Per author response, I've revised my review from 4 to 6 contingent on authors including the additional evaluation & clarifications they've described in the thread below
Vapur is a tool for finding related protein-chemical pairs
The idea is to (i) extract & normalize entities using BERN, (ii) identify relations using a ChemProt-supervised model, (iii) index papers based on related entity pairs
1) Make Figure 3 --> Figure 1.
2) Spend more time walking through how a user should use Vapur for search/exploration/discovery
3) Add to Related Work a review of other similar relation-oriented COVID19 systems
  Some examples include KDCovid (http://kdcovid.nl/about.html), EVIDENCEMINER (https://www.aclweb.org/anthology/2020.acl-demos.8/), and SemViz (https://www.semviz.org/), which all surface relational information between bio-entities for COVID-19
 I believe Vapur is sufficiently different from theirs because of the use of a relation-based inverted index, but I recommend the authors try to articulate this clearly earlier in their paper to help distinguish their work.
4) Move the details about the Methods (sec 3) to after the reader has familiarized with the system
- Table 6 is not discussed in the text.
 Don't make us go look for it.
- Why did you decide to use 1/0 binary classification of relation instead of predicting the class labels?  Were the relation class labels not useful for your particular user interface? This would be very interesting to discuss
- It's good you performed an evaluation for the relation extraction module
 When checking the Appendix, I realized the evaluation was on the binary level (i.e
does this express a relation or not?)
 While this is fine to identify that errors are coming from entity normalization, I think your system could benefit from an error analysis on the sentences itself (i.e
what types of sentences does the system tend to fail on?  maybe split this out by the existing relation types in ChemProt?)
 When we build such tools, it's important to get feedback from real biomedical researchers (e.g
given they issue K queries, do they find the results sensible/helpful?)  It doesn't have to be extensive (given this is a workshop submission), but some indication of having verified that this tool is actually something biomedical researchers would find helpful would give readers more confidence that system designs like Vapur are a good idea.
This paper is about the development of a search engine for Proteins-compounds extracted from CORD-19 dataset
The pipeline of the work consists in extracting the entities from the abstracts using BERN and normalizing these entities
Then the system uses the BioBERT, previously tuned using the ChemProt dataset, for relation extraction between the entities (e1 for chemicals and e2 for proteins)
Finally, they create an inverted index
The related work section needs more work
This section talks about NER, BERN, but it does not present RW related to search engines, thus, when the authors state that this is the first search engine that “uses relation extraction to construct an inverted index of related biochemical entities”, I thought that I need more info about search engines provided in the RW
In Methods, the paper presents very well the datasets used, I have just some issues:
- Why do you consider CPR:0 as 1? Is there a certain relation there but ChemProt doesn't know what is it?
- Here the authors introduce new concepts, such as GENIA, and finally say what is BERN (a neural NER architecture with an integrated normalizer), but I think this should be said previously in the paper
- I didn’t quite understand what was used for the normalization
Are you planning some online evaluation? 
In Results and Discussion the authors only evaluate BERT vs BioBERT performance, which seems a bit limiting and expected that BioBERT performs better in a Biomedical related dataset
- In vapur there is the display of similar entities, for example, similar genes as in Figure 3
I didn’t understand how the similarity is calculated here
- Are you considering the use of the full text? Some precious entities may be enclosured there
Figure 1 show the overall architecture of the work but the main component which is CORD-19 is not explicitly included
For this sentence “Vapur indexes only these abstracts, but it is able to return the linked full-paper,
as well.” What does it mean? As a reviewer I had to decipher it to mean this “ Vapur indexes into these abstracts, and is also able to return the linked full-paper.” Is that correct? (Only and as well do not go together).
Where Chemprot dataset is introduced include the number of records (publications).
Table 1 would benefit from having a total row.
Both Results and Discussion and Conclusion sections have Vapur explained in them
The authors latest response addressed many of my concerns, and I'd be happy to push for acceptance *contingent* on author commitment to make certain changes in the manuscript.
 Many more questions still remain (see my follow-up to author response).
The authors provide a detailed documentation of their participation in the TREC-COVID challenge (Rounds 1-3)
 Their system: (i) concatenates BioBERT sentence embeddings (Reimers and Gurevych 2019) with a standard inverted index over documents, and (ii) ranking documents using an averaging of BM25 scores and cosine similarities over the neural embeddings
 The system achieves moderate performance among automatic runs on the official TREC-COVID results
Regarding the method, there is not sufficient Related Work provided to understand how this method fits within the broader literature  (has this been tried before -- if so, citation? is this similar to something else w/ some modification?  -- if so, citation & explanation/motivation of differences?  is this entirely novel?)
Regarding the relevance feedback (RF) baseline, why is this a sensible choice over (a) BioBERT sentence embeddings & cosine similarities without BM25, and (b) BM25 without the BioBERT sentence embeddings?  Comparing against the latter baselines would be able to help explain how/where the described system is performing well/not
Regarding the evaluation, Table 1, while it is an accurate reporting of the official performance by this team, because the nature of the submissions differs between Rounds, it's hard to understand what learnings we can gain from this
 For example, RF appears for Round 1, but RF-RR for Round 2, and no baseline for Round 3
 Fusion appears for Round 3, but not Round 1 or Round 2
 I believe the authors should've conducted separate controlled experiments to demonstrate efficacy of their method in a controlled setting, and supplement those results with the results from TREC-COVID, not base the entire paper on the TREC-COVID results.
Regarding the analysis in "Where does the model succeed or fail", the point about bias in TREC judgments is very interesting -- that is, their system returned a document at a high rank but was un-judged because no other system returned that document
 Nor does it perform enough careful experimentation/detailed analysis to glean any generalizable learnings from their experiences (which would also be great).
This paper describes an approach to the TREC COVID Search challenge
The proposed method combines an inverted index score using BM25 and a cosine similarity score based on neural representations derived from BioBERT-NLI
The paper's main contribution is the proposal of a hybrid system that can perform document retrieval without separate search and re-ranking steps using a pre-trained neural indexer.
The paper denotes NIR_{AVG} as its proposed method, but the model is neither the official submitted model (NIR_{[CLS]}) nor the best performing model in the additional run (ClinicalCovid-NLI)
What's the rationale behind this choice?
It would have been nice to see quantitative analysis of models' performance regarding "Where does the model succeed or fail?"  The authors provide a qualitative description of a case where the model had lost points due to unjudged documents, but the explanation's qualitative nature leaves many questions open
Is it always the case that the model's low NDCG@10 scores in different topics due to this phenomenon? How does it improve as the base model changes to a stronger model?
One can guess that it's related to other teams' submissions, but it would be better to have the exact criteria described in the document.
- Why doesn't Table 2 have the numbers for Round 3?
This paper proposes a simple and effective model for the COVID-19 challenge that combines similarity scores computed from neural representations and traditional inverted index scores (BM25)
The details are extensively provided however the organization can be improved, as it's unclear what are the motivations for technical choices and baseline choices.
It seems the biggest discovery is that BM25 score is very important for the model (and potentially for other models that participate in this challenge)
While this can be a contribution, but the argument sounds weak to me
Isn't this a widely used tool even outside of this challenge? Furthermore, in the ablation study, when taking out BM25 components, the performance dropped to a point of unusable
This questions whether the neural part of this model is effective
It looks like the BM25 is doing heavy-lifting there.
What does the NIRR model add to this paper? It seems it performed worse than NIR but the paper does not offer any analysis.
It seems another contribution buried in the technical details is that "the NIR model...performs on par with the RF run with neural reranking, without the need for tasks specific training data." So the proposed method is actually unsupervised?  I am confused
Did it use round 1 judgement during round 2? and in round 3?
This article describes IR methods and their evaluation on the context of the TREC-COVID challenge
Given the space constraints, I believe the information is very well summarised.
With regards to the technical contributions, the methodology they applied to build and evaluate their models is sound, and the error analysis is insightful, providing useful ideas for future work
Maybe it would have been interesting to provide more details about the systems in Table 2
For instance, the different NLI and BioBERT versions should be explained, and also the difference between NIR_avg and NIR_cls (not just in the table caption)
I think it would be more relevant to have a paragraph covering the Table2 variations than keeping Figure 2 (the example is interesting, but the figure takes too much space in my opinion).
- NIR_avg has slightly different numbers for Round 1 NDCG@10 in Table 2 (0.614) and Table 3 (0.615)
- P1: "gains achieved with neural reranking are debated until recently": a bit of elaboration would help provide more context.
- P1: "large neural models pre-trained on language modeling—specifically BERT which uses bi-directional transformer architecture— achieves": should be "achieve"
- P4: "the document at rank 3 for shared no keywords": remove "for"
We might say, neither NMF nor HNMF is a novel idea in topic inference, regardless the known LDA in history and powerful Transformers variances (If one use BERT or SciBERT to augment semantics in the research) in current days
However, this work is fairly well designed, and I believe it reaches sufficient quality level published in EMNLP workshop.
The overall scientific design is reasonable
In the implementation section, algorithm 2 is carefully designed and it assures the reasonability of topic amount selection
It seems very doable for this work to provide "raw data/codes" reproducibility, but authors did not mention it
Introducing Word2vec in WMD makes sense, that is true
However, in the meantime, currently there are quite a lot new focuses in semantics representation of phrase or sentence, instead of single lexicon
Conventional method could be augmented with new idea so as to explore more in depth.
Somehow, some baseline methods could be introduced, implemented and tested
Before jumping into the analysis of the final 52 (sub-)topics, it is of utmost importance to make the HNMF algorithmic part convincing
This paper presents a topic modelling system based on hierarchical nonnegative matrix factorisation
The accompanying website is easy to use, though can be improved in terms of visual representation.
Though the paper does not introduce a novel method, it makes several meaningful contributions including the following:
The authors augment Kaggle's COVID-19 dataset by additionally searching 4 databases, removing non-English publications and articles without abstract or full text, which helps to build more meaningful and relevant topic models
The resulting model helps to avoid flat structure and discover meaningful subtopics on as many as three levels of nesting
Some subtopics can belong to several major topics and the authors provide examples to justify this
Overall, such hierarchical structure can be useful for researchers to explore the topics of interest in more detail.
The topics are thoroughly analysed in terms of major issues discussed in COVID-19 literature, thus presenting a valid use-case scenario
However, there are some weak points in terms of reproducibility and implementation:
It is not clear how the texts were preprocessed
Again, this makes the paper impossible to reproduce and can potentially affect the results of the topic model
For example, if the authors manually checked the texts for “irrelevant” words and then removed them, that would lead to much more relevant and coherent results and thus we’d overestimate what the model can do.
More of a suggestion - though the topics look coherent by “eyeballing”, you might consider using a more reliable coherence metric than Mimno et al
2011, which has the worst correlation with human evaluation (Röder, M., Both, A., & Hinneburg, A
Exploring the space of topic coherence measures
In Proceedings of the eighth ACM international conference on Web search and data mining (pp
The paper describes the creation of a dataset with scientific papers about COVID-19 that have been divided in a three-layer structure based on the topics covered
The authors have used hierarchical nonnegative matrix factorization to organize the topics
A total of 52 major topics were found
The authors evaluated the results trough different measurements
The work is of high interest given the ability of "classify" papers based on specific and concrete topics, allowing to filter the large amount of literature that has been produced in the last months with respect to COVID-19, being an useful tool
My only concern is that I cannot see where is possible to download the dataset used to create the topic structure, or if could be possible to use the tool for new articles classification that will appear in the future.
This paper proposes a dataset that is annotated via a standard active learning framework, aiming at providing supervision for models to filter COVID-19 articles that are out of expert interests
This paper is well-motivated and well-written
When the paper says "...however, the precision of the aforementioned query is highly arguable", there should be some statistics to justify this point
Afterall, this point establishes the motivation of this paper
In Algo1, line 5, "i<= max" should be "i <= t_max"
Further, it would be useful to present a curve of the acc(f_i) over time
This would illustrate the effectiveness of using active learning
It would be interesting to see if this is a log-like curve or some bumpy curve.
For a closed dataset, even if it is available during the reviewing period, I would give it an immediate rejection, no matter how good the work is.
What is Fig 5? Not mentioned anywhere.
It would be perfect if this paper can even gently touch on this point: What is the impact of this dataset on other COVID challenges? This would highlight the usefulness of this data
There could be some domain issue that hinders its application elsewhere
This paper introduces a task of filtering scientific literature for relevant artciles
The authors frame the task as a binary classification problem, and use both BioBERT based classifier and conventional classifiers to solve the task
The second contribution is that the authors release an expert-curated benchamark set for the task.
Active learning is used to develop the training set, where BioBERT based classifier is used as the base classifier.
- Although the proposed task focus on meeting the information needs from medicine, biology, chemistry and bioinformatics researchers, the task of filtering out scientific literature should have broader audiences
The proposed framework is generic and should be able to find use cases in other domains.
- The authors mention the dataset is available upon request
- The binary classification setting seems oversimplified
If the task is framed as a multi-class classification problem, it may further solve the overload issue.
- It would be interesting to see the performance of a simplest baseline that uses only query in Figure 3
In other words, you can use only the IF part of your Algorithm 2 (line 4) and assign all others `relevant'.
- Page 3: 'keyword query is used on the metadata of the articles' vs
Figure 1 caption 'Papers that match on these keywords in their title, abstract, or full text are included in the dataset.'
Are these two conflicting? Is full text used?
- Page 4: Line 4 in Algorithm 1, measuare accuracy on which set?
Maybe make it clear in Section 3.
- Page 4: is the seperate regression set randomly sampled?
- Page 5: The first part of Algorithm 1 is confusing
Should the training of BioBERT classifier not involve active learning? and the classifier is trained only once on the complete training set? The second part of Algorithm 2 (line 3-8) looks unnecessary.
Also, you may add \bottomline to Table 3.
- Table 2, 3: reporting only results on Relevant class should be enough, because it is a binary classification task and relevant class is what we really care.
- Page 8: suggest to add citation after batch-aware methods, or explain what it is 
- Page 8: 'By conducting novel keywords extraction from the recent scientific literature, the CORA keyword-based query can be enhanced automatically with new terminology.' this part sounds not about future work
It is a bit of confusing
My understanding is at this stage you fine-tune BioBERT based classifier on the complete training set, and this process does not involve training the classifier several times (such as the while loop in Algorithm 1)
The authors generated a benchmark dataset of relevant scientific information to COVID
They used active learning to build the training set and classified literatures into COVID relevant and non-relevant with high precision and recall
The paper is well written and includes sufficient technical details
The authors not only evaluated the performance the purposed method, but also evaluated the benchmark dataset using other classifiers
 The classifier fine-tuned on BioBERT outperforms other classifiers
Please clarify the following information in the paper:
The purpose of using active learning for classification is to reduce the number of labeled samples
Please provide the number of samples labeled by experts in Algorithm 1 and show the performance of classifier with the increase of labeled samples
The performance of classifier on two relevant and non-relevant sets were reported
In each of set, relevant and non-relevant literatures were included
What is the purpose of generating two sets?  
Two datasets were used: training and regression set
Both datasets were annotated by expert
What is the purpose of using two datasets and why only the regression dataset was used to compare with other classifiers?
The work compares traditional topic models based on word tokens with topic models based on medical concepts and suggests several ways to improve topic coherence and specificity.
Latent Dirichlet allocation is the most commonly used method for topic-based analysis of documents
The authors have pointed out that conventional LDA models work well for topically-diverse document collections but they are less informative in narrow, knowledge-rich domains like medicine and specifically when the corpus consists of documents related to one broad topic, such as coronavirus-related literature
The experiments have been conducted on the CORD-19 dataset
Three different models have been considered based on three different input representations of the text: word tokens using NLTK tokenizer, concepts based on the Unified Medical Language Systems concepts and non-generic concepts with more general concepts filtered out.
Human evaluation was performed by two annotators to evaluate the quality of topics by labelling them as incoherent, specific or generic
Although the definitions of these labels are quite vague, the Cohen’s Kappa measure of inter-annotator agreement was 0.87
The necessity of human evaluation has been well explained.
After the models are trained, the dataset is subdivided based on the most prevalent topic in each document and then an LDA model was trained on each subset
Two approached have been proposed for further experimentation: first one based on non-generic concepts and the second one based on non-generic concepts re-weighted based on the log-likelihood.
It has been empirically shown that non-generic concepts based topic modelling makes the identified topics as more specific and less incoherent and less generic.
In this work the authors present different approaches to improve the topic coherence and specificity of topic representations using LDA with an application to the CORD-19 dataset
Most of the details of their methodology are found in the presented text (document preparation, number of topics), but a few things could be expanded further as other reviewers have pointed out
The evaluation part is very brief and key details about the annotation/evaluation instructions and specifics of how the topics were separated into generic and specific is definitely needed
The point of using humans instead of automatic metrics for the topic coherence evaluation is essential for this kind of work
 The experimental evaluation nicely characterizes the need of a certain 'guide' (while not being exactly guided-LDA) for the LDA models to be coherent by feeding them concepts of higher relevance in the clinical context
Table 1 is an excellent result
The topic tables on page 4 are nice, but should have been reduced as this didn't leave much space to the authors for a longer discussion and conclusion sections This last section seems to be missing and replaced with an odd "Related Work" section at the end which is not really a satisfactory conclusion to this paper as some of the test should be featured earlier on the paper
 Overall, very solid start of the paper that seems to run out of space near the end, and leaves important details out.
This paper describes how an LDA topic model was improved by using concepts
The evaluation - performed by humans - shows that the more refined topic models (using medical concepts) are indeed "better", meaning here more specific and less incoherent.
The conclusion is not by itself surprising: LDA models are known to be very sensitive to pre-processing (including lemmatization and stop-word removal)
This is one of the reasons why embeddings became so powerful as they are less impacted by that
In this sense, comparison with a neural topic model would have been an interesting, and helpful to guide practitioners as well as to have a data-point of the usefulness of old-and-tested LDA vs more novel models.
I found the paper an interesting experience nugget..
In addition, I have doubts about the following:
 - what exactly are the documents that are provided as input to the concept-LDA? Are the words enriched with concepts, or are those documents just a sequence of concepts?
 - so what? It is not clear to me why having more "specific" topics would be helpful in the end
Can you relate the 7 specific word-topics to some of the concept-tokens (eg, by comparing the probability distribution over words)? Do they subdivide the space further, or is it a totally different soft clustering? How will this help the final application?
This paper proposes an improvement to LDA-based topic modelling by using only important concept words
It proposes to test whether limiting the vocabulary to UMLS concepts, and a smaller set of non-generic UMLS concepts improve the topics extracted by an LDA method on the CORD-19 dataset
The paper finds that non-generic UMLS concepts provides improved topics.
While I do find it believable that non-generic UMLS concepts improve over normal LDA methods, I think this paper could do more to evaluate this compared to other methods
Furthermore, it is not clear what this paper adds to the COVID-specific text mining area
- The core issue is that this paper cites multiple other extensions to LDA but doesn’t attempt to evaluate them as comparisons.
- Just a note, but why aren’t both evaluation annotators authors on the paper?
- The annotation task didn’t sound pretty vaguely defined so I’m impressed by the high agreement between annotators
- I think a stronger case has to be made for fixing the number of topics across all methods (to 25)
While it makes the methods comparable using raw counts, it isn’t clear that good methods won’t be unfairly penalised by this.
This submission proposes a new Korean-English test set and multilingual neural machine translation model for contributing to the translation task of COVID-19 related texts
The single model was heavily trained on 4 selected languages from Corona Crisis Corpora (TAUS 2020) and a Korean corpus via back-translation
The authors implement the model with a standard toolkit with a few adaptations based on the recent work
Reasons to accept: The proposed model beats against the state-of-the-art (SOTA) models (results were from Berard et al., 2019; Ng et al., 2019, Bawden et al., 2019) on major test sets
The model has a significant performance improvement than the publicly available OPUS-MT model on the handpicked Korean-English sentences
The Korean-English test set is valuable and supplementary to the existing multilingual translation corpora, Corona Crisis Corpora
A significant contribution is the new test set
However, the data quality is a big concern
First, how the sentences were handpicked is not clear
For example, the KCDC document has many pages, but the selected number is only 258 sentences
Does the sentence share similar format with the training set? Will the authors provide detailed guidelines that the confidence of data selection? Did the translators have a high agreement on the translation quality?
The TAUS 2020 provides 6 languages (checked at 09-28-2020), including the 4 languages in this paper and the other 2 languages (Russian and Chinese)
Is that because when the authors trained the model, those two languages were not available?
This work presents a pretrained multilingual neural machine translation (MNMT) model, baseline evaluations of this model on an existing COVID-19 MT dataset called Corona Crisis Corpora (TAUS 2020), and a new test set for Korean-English sentence pairs to make up for lack of this language pair in the TAUS 2020 dataset
- Large pretrained models are costly to train, but easy to use
 Releasing such models is a service to the community & makes it easy for others to get started in this area of research
- How did you guarantee the quality/faithfulness of the Korean-English translations, given that these contained sentences from biomedical abstracts?  Did the recruited translators have relevant biomedical background?
- To help others gauge the cost/effort to produce such annotations, would it be possible to add more information about the total annotation hours spent & cost to hire these experts?  
The paper describes a new multilingual multidomain machine translation model, and a new Korean-English test set
On a neural-network level , the model itself seems to be fairly standard, with a few tweaks based on recent suggestions from the literature
The training data is of more interest - they have included the recent Corona Crisis Corpora, biomedical data where available (oversampled by a factor of 2), back-translated data for Korean (the language with the least data available), and domain tags to indicate whether the data is biomedical or not.
Their Korean-English test corpus was developed to test performance in the Covid-19 domain, and because Korean-English was the only language pair not covered by the Corona Crisis Corpora
Their model was tested against state-of-the-art systems on three test sets
On "generic" test sets (News, IWSLT) it generally outperforms the state of the art, but does slightly worse in the biomedical domain
They have also evaluated their system using their Korean-English test set, where it outperforms publically-available models by a substantially wider margin than on the pre-existing test sets
This may suggest that the model is particularly well-adapted to the Covid-19 domain.
Reasons to accept: The model they have developed looks like it performs well, and the evaluation on the Korean-English dataset suggests it should do particularly well in the Covid-19 domain
The authors stress the multi-lingual nature of the system as an advantage
I wonder also whether the multi-domain nature of the system is an advantage - Covid-19 news is likely to have more in common with the biomedial domain than most news text does, and the literature relevant to the "international impact that this crisis is causing, at asocietal, economical and healthcare level" which they are studying is likely to have more in common with newswire text than most biomedical text does
Both the model and the test set look like useful resources for the community.
The authors say "Note the SOTA models were trained to maximize performance in the very specific Medline domain, for which training data is provided
While we included this data in our tagged biomedical data, we did not fine-tune aggressively over it." This raises the question as to whether the systems optimised for biomedical text are better for the biomedical texts this model is intended for, and whether the other advantages of this model are compelling enough to overcome this.
In conclusion, this paper presents two useful resources to the NLP community, and as such is worthy of acceptance.
This paper proposes a real-time Twitter visualization tool which consists of three components: Twitter classification, geolocation extraction and interactive visualization
I agree that the proposed system can be of great value in practical use
First, there exist a lot of twitter mapping/visualization works, e.g., real-time crisis mapping of natural disasters using social media and this one: https://blog.tensorflow.org/2019/09/disaster-watch-crisis-mapping-platform.html
All these tools have similar structure as that of the proposed tool: extract location of the twitter, perform classification on the twitter content and visualize the classified twitters in a map
The only difference to me is that the target labels in the classification layer is different
The author should include a comprehensive review of these visualization tools and highlight the differences between this paper and previous tools
Second, a claim in this paper is that using negative L2 distances in the output classification layer outperforms using traditional inner product similarity
The marginal improvement in terms of validation accuracy (76.84 vs 76.35) is not convincing to me.
Finally, as the proposed system works in an interative and real-time manner, there should be an evaluation on the latency of the system.
 for example  “training data” is not well described
The author mentioned that the annotated data is from Middle Eastern Respiratory Syndrome (MERS) outbreak but there is no clear connection with Covid-19 data, there are several Covid-19 datasets and the author did not mention why did not use them
Main point: author did not report any classifier’s performance or any kind of evaluation metrics
 even though it is short paper but at least a table explaining the performance metrics and how the classifier performs
The classification is heart of this visualization and the performance metric s are must for any NLP or applied NLP research paper.
This work presented an interactive visualization system for COVID information including the number of affected people, prevention, treatment, death reports, etc
The system was based on Tweet streaming data with 4 COVID related filtering keywords
number of affected people), the BERT-based pre-trained model was fined-tuned on the 2014 MERS dataset and the L2 layer was employed as an output layer
To further generate the information distributions on the global map, geolocations were extracted with name entity recognition from the text contents of tweets.
This novel system provides a live information distribution that presents the changes in the pandemic situation
The authors present an analysis of the prevalence of the 6 Ekman emotion classes in Portuguese tweets, over the month of March 2020
The hypothesis they explore is that "negative emotions would intensify as the pandemic progresses"
In addition, insufficient context is provided -- what is the level of relative emotion in March, as compared to February and April? You have data Feb-May, it could be used.
If the data set is a primary contribution, then some analysis of the labelling is needed, including some assessment of the quality of the emotion classification using the identified method
As other reviews have pointed out, the count-based approach to measuring is oversimplistic; while it may be okay for large-scale statistical studies on trends where some noise can be tolerated, if it is intended to be used as a labelled dataset it must be more rigorously validated
If the collection of COVID-19 Tweets tied to Portugal is the contribution, some additional details are required
- "natural language processing and machine learning technologies offer exciting possibilities for the improvement of both population-level and individual-level mental health" -- is it really improvement of mental health that is targeted with this work, or rather monitoring of mental health? (How that information is used to improve mental health seems to require more than NLP.)
- why does Figure 3 have a y axis that goes up to 12 (120%) and at least one point that appears to be above 10? Labels for the axes are needed
What is the normalization assumed to produce this Figure? 
Ofoghi et al (2016) Towards early discovery of salient health threats: A social media emotion classification technique
Also consider from the Part 1 of this workshop:
The paper aims to conduct emotion analysis on English and Portuguese Tweets related to COVID-19 and shows that the negative emotion trend corresponds to some of the events occurred during the pandemic.
While the objective of the research - which is to monitor the mental health of the social network users - is interesting, I am not convinced that the preliminary experiment conducted in the paper can serve as a stepping stone for that.
For example, while Table 2 shows the number of Tweets, it is unclear how many unique users were involved.
The authors should consult the recent related work and techniques in the field of sentiment/emotion analysis
The level of the analysis is limited
The authors present a single figure (Fig
3) as a result of the analysis where the normalized counts for the six emotions are shown within the period of March.
It would be interesting to see why this is so.
The authors should reconsider the direction of the experiment as proposed by the AnnonReviewer2.
The authors provide a basic sentiment analysis of a set of tweets that have been generated in portugal for month March 2020
They claim that sentiment analysis of social media posts can be used as a  tool for locating individuals who suffer with psychological issues caused by the covid-19 pandemic.
Based on a psychology theoretic framework, they want to locate people that might have mental health issues.
- The authors indeed show a correlation between negative sentiment and the diffusion of covid-19 for March 2020
Even a simple correlation metric can provide additional insights
For example, although the virus is diffused and reaches it peak in the end of march, negative emotion in tweets does not increase
Authors should look at an individual's tweets in general, and see if COVID-19 waves are related to negative emotions overall
The analysis bases it self in a very simple theoretic framework, and needs a more solid framework that can lead to the detection of individuals with mental issues.
What does this information tell them? Would they contact the individual? Report authorities that specific regions might face serious issues?
Although the idea is very interesting, it is not operationalized adequately
I propose to create a more solid framework (better sample of tweets, a theory that connects the different feelings to psychological issues), analyze tweets during multiple waves of covid-19, and use more sophisticated statistical tools (e.g
time-series econometric models, potentially panel data that allow to find associations at the city level)
In this way, they can create a pipeline that provides insights about mental health and covid-19.
in the abstract it says twees instead of tweets;
in the preliminary results it is french instead of franch;
plot visualizations are poor, they could be improved.
The paper describes an application of sentiment analysis for the dutch messages from three social platforms related to the COVID-19 measures
The paper is well written, the introduced analysis is definitely needful and interesting
 Authors do not describe the classification algorithm that was used for the stance analysis
According to my understanding, authors trained FastText word vectors on their data
However, authors do not explain why they do not use pre-trained vectors, despite the very well known fact that word embedding trained on a big data performs better than trained on a specific small data
 Also, the authors mix training word vectors with classification, which is confusing
If the used classification model contains an embedding layer, it also needs to be explained
How these cases were handled? 
What is the accuracy level of the used sentiment analysis (polarity)? Despite the method that was used is unsupervised, its evaluation on some test dataset is needed (or, if it was evaluated by developers on a similar domain, the relevant paper must be cited and its performance must be reported).
10-fold cross validation is used for evaluation and NOT for training
Which model (trained on what?) was eventually used for automatic labeling of a new data? 
Which classes were "difficult" for your classifier (were misclassified)? Accuracy per class can answer this question
A small typo: "the COVID-19 related tweets *is->are* generally.
This paper describes an analysis of COVID-19-related text messages in the Dutch language obtained from three online sources (Twitter, Reddit, Nu.nl; relevant messages were filtered using a selection of COVID-19 related keywords (Twitter) or based on specific topics (Reddit, Nu.nl)) from February to July 2020
Sentiment polarity analysis (using a lexicon-based approach) as well as stance detection for the topics of social distancing and the use of face coverings (using a supervised learning approach based on fastText) were conducted on the retrieved messages
For the latter, subsets of the retrieved texts were sampled based on keyword search and manually annotated by a single human annotator
The findings show some alignments between COVID-19-related events and the corresponding reception on such social media platforms
For example, the first introduction of lockdown measures in the Netherlands in March 2020 led to a decrease in polarity on the Twitter data, and the first release measures in May 2020 caused the polarity to peak on the observed dataset.
The topic and approach presented in this paper can be very interesting to the community
However, it is unclear whether the collected online text data can be interpreted as representative for the “broader Dutch public” (as claimed in the paper) since only online data are used
For the stance detection, the used dataset is relatively small (especially for the face covering topic), and it would be interesting to see how a simpler approach (e.g., a linear model using bag-of-words features) compares to the neural network-based approach.
Furthermore, some statistics (e.g., average sequence length) on the text messages obtained from the different sources would be helpful, since the stance detection system is trained only on parts of the collected data
In Figure 2, for example, the stance detection system is trained only on the Twitter data and evaluated on all three datasets.
It would also be interesting to see the polarity results on the other datasets (e.g., in the Appendix), only Twitter data are shown in the paper.
Section 3.2: For the stance detection task, is there a reason for why only Twitter and Nu.nl data was used for the human annotation?
Which topics and articles were considered for the analysis of Reddit and Nu.nl data?
In Section 2 different tenses are mixed up when discussing related work.
The axes in Figure 1 should be labelled and explained in more detail
Also, the letters A and B are hardly visible in the figure.
Section 4.1: Missing word “Some interesting links can [be] found…”
The study was based on the datasets from three types of social media: Twitter, Reddit, and Nu
Datasets are collected and filtered using a set of COVID related keywords from late February 2020 to July 2020
Sentiment analysis included two parts: 1) polarity analysis based on 3918 Dutch polarity words; 2) stance analysis--annotating and then classifying posts related to social distancing and face mask-wearing
For stance analysis, the trained classifier (with fastText) is applied to the entire dataset
The study showed the policy support value declined until June and then increased, which was consistent with the pandemic situation in the Netherlands
For the Reddit dataset in Table 2, is it the number of submissions(initiating posts)?
Is it trustworthy to have only one annotator without any adjudication process?
This study aimed to identify public figures involved in the discussion of medical information on Twitter, examine the transformations of their initial tweet content, and trace the dynamics of attitudes within information cascades.
What does the literature suggest? 
The approach to detect information distortion lacked transparency, reliability, and validity
Please fully explain this process and demonstrate how it was determined to be valid.
But as the paper itself points out (“preliminary and qualitative study”), the study is still at a very early stage to me in several aspects
First, the concept of “information distortion” is replaced by  "attitude/sentiment shifts"
Although the two concepts may overlap to some extent, but are not equal to me
Secondly, the experiment setting is simplified (only very influential figures are included) and only qualitative results are given.
The quality of the paper can be greatly enhanced by improving its experiment section.
They detected the major public figures spreading the information and also attitude shift within cascades
However, only descriptive analysis is provided
The paper will be improved if the authors adopt automatic approaches for detecting distorted medical information in large scale
For example, tools such as InfoPath (http://snap.stanford.edu/infopath/) can be used
Gomez Rodriguez, Manuel, Jure Leskovec, and Bernhard Schölkopf
"Structure and dynamics of information pathways in online media." Proceedings of the sixth ACM international conference on Web search and data mining
EMNLP COVID-19 WORKSHOP — 26th Sep 2020
This short paper/abstract describes the development of a corpus of Arabic COVID-19 related tweets from Saudi Arabia
 Data was collected between Jan 1st 2020 and Apr 10th 2020, and consisted of 3.8 million tweets selected based on Arabic COVID-related hashtags
 The case for publication would be stronger if the corpus was manually annotated and/or there was a more extensive analysis of corpus characteristics.
 Generally, the language is a bit hard to follow
 It is not clear how you identified the hashtags
The paper presents a dataset of over 3 million tweets in Arabic collected during the initial months of the COVID-19 pandemic
Authors selected relevant hashtags related to public policies, measures, solidarity messages, etc
The paper presents the main statistics of the corpus and briefly discusses the data collection methodology.
The creation of new linguistic resources is always a time-consuming process, but it is extremely valuable for research in NLP
Furthermore, resources in languages other than English are necessary to diversify our methods and technologies and ensure they are scalable to these scenarios
In this respect, I consider the work relevant.
However, my main concern is that I think the work is still very initial to be considered for presentation in EMNLP
At this moment, the authors have "only" collected the data (which I agree is an important part of the research), but it remains to be seen how this data can be used to successfully answer some interesting questions in NLP
Furthermore, the description of the dataset and data collection methodology is not sufficiently detailed in order to make this process as transparent and reproducible as possible
#### Some questions I consider could help the authors improve their work:
- How was the selection of hashtags and categorization? Was it the work of a single person, or was some sort of committee designed, and in that case, how was disagreement dealt with? If the selection was performed by a single person, then it is harder to argue that the hashtags and categories are objectively meaningful, but at least is important to disclose this information.
- Is there some classic NLP task that the authors can show, at least with simple baselines, can be aided by or performed in this dataset? For example, apply sentiment analysis to the tweets and estimate a percentage of agreement/disagreement between retweets and original messages
- Can authors provide additional statistics of the dataset, for example, timelines showing the prevalence of different hashtags correlated with major events in the Arab world (e.g., curfew application)
The idea is to give the reader a view as deep as possible into the content of the dataset such that they can judge if it will be useful for some specific task.
- Line 43: Update to more recent statistics and include the specific date to which "to date" refers.
- Line 233: I couldn't find the dataset repository URL in the document.
I think this is a good resource (esp
given it is in Arabic and thereby focuses on other-than-English data)
- line 45-47: consider updating the infection numbers or state the date when the numbers were accurate
- line 135-137: how were they categorised? By whom? What was the inter-rather agreement?
- line 233: the link is not stated so I cannot access the data
Overall: this paper would do well to also include a section on previous work - esp that on COVID-19 and NLP, many of which has been published at the COVID-19 ACL workshop
Relevant datasets, studies and findings should be discussed.
I would also want to see analyses on the dataset
EMNLP COVID19 — 26th Sep 2020
This paper seeks to use Twitter to better understand the emotional responses of the Saudi population to COVID-19
 As a first step, the researchers built an Arabic sentiment lexicon from several sources and augmented by some manually analysis to identify sentiment terms specific to the Saudi dialect
 Second, they geolocated the tweets using CrimsonHexagon
 Third, they assigned sentiment scores to tweets based on a process of (a) filtering tweets using the lexicon developed in step 1; and (b) applied NB & SVM to an annotated subset of these tweets [Note that this step is somewhat unclear]
 Second, hashtags were used to home in on specific themes (e.g
support for decisions, social solidarity) should general support for government COVID-19 containment policies
 Third, the method was able to identify specific geographical locations that exhibited in their tweets greater than average negativity.
 The methodology is not entirely clear.
 The writing can be a little hard to follow at points and generally would benefit from tightening up
 If we take the abstract as an example, there are a number of issues present:
	* “The COVID-19 had a great impact” 
Drus and Khalid, 2019) should probably be without brackets (i.e
This work described the sentiment analysis for Saudi Arabia based on the Twitter data from January 1, 2020, to April 10, 2020
The Twitter dataset was processed by Crimson Hexagon to identify both hashtag topics and geolocations of the posts
To identify the sentiment of the posts, sentiment lexicons were expanded based on the Arabic sentiment lexicons dataset by 4 annotators
To generate the ground truth sentiment dataset, more than 129K tweets were samples and annotated according to the expanded lexicons
NB and SVM models were trained and tested on the ground truth data
Based on the results, both sentiment distributions over time and topics were discovered
It also showed sentiment variations in different cities
The 'Method' section needs more explanations and improvements
To expand the lexicon, 4 annotators extracted words expression emotions
After annotation, do they reach a certain agreement (Kappa statistics)?
Overall: a good attempt to show NLP-related work on COVID-19 for Arabic language data.
- the introduction could do with some references of relevant work on social media studies around COVID-19 (incl
- line 114: reference missing --> "?"
Why is it interesting/necessary/worthwhile to look at sentiment? Why is it important to know the geographical distribution?
- section 2.2: this section seems redundant and could go I think.
- line 317: the section numbering is off
- line 322: provide stats for the exclusions and general corpus descriptives
- related: how was the labelling done? Provide details of the procedure.
- table 4: provide more detailed performance metrics
- fig 2: are the differences really meaningful? Statistical testing would help understand this.
- fig 3: not really clear what the figure means and the use of spatial statistical models would be more appropriate here.
- lastly: at a few places, the wording is off.
In this work, the authors use a dataset built from reddit posts to identify pandemic-related stress factors
The framing of the problem is nicely outlined, and the focus on college students is a relevant one
Seems rather odd that on Figure 4 there are no neutral posts related to family
The study collected posts from Reddit and use common NLP techniques (e.g
The findings are interesting and intuitive, but I am concerned about two things: 1) Is the data source limited, why not include other social media like twitter? Is there a fact that young people use Reddit more often than other social media ? 2) The authors put much effort in topic modelling, it would be better to give some qualitative analysis rather than only quantitative insights
3) Not sure what is the goal of sentiment analysis ? what is your accuracy then ?
This study aimed to analyze depression-related issues reported by Reddit users using statistical and NLP techniques
Strengths of this study include the methodologies used to systematically analyze content using linguistic techniques to find out the stress-inducing factors
Weaknesses of this study include the lack of supporting documentation suggesting Reddit is a place where those suffering from depression and/or related symptoms express themselves, a lack of justification for choosing Reddit over a traditional survey-based approach, and the failure to rely on previously established terms to identify health concerns
The authors have developed rule-based strategies to generate term variants for the SARS-CoV-2 virus and COVID-19 disease and examined the usage of these variants in the literature.
They distinguish the task from NER; I agree it is not NER, but I do think it is comparable to normalization to a very small set of (two) identifiers
In that light, some assessment of the quality of the terminology in terms of Precision/Recall would potentially be valuable
Are any of the terms ambiguous? The authors also assume that if a term is instantiated in the literature, it is valid
Is it possible that there is a spurious match between a term and the literature? Furthermore, given the comment about the need for regular updates, why not just leave the terms in the dictionary? Perhaps those terms can anticipate variation?
I'm a little bit confused by the analysis in Table 1
If the Union of all of the resources is considered the "complete" dictionary, shouldn't we be interested in how many terms are unique to each resource? That is, how many terms are added from each source that contribute to the final set of terms? And, shouldn't we consider the term-level overlap between them in order to establish agreement, e.g
which terms are in all 3, 2, or only a single resource? 
Figure 2 could be more interesting if we were also told the proportion of articles each of these unique terms appeared in, because the literature was also increasing during that time; this would give a sense of the rate of adoption of these 'novel' terms
Perhaps some 'error bars' could suggest the variation in the distribution of the terms.
The authors in Section 2.1 identify 3 types of entities: The SARS-CoV-2 virus, the disease it causes (COVID-19), and the pandemic
Some are obviously highly specific to this virus, but others such as adding "infection" to the name of a virus to produce the disease, or adding "disease" to the end of a base disease term, seem like they would be generalizable
It would be interesting to see whether such patterns apply for other virus-disease associations.
All in all, a useful resource in a narrow context
It would be nice to understand the broader utility of the methodology.
This paper present a technique to expand gazetteers referring to Covid19 (disease) and Sars-cov-2 (virus), consisting of a set of rules.
In addition, there is an analysis of the result applying it to a collection called LitCovid.
However, that utility is never shown
This by itself might not be an issue with a workshop paper
In that sense, Table 1 provides the wrong information
It is not the number of terms that is important, but the combined number of occurrences of those terms in a given document
As ~80% (70%) of the terms referring to Covid19 (Sara-cov-2) are covered by the basic expression, it would seem that the percentage that is covered by your expanded list might be very small
3 addresses this with the top-1/2/5 terms, but it is not clear how many additional terms (or even documents) are captured with your expanded list.
There could be value in analyzing in what situations non-standard terms are used, and if those are preferred by a certain public or region
However, besides Fig 1 (which I found one of the most interesting parts of this paper), such an analysis is absent.
This list would be very helpful, and will hopefully be provided as a push-request to the terminology list referred in the paper (https://github.com/Aitslab/corona)
  - What is LitCovid: in order for the paper to be self-contained, specifying the type, source and some statistics on the collection would be helpful
The paper describes the generation of a dictionary of terms related to COVID-19 based on a set of rules
The rules allows to create a set of different combinations using a set of base-terms that allows to identify mentions to the disease by detecting terms that are not very common
The results showed that the approach developed by the authors can have interesting applications for example in bibliometrics as the detection of those concepts less used can help in this field
From the NLP perspective, the technical advances are very limited although the results provided are reasonable useful.
This paper proposes two separate models for multilingual emotion analysis and misinformation classification using Tweets
The authors use publicly available datasets for both tasks, and add Conv1D and Dense layers for fine-tuning the pre-trained multilingual BERT model
After briefly reporting the performance of the two models on the test sets, a bar chart is plotted for each task to show the distribution of emotion labels and misinformation classes.
Add whitespace before the left bracket and remove whitespace before the full stop.
However, due to the lack of baselines, it is difficult to evaluate the effectiveness of the proposed models
As the main contribution of the paper is the proposal of the multilingual models, the strength of the models in comparison to other baselines should have been presented clearly.
Also, it would be informative if the performance scores were given for each language as well
From my experience, the vanilla tokenizer provided by the multilingual BERT tends to over-tokenize the sentences in low-resourced languages; and their corresponding embeddings are less robust which may result in varying levels of performance for different languages
I am wondering if the distinctive patterns in the distribution of emotions for each language are due to the varying performance of the model or there is actually something distinctive about the Tweets.
If possible, a more in-depth analysis using the trained models would be nice.
This paper proposes a multilingual framework for tweet analysis
It considers two types of tweet analysis: emotion classification and misinformation classification
The proposed framework trains a multilingual BERT model on labelled corpus and uses the trained model for emotion/misinformation detection
There is no doubt that the task of emotion and misinformation analysis is important during the pandemic period, a fast-changing time period with explosion of information
Further, from a practical use point of view, the adoption of multilingual approach is potentially a good solution to address the problem of limited data availability
That is, a model trained on a corpus in one language can be transferred to use in different language context
However, the language setting is different for emotion classification task and misinformation classification task
The emotion classification model is trained a corpus that only contains English tweets, but the misinformation classification model is trained on a multilingual dataset
This setting may be due to data availability, but I think the authors can improve the experiments by comparing the results with a monolingual BERT model
Does the multilingual BERT model make contributions? 
The authors should do a thorough proof reading
This work reports some experiments using the multilingual BERT model on Twitter data from different datasets in several languages: the COVID-19 Real World Worry Dataset (Kleinberg et al
2020), the Coronavirus Facts Alliance Dataset (Alam et al
2020), and the dataset of tweets collected by Chen et al
Although some of the ideas are interesting and the initiative to analyze multilingual data is compelling, several issues prevent me from accepting this work at its current state.
Authors tested the multilingual BERT under the assumption that the same pretrained model could fit all datasets, regardless of the language
As if the same multilingual model fits all
The article could have been improved if this assumption had been validated or contrasted, by comparing the results when monolingual models are used in each data set
For example, by applying the monolingual English BERT model to the English dataset, a French monolingual BERT model to the French dataset..
Does a multilingual BERT model help or causes noise in some datasets?
- From a qualitative point of view, the analysis of Twitter data, enhanced through NLP, gives an enriching panorama of the public opinion regarding the COVID-19 disease
French, Indonesian, Japanese, Portuguese, Spanish, etc.)
- There are methodological issues regarding the experiments, which may induce to think that results are not reproducible or generalizable.
Abstract: What does CMTS stand for? Is it a typo of "CMTA"?
4.1.1: The authors seem to refer by "tokenization" to 2 different processing steps
The "tokenization" step refers to the segmentation of text or sentence items into individual items ("tokens"); it does not imply obtaining vectors
After tokenization, each token is then converted to a vector representation
4.1.3: the authors mention that they used "dropout layers", but what value exactly? 
What criteria did they follow to replace the OOVs? Did they use synonyms included in the vocabulary? Or do authors mean that OOV abbreviations and acronyms were expanded to full words in the vocabulary?
Regarding results, authors only reported scores on one round of experiments; the outcomes do not seem solid enough to be generalizable
A good methodological approach is initializing the model with different random seeds and test it in several experimental rounds; then, authors should report the average F-score and standard deviation.
5.3: "The bar plot shows..." -> Which one: Figure 2 or 3? The same happens in Sect
Figures 2 and 3: the colors do not distinguish well each class when printed, please use different patterns (horizontal, vertical lines...)
- The acronym "CMTA" should be defined since the beginning of the article.
(not exhaustive): "various task" -> "tasks" (p
1); "two separate deep neural network model" -> "models" (p
1); "have use" -> "have used" (p
3); "contained noises" -> "noise" or "noisy content" (p
1): "(Matsa and Shearer, 2018)(Hitlin and Olmstead, 2018)" => "(Hitlin and Olmstead, 2018; Matsa and Shearer, 2018)" 
- Footnote 4 and 6 are not needed, the URL is provided in footnote 2
The same happens for footnotes 5 and 7.
- "it is divided into four phases" -> I counted 5
This work introduces a light weight retrieval and reranking model, as well as an easy-to-use web search service
Although the model has only 620 parameters and does not use the sophisticated Transformer variants like Bert,  it still ranks the second position in the third round of submission.
I believe that the work is innovative.
There are still several weak points worth mentioning
It seems that the preliminary work of the model  come from (Almeida and Matos
This work should be cited at the first moment when mentioned
Say, the third paragraph of the introduction, "BioASQ system in our work".
Please add citation to “BM25” in the section 2.2.
So, if "yes", declare them clearly earlier, pls
The effect of the re-ordering model on the third round of Baseline is weak, so does the UIowaS team
This paper proposes a very light-weight reranking model for retrieving documents from the CORD-19 dataset
The impact is that the proposed model would encourage wider application in cases where computation resources are a constraint
The proposed model, while only having 620 parameters, performed surprisingly well (ranked 2nd in the 3rd round).
The paper is well-written and well-motivated
I specially like the 3-round of testing and corresponding "lessons learnt" parts
It offers some insights into the technical details that would cause substantial difference in the evaluation performances, and thus inspire future works.
The paper mentioned a bug in feedback data but still not sure why the baseline performance is missing
it should correspond to our phase-I baseline results", but where are the numbers? How far is it off from the system "ours"?
This paper presents a currently in-development system for information retrieval in the CORD-19
Authors present a pipeline that is being evaluated in the TREC-COVID challenge and shows a clear advantage with respect to other approaches despite (or thanks to) being based on a simpler neural ranking component
While it may not necessarily be the top-performing approach, authors claim that the simplicity of the neural ranking component and the overall pipeline makes it more feasible to deploy online and use for real-time queries
the TREC-COVID leaderboard links provided in the paper) positions this system with a very good performance, among the top-3 in several metrics
On the other hand, I would appreciate some experimental results to back the hypothesis that this type of system is easier to deploy and faster to execute than the alternatives.
**Reasons to accept:** The paper presents a clear result with demonstrable effectiveness, as validated by experimental data that is publicly available and a prototype system available online.
**Reasons to reject:** Some claims regarding the scalability and practical advantages of this system require further experimentation to be completely justified.
The authors aim to identify the mental health dynamics on social media during COVID-19 pandemic
They create a dataset by using a pre-existed distant-supervision methodology including cases (depression) and controls
They investigate the classification performance on SVM, and then turn to 8 classification models.
The authors create a distantly supervised dataset and perform a careful investigation on the dataset, especially from the view of data imbalance
The algorithmic part is less novel nor informative, as the author mainly use the classical classification models
I am personally opt to regard the dataset created in this research as the main contribution
So, more accurate description of this part is expected
The dataset in this paper is created by a "distant-supervision methodology", but the details are not sufficient
More examples and the quality evaluation is expected.
The authors could enunciate the performances comparison at section 3.2.2.
In this paper, the authors attempt to develop a methodology to predict population depression rates across different countries from social media data (Twitter)
The discussion section provides a nice overview and is a good example of how machine learning results can be interpreted using domain knowledge
The authors apply previously developed methods in a well-designed set of experiments, however, they manage to achieve only moderate performance
- The concept of a “distantly supervised dataset” should be clearly explained.
- Why were the classifiers described in section 3.2.2 trained for only 1 epoch? Training for longer might improve the performance of the classifiers.
- The authors also specify that Diagnosed samples were treated as 5 times more valuable
At what stage was this weighting implemented? How was number 5 chosen? 
- How was the data split into training and validation sets?
- The authors should specify the 2-week period within which the data described in section 3.1 was gathered
- The false-negative example in Table 6 doesn’t sound like the person has been “diagnosed with depression”
Was it labelled as positive because of other, more affirming posts by the same user within a certain time period?
- In section 4, were tweets from different countries analysed by respective pre-trained classifiers?
The paper aims to provide "organisations with a methodology for monitoring and analysing temporal mental health dynamics using social media data"; however, they do not gather enough data or train a sufficiently successful model to do that successfully
The paper does a good job of applying established methods for mental health analysis on social media data and the inclusion of a BERT classifier is compelling
The paper over-represents its usefulness in several areas.
The paper may be better suited as a 4-page short-paper
In that condensed format, the paper could focus on its unique contribution of analyzing data across national boundaries
This claim should be supplemented by country specific analysis of depression and suicidality
These are known to vary widely by culture.
Lastly, the authors should include a discussion of the ethical principals informing their decision making during this study.
Two additional nits: (1) the authors should include the AUC metric, which is conventional for this space; (2) the authors should use effect size or confidence intervals instead of significance testing, which has flaws on the massive N found in NLP.
- Methodologically consistent with the literature 
The methods are conventional--well explained here and in previous works--however, the deep models are under documented and should be supported diagrams, more detailed description, or code
Those models, likely, could not be reproduced.
This paper proposes a system for biomedical entity recommendation
The system performs entity recommendation in a pipeline manner with three steps: named entity recognition, relation extraction, and entity recognition
The paper shows that the recommendation accuracy can be enhanced if multiple ontologies are used for recognising and linking the entities
The paper is well-written and easy to follow
My major concern is that whether the experimental results provide sufficient evidence to support the hypothesis that the authors formulate: if multiple different ontologies are used for recognising entities, the accuracy of entity recognition increases
As shown in Figure 2, the accuracy of using all ontologies (ALS-all) outperforms any other dataset with only one ontology (e.g., ALS-chebi which only includes entities in CHEBI ontology)
This might be a positive sign showing that other ontologies are contributing to model’s understanding about how to recommend CHEBI entities
However, it might be also simply because of the model’s learning curve: the data in ALS-all is a super set of ALS-CHEBI, and the model simply learns more with more available data, no matter which ontology the extra data comes from
And this can be evident by that the accuracy in Figure 2 (ALS-all>ALS-chebi>ALS-go>ALS-hp~=ALS-do) is actually roughly proportional to the data set size if we look at the size of each data set in Table 4
As such, the authors should consider to another baseline scenario where the entities are randomly sampled, without considering the ontologies.
Secondly, the rating matrix is not implicit feedback
Implicit feedback means the scenario where rating is not available and only users’ actions are available
In implicit feedback setting, the interaction between users and items is only recorded as 0 or 1 (interacted or not)
Therefore, pairwise ranking is pervasively used for learning
The ALS algorithm is usually preferred in the setting of explicit feedback, rather than implicit feedback
Reasons to accept: well-written, data set can be useful
This paper proposes a pipeline for the automatic extraction of entities and relations from the CORD-19 dataset and recommendation of relevant entities in those papers
The authors apply a system (MER, Couto and Lamurias, 2018) for the automatic extraction of entities which is seeded with 4 ontologies in the biomedical domain
A total of 9000 documents from CORD-19 are automatically annotated with entities, from which 100 documents are manually curated and a small subset is used to evaluate the inter-annotator agreement and manually annotate relevant relations (relations are defined in the ontologies used)
Afterwards, a recommendation technique (LIBRETTI) is applied by considering author-entity-mention counts as the user-item-rating triplets
Thus, as I understand, the overall pipeline proposed has the purpose of suggesting an author of a paper in the annotated dataset with novel entities that might be of interest for that author and not mentioned in their papers.
The main hypothesis of the paper is that the use of ontologies from different domains for the NER phase improves the quality of the recommendation
With respect to the results, the significance of the metrics provided is hard to evaluate without some comparison with alternative systems or at least some baselines.
Although I consider there is value in the proposal, since building such a recommendation system at a large scale (e.g., at the scale of Semantic Scholar or Google Scholar) would be extremely valuable for researchers, I still consider a significant effort is necessary both to improve the extraction and recommendation procedures, but also regarding manual annotation, evaluation, and the actual development of such a tool
I recommend working on improving the motivation of the research; at least personally, it was hard for me to understand the underlying motivation for recommending entities, and I think this is a very important point.
**Reasons to accept:** The automatically and manually annotated corpora can be valuable resources for the community
Also, the source code is provided, which improves reproducibility and fosters collaboration.
**Reasons to reject:**  The significance of the results is hard to evaluate since there is no comparison with alternatives or baselines.
- About the phrase "an IAA of 0.2978 which indicates fair agreement" -- It is my understanding that a fair Kappa agreement is considered above 0.70 unless the authors are using a different formulation, in which case I would recommend the authors to explicitly state and justify the threshold above which they consider the agreement "fair".
- The Precision, Recall and F1 scores are considered "high" by the authors, however, since there is no baseline or comparison it is hard to determine the significance of these measures.
- Recurrent typos in page 2: *"¡article,topic,cardinality¿"*, *"¡author,entity,rating¿"*, in Page 4: *"¡user, item, rating¿"*, ..
The authors develop datasets for NER and RE extraction of COVID-19 literature
They used the generated data to generate concept recommendations.
- The developed datasets might be valuable to further researchers
It might be because of my limited expertise on the field.
The paper explores the task of experts extraction and expertise topic extraction from COVID-related literature
While the methods seem reasonable, the evaluation is quite problematic, as explained below:
* For term extraction, the evaluation is mainly based on the MeSH terms, which I don't think is appropriate
Based on "When labelling an article, indexers select terms only from the official MeSH list – never other spellings or variations" ([quote from here](https://onlinelibrary.wiley.com/doi/full/10.1111/ijcp.12767#:~:text=MeSH%20terms%20are%20official%20words,never%20other%20spellings%20or%20variations)), the evaluation using MeSH terms risks penalizing words extracted that share the same topic but different surface forms with the MeSH terms
For example, "heart attack" would be not be recognized as a successful extraction since the word used in the list is "Myocardial Infarction"
* For expert extraction, there are two ways as evaluation:  ERC panel member matching and h-index
Both evaluation methods are quite problematic
    *   For ERC panel members matching, the authors have already pointed out its limitation in the discussion "The ERC panel members on the contrary contained only a few Asian specialists ..
 the panels being rather centred on Europe and North America"
    * The h-index tells mainly the productivity and citation impact of an author
For example, a person has a higher h-index but few work related to COVID-19 would potentially be recognized as a top expert by this metric
> "Typically, researchers rely on metrics of publication and citation impact to evaluate the level of expertise of their peers, but these metrics are not as reliable nor directly comparable across scientific fields and bibliographic databases"
* There is no comparison between the proposed method and any other baseline
So it is hard to interpret all the reported results
As an NLPer, I had a hard time finding the definition of MeSH terms and the annotation criteria for MeSH terms
It would be better if the authors could add some definition and details about MeSH in the paper
This study proposes a term extraction method to extract expertise topics from coronavirus related papers which aims to find experts accordingly
 My main concern is on the evaluation part.
First, the study does not compare with any baseline methods or other term extraction methods
The effectiveness of the proposed method is thus not clear.
Second, it is confusing that using MeSH terms for evaluating extracted expertise topics
MeSH terms are not designed for expertise topics at all
It annotates the terms wherever applicable to the ontology, not key terms or expertise terms
For instance, https://pubmed.ncbi.nlm.nih.gov/32134205/ has a MeSH term 'Humans', which is not the main term of the study at all
Having an overlap between MeSH terms does not necessarily imply the expertise topics are extracted.
Third, it is even more puzzled on using h-index to 'quantify' the expertise..
Is it a reasonable metric? Why an h-index of 20 was selected?
Fourth, looking at the extracted terms, it seems that they are not COVID-19 specific
For example, 'infection control'  and 'antibody response' are general terms for different types of coronaviruses
Whether it can find specific expertise terms for COVID-19 is not clear
The study could apply the method to LitCovid (https://www.ncbi.nlm.nih.gov/research/coronavirus/) which consists of PubMed articles that are on COVID-19 and it has curated topics to conduct a case study for evaluation
For instance, compare the extracted terms from COVID-19 articles with the terms from articles on other coronaviruses
The authors combine an off-the-shelf term extraction system (Saffron) and manually collected MeSH terms with a tf-idf based method (Bordea (2010, 2013)) for identifying scientific experts in under-indexed literature in the CORD-19 dataset
- the authors apply existing technologies to a new problem
I would like a proper metric here instead of a series of ad-hoc approaches
As an example h-index is not a good measure for expertise in a novel and/or developing area, which is one of the main focuses of the paper
- The authors present a pipelined approach for this technique, but they don't investigate how errors propagate within the pipeline
What happens if we strictly use MeSH terms and manually tagged documents? How does this impact the overall expert identification? How sensitive is the expert identification scoring to the h-index; what's the distribution of scores?
- this is a straightforward application of existing technology, not a methods development (this is a much weaker complaint than the evaluation angle)
choosing how many extracted terms to use?
- the subarea splits and use of ERC experts seem to mostly distract given that there seem to be few extracted ERC experts, rendering both the ERC and the split moot.
- what instructions was the expert annotator given for the MeSH terms?
- low quality term extraction may lead to overly aggressive MeSH term matching
How did you compensate for this?
- what proportion of researchers are ERC panel members? This is a crucial point to understanding the evaluation of expert identification
- what was the h-index threshold you settled on?
- why use this word-mesh-term overlap instead of an existing MeSH tagger?
I think that the idea of expert discovery could be explored on better tagged datasets (e.g
the portion of CORD-19 that has already been indexed, or any other subset of PubMed), allowing for easier measurement of the decay for any portion of the pipeline.
The paper presents an approach to detecting COVID-19-related symptoms in social media data (Twitter)
The authors use word embeddings generated by BERT and build a graph of tokens based on their similarity to the learnt context embedding which allows them to iteratively identify more and more symptoms
The study presents an original methodology
Since the method is unsupervised the authors validate its performance by manually calculating precision for a few identified symptoms
They also test the method on a previously developed and annotated dataset for adverse drug reaction detection and show that the method generalises quite well
- To improve the reproducibility of the study, the details about BERT implementation should be included.
- It is unclear what the similarity scores reported in table 1 represent for manual and graph-based approaches
Are these the cosine similarities between a given token and the seed word “cough”?
- I am not sure what this notation means “##raine”?
- Legends should be added to Figures 2 and 3 to show how node colours correspond to similarity values
- For the annotated ADR dataset, is it possible to calculate other metrics (e.g
The author represented an unsupervised graph-based approach for the detection of symptoms of COVID-19
1-The propose method will have several useful applications if it can be generalized and better than traditional methods.
2- The author test their method on 2 different dataset 
1-The methodology needs further explanation 
2- There is no comparison with traditional method that are able to capture such symptoms for instance LDA , W2V approaches
Sometime a simple method works better than complex one , particularly when the language context is simple 
Graph-based relations between a seed word and the more similar words are obtained by means of a similarity measure; the context embedding value is used to represent each word and compute the cosine similarity value with regard to the seed word
The authors applied their method to a collection of COVID-19-related tweets (using the word "cough"), and also conducted their experiment on an ADR corpus (Sarker & González 2015), using the term "pain"
Their method obtained similar symtom words in both cases and yielded promising results that could be pursued in other pharmacovigilance tasks.
- I found this is an original method that will interest the audience of the workshop.
- The article is clear, although sone methodological aspects need further clarification and some excerpts minor need revision
- As authors already point out, the method currently detects only single-word terms, not multi-words, which may be essential to detect fine-grained description of symptoms (e.g
- The experiment is only conducted using one word ("cough"); it could provide more solid results if more terms related to the COVID-19 symptomatology had been explored.
Healthcare professionals could validate these lists of medical terms
This could yield a quality reference dataset, and vice versa, healthcare professionals could get valuable, unknown data about symptoms to take into consideration (and probably not reported to date)
The authors could develop this aspect
Namely, developing an interface to explore and show these associated terms would be a great contribution of this work.
The first column in Table 1---"Cough (Manual)"---needs more explanation
Do authors mean that they selected manually a set of words related to "cough", then computed the cosine similarity value? Please, provide more details in the running text.
In Section 2.1, authors have to provide more details about the BERT model they used: e.g
BERT cased or uncase? Did authors use the models released in the paper by Devlin et al
2019? Did they resort to domain embeddings such as SciBERT or BioBERT? Or did they train their own model? What were the hyperparameters used (learning rate, dimension of hidden state, use of dropout, batch size...)?
1, Introduction: "SARS-nCOV" -> "SARS-CoV-2" or "SARS-nCoV-2"
1: colors are not distinguished when printed in black and white; authors should think of an alternative to improve this.
This paper proposes an end-to-end phrase identification and linking model for COVID-19 concepts
Experiment results look pretty good in terms of F1
Some more explanation and analysis would substantially benefit this paper.
in an end-to-end fasion,..." No, it doesn't have to
Being end-to-end only makes it look nice.
This might be due to domain issue
But why is this also the case for SciBERT-FT? Maybe there are some limitations in the process/model that overkills candidates, but no explanation/analysis on this.
This paper presents a new dataset of COVID-19-related mentions extracted from Wikipedia, and some experiments for concept mapping and entity linking
The dataset was annotated by using the available curated links to COVID-19-related concepts, and authors conducted some postprocessing to remove annotations of non-medical mentions (e.g
This dataset was then used as an experimental resource to test entity linking experiments, where concepts are restricted to the COVID-19 disease and occur in Wikipedia ("Concept wikification")
The authors applied a 2-step approach for the task: first, unambiguous entities are string-matched, then ambiguous entities are resolved through 3 methods: a majority rule-based voting scheme, a BERT-based classifier using SciBERT, and a SciBERT-based classifier fine-tuned on the dataset
By using this latter approach, the authors obtained promising results.
I liked the article and I think could provide an interesting contribution to the workshop, especially owing to the release of this annotated dataset and the trained models.
The weaknesses I found are related to the experimental methods: because results were only reported on one round of experiments, the generalizability of the outcomes are not solid enough
A good methodological approach is to test the model in several rounds with different random seeds or random initializations, and then report the average F-score and standard deviation.
Finally, I think a more thorough description of the dataset is missing (e.g
number of tokens, examples of entity types...)
I provide some other comments below.
4: authors could also take a look at BioBERT, which was trained on PubMed article abstracts
Lee J, Yoon W, Kim S, et al
BioBERT: a pre-trained biomedical language representation model for biomedical text mining
1: References need to be sorted: e.g
"(Ratinov et al., 2011; Lin et al., 2017; Nguyen et al., 2016)" => "(Ratinov et al., 2011; Nguyen et al., 2016; Lin et al., 2017)"
This paper addresses the wikification of COVID-19 concepts
- The result that fine-tuning on the dataset improves performance is interesting.
- It would have been nice if the authors had provided more details about the dataset, like the distribution of concepts or a list of the most common concepts.
Why? Some error analysis here would be helpful
Does this suggest that F-score is not the best metric here? There seems to be a big difference between precision and recall for the baseline.
- As for the evaluation, it would have been an interesting baseline to compare SciBERT with BERT-base, although this omission does not warrant a rejection in my opinion
Please include it if you're able to run the experiment.
This direction of study would offer insights to the community behaviors w.r.t
COVID-19, in addition to the related works that focus on Biomedical literature
However, justifications and explanations in this paper are very unfortunately insufficient.
The word "explain" appeared many times across the paper, but they should actually be "correlate" because the methodology used in this paper does not constitute an explanation.
Although the paper presents lots of technical details on neural models and text preprocessing, it lacks a proper explanation of how data are filtered, e.g
In the 2nd paragraph of Sec 3, how does having the county-specific model "combat" the large variation of word embeddings trained on small data? This is not explained at all.
This paper studies the relationship between online language relating to COVID-19 and counrty-level behavior patterns in the United States.
The authors show that the variation in how people perceive the virus can reveal people's stance towards social distance measures, as well as towards Trump.
Suggest to provide more intuitive explanations.
Details about data, method, setup are missing.
* Footnote 2, 5: period missing
* Suggest organize Section 2 into Section 2.1 explaning tweets and Section 2.2 explaning mobility data
* Page 3, 'we fine-tune that baseline model', do you mean 'continue training'?
* Suggest to add more intuitive explantions about the results, helping readers to understand these correlations.
The paper presents an interesting analysis on the correlation between a population’s tendency to adhere to social distancing policy and the association of the virus with the political left, fraud and ultimately, individuals’ political party identification
Utilizing Google community mobility reports, the authors approximate the amount of people staying at home by change in their mobility in residential areas
The second part of the work supports the initial findings by demonstrating that (based on twitter data analysis) counties that tend to social distance less, are those that supported Donald Trump in the 2016 election.
Section 2.1: the (presumed) positive correlation between residential mobility and social distancing needs to be better explained; it not entirely intuitive; yet, is one of the key assumptions of this work.
Building county-specific embeddings could be done using distributed representation for geographically situated language, as described in https://www.aclweb.org/anthology/P14-2134.pdf
Table 1 needs more detailed interpretation IMO: say explicitly what are independent and dependent variables, what the numbers in the table represent (regression coeffs? pairwise correlations?)
Also, the somewhat low R^2 in the first three models needs better interpretation.
Section 4.4: more details re the generation of an experimental document would be appreciated – *how* do you manually alter tweets in the control document? Some examples could help at this point.
Table 2: are the differences in control vs
# [REVIEW] COVID-19/EMNLP Workshop -- Sentiment and Moral Narratives during COVID-19 [paper 18]
 The paper presents 2 core results:
Tweets pertaining to loyalty fluctuate over time
 “ Containing the pandemic urged for rapid measures, the success of which hugely rely on mass cooperation” and“Understanding indepth the human values triggered and who may resonate with those, is essential to fight the infodemic.”
* Suggest listing 4 exemplar countries on first page in the body of the paper (although they are currently listed in the abstract)
* The use of embeddings to expand the lexicon seems reasonable, but was there an attempt at evaluating the words identified?
packages used for word embedding work)
 However, I think the choice of MFT is sensible given your goals, but could be expanded on a little
 Also, it would be helpful to explicitly state that the MoralStrength lexicon explicitly draws from MFT
* Minor point, but in Section 4, \citet{} should probably be used to reference the Shanthakumar paper.
* “indifference, happiness, sadness, emotions are included, essential for getting a holistic understanding of the users’ intend.”  gaining a holistic understanding?
* “Finally, this group is more authoritative with a pronounced feeling related to the obligations of social relationship” I’m not sure if you mean authoritarian here?
* I’m not clear on how reliable the Grouping of hashtags is
* Figure 1 — text is too small to read - also, I’m not sure how useful the confidence intervals are
* Figure 1 — emotions (i.e
loyalty, etc) should probably be presented on the graph, not just in the caption
The choice of categories chosen — loyalty, fear, inspiration — don’t seem to map very well to MFT
* Evaluation is currently somewhat lacking
Some examples: “with the in four exemplar countries” , “communications Campaigns”, “derived to a total number Of”, “we were based on”, “being 5 the”, “understanding of the users’ intend”, “is a quite big fluctuation”.
The choice of hashtags for breaking the tweets to two behavioural groups needs to be explained.
It could be useful to give some examples of how the scoring is done
Need to be clarified if only 273,924 out of 3,163,500 tweets were used for the analysis
If so this need to be explained in the data collection section.
Figure 1 the y-axis could be labelled with (loyalty,fear and inspiration).
The answer to question two is based on the negative correlation of fear and loyalty but Loyalty does not seem to follow the fear trend
Even when fear goes down loyalty does not keep going up
Given we cannot explain the fluctuations in the loyalty the answer to question 2 is not satisfactory
 The data analysis was performed according to two lexicons: 1) MoralStrength lexicon for moral valence based on Moral Foundation Theory (MTF),  2) DepecheMood++ for sentiment scale calculation
Moral lexicons were expanded based on word embeddings
 Tweets frequencies were then calculated for two behavioral groups: Social Distancing(SD) group and the Misinformation propagators(M) group
 Average morality values were further compared for the two groups
 Loyalty (Morality), fear (Sentiment), and inspiration (Sentiment) evolutions were studied from February to May based on 4 different countries: China, New Zealand, Italy, US
It provides special insights into the relationships between behavior and morality and sentiment for the COVID dataset
Please clarify how the annotation is performed
In Figure 1, for each line, there is a shaded area
Is it the range of scores?
From Table 1, the sum of the number of posts for M and SD groups is far less than 3,163,500 (the total number of tweets being collected)
Please clarify after grouping to M and SD, where do the rest of the posts go?
This paper introduces a very interesting and useful question-answering resource about COVID-19
The corpus consists of 2200 frequent question-answer pairs, extracted from 40 trusted online sources plus 24k social media questions semantically-aligned (by experts) with one of the former dataset.
I missed some information on how BM25 was trained
Regarding the chatbot to be developed, it is important to notice that the answer to many questions  about COVID may change along the time
For instance, during quarantine, the question “Can I go to a bar?“ would be negative, whereas after the end of the lockdown would be positive
I am not sure how you can control the update of 2200 questions automatically
It is important to think about that in order to do not spread misinformation.
This paper describes the curation efforts to create a QA dataset on COVID-19
Then the provided answers were further manually annotated (whether they are relevant)
If an unanswered question is a new topic of COVID-19, the top retrieved answers will be irrelevant, and health experts will not provide new answers based on the description
Adding these questions will enrich the datasets
In contrast, if a question is similar to the existing questions, adding it will just have one more similar instance
Second, the study needs to explain how to use this dataset in detail
How to train a QA model in this case?
Third, from the methodology level, while BM25 is effective, it does not capture the semantics of the questions
Did the study also try to use other methods to calculate the semantic similarity between the questions such as using word or sentence embedding?
Also importantly, the dataset is not public for now
More specific comments cannot be made
The paper presents a potentially useful dataset of COVID-19-related frequently asked question-answers
The link given in the paper does not yet allow the access to the dataset
(https://covid-19-infobot.org/data/) Presumably this will be made available upon publication?
Introduction: When explaining the three aggregation efforts, what does the 1st mean by "generating high quality information"? 
In this sentence “To aid in this effort, we aggregate factual information in the form of verified questions and answers to help answer frequently asked questions about the pandemic.”
Better to use question-answer pairs instead of “questions and answers”.
Section 2:  This sentence is vague: "abstract away adding this information to our set"
Figure 2 is not a clear example
The answer to the question of "What is COVID-19?" is not correct – COVID-19 is an acronym for “Coronavirus Disease 2019”
The example also does not have any indication of the quality of the answer, and the impression gained is that this might not be a very useful resource at all
It might be more accessible to show a few records in the database, or at least to use a very accurate example from the documentation together with a snapshot of the answer that would be made available
The figure's title indicates the inclusion of metadata, but it does not have the "date/ update date" metadata mentioned under section 2.1.
Section 3: Repeated the “We list the the number of question-answer”
Section 4: Need correction “We additionally collection”?
Here Qorona and CovidFaq are mentioned, but there is no other reference to them later as if the questions from these sources are used or not? If yes how? If no, why they are mentioned?
Section 4.1 After Twitter data is collected, what technique/tool is used to extract questions? What indicated that a tweet does contain a question?
What techniques is used to group semantically similar questions? Is it topic modelling or something else?
This paper introduces a multilingual dataset of 6871 utterances across 4 language + a mixture of English and Spanish
The dataset itself is an interesting contribution for the research community, which the authors complement with the study of different cross-lingual transfer learning techniques
In terms of methodology, could the authors elaborate more on the choice of the 16 intents? What was the basis for chosing them over others?Also, I would have liked to have more details on the ontology they mention in the beginning of Section 2
The C Section of the Appendix is a bonus point in terms of reproducibility of the results
However, I do not understand Table 6 in Section B of the same Appendix
Why was German considered less relevant than the results in Spanish and French (Table 3)
For example, "I traveled to new york recently" could also be understood as "travel".
This paper introduces a multilingual dataset for detecting COVID-19 specific intents and a Spanish test set for code-switching
The strength of this paper is that they introduced a dataset to the community, provided several baseline models in the experiments, and showed that using cross-lingual representation is useful in this task
However, there are some major problems about the dataset which the authors need to elaborate more on
* The paper presents to the community a public multilingual dataset for detecting covid-19 specific intents in user utterances
* The paper shows that cross-lingual representation could improve the baseline models in this task
* The paper is well written and provides relatively comprehensive comparison between baseline models
It also provides analysis about the performance of these models
* The dataset is synthetically created by annotators based on an ontology, but there is no description about the ontology
Furthermore, there is no discussion about how close this synthetic dataset is to real data
* The authors did not mention how and why the 16 intents are chosen, and also lack the definition of these intents
Is the formulation of these intents based on the real data or the synthetic data? Do most of the real data fall into these intent categories?  
*  The authors need to elaborate more on the details of the annotation process, including the number of annotators to annotate each utterance and the inter-annotator agreement among annotators
    In general the authors need to provide more details about the dataset
Otherwise it is hard to assess the quality of the dataset and makes it less useful to the community.
* I would suggest the authors give some significant test statistics when they claim the performance scores are significantly better
* It would be interesting to see the breakdown of scores(Precision, Recall, F1) for each intent category and some analysis of the difficulty/easiness to identify each category
Showing cases with correct/incorrect predictions could also be helpful
This work presents a multilingual dataset for intent detection of COVID-related utterances
Besides, the authors built various baselines and experimented with code-switched data (Spanglish)
- The dataset is open-sourced and could be helpful for further COVID-related development.
- Reasonable analysis and discussions on baseline results.
- Hyperparameters of models were provided.
However, no details on how synthetically created, the information about annotators, the ontology and how it was created, and the seed examples were provided
There is also no discussion on how close the dataset is to real utterances
These issues make the dataset, i.e., the main contribution, less useful
- For Table 3, it might be better to have a plot instead of a table to show the trend when increasing % training.
- The authors should discuss why zero-shot performance with XLM-R Large is better than using 10% Spanish data, and why it performs worse than the base model under the setting of 80% French data.
Authors has used data distillation methodology to augment the data
However, the contribution needs more evaluation
For instant the authors need to plot learning curve to know how Bert model improves with different % of dataset
Second author may investigate how many manually labelled data is required to reach the current performance with data augmentation and see how worthy data augmentation is compared to increase of manually annotated data (how many time and efforts it can save)
 Third, I would recommend the authors to investigate the data they have used and check if they are annotated based in robust guideline, it is worth to try this approach in well representative data to see its feasibility
the main issue of some annotated data is that not because it is small but because it is not representative, and they are more vulnerable to noise and overfitting when they are augmented and the performance on test data is likely not a real improvement
The author adopt data distillation model into the tweet stance analysis in support of monitoring public opinion on COVID-19 intervention measures
The research design and experiment results are of good quality
Some details of the experiments are expected to answer: 
* How many rounds for the teacher/student models in data distillation achieves convergence in performance?
(306 Against, 223 Support and 565 Neutral)?
* Figure 2 is not informative sufficiently to support the result.
This study aimed to use Twitter data to better understand stance toward COVID-19 intervention measures
The topic is timely and experiments seemed appropriate
However the approach taken to develop the manual labels was not fully described
In other words, how were the manual labels determined?  It was unclear whether the authors conducted interrater reliability to obtain an understanding of how reliable the manual coding was
Given the models were based on these labeled data, this is rather important to document and demonstrate that the manual data coding process was done in a reliable fashion
To accomplish this the authors need to write out the rules for ’Support’, ’Against’, and ’Neutral’ as well as provide examples (either example tweets or words from the tweets for each case)
Without this context it is hard to determine this study ultimate value to the field
- which communities are targeted by hate speech in the pandemic context? 
Here are some suggestions to the authors:
The fluency of the paper will increase.
How were the query search (COVID-19, COVID, Wuhan…) and seed words (China, Bolivia, etc.) defined?
How many data was annotated so far?
Overall, I agree with the sentiment of Reviewer 2 in that the direction and questions are interesting but a more comprehensive result if needed for publication.
I think figure 2 could be seen as a contribution if the authors had explained why the findings that they purport are significant and what question they answer
Once the dataset is collected, the authors should describe the dataset in detail and provide examples to help readers understand the value of their collected dataset
Finally, the paper presents preliminary results from a separate machine learning system.
Unfortunately the additional data analysis (in sections 4.2 and 4.3) does not link well with the proposal to provide any support or specific research direction
Some further issues with the paper are outlined below
- The paper mentions Bender & Friedman’s data statements but does not provide the long form of a data statement (or as close as possible at this stage) as suggested by the cited paper
- Apart from one research question in Section 4.1 (discussed below), the paper doesn’t provide reasoning why hate speech should be studied specifically during the coronavirus pandemic as opposed to any other time.
You state that your results outperform the current best methods for the SemEval 2019 HatEval challenge
Oddly, you then claim that it “does not perform as good as in the original domain”, Maybe I’m misunderstanding, but that performance is good, right?
Furthermore it is not clear if the data collected will contain this information.
Is there an interesting reason behind that?
The paper asserts the importance of translation technologies in crisis situations and describes the TICO-19 benchmark for validating and testing translation systems for low-resourced languages
Also, other additional resources such as translation memories and in-domain monolingual data are presented
The paper concludes with initial benchmarked results using publicly available pre-trained models.
I think the work is well-aligned with the workshop's objective
I do have a few questions:
What does the 95% rate correspond to? Does it correspond to the direct assessment scheme employed in WMT?
Were there any domain-specific evaluation guidelines?
If I understood the paper correctly, the additional resources in Section 5 are not used at all to train/fine-tune the "Our OPUS" and "Our TED" models? Is this correct?
English documents were translated into 35 languages with terminologies provided by Facebook and Google, where the OPUS-MT system was employed.
The MT is pre-trained on OPUS parallel data
It is not clear how do you use the resources in section 5 to adjust the pre-trained system.
Can you please clarify the structure of the pipeline system? After collecting the data and specifying the target language, how do Facebook, Google terminology datasets being used in the MT system?
The authors describe the TICO dataset, a development and testing dataset for multi-lingual medical translation
They describe in great detail the creation of this dataset, and the QA measures they undertook
I am particularly a fan of Table 8 which includes specific measures and details about which dataset underwent which parts of their QA process, although I note there appears to be an extra empty aggregate row.
The authors provide a great overview of the varying languages, grouping them for relative importance in the translation effort, and making a qualitative assessment of the resources available for each of them.
Overall I think that this is a high quality development and testing set
I disagree with the relative downweighting of the importance of the parallel data in the paper - the authors should document the sizes of the data they found, even if small
 Licensing concerns are relevant for others using the data, but a proper description of it, including sizes and genres, belongs in this paper.
I am concerned that the relative importance of the PubMed data in the evaluation datasets, by a small number of documents providing a large number of sentences, will primarily reward systems for a somewhat random game of vocabulary whack-a-mole instead of rewarding the ability to produce a high quality translation over a diverse set of inputs
While adjusting this is likely impossible at this point due to expense or annotation effort, I would like to see an explicit treatment of this potential issue.
Wherever possible I would request that the authors create wayback or archived versions of the links in the paper and to prefer full links over shortened ones instead of relying on bit.ly's promise to be around forever
This will help ensure that their work remains accessible from a long term perspective.
This paper describes a question-answering system that is particularly designed for COVID19-related questions
The system is capable of providing two types of answers: potential spans containing the answers to the questions and potential bio-medical entities that the query user might be interested in
The system design of the paper is well justified, and a online demonstration of the system is provided
I just have several questions as below.
I tried several entities including the example shown in the paper but did not get any returned results in terms of the related entities.
Second, I like the idea of retrieving both answers and related entities for query users
Do the authors think that it is possible to use the two systems to enhance the performance of each other in the future? Now the two systems seem disjoint to me.
Finally, the authors claim that one of the highlight of the system is that it's real-time
Of course, I've tried the online demonstration and did not feel significant delay in getting answers
But it is necessary for the authors to provide evalution results on its efficiency
This work presents a real-time QA tool covidAsk, which provides both the answer/supporting documents, as well as related entities
The framework mainly uses two previous works (DenSPI and BEST) with BioBERT on CORD-19 and PubMed
Despite some issues mentioned below, the paper contains substantial content that could promote the development of COVID-related QA/IR systems.
I have tried on multiple days with multiple ips, but only the cached examples work
For example, https://covidask.korea.ac.kr/search?query=What+can+I+do+to+protect+my+family+against+COVID-19%3F will return an internal server error.
- For a *real-time* system, there should be evaluations on latency, computing resources needed, etc
- I didn't see any evaluation on the entity search engine side
In that sense, would it make more sense to use one of the QA systems plus a regular search engine?
- Section 1, Paragraph 5: "The first challenge is that there are no QA datasets that are tailored specifically to COVID-19
" there is at least contemporaneous work COVID-QA (https://openreview.net/forum?id=JENSKEEzsoU).
- Section 3.4 NBCI -> NCBI
This study presents a COVID-19 QA system, COVIDASK
It uses two models previously developed (DENSPI and BEST) as the basis and adapted accordingly to the evaluation on a manually crafted COVID-19 QA dataset
The manuscript is easy to follow
First, why the indexing is at phrase-level? From Table 5 (the evaluation on the IR part), it seems that the search effectiveness on phrase-level indexing is lower than other systems indexing at passage or paragraph level
Second, the evaluation dataset for the QA part was created by the authors themselves
Was the system evaluated on public COVID-QA datasets such as https://github.com/deepset-ai/COVID-QA and https://github.com/castorini/pygaggle/ ? The evaluation results on these datasets will be more representative since different COVID-19 QA systems can be evaluated on the same datasets.
Minor comment: please include more detail on the system such as the update frequency and any APIs supported.
The paper describes the construction of a website (system) that provides users with COVID-19-related information aggregated and translated from multiple reliable sources
Extensive use of crowdsourcing is done to check the reliability of the resources as well as to annotate them
The resources in languages other than English and Japanese are translated using a machine translation system called TexTra.
It is both costly and labor-consuming to build such a dataset.
A keyword-based baseline sounds too weak in comparison with a BERT-based neural network
A more sensible baseline would be an embedding-based similarity measure.
Also, even with the BERT-based topic classifier, the precision of some topics is very low
It would have been interesting to see if the inter-annotator agreement is also low for such topics
I think this is especially important because, in order to justify the validity of the dataset, the authors should have posted the reliability of the annotations among the annotators.
In addition, as the system makes heavy use of machine translation, a simple result table about the quality of the translation would be have been informative.
However, its potential value of the dataset is not convincingly presented in the paper in its current state.
This paper addresses an important topic
I visited the website and it certainly seems like a complicated and useful tool.
What are the details for the topic classification? E.g., what is the distribution of topics and how were they annotated, and with what annotator agreement? Did some articles belong to other topics? What was the ground truth? It could be helpful to show a confusion matrix for the BERT classifier.
Is there any evaluation of the translated text?
The main area of improvement that I see in this paper is that there is not much analysis of such a large dataset
Are the topics that are annotated with the articles the same or similar to articles that could be discovered by unsupervised clustering methods? Did they vary for the countries based on the number of cases at the time?
Any possible analysis on the distribution of n-grams?
The paper describes a system that aggregates news and official sources on Covid-19 and presents them with a web interface to the public, as a way of providing reliable, comprehensive and up-to-date information
The system is international, making extensive use of machine translation
It makes use of crowdsourcing in two places
The first use is to solicit recommendations for good information sources
The second is to gather training and test data on individual articles, asking about relevance to Covid-19, helpfulness, the quality of translation and membership of 9 topics
The system also makes use of NLP components
A classifier, based on BERT, classifies articles as to whether they are relevant to Covid-19, and assigns them to the 9 topics
Furthermore, they deploy a pre-existing machine-translation system.
Reasons to accept: The system being described potentially meets a need for citizens to have access to reliable, relevant and up-to-date information; the value of this paper largely depends on whether the system lives up to this promise
Also, the system is of some interest to the NLP community
The article classifier appears to get good results for relevance-to-Covid-19 - an F score on 0.84, substantially above a keyword-based baseline
The list of reliable information sources that they have collected is 
Furthermore it is often interesting to hear about the contexts into which NLP systems are to be deployed.
Reasons to reject:  In general this paper suffers from a lack of evaluation, or even thought about evaluation, except for the NLP components (the evaluation of the classifiers is OK and the machine translation is done using a published system)
Relatedly, the F scores for classification into 9 topics are a lot lower than for the relevance-to-Covid-19 classifier
It is not clear whether this is caused by the classifier not being so good at the task, or whether the low scores reflect a task that is hard for humans to do consistenly
Some discussion of inter-annotator agreement would be helpful here.
Conclusion: This paper describes a system that has a lot of promise, but provides very little information about how well it lives up to its promise
This paper presents a system designed to help medical professionals to quickly get short answers about COVID-19 based on multiple source documents
The system took part in a related Kaggle competition and won one of the tracks, which means that the answers for one particular question regarding Covid (What has been published about information sharing and inter-sectoral collaboration?) were considered to be the most relevant by medical experts
The system consists of a three modules: Document Retriever, which includes Query Paraphrasing and Search Engine (Anserini), Relevant Snippet Selector (HLTC-MRQA and BioBERT) and Multi-Document Summarizer
The authors released their tool and source code, which is undoubtedly very helpful for both medical professionals and NLP community working on similar tasks
The search/snippet extraction/ranking part follows an established pipeline, and the authors improved the extraction results by using the ensemble of a domain-specific model (BioBERT) with generic one (HLTC-MRQA)
However, the parts of the system which sound more novel raise some concerns.
The Query Paraphrasing subsystem - the authors paraphrased the Kaggle task questions manually to improve the retrieval results, since, as they say, automatic question simplification methods did not help to improve the retrieval
One concern is that such paraphrasing could have introduced bias for Kaggle questions and is obviously not scalable for arbitrary questions; another question is why would you call it a “subsystem” instead of stating that the original questions were too complicated for the system to retrieve meaningful results?
The summarisers (both extractive and abstractive) are evaluated on datasets not related to COVID-19 or medical literature datasets
Though it is true that there is no multi-document QFS dataset for COVID-19, the authors could have used a medical QFS dataset such as Mollá, Diego, and Maria Elena Santiago-Martinez
[https://sourceforge.net/projects/ebmsumcorpus/], which would give a better indication of summarisation performance than unrelated news/debates datasets
Moreover, though the suggested extractive and abstractive models improve over the baselines of Lead and vanilla BART, the reported results for DUC 2005, 2006, 2007 datasets are worse than that of old non-neural extractive models [see, for example, (Ye, S., Chua, T
Document concept lattice for text understanding and summarization
Information Processing & Management, 43(6), 1643-1662.] and recent neural abstractive models [for instance, (Zhu, Haichao, Li Dong, Furu Wei, Bing Qin, and Ting Liu
“Transforming Wikipedia into Augmented Data for Query-Focused Summarization.” arXiv preprint arXiv:1911.03324 (2019)]
There is no qualitative evaluation of the results apart from the claim that “effectiveness of the system has been proved by winning one of the tasks”
If one checks the results of the system for other questions, say “What do we know about asymptomatic transmission”, 4 out of 6 sentences of the abstractive summary have no relation to the question but rather talk about paediatric patients and PTSD
If quantitive evaluation against the COVID corpus was impossible, the authors could have performed a qualitative evaluation of relevance/factuality of the resulting summaries in line with recent tendencies to evaluate factuality instead of relying on ROUGE scores.
A suggestion rather than criticism - the results could have probably be improved by incorporating more biomed-related resources, for example, by fine-tuning on PubMed or CovidSum dataset instead of CNN/DailyMail 
The paper is very well-written with enough detail across the pipeline comprising three modules: 1) Document Retriever, 2) Relevant Snippet Selector, and 3) Query-focused Multi-Document Summarizer
They provide good evaluation with baselines and analysis
Open-sourcing the code along with clear instructions is commendable
Related work section could shed additional light on similar approaches or settings, but is understandable given the page lmits
Have authors interrogated cascading errors due to the nature of the pipeline? It’d be good to briefly comment on that
This paper describes the CAiRE-COVID QA and query-focused multi-document summarization system for COVID-19 literature
The system links together several SOTA models in QA and summarization in a novel and effective way
Overall, the modeling decisions are well justified, and the performance of the system is quite impressive.
* The authors do not remark on factuality in abstractive summarization, though it would seem to be an important consideration here due to the domain being of healthcare and public health consequence.
* Related: for both extractive and abstractive multi-document summarization, especially in the scientific domain, contradictions can be a problem
Summary: This paper presents a conversational system, Expressive Interviewing, that is tailored to assist people in coping with the impact of COVID-19 on mental health
Before and/or after interacting with the system, users are asked various questions about their emotional states and their interaction experience (e.g., stress levels, life satisfaction, meaningfulness of interaction with the system)
The system uses word occurrence frequency statistics from user inputs to select appropriate questions for specific topics or word categories
To validate the system’s functionality and usefulness, the authors collected interactions with 174 users and report a thorough analysis on the impact of using the system based on the collected data
Furthermore, the system’s usefulness for user interaction related to COVID-19 is compared Woebot, a related general-purpose mental health application offering a conversational agent
Based on data from 12 participants discussing topics related to COVID-19, the authors show that using Expressive Interviewing, participants report lower stress levels as compared to using Woebot, and that it is easier to use, motivated the users to engage in the conversation more strongly and led to more meaningful interactions
The paper is well-structured and easy to read, the interface is straightforward and easy to use, and the idea of providing such an online tool is interesting
The quantitative and qualitative analyses are thorough and provide insights into how users interact with the system
The system’s internals are only described in the text, a figure or table to illustrate its functionality would be very helpful to better understand the underlying processes.
The comparison against Woebot furthermore suggests that Expressive Interviewing is better suited for users discussing their concerns related to COVID-19
However, the authors do not provide a justification for why Woebot was selected for comparison and why their system was not compared against the Wysa tool
Is there a reason for this? Furthermore, the sample size is very small and statistical analyses would support the findings of this experiment.
In sum, I believe that this is an interesting tool which has the potential to find applications in in the context of COVID-19, but I would encourage the authors to better illustrate the system’s design and to conduct more detailed analyses on the comparison to the Wysa/Woebot conversational agents.
- Table 1: there’s a typo in the caption (“ratinga”).
- Table 5: in the caption, there’s a missing “of” after “Comparative evaluation” and a typo “...with scores > 3 in a 7-point…”.
Overall - I like the idea and the paper is well-written
The online system is great - I did it and I much appreciate the authors' effort on making it available.
I feel that some comments need addressing;
- p2 (paragraph on MI): maybe add example questions used in MI sessions
- general aspects about the chat interaction:
- - how long was the interaction?
- - how often does the system follow up?
- - what are the rules/criteria for following up (or not)?
- - what were the criteria for detecting emotions?
- - how was the detection of topics and emotions validated? And what was the performance?
- - how often were responses flagged for the reasons given on p3 (right column, middle)
- - all in all: the comments above all require more detail on the system
- related: the sample is quite small: why not use it in an online study with a few hundreds? This would allow for a more representative view on the system and its comparison tools (see below)
- - since you present multiple correlations, you should correct the significance alpha level by the number of comparisons (e.g
0.05 for 1, 0.025 for 2 comparisons, 0.005 for ten comparisons, etc.)
- - absent that Bonferroni correction the findings capitalise on chance and might contain FP statistical artifacts.
- - the majority of the results section would thus need rewriting
how does your system compare to simple writing?)
- p7 (left col, bottom): "more associated with the group [...]" than what?
either they get A or B and then you assess which one was better).
- - both points above will be solvable with a proper, large sample
- I appreciate the scope of this paper is on introducing the tool but I'd really like to see a section on linguistic analyses of the responses (what did they write about, did emotions change during the coversation or depending on the topic?)
The tool is great but it requires more adequate statistical tests and more analyses to fully introduce the system (and its limitations).
**Recommendation:** Authors must include system description that would allow for reproduction
Comparison to Woebot is promising, but not thorough
Authors should prefer interval or effect size over significance tests
Authors should discuss how they intend to assess their system earlier in the paper
The authors introduce a COVID-19 specific mental-health support chatbot that focuses on motivational interviewing and expressive writing
Additionally, they compare user-ratings of satisfaction against a standard system: Woebot
The paper has major shortcomings in terms of system description and analysis.
The authors should describe which NLP techniques are used for the various parts of their chat system and how these pieces interact with one another.
Additionally, the paper's analysis is lacking
They should prefer effect size or confidence intervals
Additionally, no statistical testing is done for the comparison with Woebot; however, this does not stop the authors from making claims about differences between the systems' performances
Authors should review [Yan, Song and Wu (2016)](http://www.ruiyan.me/pubs/SIGIR2016.pdf) as example, noting 
The paper describes AskMe system which is a query engine that searches COVID-19 literature and additionally offers further processing of the results using NLP tools.
It outlines the system briefly and makes a comparison with other related search engines, namely, LitCovid and iSearch.
While searching over the latest literature is indeed an important aspect, the search process is keyword-based (as opposed to vector-space-based) and limited to titles and abstracts.
In addition, as the authors acknowledge in the paper, only a preliminary evaluation is performed, and the result is not yet convincing.
Although the work in its current state is not yet mature, I do applaud the authors for building such a system that has a great potential to benefit medical researchers in the field.
The authors describe the AskMe system – a search engine utilizing the CORD-19 dataset to retrieve documents per natural language queries on the pandemic
The paper describes the engine in detail and provide a qualitative evaluation of its retrieval results to those of the LitCovid system
The paper is generally clear and well-written.
The weaknesses of this work lie in two main aspects: (1) scoring algorithm for relevant document retrieval, and (2) qualitative (and not entirely convincing) evaluation
Re (1): Using only title and abstract for scoring a document is not considered sufficient for a retrieval task and would naturally render the proposed system inferior to others
Additionally, I appreciate the retrieval algorithm presented on page 3, but wonder how it compares to traditional (SOTA among them) IR approaches, including vector space, probabilistic and language models
The authors could attempt, for example, the TREC-COVID (CORD-19-based) benchmark (https://ir.nist.gov/covidSubmit/index.html) and test how their engine performs on this labeled data, when compared to other search engines.
This work describes how the LAPPS Grid was augmented to support querying over the CORD-19 corpus
This extension makes available filtering features in Grid, as well as the suite of Grid NLP tools, for querying and accessing data in CORD-19
The authors suggest that Grid retrieval performs at a comparable level to other COVID search engines like Vespa or CORD19.aws, though no significant evaluation was performed.
It’s hard to gauge from the presented evaluation exactly how this system performs relative to other systems
The weakest part of the paper is the evaluation, which currently relies on a single expert evaluator comparing results from different search engines
The authors should consider evaluating on available CORD-19 retrieval datasets such as TREC-COVID (https://ir.nist.gov/covidSubmit/index.html).
This article describes experiments on MT of covid-related documents into low-resource languages
The article is well motivated, but it would help to focus on the challenges and current approaches for low-resource MT, including unsupervised methods
For work on the unsupervised paradigm, see for instance Artetxe et al
For gaining space some parts of the motivation (such as Table 1) could be removed
Another section where more information would be desired is on the used dataset
There is no mention of the process to translate the data and its quality, or whether it has been used for MT in previous work.
With regards to the experiment, it is limited to applying an standard supervised solution exploring only the use of factoring
It would be interesting to explore more creative solutions and evaluation frameworks
For instance, they claim that the training data is insufficient, and they could test a learning curve to provide more clarity on this aspect
Monolingual data could also be used to test extensions to the model.
On the evaluation, it would be interesting to provide context on what the BLEU scores mean for a practical system, and provide at least some qualitative error analysis.
- P1: "very essential": remove "very"
- P1: missing reference for English-Luganda SMT
- P2: "one of the major languageS"
- P2: "one of the official languageS"
- P2: "low resolution": low resource
An Effective Approach to Unsupervised Machine Translation 
Mikel Artetxe, Gorka Labaka, Eneko Agirre 
This work reports on a series of experiments using neural machine translation systems for translating English into five low-resource languages (Lingala, Nigerian Fulfulde, Kurdish Kurmanji, Kinyarwanda, Luganda) focusing on information related to COVID-19
To do this, parallel corpora of approximately 3000 sentence pairs for each low-resource language are applied to two NMT models.
While this paper discusses an important topic, the datasets used to train the specified translation models from English into the low-resource languages are too small, and a larger amount of sequence pairs and/or a different approach (e.g., using domain adaptation techniques) to this task is required
There is a missing reference at the end of Section 1 (English-Luganda SMT system).
Some of the information in the Introduction could be moved to the Related Work section.
The BLEU metric mentioned in the paper requires a reference which is not provided.
The NLTK library used in the experiments requires a reference which is not provided.
In Section 2, is the phrase “low resolution” mistakenly used for “low resource”?
This work presents NMT models for COVID-19 related documents for five low resource languages in Africa and Mid-East Asia
The topic presented is definitely crucial, and the paper is easy to follow
I appreciate the author detailed all procedures to train and evaluate the models
However, there are a few key issues that should be resolved before moving forward:
  * The translation model only takes <3k data for training
I doubt if any useful information could be learned with this limited data
The author may consider transfer learning and data augmentation to mitigate the scarcity of the data
  * The vocabulary contains only 300 tokens
Is there any specific consideration for this?
  * The paper uses unigram BLEU for evaluation
However, unigrams only capture limited signal, and it is hard to make system comparison and justify that factored NMT is better with a rather limited metric
BLEU-4 seems to be a better option.
  * Given the test dataset has only 100 sentences, it is important to obtain confidence intervals to support the argument of one system is better than another
In the long run, it is crucial to check the correctness of the translated sentences to make sure all critical information was translated correctly.
This paper introduces an application that enables researchers to explore COVID-19 literature
The functions of the application include finding similarity between publications, targeting central sentences in a publication, and extracts NER from publications
* This application could be useful for researchers to conveniently explore publications related to COVID-19, or maybe any other topics not limited to COVID-19
* While the proposed methods are reasonable, the lack of direct evaluation raises the question how well they  perform
For example, STS seems a key component in constructing the publication network and finding central sentences, but there is no direct evaluation how well the proposed method finds similar publications
Despite the indirect evaluation using "one corpus of the topic of
transcriptional-regulation", the performance of SciBERT may not be consistent in STS corpus from other domain
 If the STS is already poor at the first place, errors could propagate and amplify in the pipeline and make results less accurate
Also in finding central sentences, how good is using STS by SciBERT compared to other traditional methods, such as traditional text ranking algorithms? And there is no direct evaluation of the OGER in COVID-19 publications
Could OGER be poor in recognizing COVID-19 NERs that have not been seen in the training data of OGER? Without any quantitative analysis to answer these questions, the application is less useful to the community
* How did the authors chose 0.35 and 0.65 for Eqn.2 ? 
This demo paper presents a system for searching COVID-19 literature and relating similar sentences both within and across documents
The search component is augmented with concept search
The goal according to the authors is to facilitate skim reading by visualising  salient connections between sentences.
I find the eagle-view of the internal and external links interesting and definitely worthy of further exploration
In section 3.2, a corpus on transcriptional regulation is introduced, but little is said about it and the annotations it contains
Similarly, a short description of DistilBERT is in place, as well as motivating the decision to include this model in the present study.
I find interpreting the results from Table 1 quite difficult: How exactly was this study involving correlation carried out? What does e.g
the row including SciBERT tell us? Is it about correlation of cosine scores to human judgements in the transcriptional-regulation corpus? 
In 3.2.2 (Network of publications), could the authors say more about the directionality of the graph? Is it not the case that edge weight e(A->B) should equal e(B->A) since the set of sentence connections is the same in both cases, and the cosine is symmetrical? Similarly, eq
(2) distinguishes between s_i->s_j and s_j->s_i, but I fail to see how these can be different.
If I wrongly understand STS to be the cosine similarity, please make that clear in the paper.
The coverage of the tool appears rather limited; how will the tool scale to larger collections? 
For computing semantic similarity over a large number of sentence(/document) combinations, Maximum Inner Product Search (MIPS) algorithms may be relevant
These are used to find the approximate top k documents, using running time and storage space that scale sub-linearly with the number of documents.
There is little evaluation in the current paper, both in terms of the appropriateness of the STS measure and the usability, and no related tools are discussed either
This paper contains a substantial related works section, which may be useful: https://arxiv.org/pdf/2008.07880.pdf.
Also, try to provide a more informative title for the paper, e.g
“exploring COVID-19 literature with semantic similarity networks”
Finally, the paper should be proofread (e.g
firsts → first, and many more), and checked for coherence, e.g
The study authors describe an NLP platform that aims to identify central knowledge items within a collection of publications by: 1) highlighting the central sentences in an article; 2) creating a semantic connection graphs of related articles 3) enabling browsing across semantically related statements an; 4) and enabling named entity searches
To achieve these tasks, the authors use Semantic Textual Similarity (STS) and Named-Entity Recognition and Disambiguation
While the concept is interesting, the authors do not demonstrate its accuracy or conduct any evaluations of the platform on COVID-19 literature, but rather perform an indirect validation of STS on the topic of transcriptional regulation
In the results section, the study authors primarily describe the features of the tool
From the information that was presented in the paper, it is difficult to determine the applicability or the effectiveness of the platform in aiding knowledge identification/discovery/exploration and summarization as it relates to a diverse and rapidly expanding evidence base
In building the application, the authors use exising NLP platforms, namely SciBERT and OGER, a biomedical/genetic entity recognizer
In order to better assess the publication, the authors would need to provide further examples of the tool's performance
This work is centered on automatic detection of tweets containing self-reported symptoms by COVID-19-positive patients
The problem is framed as a binary classification task
They use dataset from a previous study and train a BERT classifier to classify the tweets
Their focus on developing an early warning system is commendable.
Paper lacks detail on the technical contributions
Given the data imbalance issue, it would be better if additional metrics like Cohen’s Kappa are computed and ROC Curves are shared
Dataset section could have benefitted from more detail and the Experimental Dataset is on shaky grounds both from a timeline standpoint and confusion with flu symptoms standpoint
Given the broader overlap of COVID-19 symptoms with flu symptoms, authors should consider dedicating a subsection to this point and do further analysis on false positives and false negatives
Authors should also share the rationale behind picking BERT-large model and consider doing more baselines.
In this work, the authors present a text classification approach for the automatic detection of twitter posts containing self-reported COVID-19 Symptoms
The contribution is well framed and nicely presented alongside other similar approaches or related works
However, these first sections are a bit long for a short paper, taking space away from other more insightful section like having a proper dataset description, which is a bit anemic in this paper pointing out to a different paper, and only having a Table 1 to observe the massive class imbalance in the set (1 positive to 9 negatives)
Using a dataset limited to February, is a minor weakness as there are plenty of other larger and more comprehensive datasets available
One of the initial issues is why apply the models to data from Dec
A held-out set from further into the main pandemic months, or at least before the conception of the paper, would be a more realistic validation in terms of performance
The classifier built is well described with the results reported on Table 3
These numbers show that the classifier is performing unsurprisingly well on the most represented class, which was the negative one
The discussion for an early warning system is interesting and the dataset used might be ideal for this, however the performance seems to not be quite there, more tuning and playing around with the architecture might give improvements
This work is about building a classifier to detect tweets that contain self-reported symptoms by COVID-19-positive patients automatically
The authors built a BERT-based model that achieves reasonable performance.
- Mostly reasonable choices of data/model/analysis
Overall, the work is still preliminary: the dataset was a contribution from another paper from the authors; the model is straightforward BERT; the analysis isn't sufficient (see below for details).
However, it would be better to include justifications on how much gain there is compared to a smaller model (BERT-base), which is more feasible for detection tasks.
  - No baseline is included in the paper, making it hard to justify how well the model performs
I recommend the authors to add simple baselines, e.g., lexicon-based methods, for comparison.
It would be hard to deploy systems with this error rate (precision = 0.65, recall = 0.8) to the wild
The authors may also consider factoring the discourse and user demographics about the users into account to see whether there is a gain.
  - Given that the negative class is the majority, it seems less useful to include accuracy as an evaluation metric
The authors should also consider emphasizing the positive class instead of highlighting the good performance of the negative class.
The authors should discuss why the false positives/negatives happened with their classifier.
  - More details on model training should be provided
The authors investigate twitter user’s reactions and concerns about COVID-19 in the US and Canada through topic modeling and aspect-based sentiment analysis
They report the change in the prevalence of 20 specific topics over time as well as inference of sentiment based on 545 aspect terms and 60 domain specific opinion terms
From the information provided in the paper, it is not possible to determine if the model does actually preform as expected and if the proposed approach can be reliably used to inform public health decision-making
While, theoretically, information obtained from sentiment analysis can be helpful in steering public health interventions, there is also a significant risk that false information can be detrimental by misguiding action and depleting limited public health resources
For this reason, evaluation of models that are used by health authorities requires rigorous validation before it can be considered for use
This work conducts experiments with topic modelling (using LDA) and aspect-based sentiment analysis (using a weakly-supervised approach) on a corpus of 320,000 tweets related to COVID-19 from Canada and the United States posted between January and May 2020
The findings show that there are both similarities and differences between the most popular topics for the Canadian and United States tweets
An analysis of topic changes over time reveals that topic popularity is similar between tweets from both countries, and that the popularity changes match with public health activities related to the specific topics (e.g., social distancing and the number of tests and cases)
The sentiment analysis experiments show mixed sentiments for a few selected aspects
The experiments for aspect-based sentiment analysis could be described in more detail
For instance, it is not specified how many public health experts edited the suggested lexicons and what the exclusion criteria for certain keywords were
It would also be helpful to provide a larger sample of identified aspects
Furthermore, the authors compute sentiments for 545 aspect terms, but only show results (Fig
2) of a few selected aspects
Here, it would be interesting to see, for example, averaged results across aspects
It would also be interesting to analyze how the sentiment changed over time (similar to the topic modelling approach).
Overall, the proposed approach can be valuable for better understanding how people on social media react to the pandemic and corresponding governmental decisions
However, the analysis could be more fine-grained, and additional experiments would be beneficial to better demonstrate the potential of such an analysis.
Dataset and method are clearly described
Approach is not novel - similar experiments have been performed on similar datasets
Proposed connection to potential interventions is not demonstrated
Not obvious how human-in-the-loop approach benefited anlaysis
The authors perform a topic analysis and sentiment analysis of an open Twitter dataset on the topic of COVID-19
The authors identify differences in the preference for topics between Canadian and American Twitter users
Additionally, they identify differences between sentiment across topics, including negative sentiment towards the "Asians".
As many as 14 papers have performed topic analysis COVID-19 Twitter data as of early May [Ordun et al., 2020](https://arxiv.org/abs/2005.03082)
Overall, the paper does is not novel enough in its approach, data, or findings to justify inclusion.
**Reproducability:** Researchers could easily reproduce this paper
The authors describe the development of a task-informed document retrieval framework that leverages latent factors learned through topic models
The framework is applied to the CORD-19 corpus and compared to a naıve keyword-based document retrieval approach
The authors note that they are not able to quantitatively compare the keyword and topic-modeling based approaches on the basis of precision and recall in the absence of relevance labels
The key intent of the CORD-19 dataset was to facilitate automation of knowledge synthesis for specific questions
Given the exponentially increasing number of studies, automation of knowledge synthesis would require a model to, as precisely as possible, identify evidence that is most relevant and contains information which can be analyzed to provide answers to those questions
While the idea of topic modeling is interesting, the authors do not provide any indication that such an approach, as described, would be able to support any relevant evidence synthesis
Therefore, it is impossible to comment on the model and draw conclusions on its usefulness or advantages when compared to other information/document retrieval methods.
The paper presents a task-informed document retrieval framework for search over the CORD-19 dataset
The authors suggest that latent variables found in topic distribution of each task and document can contribute to the higher quality retrieval, specifically, they score each document by the JSD between its topic distribution and that of the task at hand
The approach is well described and sufficiently detailed throughout the paper, but lacks better motivation – why the authors think this method is likely to outperform the keyword-based search? Some concrete examples could be helpful
Otherwise, the paper is clear, coherent, and very well-written.
My main concerns about this work lie in two main aspects: (1) the lack of comparison to traditional methods in IR, e.g., vector space, probabilistic and language models, and (2) insufficient evaluation.
Re (1): Why using a somewhat simplistic keyword match model as the baseline while there exist established and successful approaches to IR? I found the proposed methodology creative and potentially novel and would be curios about how it compares to other IR techniques.
Re (2): Considering the two approaches presented in the paper, the evaluation (presented mainly in section 4.2.3) makes is difficult to draw any conclusions: figures 4-5 present highly expected results (“by the virtue of the ranking”); surprisingly little documents are in the overlap of the two approaches, which highlights the need in adequate evaluation; no actual comparison between the two approaches in conducted
The authors could use, e.g., the TREC-COVID labeled dataset for evaluation.
With an adequate evaluation (and proven benefits of the suggested approach) this could be a very good work.
Section 4.1.1 could be shortened or omitted; topic model coherence seems a bit off-topic.
Figure 2 could be clearer if presented in negative (i.e., light) colors.
Page 3: “pipeline pipeline” -> “pipeline”
Page 7: “include include” -> “include”
Page 4: why 35 was chosen for k?
The study proposed to use NMF to extract hidden topics from the COVID-19 Open Research Dataset  for 17 tasks, which was used for information retrieval for each task
 The method is quite standard and straight-forward
Strength of this paper is : i)it showed the effectiveness of NMF over keyword approach
 ii) the examples given in section 5 are inspiring.
However, there are some weaknesses: i) The keyword approach is rather a simple baseline, authors may consider other vector space models or language models for IR
ii) For evaluation, the paper did some effort in assessing topics, it would be better to evaluate the IR results further
e.g., do some extrinsic evaluation on down stream tasks and see if the retrieved results are really beneficial; iii) Not sure whether the two key word sets are good or not
Especially in the second set, it returned some noisy words (e.g
best) using tfidf for key word selection.
Some questions for authors: In algorithm 1, what is ψ exactly ? how did you do the dimension projection (reduction) ?
This paper anchors on interpretability of machine learning solutions in the context of hate speech detection around COVID-19 social media
This problem is extremely crucial to address, especially in the current times
They leverage the global feature importance from a model’s training dataset to reinforce or penalize new predictions when their local feature importance varies from the learned global values
Contributions section doesn’t clearly highlight why the proposed method is impactful
They should consider adding more qualitative samples to drive the point home on both the approach and interpretability aspects
The dataset collected in the context of COVID-19 (Section 3.2) deserve more analysis and isn’t described adequately
While leveraging “proven methods” is understandable, additional context would be helpful as to why other approaches were not considered or evaluated
Finally, I would suggest them to remove Figure 1
and consider providing multiple anonymized examples to illustrate the problem they are trying to solve.
In this work the authors present their approach for hate and toxic speech detection in the context of the COVID-19 pandemic
For a short paper, the authors frame their solution well and discuss some of the relevant approaches by other researchers
One of the major drawbacks here is that almost no mention of transformer solutions, which have been more popular and better performing in the last few years
While the focus is put more on the contribution of using SHAP (SHapley Additive exPlanations) to focus on the importance of the features in the general context
The biggest issue with this paper comes with the lack of clear and concise experiments (baseline evaluations on the non-covid datasets), and a proper way of evaluating their methodology on the COVID-19 set, which of course has no gold-standard at the time
This paper proposes a method for detecting hate speech that supposedly improves performance and explainability.
First, if there is a quantitative evaluation of whether their proposed method improves performance, it was not clear to me
For example, having a Table with performances of the baseline versus their method (with respect to some ground truth) could clearly show the improvement in performance
Or plotting an ROC curve could be appropriate here
The datasets that were used are not clearly or adequately described
What is the ground truth and how were they annotated? What is the relationship between the training sets (which do not appear to be COVID-related) and the test set? Is there a data distribution mismatch due to the different topics?
Additionally, it seems that the authors proposed method shifts examples towards the "none" class, but it is unclear how this correlates with better performance
But was the number of false positives the same? 
Authors should explain why they have chosen to use CNN instead of BERT as the text classifier.
This paper is built on their previous work on a parallel architecture to perform named entity recognition and normalisation simultaneously
They use a dictionary-based system in conjunction with two models based on BioBERT, whose outputs are combined as per the entity type
They also use a manually crafted dictionary optimized for the new COVID-19 concepts. 
Their willingness to share the code, outputs, BioBERT models and even the annotations is appreciable
However, the paper lacks novelty, rigor and doesn’t attempt to compare against other baselines
They should definitely try to add additional baselines in the event that the paper is accepted
This study aimed to improve the CORD-19 dataset and provide documentation on how the public can access their pipeline
This study understated the problem and the methods used to improve the CORD-19 dataset do not appear novel
 While this study provides an open source that may be useful to the public and scientists, its technical contributions were unclear
The authors describe a pipeline for Named Entity Recognition and Normalisation on the Covid-19 literature, with the results being distributed to the public via various means - PubAnnotation, EuroPMC, their own BRAT webservice, and downloads in four formats
The code to run the pipeline and the trained models are also made available
The system continuously responds to updates in the LitCovid dataset
Much of their work uses a system that has previously been evaluated in a shared task - however they also add a Covid-19 specific terminology, COVoc.
Reasons to accept: This paper presents a useful resource to community, in a way that is ready for use
Most of the methods used have been previously evaluated, so the quality is known
COVoc is also a potentially useful resource.
Reasons to reject: Lots of the paper is spent describing the technical details of the parts of the system that have already been published and evaluated - space that would be better used to describe the novel contributions
For example the whole description of COVoc is "Additionaly, we employ the manually curated, COVID-19 specific terminology COVoc, containing over 250 terms" and a URL for obtaining COVoc - substantially more on COVoc would be welcome
Much of the system _has_ been evaluated as part of a shared task, but no other teams contributed to that facet of the shared task, making it hard to know how the system compares with the state of the art
The authors state that the pipeline "with some effort could be modified using OGER's format conversion to process other dataset such as CORD-19" - CORD-19 is an important resource and having ready-to-go annotations for this would be very useful.
Conclusion: Despite my misgivings about the level of novelty in this paper, I think the public annotations are potentially of use to the community, and having a citable publication associated with the annotations would expedite use of these annotations, in an area where time is of the essence.
The authors apply an existing transformer encoder architecture to what may be a new QA dataset for COVID-19.
- the authors collect(?) a potentially useful QA dataset.
- I can't tell if the authors collected/aggregated new data beyond a wget or if they used an existing dataset
If they collected a new dataset, it needs much deeper documentation in terms of both quantitative measures (number of questions, breakdown into categories, length of questions and answers, distribution of languages), as well as qualitative measures (high level gloss of what the questions and answers look like, sources of information)
- The JHU-COVID-QA model is underdescribed
It's not clear which of Poly-encoder and JHU-COVID-QA is the intended baseline.
- why ignore the non-English languages?
- do you attempt to group together identical or similar questions?
- Task and model descriptions do not belong together
The formulation of a task and a solution to the task are independent.
And then trains adapts a polyencoder for a QA task
The given focus of the paper is definitely topical.
QA tasks are a problem of verified standing in Natural Language Processing and definitely something to be looked at.
The paper uses multiple evaluation metrics to evaluate the task they are undertaking.
Work done for the QA has been made available
Working of the PolyEncoder has been explained in detail.
For instance the answers were verified by professional at John Hopkins University or the Bloomberg School of Health
Were these answers purely medical or clinical? If not how was the verification done? What was used for fact checking?
For instance if a question is asked about what is the current situation in XYZ place, a response appears saying "COVID 19 news is always changing you can find more information about COVID19 followed by a generic URL).
For QA datasets what were the annotation metrics? Was it binary (Trusted vs Not Trusted) or on a Likert scale (5 points) or different? What were the labels? How was the correlation between features of the dataset built?
The paper talks about building a QA dataset, testing and evaluating the same using standard metrics available
However, details such as how the scoring of Q-Q pairs were done needs more light
"We scored each of the question-question pairs through the experts" seems very high level for a paper which includes building the dataset as a major contribution
Further description and analysis of the results are also needed.
Overall the paper has talked of a task which has potential and needs to be revised and presented in a better and more organized way
The sections need to be arranged properly with possibly elongated sections on annotation, and analysis of results
This paper presents a Question-Answer (QA) dataset for COVID-19 related questions and also a QA machine learning system using a poly-encoder approach
I would strongly recommend an editing service or at least the use of an automated system such as Grammarly in the future.
The ordering of the paper is challenging and it is unclear if the entire dataset construction, QA system and chatbot are within the scope of this paper, or the focus of this paper is only supposed to be the QA system
Then, earlier in the paper, it describes the poly-encoder architecture used for the QA system
And the linked website, and Github page (https://github.com/sseol11/Parlai_ver2/tree/master/parlai/tasks/covid19) show that this is part of a chatbot, which is not clear in the paper.
The paper presents results of the Polyencoder system (with ParlAI) using this JHU COVID-QA dataset and the results seem reasonable
- At first, I was confused whether references to COVID QA referred to the COVID QA dataset presented at the previous NLP COVID workshop (https://github.com/deepset-ai/COVID-QA)
It’d be preferable to discuss this dataset with some sort of comparison too.
- The paper also proposes another dataset (Q-Q) based on question similarity
What is the purpose of question similarity for this task? More details are needed for how the dataset is constructed, e.g
- Unfortunately the description of the Polyencoder architecture is particularly difficult to follow
- The example dialogue on the Github page (https://github.com/sseol11/Parlai_ver2/tree/master/parlai/tasks/covid19) includes a question about wearing a mask
- What does 20(ft) mean in the name of the model?
This paper proposes a methodology for curating a topic model from CORD-19 documents to improve downstream NLP tasks in this domain
Authors argue about the importance of curated vs fully automatic models and outline a set of requirements that a high-quality topic model should have, in order to be useful for end-users
The paper also provides some preliminary analysis on the current state of this endeavour, but details are still too superficial to evaluate if the quality of the curated topic model is indeed a significant factor for improving other downstream tasks like QA or IR.
I consider this paper could spark an interesting discussion in the workshop given the inter-disciplinary nature of the work, and this type of discussion is very necessary if the biomedical NLP research community pretends to produce results that are usable in practical scenarios
My primary concern is that the results are still very initial to evaluate the feasibility of the proposal or estimate the value that this resource could provide
The authors appear to have previous experience in this type of task, which is an argument in favour of their success
However, I would have appreciated more details that could help quantify the expected returns of this effort.
**Reasons to accept:** A very interesting and important discussion that can potentially benefit the research community and a very good exposition of the necessity of this resource and its requirements.
**Reason to reject:** Preliminary results are too superficial to allow evaluating the feasibility 
This paper presents a description of ongoing work involving curated topic modeling for improved exploration of COVID-19 literature
The idea is based on including humans in the loop, so that the initial topic model can be better fit to experts’ needs
The authors reflect on a list of desiderata that would make the use of topic models more acceptable to medical practitioners, such as the use of readable labels and structured search
Then, they discuss their ongoing efforts in this direction
I especially liked the discussion of the desiderata that would enable easier adoption by medical experts, and the description of the intended use where the topic labels would be used in a way similar to MeSH terms.
And, how are the topic labels assigned? More generally, since the document collections are ever growing, how do the authors plan to adapt the topic models when the new data comes in?
I would like to draw the attention of the authors to a related paper at this workshop, which also studies medical multi-word expressions in topic models: https://openreview.net/forum?id=c-TkXmZC-Yk (Improved Topic Representations of Medical Documents to Assist COVID-19 Literature Exploration)
The paper also contains other ideas about refining topic models for COVID-19.
When introducing the last desideratum, the authors mention that special purpose category systems are needed for COVID-19, but the link to topic models was initially opaque to me
It  became clear only at the end of the section.
The acronyms DSM and RDoC may be unfamiliar to the reader.
Overall: Although I would hesitate to accept the paper for its empirical contributions, I do think the paper could lead to interesting discussion at the workshop due to its contemplating nature about the inclusion of an NLP component into medical literature search.
This paper presents preliminary work towards a topic model for use by Covid-19 researchers, being developed with a human-in-the-loop process
It explains the needs of the subject matter experts (SMEs) who are to use the model, and how fully automatic topic models do not meet their needs
They present the work they have done so far in constructing automatic topic models as a starting point, and describe their plans for the involvement of humans.
Reasons to accept: Their analysis of the desiderata for topic models, and their anticipated uses, is extensive, highly compelling, in line with my own experiences of attempting to deploy topic models and similar systems, and likely to stimulate interesting discussions at the workshop
The initial NLP work looks reasonable, although the details there are vague
The final resource is likely to be of value to NLP workers and SMEs alike.
I get the impression that plans have not been finalised
The vaugeness is also a problem with their descriptions of the NLP work already conducted - the descriptions are a long way from providing reproducibility
Finally, the description of the specific challenges of working with Covid-19 research, rather than biomedical research in general, is minimal.
However, much depends on what the organisers aims for the workshop are; it would make for a stimulating presentation and lively discussion on the day, and if the organisers prioritise this above making a long-term contribution to the literature, then maybe it should be accepted.
The authors label a part of a covid-19 research literature dataset and train a ML pipeline for annotating covid-19 research
They apply the developed model, and with simple text analysis and qualitative interpretation they create an ontology of covid-19
- The authors use a large group of humans (21) to annotate data
- They follow a simple straightforward modeling process (shown in Fig.2) to label total corpus.
- The pipeline, although takes a small labeled dataset provides promising results (see Fig.4 covid-19 ontology)
- The paper is well-written, and provides in detail the scope of the pipeline, how they labeled the data, and how they performed model deployment and the evaluation of results.
I suspect this is due to the relatively small training-dataset (500 sentences).
- The ME, and ML models are CRF and a logistic regression
The authors did not try different model architectures and do not report the predictive ability of other baseline models (e.g
naive bayes) in order to appreciate CRFs and logistic regression's performance.
- Because of pipeline's low performance, they have to perform a qualitative analysis to faciliate COVID-19 knowledge discovery
Have you improved the pipeline? Did you label more data until now?
- Since data are available in Kaggle, and many researchers have analyzed this data, are there other models by researchers that performed the same task? If yes, please compare your pipeline with theirs, in order to show why is yours is better.
This paper is a study of a use-case on using an ontology of information extraction to create a knowledge graph around Covid-19 scientific literature.
It is hard to get information extraction "right"
Those frameworks that allow for high performance (F1) are often too simplistic, and those that are too fine-grained do not obtain a performance level which is useful
This is a nice example of a middle-ground: a rather fine-grained extraction of relationships, with detailed performance analysis
While it remains low, the analysis shows how to tune recall vs precision.
 * a good example on how to leverage existing research frameworks (in inf extraction in this case) for a new use-case
 * an annotated dataset which can be helpful for future research
QUESTION: will this dataset be released? (I found the *.ann files on your github: are those all the annotations?)
 * a complete study: annotations, algorithm, performance analysis, first results
 * The final outcome is obtained by gluing a number of parts together
While this is very pragmatic, it raises a number of questions (see below)
This is, if the end goal is to obtain a general graph, then a from-scratch approach might look different from what is proposed
Of course, a from-scratch approach is not what this workshop is looking for
The weight these days is to justify non-neural methods, and no ablation study of the features used is mentioned
The current pipeline approach consists in (i) extracting entities, (ii) extracting relations, (iii) constructing the knowledge graph
Many things could slip through the cracks between all those steps, but the magnitude of this is unknown, and despite the existence of joint extraction models they are not mentioned.
        * Is the annotation of relations done wrt the original gold annotations, or with respect to the result of the entity-extraction step?
	* “selected by the authors from the CORD-19 corpus”
What does this mean? Did you interact with the authors of that corpus?
	* What are the output classes of M_R? Are those all 10 relations of Table 2 + None? Following your pipeline approach, did you try with first predicting if there is a relation, and then which relation it is?
	* The BILOUV encoding is redundant I believe
Wouldn't you obtain better performance by predicting a simpler encoding?
	* Fig 3 shows that the threshold obtaining the best trade-off is different for each entity type
This might indicate that independent sequence prediction models could obtain better performance, in particular as a conflict in the extraction of entities (eg, one span annotated as Concept and also as Action) is not really an issue for your final goal of displaying a graph of knowledge.
	* How does the number of supporting documents enter in your knowledge graph? This is, how do you combine the confidence of a prediction of your model, with the number of times that prediction was made (eg, <lymphopenia, indicator, Covid-19>  predicted only once with high confidence vs predicted often with medium confidence)
	* Instead of Fig 3, a prec-recall curve (or better, a prec-recall-gain curve [1]) might be more indicative of the efficiency of your 
[1] Flach, Peter, and Meelis Kull
"Precision-recall-gain curves: PR analysis done right." Advances in neural information processing systems
This paper presents an approach to "knowledge discovery" on the Covid-19 literature, based on previous work in the more general biomedical domain
Its main contribution is a collection of annotated sentences, annotated by volunteers recruited via social media, for use as test and training data, complete with inter-annotator agreement scores
It also includes a baseline system trained on the annotated sentences, an analysis of the corpus, and the source code and data are available online.
The paper builds upon a body of work using the SAT+R (Subject-Action-Target plus Relations) formalism, a general purpose semantic model that has been applied to the biomedical domain, including its deployment as a standard in shared tasks.
Reasons to accept: This seems like a reasonably competent application of an existing research programme to Covid-19
It is always good to see datasets and evaluation resources
Their crowdsourced annotation is interesting - but the success may be dependent on extra goodwill being available in the time of Covid-19, so it's not clear how it will transfer.
Reasons to reject: The value of SAT+R annotations is unclear to me
The inter-annotator F scores they have achieved are not great (.64 for entities and .49 for relations, with some relations having agreement as low as .08) - this may reflect the intrinsic subjectivity of the task, or a lack of developement of guidelines, or may be a result of their crowdsourced annotation approach
The authors do state that "Previous attempts to apply this annotation model to medical text show that it is possible to obtain near-human-level accuracy with as little as 600 training examples", but even if F scores comparable to the inter-annotator level are possible, those scores are not especially high
Given that the application for these Subjects, Actions, Targets and Relations is to build graphs, the probability of errors in even a small graph becomes very high
It may be that these efforts are part of a long-term research program which will reach the appropriate levels of accuracy by incremental progress in the fullness of time - if so, I question its application to the Covid-19 domain, where applications ready for use by biomedical researchers etc
Conclusion: The work looks like a competent extension of an already-existing research programme to the Covid-19 domain, and the evaluation resources it has produced may be of use to the knowledge discovery subcommunity
This paper considers the hypothesis that the impact of COVID-19 in public mental health (specifically in individuals with mental health issues) can be measured by analyzing the users' behaviour in online forums related to mental health
Specifically, the authors focus on 3 mental-health-related forums in the Reddit social network, measuring the increase or decrease in both the user activities as well as in the relative prevalence of certain keywords of interest
Authors apply time series analysis to evidence a significant difference in user's activity post-COVID with respect to the behaviour predicted by autoregressive models
Furthermore, they analyze the relative change of word usage in certain categories, using LIWC clusters and an LDA topic model
The main findings indicate that these forums indeed exhibit abnormal behaviour, i.e., it does show an increased activity in these forums that coincides in time and textual content with the COVID-19 pandemic
However, two of the three forums analyzed actually exhibit a decrease in user activity, an issue which is noted by the authors but there is no attempt to explain it
I also consider that the techniques and analysis applied are an initial approximation, but they lack the sophistication expected in a top-level NLP venue
Specifically, the paper relies on descriptive statistics (word counting of LIWC clusters) and topic modelling techniques (LDA) which fail to account for complex linguistic phenomena that may be acting as confounding factors
Reaching this conclusion would require a deeper semantic analysis of the text.
As such, I believe the paper establishes a clear and important hypothesis, provides initial evidence for it, and it is clearly written, but the contributions are not sufficient to demonstrate an actual increase in mental health issues in users of these platforms as opposed to an increase in the general discussion of topics related to the COVID-19 pandemic
**Reasons to accept:** The paper is clearly written, the methodology is thoroughly explained and carried out and the problem under discussion is relevant to the workshop.
This paper describes the analysis of messages published on Reddit under a set of specific subforums related with mental health topics
The authors have performed several analysis to understand the levels of preoccupation of the population on these forums with respect to the pandemia and how the people that use these forums rise
On the other hand a second analysis about the language and topics of discussion has been performed
The topic addressed is interesting and the technical background is solid
My main concern is refered to several points where some conclusions were arisen by the authors with respect the implications of the results obtained
Paper has room for improvement, but makes clear additions to the field.
Use of LIWC to supplement topic-modeling was a good decision
Traditional **Discussion** section is intermixed with the section 5: **Findings**
An embedded topic model (Dieng, Ruiz, and Blei 2019) offers benefits over LDA 
Discussion of LIWC and depression/anxiety prediction for NLP is not particularly thorough
The authors examine how the content of three subreddits related to mental-health help seeking changes as a result of the COVID-19 pandemic
To do so, they combine established methods for topic analysis (LDA) and psychological assessment of text (LIWC) with a time-series prediction approach (Prophet)
Their analysis contributes to the field and their approach is satisfactorily documented and executed; however, the authors could improve their work with better organization and use of more current methods.
Useful to show that NLP can be used to the same end
The authors longitudinal analysis of established NLP techniques, LIWC and LDA, usefully allows them to identify topic-changes across the various subreddits.
The paper could be made more accessible through a more typical distinction between analysis and discussion
The authors could also update their methodological choices, e.g., using [Embedding Topic Modeling](https://arxiv.org/pdf/1907.04907.pdf)(Dieng, Ruiz, and Blei 2019) in place of LDA
fastText embeddings would be a natural choice for this, as the authors have already cited Wolohan, 2020.
Lastly, the authors could also discuss more thoroughly the history of LIWC and word-count approaches in NLP for depression / mental health detection
(2004) is cited but only as a possible explanation for a finding
That paper is a significant forebearer to this work.
This paper is highly reproducible, though associated code or Reddit post-IDs would help.
The study converted the covid19 suggestion seek/give problem into a social media text classification task, the data is from Reddit and the experiments showed some average weekly emotional support sought/given changes
I found the topic is interesting, but the presentation needs to be improved
it would be better if the authors can give more statistics of the collected dateset, show the  agreement level of the annotators, given some reasons of the feature selection,  illustrate basic parameter settings for the model Random Forest (e.g
how many trees and splits are there), and give the model performance level
In general , I think the work can still be improved
This paper addresses the topic of COVID-19 online support forums with the purpose of automatically classifying posts that either seek or receive emotional or informational support
While the topic of the paper is of interest for this workshop's audience, the paper could be improved both in terms of presentation of the methodology and results (as well as their discussion).
Section 3.1 should be discussed in more detail
Which LIWC categories were considered reevant to emotional and informational support? How were the posts involving request or advice identified? What were the 20 topics generated using LDA? The results in Table 2 deserve a more thorough discussion
Interesting paper and with addressing the comments below this could be a candidate for this workshop.
- line 100: provide full details (of how the support types are distributed across your sample) in addition to the two examples
- related: consider adding examples for each type being low/high.
- line 155: great to see proper agreement evaluations!
SVM, simple linear regr., LASSO, etc.) - also: please add detail on the model used (did you treat the Likert scores as "classes" or predict continuous values?)
- line 200: the x-axis of the plot could do with refinement (e.g
- line 227: the problem (predicting Likert-scale scores) can be formulated as a regression problem, so in addition to the r reported, I'd like to see regression performance metrics (at least: R-squared, RMSE and MAE)
Please add a discussion of the prediction analysis and provide more rigorous testing of the plots.
is it driven by specific posts /outliers; what is a possible explanation for that divergence).
Lastly: the interpretation of the RF findingfs (Table 2) deserve some review: r=0.40 means that the model merely explains 16% of the variance in the outcome variable (here: degree of support) - since R-squared = 0.16
This is not a lot so there will be many other factors aside from features used - I'd like to see this discussed in the paper.
The paper describes a COVID-19 QA system
It uses BM25 for the IR component and BERT models for the QA component
It provides both quantitative evaluations on the COVID-QA dataset and in-depth manual error analysis
It would be more robust to provide the performance of baseline systems and other systems on the COVID-QA dataset
This will make it clear on whether the proposed system is indeed effective.
The manual analysis part is interesting
More details are expected: any suggested changes on the evaluation metrics? What are the recommendations from the study? This is critical for QA systems in production
In this work the authors introduce their QA system that won one of the tasks in the round one of the Kaggle CORD-19 Challenge, and also make the case of the suitability of automatic metrics
With an ambitious scope, the authors very effectively frame the problem and their QA system in contrast to others in the same competition
seems a loose and needs more refinement, as for example Table 1 just cherry-picks some other systems IR module to compare against (the top 5? what number was each? it just picks 3 out of 50 randomly)
The evaluation of the QA system in section 4.2 is quite interesting and while brief, it shows the results are unintuitive and lead to the interesting idea of the suitability of automatic metrics after they performed a manual review
The paper presents a question answering system that won one of the tasks in the round one of the Kaggle CORD-19 Challenge
The authors describe the two system components – IR (retrieving the most relevant paragraph given a question in natural language) and QA (extracting the answer from the paragraph)
The paper further analyses a surprisingly low performance introduced by the system when fine-tuned on SQuAD and QuAC on an annotated dataset, revealing that the inherent preference of short answers by evaluation systems causes the inspected drop in accuracy.
The main contribution of this work is summarized in section 4.2 – the analysis of automatic evaluation of QA systems w.r.t
long answers, reflecting essentially the same information
My main concern is that the conclusions are drawn based on 50 or so annotations done by a single annotator, who may well have preferences re short vs
Which leads me to my second concern – the two examples in figure 1 (“how many” and “when” questions) call for a short fact-based answer IMO, or, at least should rank a short and precise fact providing an exact answer to a query not inferior to a long paragraph including the answer plus some bits of potentially unnecessary information
Can the entire drop in EM and F1 in the last row in table 2 be attributed to the analysis in section 4.2? Are there any other potential factors that could affect the QA accuracy?
I appreciate the authors’ analysis in figure 2, interesting!
All in all, I think that’s a good work
The fact that the proposed system (when fine-tuned on SquAD only, third row in table 2) outperforms ROBERTA-based solutions highlights the contribution of this work, which is sufficient to recommend for acceptance IMO, despite the two issues above.
The paper presents a QA engine that takes COVID-19 related questions and outputs a span-based answer in a piece of text belonging to the relevant article
When a set of articles relevant to the question is obtained, a general-domain BERT QA module outputs the span of text corresponding to the answer
The pipeline separating retrieval and answer selection in general makes a lot of sense, and is clearly presented in the paper
Also, the authors consider a variety of COVID-19-related questions (from two different sources), and document them well in the appendix.
I discuss those in the order as encountered while reading the paper.
I would be curious to see some statistics about how often the model selects the answer from a chunk (QA step) that was not previously selected (retrieval step) as the maximum scoring one.
The similarity calculation gives preference to locally maximally similar snippets of text with respect to the query
As an alternative, instead of just looking at the snippet maximising the cosine similarity, looking at the distribution of similarities could work better, e.g
when there are multiple snippets answering the query.
The authors mention that they carried out an empirical evaluation to single out DistilBERT as the best architecture, but do not mention how this evaluation (albeit “only” qualitative) was carried out
The same criticism applies to selecting the QA model
Because of this, there is little to learn from the paper---the comparison of model components was done only at face value.
Similarly, the answer evaluation has methodological issues, e.g
Some examples could be provided for each answer score.
But I’m wondering—improvement over what, and how was it measured? If the proposed tool can provide at least one precise answer ~60% of time, is this good or bad, and why? Further, more analysis could be done to show from which chunks (location in the text) the answers come from.
Finally, there is a lot of work involving QA for COVID-19, but the paper fails to address any of this work
See this as a starting point: https://openreview.net/pdf?id=0gLzHrE_t3z.
This paper describes a QA tool for COVID-19 built on top of two components, a Sentence-BERT model for filtering papers by relevance, and a BERT-based QA model tuned on SQuAD
The authors test their model on two sets of questions, one created by the authors, and another set contributed by a regional healthcare facility, asking for human evaluators to score model results on a 4-point scale
The list of questions provided in Tables 1-4 are a good contribution, though the paper is lacking in implementation and experimental details.
The paper lacks experimental results, though the authors repeatedly refer to empirical comparisons
For the filtering step, the authors use Sentence-BERT on chunked papers, but do not compare to other baseline retrieval models, such as Anserini, CovidBERT, BioBERT, SciBERT etc, all of which the authors mention
The authors say Sentence-BERT was empirically better, but there are no results to support this
How much better? Similarly for QA, there are no numerical results showing that one model is better than another, just a statement that comparisons were performed
All modeling decisions therefore seem incredibly arbitrary.
There is also no way to understand how well the model performed
The authors report evaluation results, but are these numbers reasonable? Are the methods reported here comparable to what would be expected in a general domain setting for example? Or the COVID-19 domain? There are datasets on COVID QA (Tang et al 2020) that could be evaluated on to give a sense of the performance of this model.
This paper builds a question-answering model which answers questions from the recently available CORD-19 dataset
While the work promised potential, major steps are missing in both experimentation and writing
In the pre-processing step of the paper the authors mention they use a list of keywords identifying the COVID-19 virus
However, this list of keywords needs to be released or revealed to facilitate repeatability and better understanding of the pre-processing step.
Similarly - the authors mention using k number of tokens less then 128 owing to the maximum capacity of the Sentence-BERT network
However, there is confusion in the above step
The authors mention that they choose a k-value slightly less than 128, however, in the same paragraph they also mention that they choose a k-value sufficiently smaller than 128
For repeatability purposes and for better understanding, revealing this number is required.
The literature review section of the paper lacks specific numbers
Since the primary contribution of the paper is an artificially intelligent question answering model which works on existing datasets, it is important to know exactly where this model stands in regards to a performance benchmark against current networks
It has been mentioned by the authors that owing to a lack of time and resources an empirical evaluation has been provided instead of a quantitative one, it does not show how the model in the paper stands up to existing work.
Section 2.3 mentions RECORD (the model proposed) extracts additional metadata such as title, authors, and so on to highlight reliability
Is this so that the person asking the questions knows that the answers are from a published source? Shouldn't that information already be clear owing to the dataset the model was trained upon? If that is not the case, then how does the additional data provide reliability? This needs to be specified in detail.
In Section 4.1 - the authors mention that humans were asked to annotate a score for each provided answer
How were the annotators selected? Were they annotators familiar with the model or the dataset or current literature and a ground truth? Was there any inter-annotator agreement between scores on a particular answer? If so, what was that score? Or did one annotator score only one answer?
While the questions are detailed, it is suggested that the authors provide some of the sample answers from each of the classes
Maybe a table showing a few questions with answers spanning scores 0-3 would be a very useful addition to this work.
All the questions and all tasks are very well organized and detailed in appendix A.
The bar charts and graphs provided are clear to understand and shows the claimed performance benchmark.
The work is topical and promising
There is a lot of scope to improve and contribute to future venues.
In conclusion, although the work described in the paper promises to be full of potential, the paper lacks a lot of crucial details
The paper has very well presented supplementary material
The performance of 61.3% needs to be evaluated against other pre-existing benchmarks
The paper introduces a large-scale dataset with posts related to COVID-19 collected from Weibo, a social media in China
It describes in detail the process to collect the dataset and provides some interesting analysis of the dataset
* The dataset  is large and contains rich information such as location and post time
This could be helpful for further COVID-related researches
* This paper provides interesting analysis about the dataset such as the daily distribution of COVID-related posts and word clouds at different phases
* There are no deduplication steps to deduplicate tweets in the dataset
The duplicated tweets could bias the statistics of the dataset and further studies
* While the main contribution of this paper is a large-scale dataset, there is no direct evaluation of its quality
It is unknown how many tweets in this dataset are directly related to COVID-19
person and location) could potentially pick up non-COVID-related posts (e.g
 Another example is that a portion of words from Fig.4c and Fig.4d are not directly related to COVID-19, such as "汇算 清缴"(settlement and payment), "梵高 画作"(paintings of Vincent Van Goph) and ”后街男孩“(Backstreet Boys)
 The precision of this keyword-based approach to collect COVID-related tweets remains unknown, which makes the dataset less useful to the community
* Some Chinese keywords do not correctly match their translation in English in the appendix table 
 For example, "美国 AND 例" should be translated to "USA AND cases " instead of "USA COV-19 AND cases"
- Why was the data collected only until the end of April? The authors claim that the pool of active users is dynamically maintained and if so it would be better to provide both the users and the readers with a more recent version of the dataset
- What does crawling time mean in this context? Please either specify in Table 1 or add the definition to the main text
It would be nice to correct those to improve the readability of the paper
This paper describes a dataset for COVID-19 built from Weibo, a Chinese social media platform
The authors first maintain a Weibo active user pool -- around 20 million active users, and then collect their tweets using a list of specified keywords relating to COVID-19 (179 in total).
- A large scale dataset, containing tweet content, interactive, location and retweet information
Not only text-based but also social network-based analysis can be potentially conducted on the dataset.
- The authors describe in detail how they build the dataset
It may be worth analysing those users as well.
For example, is the result filtered using keyword 'Doctor AND Li Wenliang' a strict subset of result using keyword 'Li Wenliang'?
- The title of (Chen et al, 2020) paper has been updated to 'Tracking Social Media Discourse About the COVID-19 Pandemic: Development of a Public Coronavirus Twitter Data Set'
The authors develop a large dateset of 86 misconceptions along with 5000 annotated tweet-misconception pairs from COVID-19 related tweets that were generated in March and April of 2020
The researchers then evaluate the performance of the existing NLP systems on the dataset.
While the authors provide a good framework for developing and testing misinformation in social media, given the rapidly evolving/changing evidence base related to COVID-19, strong consideration should further be given to developing more automated systems for classifying social media content (e.g
In the meantime, COVID-Lies provides a great starting point for the development of robust misinformation detection systems and their evaluation
The authors generate a dataset containing 5000 related to 86 covid-19 misconceptions
They also deploy a two-step process for classifying tweets
At the first step, they deploy a semantic model that tries to define the similarity of a tweet to one of the 86 misconceptions
They use different models and techniques coming from NLI to investigate the abilities of models in misinformatio detection
* Authors generate an new benchmark dataset for covid-19 misinformation detection.
* They create a smart two step process for detecting misinformation
* They investigate if general NLI archtiectures that are not domain adapted can help in misinformation detection
* They use different architectures and evaluate their accuracy
* They definitely provide new knowledge and interesting insights on how misinformation can be detected.
* Models accuracy is generally low
That could be due to the limited amount of observation (5000) and the high amount of potential classes (86 * 3)
One thing that I would additionally want to see is comparing your results with these of a standard fine-tuned language model classifier
I would assume yours would be better, but such a comparison would provide clear evidence why shemantic association and stance detection is a better process to follow.
This paper purposed a method for COVID-19 related misinformation detection
The authors contribute a datasets with 5K annotated tweets on 86 COVID related misinformation
They evaluated many approaches for misinformation detection and stance detection, and the domain adapted BERTscore  and SBERT achieved the best performance in these two tasks
The paper is written in a clearly and concise way, and well motivated.
Some further work is suggested for the authors to consider
The Number of English tweets used in this study and the duration of data collection needs to be mentioned
It is mentioned as 424 million, where only 67% is English
Its not stated how many of the tweets went through labelling process for COVID drugs
In the phrase “ For each term, each alphabet…” its more clear to use character/letter instead of alphabet.
Explanation of the third methodology seems to assume prior knowledge of the technology used
Zinc is stated as being irrelevant whereas it is talked about as enhancing the clinical efficacy of chloroquine.
Table 1 is unclear as what the “total annotated terms” represents? Is it the number of tweets?  And what is delta in the last row showing? 
In Table 2 the total frequency is around 472,000 so where did the rest of 723,129 tweets with mentions of drug go? It also would be helpful to show the total number of misspelled drug names frequency.
In table 5, the calculation of percentage increase column needs clarification
The total of 222,428 is calculated as 15% of what number?
The paper points out that spelling correction is an important step in preprocessing covid-19-related tweets and enables identification of additional data
The authors explore several approaches to correcting spelling mistakes in drug names using a corpus of covid-19 related tweets
The results suggest that several spelling correction techniques should be applied together to comprehensively address the problem.
The paper does not provide a characterisation of potential treatments but rather lists the most commonly mentioned drugs
Moreover, both the title and the abstract put emphasis on using machine learning, however, only 1 method out 4 is machine learning-based.
Overall, the format of this study seems more appropriate for a short paper
For a long paper, I would expect the authors to further analyse why the implemented algorithms give different results, what potential errors might be introduced as well as show some validation of the approach.
- When describing the second method used to generate misspellings the authors state “For each term, each alphabet is replaced with the closest alphabet on the QWERTY keyboard.” It is not clear what the authors mean by an “alphabet”, I am assuming it means a “letter”? How the closest letter on the keyboard is defined? Do variations include digits? Where is the recursion in this approach? I suggest the authors either cite a related research paper or provide a more detailed explanation.
It is unclear whether these numbers were calculated after applying spelling correction techniques or not.
- I suggest moving the clarification for the data reported in Table 3 from the bottom of p.3 to when the table is first mentioned.
- It would be great to see more analysis of why the overlap between the three methods is only 15%, ideally, with specific examples.
- Table 5 provides the numbers of additional terms identified using the described methods
- The abbreviation for the Social Media Mining Toolkit should be introduced in the main text
- It would be helpful if the authors could clarify whether the total number of annotated terms reported in Table 1 includes only drug-related terms or not? 
In this paper the authors mine the COVID-19 chatter tweets previously released to identify the usage of COVID-19 drugs related vocabulary
The number of experiments are adequate, the results are clearly presented and the discussion of each of the steps of the methodology makes it easy to follow
The same idea is further reinforced in the conclusion, which refers to potential drug treatments available for COVID-19 patients
However, the experiments led by the authors only take into consideration drug vocabulary frequencies, so I fail to see how are these indicative for potential drug treatments
In this work the author presents a novel BERT architecture with the goal of provide a summarization of lengthy papers, applied to the CORD-19 dataset
The proposed model description is intuitive and seems reasonable, however, figure 1 is a bit difficult to read as it compressed for space constraints
The dataset description is not clear at all, only discussing about the abstracts used from the CORD dataset, but then table 1 mentions 3 other datasets
On the results section the author only presents results on one dataset, with the proposed model not being the best performant
While the ideas in this paper of the novel model are interesting, the work seems rushed and not at the publication stage yet
The author combines two bert models and adds a transformer on top to perform extractive summarization
The model architecture is optimized to avoid catastrophic forgetting
* The author creates a hybrid architecture based on a state of the art language model.
* The example shows that the model satisfactorily generates extractive summarizations.
*  The author does not provide evidence why their model is either better, or faster, or more computationally cheap than other alternatives
It shows that the proposed technique is slightly worse than BertSum.
I  cannot assess if the paper provides a methodological contribution, given my limited level of expertise in language models.
The authors present Continual BERT, a model designed for being able to continuously update and summarize COVID literature
It is trained using CORD-19 to predict abstracts from document text.
- the idea of online learning is interesting and has interesting properties
- The exposition of model is inconsistent in its detail - going into depth about relatively trivial modifications to a recurrent layer but glossing over important details in overall model structure
Section 3.1 should be revisited and expanded in depth
Additionally, the model figure 1 is hard to decipher.
- in terms of ROUGE scores, the continual BERT lags significantly behind standard methods (BERT Sum)
- predicting abstracts is perhaps a poor choice for extractive models
An upper bound for ROUGE scores from extractive methods would be informative.
This paper describes the development of a visualization tool that provides an overview of domain-specific named entities extracted from a subset of the COVID-19 Open Research Dataset (CORD-19)
A pre-trained NER model from the ScispaCy library was used to extract named entities of CORD-19 articles, which are then visualized by the tool
In general, the idea of providing a visualization tool for efficiently navigating CORD-19 is interesting
via weblink), making an extensive evaluation more difficult
Furthermore, the description of the proposed tool is vague.
Overall, I think the paper needs to be clearer in describing the tool and its functions, and an online access to the tool would be very helpful for a proper review.
In many occurrences, text is not whitespace-separated from the reference (e.g
In line 120, the term “dataset” is spelled as “data set”.
In line 151, the sentence is incomplete.
None of the figures (1-8) shown in the paper are mentioned in the text, and a short explanation would be helpful
Furthermore, figures 2-8 are very hard to read in the current resolution
Also, what do the colors in the plots (blue, yellow, red) refer to?
This study provides an online visualization tool for discovering CORD-19 dataset
It applies scispaCy to annotate the concepts mentioned in the papers and then applies scattertext for visualization
My major concern is none of these steps was evaluated
The pre-trained models in scispacy were trained using the biomedical datasets, but may not capture COVID-19 specific entities effectively
Are there any evaluations on the precision and recall of the models specifically for COVID-19 related entities? Similarly, the identified relations were not evaluated either
Given the significant amount of NER time, how frequent is the tool updated? Are APIs available?
Also, the provided functions are limited
For instance, CORD-19 has papers that are not on COVID-19 (e.g., MERS)
Visualizing the entities and relations on the whole dataset may not accurately represent the characteristics for COVID-19
Providing more filtering functions would be helpful.
For instance, why providing 6 separate figures just showing different percentages of entities? A figure with sub-figures on selected percentages would be sufficient and the remaining figures could be used for demonstrating other functions
The author does not discuss its use case (beyond mentioning "the study of patterns") nor provides the perspective from a medical professional: How could a medical doctor reuse these "patterns" provided with the Scattertext library? How a healthcare professional would know which "patterns" to explore and look for with more detail in the literature?
Some sentences or paragraphs could be rewritten and condensed for sake of clarity.
-> What do authors mean by "patterns"? Co-occurrence of mentions of diseases and drug entities (which might reveal a therapeutic relation)?
named COVID19" -> This is not correct; please, change to "SARS-CoV-2"
The coronavirus was named "SARS-CoV-2", whereas the disease was named "COVID-19"
-The paragraphs starting in line 42 through l
64 seem to revolve around the same idea; it seems more adequate to rewrite them into one more synthetic paragraph.
81: The reference to the Scattertext library (Kessler 2017), and the URL link to it, should be provided the first time it is mentioned, not in line 107.
Moreover, the updated bibliographic reference is:
    ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing
    Mark Neumann, Daniel King, Iz Beltagy, Waleed Ammar
of the 18th BioNLP Workshop and Shared Task, 2019, August 1st, Florence, Italy, pp
-Figures 2-8 are too small to read the entities shown
I think this is an issue related to the Latex code.
-"Dataset" is repeated in "COVID-19 Open Research Dataset (CORD-19) Dataset"
-Abstract: "easy to use" -> "easy-to-use"
-I think the CORD-19 dataset should be cited in the first mention (l
118: "For this analysis, The custom license" -> the (lowercase)
137: "Scispacy Named Entity Recognition Model" -> "The sciSpacy Named Entity Recognition Model"
For instance, the paragraph starting at l
This could be fragmented in shorter sentences for the sake of clarity.
151: "...100 tokens were ." -> something is missing.
This paper presents an analysis on tweets posted in Persian/Farsi about COVID-19, including
 -- topic analysis using Latent Dirichlet Allocation
Don't feel Appendix B.1 fits the paper.
 -- The writing can be improved, especially when describing methods
Add citation and explain in details methods.
For example, the explanation in Section 3.1.1 and 3.2 are lack of detail and not convincing.
- Section 2.2.1: When giving examples about these topics, it would be better to refer to Table 4, explaining why it is the case
- Table 6, suggest to add English translations
- It seems a paper that should have broder group of readers (ordinary people) rather than only NLP researchers.
this study is organized as following), missing punctation and long sentences
The structure of the paper is in reverse of usual order in this discipline
The section talks about tweets related to COVID-19 and tells us to read section 4 to get an insight into the process
If we read section 4 we find that we must refer to Figure 7 to see what search terms were used – and we find that none of the English translations use the term “COVID” – rather they mostly all use variations of “corona”
Do Iranians not use the COVID-19 terminology? This needs an explanation.
Figure 2 seems to be redundant as the trend is already in Figure 1 and because of the lack of importance to Nowruz for the study – could the annotation of March 13 be added to Figure 1? 
Section 2.3, Figure 5 presents a clear tweet distribution over some topics but the discussion before the figure does not clearly explain where the distribution comes from
The discussion seems to imply that the numbers in Fig 5 are based on manual labelling
Or, if not, then how were these 6 distinct categories derived? We must detour to the suggested detail in the later section 3.2 to understand these are related to a k-means clustering content analysis process
This is also confusing because this should be explained after the annotation section.
The top 25 topics out of 50 topics are listed and some are analysed here
Looking at the list of topics they seem to be very overlapping
It seems that the number of 50 for topics is too high
For instance, Basij is mentioned as one of the top topics but it is not obvious which topic in Figure 4 it is related to
Should we have seen more of the top words to understand which of the pro or anti regime topics contains this reference? We do not need to keep seeing the word “corona” in every topic – common words would normally be eliminated during topic modelling but can at least be removed from the table so we can see more of the distinctive words.
Looking at the top 25 topics, we could merge some
Section3.2 Both k-means clustering, and topic modelling seem to be used for the same analysis
I would suggest to stick to just k-means clustering and analyse the taxonomy of Persian corona-related tweets
The k-means clustering is done on 8 categories but annotation is done with 6 labels
Give the number of annotated tweets
This is a report on the conclusions from a data analysis performed on 530k tweets over a period of one month in Persian language
 - as the authors said, it can indeed become a blueprint for how to perform similar analysis in different countries
I would encourage the authors to trim the paper down (eg: focus either on the LDA or kmeans, remove Appendix B)
 - the paper would be easier if read in reverse: data-collection comes at the end and lots of material is repeated (first the conclusion (Sect 2) and then again when explained the process (Sect 3) )
While this might be standard in other disciplines, it is not the case in NLP
I think that by working on the structure the paper could be made clearer and to the point.
 - is Fig 2 just the zoom of the red curve of Fig 1? If so, it could be removed
Was it used to obtain the categories of 3.2.1?
 - you detail the features for LDA , but not for kmeans
 - I think your extrapolation of classes from a random sample of 30 of each category has issues
Depending on the size of the cluster the error might be high (increased by your IAA)
Why not train a classifier with the annotations, maybe adding some weakly-supervision? This would also strengthen the technical aspects of the paper.
As a conclusion, I propose to reduce the paper lengths substantially by focusing on the main contribution (which for me is the annotated dataset and the clustering), remove duplicate description (which could be easily done by reverting the current order: Sect 4 first, then Sect 3 and finally Sect 2 with the plots) and deciding what to do with the LDA analysis (the appendix might be a good place if it was only used to obtain the categories)
This paper used few shot learning to build a semi-supervised model on unlabeled COVID-19 tweets
A pre-trained Influenza model was used to facilitate the classification tweets into related to COVID, Self/Others infection, Infection and Vaccine
 The paper is well motivated and clearly written
The benefit of semi-supervised learning is to use a small amount of labeled data to classify a large amount of unlabeled data
The comparison between COVID and COVID* using the pre-trained Influenza model in Table 3 and 4 can not be justified based on such small test datasets
There is no baseline method in the paper
Many other semi-supervised methods such as Graph Convolutional Network (GCN) achieved good performance on small training sets
The purposed method needs to be compared with baselines including some traditional machine learning methods.
Some information needs to be clarified:
whether CNN used in Influenza classification is same as the autoencoder used for unlabeled COVID tweets
Please refer to Section 2.3 “Once a second Autoencoder on influenza data has been trained”
Please provide the number of unlabeled COVID-19 tweets trained in autoencoder and labeled Influenza tweets used in classification
The paper proposes an approach to overcoming the lack of annotated data from Twitter related to covid-19
The authors take advantage of the similarities between covid-19 and common flu and employ transfer learning to detect tweets related to covid-19 and classify them into several categories
The technical part of the paper is very well written and the intuition behind various transformations is explained in a clear and simple way
I also appreciated the comments on the extension and application of the approach for future monitoring once a vaccine becomes available.
- The major point of confusion for me is whether the authors trained one model to perform multi-class classification (as claimed in line 141) or four binary models to tackle each problem separately (as it appears in sections 3.1-3.4)
On a related note, it is unclear if the classes considered in the paper are mutually exclusive or if each tweet has multiple labels? A figure/table clarifying this for both coronavirus data and influenza data would be very helpful.
I suggest specifying the categories here one more time for clarity.
- Line 239: please provide the size of the final annotated dataset used for few-shot learning.
- Just a suggestion to reiterate in line 339 that the latent representation is denoted by z.
- Section 2.3: the authors described the process of training 2 encoders, one on Coronavirus data and one on Influenza data
It is unclear which encoder the authors are referring to in line 391
Moreover, figure 1 creates the impression that W_i denotes the weights of the pre-trained MLP model
- Please confirm that the reported in Table 4 precision, recall, and f1 score values are calculated using the test set.
- There are a few typos in the text, for example in lines 272, 343, 433, 573
While it doesn’t affect the readability it would be nice to correct those.
This paper proposes a learning algorithm which is semi supervised and can monitor twitter during COVID19
They also evaluate the use of twitter data for surveillance and improve accuracy of their model using transfer learning
The paper shows that high accuracy may be obtained from limited labelled data using few shot learning and can potentially overcome the need to have large datasets which are labelled
Figures and tables are very self explanatory in nature with good notes o them
Processes of dataset building and data annotation has been clearly explained in detail.
The process of classifying data into its labels such as awareness vs infection or self vs other and related vs non related has been explained well and has been analyzed with proper statistical metrics and tests.
The work is very relevant to the current situation and on going research
A minor comment, but may be a bit more light on to why the specific number of data-points were chosen for classification would be better.
This study aimed to improve the CORD-19 dataset by using the tools in SeerSuite to mine data from PDFs
Additionally, this study created a search engine, COVIDSeer3, to ease the exploration of this data
This study clearly stated the problem and the methods used to improve the CORD-19 dataset were clearly described and free from major flaws
This work reports some experiments of thematic clustering about the COVID-19 pandemics
The authors leveraged data from Bruno Latour's questionnaire, which brought up in public debate some questions about activities to stop or develop after the pandemics
The task consisted in cluster these segments into coherent topics
The experiment reused several sentence embedding models (BERT, Universal Sentence Encoder and Smooth Inverse Frequency embeddings), concatenate them and reduce dimensionality, then use OPTICS clustering (Ankerst et al
The more original contribution is using a logistic regression classifier to re-assign segments from "noisy clusters" to "unambiguous clusters"
Segments assigned by the OPTICS method to unambiguous clusters are used to train the model (the authors validated manually the output from the clustering )
Through manual validation of the predicted probabilities (of the "noisy" segments to belong to an unambiguous cluster), authors report promising results of topic assignment
The authors also tested their methods on data from the SemEval task.
Especially, because it seems to work with a small corpus, which is welcomed (given that many tasks lack annotated data) and could work across domains (authors could highligh this fact)
-An original semisupervised method for resolving clustering ambiguities
-The method is suitable for small corpora and seems to be domain-independent.
5) presents some methods and also the results
8) provides further details about the comparison with Wang & Kuo (2020), which could be moved to sections reporting the evaluation methods or the results
The results of evaluating with SemEval should be provided in the Results section.
-Authors state that they "release the anonymized dataset as a publicly available resource" (l
103); however, no link to any repository is provided.
The statement that the authors' approach is lighter and "runs on [a] modest software configuration" (l
BERT and other neural models are heavy to load (even though authors do not train BERT or fine-tune on their data)
-Table 6: Please, explain that "Stop" and "Dev" refer to "Stop answers" and "Development answers" in the questionnaire, respectively
"Dev" can be mistaken with the "Development" set to validate the machine-learning classifier.
The way it is presented, "p" can be interpreted as "p-value"
-Figures 2-5: the blue colors corresponding to "p=1" and "0.75<p<1" do not distinguish very well each bar section
I suggest authors use different patterns (vertical or horizontal lines...) for each section in the bars.
16: "respondents answers" -> "respondents' answers"
Other contexts to fix throughout the document (e.g
221: "does not give satisfying" -> satisfactory
-Fix grammar errors such as (not exhaustive): "consist of a Transformer" (l
397) -> consists; "the algorithm give" (l
-Caption of Table 4: "Other model's performances assessment" -> "Evaluation of other models' performance" ?
593 & Table 8: "Develop questions" -> "Development questions"? More cases throughout the article.
This paper addresses topic modelling on a small corpus of 1083 respondant texts to a questionnaire regarding societal changes that could be driven by the recent pandemic
This translates in not only not having followed the template, but more so in an increased difficulty for the user to understand those results and their impact
Moreover, Tables 1, 2 and 4 appear in the paper before being referenced and Figure 1 is not even referenced anywhere in the text
In Table 2 it is not clear which answers are we seeing
What are the questions to which the users provided these answers?
What is the SST benchmark? No description is provided
The most intriguing part about the paper is probably the fact that the authors try to sell it as a topic modeling solution for the  COVID-19 related questionnaire answers, although they end up using only 60 couples of segments out of the pool of the 1083x9 answers texts
**Recommendation:** Paper should be conditionally accepted -- revise and resubmit.
Clear description of methods and data
Use of open dataset makes work reproducible
Data / methods discussion is disorganized  
The authors use test the implications for using deep learning in a semi-automated content-analysis approach to classifying scientific journal articles
They perform four experiments with a single, neural network containing three fully-connected layers
The experiments consist of the authors varying the inputs and output targets of the neural network
They find that across all experiments, the neural network's results are promising.
The authors should revise section 4 to discuss the what of their data instead of the how
The specific format of the data (e.g., JSON or tree-shaped) is immaterial to the analysis they perform
They should also include a discussion of their methods in this section
The authors should refrain from discussing implementation-specifics (e.g., specific Keras routines used)
If the neural network architecture is described in thorough enough detail - the exact implementation should be irrelevant
Additionally, the authors miss an opportunity to summarize the research on semi-automated/human-in-the-loop/active learning content analysis
The authors would be well served to conceptualize this interdisciplinary research area and enumerate the machine-learning methods and data types used by previous researchers in this area.
Overall, the paper is interesting and applies existing knowledge in a novel way
