,Class Index,Description
0,3, Then the method is tested in several image and text data sets.
1,3, Have you considered training jointly (across the tasks) as well?
2,3,\n\nThe contribution of this approach could be better highlighted.
3,3," I would expect the notation style to be consistent within a single equation (i.e., use ||w||_2^2, ||w||^2, or ||w||_{l_2}^2)\n\n3.)"
4,3, Was that meant to be z?
5,3," If my understanding is correct, the LTMN is trained to predict the baseline solver's output."
6,3," While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology."
7,1,\n\nMinor comments:\n\u201cFor kennels with q(w) other than Gaussian\u2026 obtain very accurate results with little effort by using Gaussian approximation of q(w)\u201d.
8,3,"\n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs, "
9,3, I am supposing this is at the test time. 
10,3,"\n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC."
11,3," Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form."
12,3,"\n5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by \""efficient\""?"
13,3, My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector.
14,3, The proposed GAN generator consists of two components where one focuses on generating foreground while the other focuses on generating background.
15,3, I see a benefit in interpretation which can help.
16,3,"\n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n"""
17,3, otherwise I don't actually have any comments on the text.
18,3,"  The framework does include several components and techniques from latest recent work, which look pretty sophisticated."
19,3,"\n* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?"""
20,3, Optimal hyperparameters are usually model-specific.
21,3, It also analyses how adversarial training affects detection.
22,3," By varying the prior distribution, the framework can incorporate both generative and discriminative modeling."
23,3, Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients.
24,1, This is an important area and findings in this paper are interesting!
25,2,Preliminary and intriguing results that should be published elsewhere.
26,3," However, how widespread is this problem across other models or are you simply addressing a point problem for RN?"
27,3,"  This is fine, but could perhaps be pointed out if that is indeed the case."
28,1,"\n\nPros:\n\nThe intuition is that the ReLU network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in PCA."
29,3, Current results in Table 1 only compare the amount of data in policy learning.
30,3,\n\n? p.3: What parts are pre-trained?
31,1,  I do think the paper would benefit from experimental results
32,3,"""The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text."
33,3," As the author say, learning the actions from state transitions in a standard stochastic MDP would require to learn the model."
34,1, Networks are incrementally grown; child networks are initialized with learned parameters from their parents.
35,3, Maybe finding better examples where the benefits of the proposed regularization are stressed could help.
36,3, They discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates (though in a quite different context and with a quite different analysis).
37,3, And the whole paper seems as if written\ncirca 2010 or 2012.
38,3,"\n5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating."
39,1, The contributions are interesting and experimental results seem promising.
40,3, but nothing really serious.
41,3, It is not easy to see what the assumption means.
42,3,\nA large set of experiments are provided to support the claims of the paper.
43,1,"\n8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research."
44,1," However, the authors do prove their work, which increases the novelty."
45,3," Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown."
46,3," (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically."
47,3," \""we turn to the discussion of a second\""\n- etc.[[CNT], [CLA-NEG], [CRT], [MIN]]  \n\nQuality: High\nClarity: medium-low\nOriginality: high"
48,2,I should be happy I dont have to spend time reviewing this dreadful paper; but I'm depressed at such bad science
49,3, They introduce a concept of \u201cscatter\u201d that correlates with network performance.
50,3," Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks."
51,3,\n\nDetailed comments:\n1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper
52,3," For one thing, there is the\nirksome and repeated use of \""discrete structure\"" when discrete *sequences* are\nconsidered almost exclusively (with the exception of discretized MNIST digits)."
53,3, I would strongly urge the authors to improve the method description in the camera read version though.
54,3,"  \n- Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful."
55,3,\n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?
56,3, Results are in line with hypothesis\n\t\u2022\tThorough appendix clearing any open questions \n\u00a0\n
57,3,"""This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks."
58,3, This raises the question whether the named entity table can only work in this context.
59,3,It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem.
60,2,..incoherent babble of unsubstantiated overstatement.
61,3,"""The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks."
62,3, I recommend to follow the notation E[variable] the authors been using throughout the paper in the proof instead of dropping these brackets.
63,3," What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS."
64,3,\n\nThe initial description (section 2)  leaves way too many unanswered questions:\n- What embeddings are used for words detected as NE?
65,1," The difference in performance for Dual-AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right?"
66,3,  how was the net initialized? 
67,1,\n\nQuality: this paper is of good quality
68,3, \n\n2) The inclusion of remaining-time as a part of the agent's observations and resulting improvement in time-limited domains is somewhat obvious.
69,3,"However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages"
70,1,"\n\n- The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper."
71,2,What is a systematic review? I have never heard of an unsystematic review.
72,3,\n\nMajor comments\n=============\n1. The authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to VGG16..
73,2,this may eventually be a cited paper.
74,3,  Would this affect the distributions?
75,3,  Does this mean that the network can only be trained when a complete set of input-output examples is available (i.e. outputs for all possible inputs in the domain)?
76,3,\n\nThe model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models.
77,3,"""This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item."
78,3,"""The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs."
79,1," but they do show some value for the technique in the area of domain-specific coreference.\n\n"""
80,3,"\n\n- Compared to many existing techniques, on 9 tasks"
81,3, The main goal is to quantify mode collapse in state-of-the-art generative models.
82,3,"\n[3] Stanley, K. O., & Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies."
83,2,[H] did a few sloppy experiments and attached his name to the great pioneering work by [P]. He should not be mentioned.
84,3,"""The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks."
85,2,"I believe that the authors have done scientific work, but in the current form of the paper it is impossible to judge it."
86,3,"\n- figures readability can be improved."""
87,3, The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus.
88,3," I\u2019d like to see far more results, and some attempt at a metric."
89,3,"\n\nThis paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited."
90,3,"\n\n* Comparison to baselines: \n  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]?"
91,3," While the idea of pivot based common representation learning for language pairs with no parallel data is not new, adding the communication aspect as an additional supervision is novel."
92,2,"The abstract says absolutely nothing, and I mean this literally and not as a judgement for the content of the paper."
93,3," When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1."
94,3," The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed."
95,3, \n\nHave you considered approximations to Wasserstein distance? E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs:
96,1, \n\nThe authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments.
97,3,"""This paper introduces a neural network architecture for generating sketch drawings."
98,3," \n\nThe experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017)."
99,1, I also like the unsupervised learning through gating function and label difference cost
100,3, It seems the right resolution will be to show that after the overlap is set to a certain small value
101,3,\n\n** PAPER SUMMARY **\n\nThe authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN.
102,3,\n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)
103,1,\n  - good overview of the related literature;
104,3," It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance."
105,3," But the accuracy of both the models is significantly lower than that of their base model (BiDAF) on SQuAD, demonstrating the difficulty of the DuoRC dataset."
106,3,"\n\n6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse?"
107,3," The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on."
108,1, the choice of baselines is convincing.
109,1,"\n\nOverall, I think this paper is interesting and presents a quantitative analysis of where the errors accrue due to learning with inference networks."
110,3," This can be seen in some of the illustrative examples in Figure 5: there *is* a coarse-scale positive curvature, but this would not necessarily come through in a quadratic model fit using the hessian."
111,3,"\""\n - evaluation of the best method from the cifar10 experiments on the new dataset"
112,2,the sthanthard of writing is impercable
113,3, there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture;
114,3," What happens when you try a bigger data set or a more complex problem?"""
115,3," More evaluations are needed to verify the method, especially with natural images."
116,3,\n\nThe proposed concept is only analyzed in MLP with Sigmoid activation function.
117,2,"The authors are perpetuating misguided generalizations in the face of substantial
experimental data to the contrary"
118,3,  It seems because of other factors instead of the stochastic convolutional layer.
119,3, Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA?
120,3,\n\n4. The hypothesis are not correctly specified.
121,3, Both these models are built off of an existing model on SQuAD \u2013 the Bidirectional Attention Flow (BiDAF) model.
122,3,"\n\nAs with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1]."
123,1, It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results.
124,3,\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al.
125,3, \n(iv) How can one maximize a probability density function?
126,3,\nhe resulting multi-channel image-like structures are then feed into vanilla 2D CNN.
127,3,"\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later."
128,3," Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut."""
129,1,\n\nClarity: The paper is well-written.
130,3," Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i"
131,3, thus I would liked to see more thorough experiments here as well.
132,3," First, we really need a more thorough analysis of what this does to the learning dynamics itself."
133,3, \n\nOriginality\nThe proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm.
134,3," There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense."
135,3,"""The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder."
136,1," \n\nEmpirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks."
137,1,"\n\nWith regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations."
138,3,\n2. Compare more clearly setups where you fix the number of parameters.
139,3,"""The paper studies methods for verifying neural nets through their piecewise\nlinear structure."
140,3,"  The other loss should remain what it is in the full \u201cOurs\u201d condition (i.e., l_1).[[CNT], [CNT], [DIS], [MIN]] \n\n- The last sentence in the caption of Table 1 -- \u201cSlight improvement in motion is observed by training with an adversary as well\u201d -- should be removed."
141,3," \""The brain as an efficient and robust adaptive learner.\"" Neuron 94.5 (2017): 969-977."
142,3," This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches."
143,1, \n\nOriginality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.
144,3," Therefore, empirically, it is really hard to justify whether this proposed method could work better."
145,1," Using an architecture to learn how to split the input, find solutions, then merge these is novel."
146,3," An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter."
147,3," It has a lemma which claims that the \""minimax and the maximin solutions provide the best worst-case defense and attack models, respectively\"", without proof, although that statement is supported experimentally."
148,3, Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable.
149,3, How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?
150,3,\n\nThe work should not be evaluated from a practical perspective as it is of a theoretical nature.
151,3, The type of model used is counterintuitive for me; why use a SVR model?
152,3," \n- Furthermore, I would reformulate the theorem."
153,3, Can we afford to keep all of those frames around?
154,1," \n\nFinally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs."
155,3, \n\nAlso the main assumption is that there is an easy way to compute similarity between states.
156,3, These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline.
157,3,"""This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks."
158,1,"""Overall, the idea of this paper is simple but interesting."
159,3," Also, for each data set, which region size worked best?"
160,3," \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks."
161,3,"\n\nShafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A rational account of pedagogical reasoning: Teaching by, and learning from, examples."
162,3," Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs."
163,3,\n- It would be helpful to define t_k explicitly to alleviate determining whether it is the interval time between ordered events or the absolute time since t_0 (it's the latter).
164,3," Is there a guarantee that a same named entity, appearing later in the dialog, will be given the same key?"
165,3, It mostly leverages the existing literature on primal-dual subgradient methods to modify the GAN training procedure.
166,3," It uses two networks, parameterization network and prediction network to model the mapping from design parameters to fitness."
167,3," When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework."
168,3, This paper suggests to factorize the recurrent weight matrix as a Kronecker product of matrices.
169,3," 2. the \""approximation gap\"": the part of the slack due to using a restricted parametric form for the posterior approximation."
170,1," \n\nIn general, the proposed work is very interesting and the idea is neat."
171,3,\n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*.
172,1," Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard."
173,1,"\n\nWith respect to the rest of the paper, the level of novelty and impact is \""ok,"
174,1, The authors demonstrate that the network works well in the semi-white box and black box settings.
175,3," It appears to be motivated by improving training dynamics, which is understandably a significant concern."
176,1, The authors generalize this idea in a nice  way and present results on 1 experiment.
177,3,   What if phi_1 is o ((A v B) Until C) and phi_2 is o ((not A v B) Until C).
178,2,"Unfortunatelly [sic], the paper is very shallow. The results are at the level of a naive master thesis"
179,3," \n- missing discussion to the \""attention is all you need paper\"", which seems highly relevant[[CNT], [SUB-NEU], [DFT,SUG], [MIN]] \n\n() Typos:\nPage 1\n\""a support vectors machineS\"" -> \""a support vector machine[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""performs good\"" -> \""performs well[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""the n-grams was widely\"" -> \""n-grams were widely[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""to apply large region size\"" -> \""to apply to large region size\""\n\""are trained separately\"" -> \""do not share parameters\"""
180,1, The plots in Fig. 2 and the appendix are quite helpful in improving presentation.
181,3, I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.
182,3, The authors first introduce their suggested loss function and then go into details about what inspired its creation.
183,3," As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded."
184,3,"\n\nAnother drawback, perhaps resulted from the \""genetic algorithm\"" motivation is that the proposed method has not been well explained."
185,3, A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure.
186,3," To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable."
187,3," \n* Section 3. A summary of all the hyperparameters should be given.[[CNT], [SUB-NEU], [DIS], [MIN]] \n* Section 4.1. The number of steps is not given.[[CNT], [SUB-NEU], [DIS], [MIN]] Do you present the same graph multiple times."
188,3, What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?
189,1," it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times"
190,3, It should be supposed L is at least locally convex.
191,2,Ive never read anything like it &amp; I do not mean it as a compliment
192,3, Does that impact the MI of the final layer and Y?
193,3,"  That has have to be named after the EM process, given that EM is unsupervised."
194,1,\n\nPros:\n-\tProvide theoretical guarantee for the use of orthogonal random features in the context of PSRNN
195,3," (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it\u2019d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold."
196,3,"\nThis additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments)."
197,3,"""The paper intends to show that complex and real valued neural network are different and lead to different results on similar tasks, the complex valued network being more appropriate to 'difficult' problems and datasets."
198,3, The approach is evaluated on:\n 1) several variants of MNIST.
199,1, I am raising my score a bit higher.
200,3,"""The paper describes a deep Q-learning approach to the problem of lane changing, whereby the action space is abstracted to high-level maneuvers that are then associated with low-level controllers."
201,3," This allows the authors to turn the standard min max problem of adversarial training into a single minimization problem, which is easier to "
202,3,  Extensive experiments could be added.
203,3," However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network\u2019s ability to separate that particular class from the other ones."
204,3," On the global convergence of the BFGS method for nonconvex unconstrained optimization problems. SIAM Journal on Optimization, 11(4), 1054-1064."
205,3,"\u2019, \u2018Our two-pass decomposition provides the better result as compared with the original CP decomposition\u2019."
206,3, Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16.
207,3, The method is an extension of the GloVe method and in the case of\na single covariate value the proposed method reduces to GloVe.
208,1,\n\nThe work seems to be a little bit incremental.
209,3,"\n - On p.8, I'm a bit suspicious of the \""Is additional knowledge used?"
210,3,"\n- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5."
211,2,"While an interesting concept, in its current form, the approach taken is fundamentally inadequate and flawed for almost all use cases."
212,3,\n\n4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc).
213,3,"\n\nOn the top of page 5, \""Line 10 of Algorithm 1\"": I think you mean Line 11 of Algorithm 2."""
214,3,\n- Show the effect of the reparameterization trick on estimator variance.\n- Compare the bias and variance of TRPO estimates vs the proposed method.
215,3,"""This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text."
216,1,\nThere is no obvious technical mistake  and the paper is written reasonably well.
217,3,"""I was asked to contribute this review rather late in the process, and in order\nto remain unbiased I avoided reading other reviews."
218,1,\n\nOn the positive side: \n\nThis is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand.
219,2,"I cannot imagine any physician or scientist who has a job, who will have the time to read this paper."
220,2,Various statements seem to be sweeping and inaccurate generalizations with little robust...
221,3," If the writing was improved, I think the paper may have even more impact."
222,3," Moreover, what is the form of f_beta and how beta is optimized?"
223,1,   I'm very happy to see good application papers at ICLR.
224,1, The work is therefore original and significant.
225,3," The high-level idea is similar to the evolution method of [Real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch."
226,3, \n\nOne concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice.
227,1," However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible."
228,3, It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Appendix G in https://arxiv.org/pdf/1706.04987.pdf)
229,3, Any smooth function can be approximated by such networks.
230,3, It is more like a dependency-based version skip-thought on the sentence level.
231,3, \n\nI should mention that I'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual-based algorithm stands in relation to existing algorithms.
232,3,"""The paper presents a method that leverages demonstrations from experts provided in the shape of sequences of states (actually, state transitions are enough, they don't need to come in sequences) to faster learn reinforcement learning tasks."
233,3," \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature."
234,3,"\n[2] Beal, M.J. Variational Algorithms for Approximate Bayesian Inference."
235,3," The model is based on tensor factorization which extends GloVe to higher order co-ocurrence tensors, where the co-ocurrence is of words within subgroups of the text data."
236,3,  These do not appear in the smoother PCA version.
237,3," \n\nAt the same time, though, I see several important issues that need to be addressed if this paper is to be accepted."
238,1," It is sometimes difficult to communicate ideas in this area, so I appreciate the author's effort in choosing good notation."
239,3," That is, the authors learn all the previous layers\nby finding point estimates."
240,3,\nMore precisely there is tradeoff to achieve between the complexity of the model and its simplicity.
241,1, \n\nStrengths:\n- The method is simple but novel.
242,3, They claim to have the following contributions: \n\n1. Using a grammar to guide decoding
243,1," The experiments are interesting,"
244,3, It seems to have a negative impact for the range of values that are discussed.
245,3,"\"" Could the authors give more detail on this? A reference would be appreciated. """
246,1, \n3. Discussing about the robustness of SRM for different depth is interesting and I suggest to prepare more results to show the robustness of SRM to violation of different hyperparameters.
247,1," The choice of simulation constants (% delayed, and delay time) seems somewhat arbitrary as well."
248,3, The paper also needs more clarifications in the writing.
249,1," This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L."
250,3,"  Since only individual images were shown, the evaluation mainly measures the quality of the generated images."
251,1,\n- Experiments on three different tasks indicating the potential of the proposed technique.
252,3, This definitely would bring it closer to the way the VAE was originally introduced.
253,1, Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time.
254,3, What will be the total required training time to reach the same performance compared with single branch model with the same parameter budget?
255,3, How did you generate these stories with so many sentences?
256,3, \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot).
257,3," \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode."
258,3," At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed."
259,3,"Overall, the results are a bit mixed."
260,3, \n\nArchitecture\n- MFSC are log Filterbanks ...
261,3,"\n(4) suggests using MC marginalization and also using the \""average\"" action to improve computational feasibility"
262,1, This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models.
263,3, Maybe this is what helps?
264,3,\nThey propose to have a predictor for deterministic information generation using a standard transformer trained via MSE.
265,3,\n-A new cross domain mapping is proposed\n-Large set of experiments\nCons\n-Some parts deserve more formalization/justification\n-Too many materials for a conference paper\n-The cost of the algorithm seems 
266,3, Arrows seem to have some different meanings.
267,3," The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once)."
268,3," Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\n"
269,3,\n\nThe second step is to train the goal function for a specific task.
270,3,\n\nA major drawback of the evaluation of the different approaches is that\neverything was used with its default parameters.
271,1,.\n\nOriginality:\nThe presented idea seems novel.
272,3, This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach.
273,1,\nThe theoretical analysis is satisfactory.
274,3," I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained?"
275,3," Then, they train generative models over the auto-encoder's latent space, both using a \""latent-space GAN\"" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model."
276,3, The task is to learn whether the center pixel is reachable from the starting point.
277,3, the form of equation (3) looks like an MMI criterion to me?
278,3, \n7. More explanation is needed towards Fig.4c.
279,3,  The method relies on sentiment pre-processing from GloVe and image pre-processing from Inception.
280,1,The problem and its potential applications are well motivated.
281,3,\n- It'd be great if Plot (a) and (b) in Figure 5 are swapped.
282,3, Are you thinking of the gradient coming back through PATH as a reward signal?
283,1, Figure `1 is a fantastic illustration that presents the core idea very clearly.
284,2,They show they can account for 20% of the variance. NoÂ wonder. The usually accepted level is 50% to...
285,2,It just doesnt make sense.
286,1," \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems. """
287,1,"\n\nSummary:\nThis is a nice paper which deals with an important problem, has some nice results and while not groundbreaking, certainly merits a publication."""
288,3," In other words, the test set of a dataset that is used for evaluation might contain some classes that were also present in the training set that VGG was originally trained on."
289,3,\n\nMy first remark regards the presentation of the technique.
290,3," The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step)."
291,3,". However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently."
292,3, The proposed method simply concatenates a saliency map with the corresponding raw pixel image as an input to adversarial perturbation detector.
293,3,\n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear.
294,3,"  What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?"
295,1,\n\nPros:\n- Task of reducing computation by skipping inputs is interesting\n
296,3, It circumvents the traditional tradeoff between search space size and complexity of the found models.
297,3, And does sampling for novel inputs by sampling the residual error collected from the training set. 
298,3, Why don't you present evaluation results on all tasks in the multitask setting?
299,1," It is well written, the idea is well articulated and presented."
300,3," which could be fairly large, thus the method is computationally much more expensive than random sampling."
301,3," \n\nThe proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters,"
302,3, It is well known that the residual connections are important in training deep CNNs and have shown remarkable performance on many tasks.
303,3," This leads to a few questions:\n\n1. What was the performance of the \""regression policy\"", that was learned during the supervised pretraining phase?\"
304,3,  They use a sparse reward function of +10 for reaching the goal and -10x(lane difference from desired lane) as a penalty for failure.  This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions.
305,3,"""This paper presents an experimental study on the behavior of the units of neural networks."
306,1,.\n\nThe paper is generally clear and well written.
307,2,They arbitrarily rule out models with interactions but without corresponding main effects. Etc.
308,3,"""This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a \u00ab backward \u00bb update (i.e. from end to start of episode)."
309,3, They also should compare to Strelka whic\nh interestingly they included only to make final calls of mutations but not in the comparison.
310,3,"\n\n\n[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447."
311,1, \n\nPros:\n- The research direction in combining model-based and model-free RL is interesting.
312,1,These are very impressive numbers for neural architecture search.
313,3,"""SIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose to accelerate the learning of complex tasks by exploiting traces of experts."
314,1,\nbut the general idea of combining different specifications is quite promising.
315,3, It showed that the proposed method could mix between the modes in the posterior.
316,3, or did you spend a lot of time tuning it?).
317,2,To me the question is uninteresting
318,3," The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency."
319,3,"\n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case."
320,3, And what would they potentially lead to? 
321,1,"  The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well."
322,3,"\tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?"""
323,3,"\n2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return."
324,1,\niii) Superior performance with fewer number of parameters compared to other methods.
325,3," Since FSGM is known to be robust to small random perturbations, I would be surprised that for a majority of random directions, the adversarial examples are brought back to the original class."
326,3,"""The paper is well-written but is lacking detailed information in some areas (see list of questions)."
327,2,I do not believe or trust the data presented or the underlying thesis.
328,3,. The key idea is to maintain a chart to take into account all possible spans.
329,3," If so, would it be possible to plot the complete curves?"
330,1,"""This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm."
331,3,\n\n3. It is unclear how the actual speedup was measured.
332,1, and the paper is clearly written and easy to follow.
333,3,". \n3. the experimental setup seems quite unusual to me: \""since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\"". "
334,3," Although it only holds when light robustness are imposed,"
335,1, The 2D example seem to work very well and the convergence curves are far better with the proposed regularization.
336,1,"\n\nSome of these questions that need to be addressed IMHO:\n\n- A critical general question: if the addressed problem is the alignment between e.g. images and not image generation, why not formalizing the problem as a similarity search one (using e.g. EMD or any other transport metric)."
337,3," The DuoRC dataset offers the following challenges compared to the existing reading comprehension (RC) datasets \u2013 1) low lexical overlap between questions and their corresponding passages, 2) requires use of common-sense knowledge to answer the question, 3) requires reasoning across multiples sentences to answer the question, 4) consists of those questions as well that cannot be answered from the given passage."
338,3," \\alpha in (1) seems to be \\gamma in Alg 1 (line 5). \""L_target is a target objective which can be a negative class probability ..\"" this assumes that the example is a positive class."
339,2,I fail to see the contribution either to physics or social science
340,3,"  If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task?"
341,1," Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful."
342,3,"\n\n* Text:\n\nsec 2 para 4. \""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set.\"" ??"
343,3,\n\n- The computational complexity of this model shouldn\u2019t be neglected.
344,3," If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \\phy(x) representation more class-seperable. Is that right?"
345,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
346,3, \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works.
347,1," I am recommending acceptance,"
348,3,"It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm."
349,3," \n\n6) Besides IHDP, did the authors run experiments on other real-world datasets, \nsuch as Jobs, Twins, etc?"""
350,3,"""1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter."
351,1," It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017)."
352,3," A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs."
353,3," In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components)."
354,3," This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology."
355,3," Also, the proposed methods aim to preserve the \""colorfullness\"" of a color."
356,3,\n\nI am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient-based methods.
357,2,"Fig 3e is fanciful, verging on silly"
358,3,"  Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns."
359,3,"""- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics."
360,1,"\n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID)"
361,3, \n+ A theoretical guarantee of the efficiency of an aspect of the proposed method is given.
362,1," Moreover, we also see the learned model is consistently improved using the proposed \""Decision-boundary Iterative Refinement Training with a Teacher\"" (DIRT-T) approach."
363,3," \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank"
364,1,"""The paper presents an interesting framework for bAbI QA."
365,3," However, this does not invalidate the contributions of this manuscript."
366,1, \n(2) Pure MC methods can outperform TD methods when the rewards becomes noisy.
367,3," It is not clear what are the advantage of deep stacked RNN in that context."""
368,3,"\n\nMinor things:\n\nPlease rewrite the sentence \""When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L."
369,1,\n\nPositive aspects:\n+ Emphasis in model interpretability and its connection to psychological findings in emotions
370,3,"""The paper addresses an interesting problem of DNN model compression."
371,1,  and large number of classes.
372,3, \n\nThis is mostly a \u201ctheory building\u201d work.
373,3, The most important aspect is the capability to build a feature map of previously unseen environments.
374,3, \nand 2) the distributional discrepancy between the re-weighted source domain and\nthe target domain.
375,3," \nIt clusters features from a specific pooling layer, and then creates a binary assignment between per image extracted feature vectors and the cluster centroids."
376,1," \n\n+ The generated face images are very impressive, especially the improved 512x512-pixel outputs."
377,3, The proof of lemma 1 only establishes the Lipschitz constant of the CNN function.
378,3,".\n- Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations."
379,3,", more comparisons and analysis are required to validate the approach."
380,3,"\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016."
381,3,\n\nSec 4\nWhat are the characters embedded with? This is important to specify.
382,3," What is the \""combined\""?"
383,3,"""**I am happy to see some good responses from the authors to my questions."
384,3,"\n\nsection 3.1:\n- replace \""to to\"" by \""to\"" in the second line[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nsection 4:\n- \""This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.[[CNT], [CLA-NEG], [CRT], [MIN]]\"" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers."
385,3," Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top."
386,3, The memory consists of a 2D array and includes trainable read/write mechanisms.
387,3,\n\nFigure 4 seems kind of strange.
388,3, It will be interesting to compare more DFM with its discrete counterpart.
389,3,"  I would add Spirtes et al 2000, Heckermann et al 1999, Peters et al 2016, and Chickering et al 2002."
390,3, How many were used for training and testing?
391,3, The main reason is that the original face mesh graph that goes into the convolution/downsampling\n operations is topologically preserved through the upconvolutions.
392,3,"""This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs. "
393,2,"Table 4 seems unnecessary given figure 8. Indeed, figure 8 also seems unnecessary."
394,3,"  \n\n- The authors write \""diWdt generates slightly better results and we adopt it in our experiments."
395,3,"\n(iv) I am not sure in whoch way g is \""measured\"", but I guess you are determining it by comparing coefficients. "
396,3,"\n\nIn order to do so, an extensive framework is proposed, consisting of 3 ConvNet architectures, followed by a hierarchical clustering approach."
397,1,".\n\n--\nAfter rebuttal\n--\nAuthors have answered to many of my comments, I think this is an interesting paper, I increase my score.\n"""
398,3,"""The paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter (Spectral Density Ratio from [Shajarisales et al 2015]) to cascades of two filters in successive layers of deep convolutional networks."
399,3,"\n\nFor the versions of the model that use beam search, what beam width was used?"
400,1," The authors report improved paraphrasing of phrasal verbs, and state-of-the-art accuracy in correcting grammatical errors involving prepositions, and good results on prepositional phrase attachment."
401,3,"  This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result."
402,3, \nHave you checked your model using different length vectors?
403,3,"\"" The reward suggests that the goal is to collect as many health kits as possible, for which surviving and maintaining health are secondary."
404,3,"\n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD."
405,1,The paper is well written and easy to follow.
406,1,\n\nClarity:\nThe paper is lucidly written and very understandable.
407,3,"""The authors suggest using a variational autoencoder to infer binary relationships between medical entities."
408,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
409,3,\n\nAuthors are suggested to perform experiments on more datasets to make the results more convincing.
410,2,It is not clear whether important new insights will be gleaned - it cannot be clear until the final product is reviewed
411,3," \n\nI believe that the current method can only learn and track simple objects in a constant background, a problem which is  well-solved in computer vision."
412,3,"  Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general."
413,3,\n\n- I think it's important to state in table 1 what is the amount of distortion noticeable by a human.
414,1,"\n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space)."
415,3,"\n- When introducing CNNs, please also cite Waibel and TDNNs - they are *the same* as 1-d CNNs, and predate them."
416,1, Results look promising too.
417,3," It is investigated how several RL strategies perform on a large, standardized data set."
418,1, \n- Significant empirical advantage over TRPO.
419,1,\n\n\nI enjoyed reading this paper and would like it to be accepted.
420,3,"""Summary: The paper introduces \""Phase Conductor\"", which consists of two phases, context-question attention phase and context-context (self) attention phase."
421,3,\n\n2. One intuitive approach to task balancing would be to weight each task objective based on the variance of each task. 
422,3, It\u2019s a simple method that can be applied post-training and seems to be effective.\
423,3, It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction.
424,3,  It proposes a non-parametric approach that maps trajectories to the optimal policy.
425,3," \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search"
426,3," \n- Finally, as the matrix is not symmetric, do real eigenvalues always exist?"
427,3,\n\nEquation 1 has a symbol E in it.
428,3," For neural networks, Nguyen and Hein (2017) assume the link function is differentiable."
429,3,"\n \n\u201c,which we propose in a later part of this paper\u201d -> which we propose in this paper"
430,3, What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)?
431,3," \n\nThe \u201csoft ordering\u201d approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task."
432,1," The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST."
433,3,"\n- The reason that Deep Speech 2 is better on noisy test sets is not only the fact they trained on more data, but they also trained on \""noisy\"" (matched) data"
434,3, Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random?
435,3, It uses the image and its attributes.
436,3,"\nThe authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables."
437,3, It also produces meaningful node embeddings with semi-interpretable latent spaces.
438,3," There is absolutely nothing of interest to ICLR except for the fact that now we know that a trivial network is capable of obtaining 90% accuracy on this dataset."""
439,2,Startlingly naive and jejeune. Obviously a poorly tailored master's thesis.
440,3," In particular, the use of LSTMs helps take into account interdependencies between pattern labels."
441,1," \n\nOverall, the work is an interesting read, and a nice follow-up to Neal\u2019s earlier observations about 1 hidden layer neural networks."
442,3,  Convolutional neural networks on graphs with fast localized spectral filtering.
443,3," But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up."""
444,1," The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings."
445,3,"\n\nThe first 3 sections of the paper are very clearly written,[[CNT], [CLA-POS], [APC], [MAJ]] but the remainder has many typos and grammatical errors (often word omission).[[CNT], [CLA-NEG], [CRT], [MIN]] The draft could use a few more passes before publication.\n"""
446,1,  \n- Competitive with state-of-the-art external implementations
447,1,\n\nPro:\n-            The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning.
448,2,"Frankly, I read the manuscript about two weeks ago and don't remember the context, nor did I cross-walk one part of the essay with another to validate the thought."
449,3," For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x."
450,3,"""Summary of paper:\n\nThe paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks."
451,3," They could have done a better job explaining the quality of their final results, though. "
452,1, It seems to perform well in practice as shown in the experimental section.
453,3, This evaluation includes training the agent on a set of training mazes and testing it's performance on a set of held-out test mazes.
454,3," If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?"
455,3, If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper.
456,3, \n\nMinor points:\n- Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)}
457,1,"\n\nI believe re-thinking new learning rate schedules is interesting,"
458,1,\n\nI have a favourable impression of this paper
459,1,"\n\nAbout the technique details, this paper is clearly written,"
460,3, This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines.
461,3,  The paper proposes a GAN model to generate graphs with non-trivial properties.
462,3," If we are evaluating the video prediction task for having real or fake looking videos, the turkers need to observe the full video and judge based on that."
463,3," Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques."
464,3,"  In my opinion, one of the main features of this work is to split the NN computation to local computation and cloud computation, which ensures that unnecessary amount of data is never released to the cloud."
465,3,"  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?"
466,3," \""Training very deep networks.\"" Advances in neural information processing systems."
467,3, This stochastic version obviously requires a step size; so it would have been proper to state the stochastic version of the algorithm instead of the batch algorithm in Algorithm 1.
468,3," The accuracy on a held-out test set is not guaranteed to be monotonically increasing, right? "
469,3, Does the KL term help prevent overfitting at some stage?
470,1, Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow.
471,3,\n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate
472,3, What do you mean does not scale to nonlinear function approximation?
473,3,"\n\nIn general, I tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications required."""
474,3," Though it is quite common in the literature to assume this, it would have been interesting to see if there's a way to handle the case where it is unknown (either the process, parameters or both)."
475,3,"  In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54"
476,2,I now turn to my best guess about what the authors might be doing.
477,3,"""The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which \nuses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions:\n- the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems"
478,3, \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.
479,3," In Figure 3, it is mentioned that the network uses a U-Net with recurrent connections."
480,3,"\n\n[1] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang. Deep Fried Convnets. ICCV 2015."
481,3, Showing YF to perform very well on several deep learning tasks without (or with very little) tuning.
482,3,\n\n- The second weakness listed above might be related to the first one.
483,3," It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora."
484,3,\n\nNovelty: This is in my opinion the weakest point of the paper.
485,3, This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling.
486,2,"I recommend acceptance, provided the editors are willing to stretch the standards for publication a bit h/"
487,3," I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup."
488,3," What is meant by \""number of layers connecting the stochastic latent variables\""?"
489,3, Will this be feasible?
490,3," Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST."
491,3,"   \n\nFor the unsupervised reconstruction loss, a static background is populated with objects, one at a time, each passing its state and feature through deconvolution layers to generate RGB object content."
492,1, \n\nThe paper is well written and conceptually simple.
493,3, It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.
494,3,?\n- Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)?
495,3, The authors provide a version with batch SGD as well.\n\n
496,1,"\nAs follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem."
497,3,\n\nThe paper suggests normal pruning does not necessarily preserve the network function.
498,1, It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning.
499,3, \n\ntwo questions linger around re practices:\n1. gan is known to struggle with discriminating distributions with different supports.
500,3,\nThe algorithm learns faster than unassisted DQN as shown by learning curve plots.
501,3,\n\nThe experimental setup is 4x4 grid world with different basic shape or grey level rendering.
502,3,"\n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work."
503,3, It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague.
504,3," The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network."
505,3, our institutional review board would certainly allow self-certification of the data (i.e. removing the patient identifiers and publishing the first 4 hours of sequences).
506,3,"There are many approaches to devising optimal execution plans, e.g., rule-based, cost-based, learning-based."
507,3," The basic idea is that for fitting nonlinear models with a convex loss, if the mapping from the weights to the outputs is open, then every local optimum in weight space corresponds to a local optimum in output space; by convexity, in output space every local optimum is global."
508,3," \n- In table 4, for example, it would be nice to see the performance on the different emotion categories."
509,3, I would like to see results on more datasets and more discussion on the data augmentation technique.
510,1, \n\nThe paper presents strong quantitative results and qualitative examples.
511,3, \n\nThe basic privacy paradigm proposed seems to be:\n1. train a GAN using private data
512,1, Shapeworlds dataset seems to be an interesting proof-of-concept dataset
513,3," This learning procedure is heuristic, and there is no theoretical guarantee about the correctness (convergence) of this learning procedure."
514,3,"  If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data."
515,3,"\n3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified."
516,2,The results are as weak as a wet noodle
517,3, Analyzing the actual adaptive algorithm would be very interesting.
518,3,\nWould it be possible to include a non-deterministic baseline in the experimental comparison?
519,1,\n\nThe cherry on the sundae are the experimental results.
520,1, The example images look convincing to me.
521,3,"""Compared to previous studies, this paper mainly claims that the information from larger neighborhoods (more directions or larger distances) will better characterize the relationship between adversarial examples and the DNN model."
522,3, They show superiority of their algorithm over SSTE.
523,1,"\n\n5.  The experiments on adversarial robustness and face verification seems more interesting to me,"
524,1," \n\nFor the novel approach and the theoretical backing, I consider the paper to be a good one."
525,1," 2) Although the problem itself is interesting,"
526,3," Specifically, the author argue that the moment-generating function for the pointwise kernel approximation error of ORF features grows slower than the moment-generating function for the pointwise kernel approximation error of RFM features, which implies that error bounds derived using the MGF of the RFM features will also hold for ORF features."
527,3,   However I feel that the authors should include some discussion of other published results.
528,3," \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n"""
529,3,"  Finally the cost effectiveness training (3c), how come that the same \""car wheel\"" (as in 3b) is discovered by the EM clustering? Is that coincidence?"
530,3,\n\nHow was Figure 5 computed ?
531,3,"  In term of impact, this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case."
532,1,\n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model.
533,3," The result is not particularly different from previous ones,"
534,2,"It looks to be more of a chance for the authors to promote a product using a poorly constructed, non-replicable pilot study"
535,3," The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized."
536,3, They substitute the simple add operation with a selection operation for each input in the residual module.
537,1,"""The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version."
538,3,\n\nSummary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs.
539,1, So these methods could be applied in the real world.
540,3,"""The paper describes a method for detecting adversarial examples using a second detector classifier."
541,1,"\n\nMy impression about the paper is that even though it touches a very interesting problem,"
542,3,  It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty.
543,1,.\n\nMy comments:\n\nThe paper is well-written and I really enjoyed reading this paper.
544,3,"\n-\tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)\u2026\n"""
545,3,"  Unsuprisingly, this leads to an improvement."
546,3, (Finn et al.)\n- Prototypical Networks for Few-shot Learning (Snell et al.)\n- Matching Networks for One-shot Learning (Vinyals et al.)
547,3," In particular, they aim at generating a complete set that fully specifies the behavior of the oracle."
548,3, How to choose lambda?
549,3," \n\npage 10:\n- Figure 6: this example should be more carefully described in terms of distribution, f*, etc."
550,3," The paper first considers the \""Bayes by Backprop\"" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model."
551,1, but the hierarchical approach seems to have more advantages and seems a more straightforward solution.
552,3," For instance, why is that particular sigmoid formulation used?"
553,2,"Is Tartarstan a magical land where they make the worlds tartar sauce? If not, I can only assume the author is referring to Tatarstan"
554,1," The experiments are serious, and done using standard state-of-the-art tools and architectures."
555,3,\u2028\n(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare.
556,3, The adjacency matrix (or a subgraph of it) is first re-ordered to produce some canonical ordering which can then be fed into an image representation method.
557,3,\n\nThe proposed methodology relies on multiple choices that could sometimes be better studied and/or explained.
558,1,"\n\nFor the presentation, the motivation in introduction is fine,"
559,3, \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations.
560,1,"\n\nDespite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR."""
561,3,"""This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature."
562,2,Often sounds like a precocious high-school student who is trying to show off how clever he is
563,3," What if content(x_t) = W_1 . h_{t-1} + W_2 . x_t? """
564,1,\n\n\n2) Pros:\n+ New quantitative evaluation criteria based on motion accuracy.
565,3, Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing?
566,3,"""# Paper overview:\nThis paper views the learning process for stochastic feedforward networks through the lens of an\niterative information bottleneck process; at each layer an attempt is made to minimise the mutual\ninformation (MI) with the feed-in layer while maximising the MI between that layer and the presumed-endogenous variable, 'Y'."
567,3, \n\nThe resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples.
568,3," Reporting the training objective value makes little\nsense to me, unless the time taken to train on MNIST is taken into account in\nthe comparison."
569,3,"""The paper evaluates one proposed Deep RL-based model (Mirowski et al. 2016) on its ability to generally navigate."
570,3," I cannot quite imagine in what kind of applications we can get \u201ca set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class\u201d."
571,1, The paper is very well written and easy to follow
572,2,"The paper is okay, its not wow but its hard to reject it."
573,1,\n+ The approach is capable of theoretically handling all linked information to an entity as additional information to the link structure
574,2,The paper failed to make the reviewers more than semi-excited 
575,1, Therefore I recommend acceptance for it.
576,3," It may be the case that even a \""bad\"" generative model (according to some other metric) can still result in a classifier that produces reasonable test accuracy."
577,3,  Is this where you artificially return an agent to a state that would normally be hard to reach?
578,3,"""*Summary*\n\nThe paper proposes using batch normalisation at test time to get the predictive uncertainty."
579,3,"n\nOriginality: I'm not familiar with LSTMs, it is hard for me to judge the originality here."
580,1," Using the generated images, paper reports improvement\nin classification accuracy on various tasks."
581,3,"\n- Results are very promising, with 5x speed-ups and same or better accuracy that previous models."
582,3,"\n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair."
583,1,"\n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting."
584,3,"""This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially."
585,1,"\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process."
586,3,Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful.
587,3,"\n\nOverall, I think the contributions of this paper are too marginal for acceptance in a top tier conference."
588,1,\n\nThis paper reads well and the results appear sound.
589,1, \nThis paper presents a learning algorithm that can \u201coutperform a greedy baseline in terms of efficiency\u201d and \u201chumans driving the simulator in terms of safety and success\u201d within their top view driving game.
590,3," Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words."
591,1,\n\nThe paper mentions \u201csignificant improvements\u201d in only two places: the introduction and the conclusion.
592,2,This needs to be standardized!!! It must be converted!!! Why should we expect some kind of relationship?? It needs to be justified!!!
593,1,"""The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter."
594,3,"""After the rebuttal:\n\nI do not think I had a major misunderstanding of the paper."
595,3," Also, particularly given that the focus is on embedded speech recognition, of which the acoustic model is one part, I would like a few more details on how decoding was done, etc."
596,3," \n\n         =  {d / d {s\u2019 }  PATH } ( s,  Tau( s, th^g ),  a )    d / {d th^g}  Tau( s, th^g)"
597,3, Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.
598,3, The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit.
599,1," The paper does a good job of explaining the connection,"
600,1,\n\nStrengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations.
601,3," Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting."
602,3,"""This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting."
603,3,  The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch.
604,3,"  The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index."
605,2,"In the interest of being helpful, my suggestion is that the authors go back and review what is involved in the scientific method"
606,3," So I actually cannot tell which method is better, at least in the MNIST experiment."
607,3,\n\nMore details are needed about Figure 3.
608,3,\n\nWhy is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution (as stated in Section 2.1)?
609,2,Pacific oysters and carpenters ants are not 'animals'. Correct this word.
610,3, Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try.
611,1,\n\nPros\n\nUseful extension of an important technique backed up by behavioural experiments.
612,3," However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction."
613,3," However, to evaluate the effectiveness of the method, it would be interesting to also report results with a kernel-based classifier, so as to see how it compares to the state of the art."
614,1, and several ablation experiments show the effectiveness of these two improvements.
615,1,"  Then, the difference is crystal clear."
616,3," \nRegarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review."
617,3,\n \n\u201cQ_it maximizing the equation is designated as the optimal price.\u201d Which equation?
618,3,  Why not use one matrix instead?
619,3, What is the purpose of including two rows in table 2 with the same\nmethod?
620,3,\n\nSome detailed comments / questions:
621,3,"\n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342"
622,3,"The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods."
623,3,\n\n\nminor comments\n---------------\n\n* the paper employs vocabulary that is not common in ML.
624,3,\n* Distance preservation appears more and more like a dated DR paradigm.
625,3, During the rebuttal I would ask the authors to extend f-Hyperband all the way to the right in Figure 6 (left) and particularly in Figure 6 (right).
626,1,"  \n\nThe proposed algorithm seems novel,"
627,3,"\n\n\nFor the experiments, the following should be addressed."
628,1, \n- The dual formulation simplifies adversarial training
629,1,"\n\nMinor comments:\nSection 1.1: \""a affine\"" -> \""an affine\""\nTypo in section 3.4: \""of a of a[[CNT], [CLA-NEG], [CRT], [MIN]]\""\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse."
630,3, The addition of one or two tables with either a standard task against reported results or created tasks against downloadable contextual / tensor embeddings would be enough for me to change my vote. 
631,3,"\n\n* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without\n  block-diagonal structure)"
632,3,"""This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program. "
633,1,\n\n=Originality=\nThis is an interesting problem that will be novel to most member of the ICLR community. 
634,3," \n\n(5) Given the above considerations, there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation."
635,3, What problem does this particular set of contributions solve that is not solvable by the baselines? 
636,3, How can the proposed method be scaled to handle multiple requested labels?
637,3,"""This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents."
638,3," However, BN-LSTM outperforms BELU-RNN on Text8."
639,3,"""This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \""style\"" as used in this paper)."
640,3," It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small."
641,3," Overall, also in the light of figure 4, the interpretation that the new algorithm results in better generalization seems to stand on shaky ground, since differences are small."
642,3,"  \n\nSmaller comments\n---\n- As a stylistic thing, I would suggest not pluralizing \""attention\"" (i.e., remove \""Attentions\"")."
643,3," A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points. "
644,3," EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs."
645,3, The approximated eigenoption was simply computed as a one-step greedy policy.
646,3, The authors also show that these inferred quantities can be used to generate more effective attacks against the targets.
647,3,"I suspect this is the actual cause for it not converging to zero for Baird's, although please correct me if I'm wrong on that."
648,1," Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it."
649,1, \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n
650,3, \nIt would be interesting to see if this can improve the results.
651,3, The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.
652,1," \n\nOverall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples."
653,3,"\n-  \""to replace the softmax error function (used in deep learning)\"": I don't think we have softmax error function"""
654,3," It would be better if an example is given, which is verified to satisfy the assumption."
655,3," I believe that N is supposed to be the noise in the SCM, but then maybe it should not be called E at the beginning."
656,3,"In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms."
657,3," There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments."
658,3,"\n\nClarifications:\n1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper?"
659,2,An effort as I might imagine necessary to make this paper work (beyond flagging the insanity) would require a Swiftian talent for irony
660,3,\n\nThe architecture is competitive on SVHN and CIFAR 10 but not on CIFAR 100.
661,3, The COCO leaderboard also uses this metric as one of its evaluation metrics.
662,1,"  Thus, this Bayesian perspective can also help explain the observation that models trained with smaller batch sizes (noisier gradient estimates) often generalize better than those with larger batch sizes (Kesker et al, 2016)."
663,3,"This could, for example, be done by optimizing a joint objective which modifies the input to both maximize error in the classifier and in the detector simultaneously, while remaining close to the original input. "
664,3, Traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixels.
665,1,", which is already quite popular by now and should be compared with."
666,3,\n\nResults for linear networks are not an improvement over existing works.
667,3,\n\nThe plots in Figure 2 include performance in terms of episodes.
668,3, The paper also did not report sample quantitative measures e.g. Inception scores / MS-SSIM.
669,1," By the proposed representation, the authors are able to apply image classification methods (supervised or unsupervised) to subgraph classification."
670,3,\n\nYou have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet
671,3,"  Similary, you say \""On the other hand, the ensemble model can explain the performance improvement easily."
672,3,\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.
673,3,\n\nThe paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming.
674,3,"\nCons: no evidence that this is a \""break-through\"" idea"
675,3,"""The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations."
676,3," \""Non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain.\"""
677,3,The possibility of computing a faster approximation of ISRLU is also mentioned
678,1,\n\nExperiments on generative optimal transport are interesting and probably generate more discussion/perspectives
679,3,"\nFinally, a generator is used to generate entity pairs give a relation."
680,1,"  The introduction is particular well written, as an extremely clear and succinct introduction to optimal transport."
681,3, The only novelty I can pinpoint is the proposed visual Turing test.
682,1, \n\nPros:\n+ The results are very pleasing visually.
683,1,"\n\n4) Two drawbacks of previous methods motivate this work, including the bias of\nrepresentation learning and the high variance of re-weighting."
684,3,"\u2028In addition to that, one could fine-tune the representation during forward model training."
685,1," Also, the experimental results look quite promising"
686,3, \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).
687,3," It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret."
688,3, Too early to tell about significance.
689,3," which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases."
690,3," In few-shot classification, the sequence length can be known a prior."
691,2,Didnt like this one
692,3,"""Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess."
693,3," Furthermore, in the matrix completion scenario, you have O(log^2n) entries per row on average, which means with high probability few rows should have a constant number of entries."
694,3,"""The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues."
695,1,\n- I like the new attention over chart cells.
696,3,"""  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss."
697,2,To justify these conclusions and spur the authors onto the better things they are undoubtedly capable of I append some details.
698,3, What is the reconstruction error during the second CP decomposition in 2?
699,3,"""In this paper, the authors propose  a new network architecture, CrescendoNet, which is a simple stack of building blocks without residual connections."
700,3,"\n[2] Zhang, Cheng, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. \""Advances in Variational Inference.\"" arXiv preprint arXiv:1711.05597 (2017)."
701,1," \n\nAfter rebuttal:\nThe writing of the paper greatly improved, still missing insights (see comments below)."
702,3,"\n\na useful reference:\n\nStrom, Nikko. \""Scalable distributed dnn training using commodity gpu cloud computing.\"" Sixteenth Annual Conference of the International Speech Communication Association. 2015.\n\n"""
703,3," The set is optimized to be as hard as possible (maximize loss), which results in a min-max problem."
704,3,The method is evaluated on heat sink design and airfoil design.
705,3," \"" -- please clarify; better results on a development set, I hope?"
706,2,I'm a bonehead so maybe I missed this?
707,3,\n- No comparison with state-of-the-art techniques on the experimented tasks and datasets.
708,3,"""This paper proposes to combine reinforcement learning with supervised learning to speed up learning."
709,2,The results section is not great (boring).
710,3,".\n\n5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space."
711,3,"\n\n* On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets."
712,3,  It could use a couple more passes with an editor.
713,3, \n5. Only early stopping seems to constrain their model to be near identity.
714,3,"  The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.\n\nHowever, the method by which these confidence estimates are refined could be better described."
715,3," In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC."
716,3," What is done here is quite different, but I think it would be worth discussing these relationships in the paper."
717,3," I found that the answers are usually very short, which is more like factoid QA."
718,3,"\n\nIt is highly suggested that the method is called as population-based method as a set of networks is maintained, instead of as \""genetic\"" method."
719,3," It is said that *a* random direction was used from each minimiser to plot the loss, so how the 2D directions obtained ?"
720,1,Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.
721,3,"\n\n\""learn a policy that satisfy\"" -> \""learn a policy that satisfies\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""HRL, We introduce the FSA augmented MDP\"" -> \""HRL, we introduce the FSA augmented MDP.\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\"" multiple options policy separately\"" -> \"" multiple options policies separately\""?[[CNT], [CLA-NEU], [QSN], [MIN]]\n\n\""Given flat policies \u03c0\u03c61 and \u03c0\u03c62 that satisfies \"" -> \""Given flat policies \u03c0\u03c61 and \u03c0\u03c62 that satisfy \"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""s illustrated in Figure 3 .\"" -> \""s illustrated in Figure 2 .\""?"
722,2,I find the title and the main premise of the abstract confusing and illogical. [key concept X] has the logic of a Monty Python sketch
723,1,"Finally, while it proposes an interesting formulation of a well-studied problem"
724,3,". Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points."
725,3, These are important questions that should have at least an empirical exploration.
726,1,\n- the second GAN solution trained on reverse codes from real data is interesting 
727,3," For example, could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural?"
728,3," \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data."
729,2,"How does [redacted theory] explain your results? Or really, your underwhelming results?"
730,3," And in order to achieve three important targets which are auto completion, auto error correction and real time, the authors first adopt the character-level RNN-based modeling which can be easily combined with error correction, and then carefully optimize the inference part to make it real time."
731,3, The standard LQG setup does not have this restriction.
732,3,"\nConcerning the contribution of the model, one novelty is the conditional formulation of the discriminator."
733,2,"The authors presents a flurry of statistics, but they do not explain why or how those are relevant to the study"
734,3," For instance, to a spectral clustering method for the semi supervised clustering, or\nsolving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006."
735,3, Experiments demonstrate that their method is promising compared to the competitor \u201cNormProp\u201d which explicitly normalizes the weights of neural networks.
736,3," Instead, the conditional density p(y_t|y_{1:t-1|, \\theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM."
737,1,. The revised version does address my concerns.
738,3,"""This is an emergency review, as the replacement of an overdue review.[[CNT], [CNT], [CNT], [GEN]] \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit."
739,2,Are we modelling an astronomical object here or an abstraction?
740,3," Take for example the case where a particular transition (s,a,r,s\u2019) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced."
741,3, Should it allow better classification performance?
742,3, Does single task mean average performance over the tasks?
743,3, It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span.
744,1,\n\nThe paper is to be commended for the following aspects:\n1) Detailed description of GGNNs and their comparison to LSTMs
745,3,\n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow.
746,3," One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes."
747,3,"\nMulti-Batch Experience Replay for Fast Convergence of Continuous Action Control (Han and Sung, 2017)"
748,3," Moreover, some databases could be used for different tasks, such as WSJ or ImageNet."
749,3, The network learns unsupervised using a predictive coding setup.
750,3, Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B.
751,3, The paper seems rushed to me so authors should polish up the paper and fix typos.
752,3," If I understand it correctly, the model needs to compute O(N^3) LSTM compositions."
753,3,"""Summary\n - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism."
754,3," Given a reward function, one can define the Bayes decision rule."
755,3," In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations. "
756,3," This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation."
757,3," This extends up to page 4. I would argue, that this is quite a stretch, as the free energy principle is essentially blind to the idea of rewards and preferable states such that all tasks are essentially evaluated in terms surprise reduction."
758,3,"""My review reflects more from the compressive sensing perspective, instead that of deep learners."
759,1,\n\nOriginality:\n\npros:\n-The paper while simple sheds some light on important problem with the prior distribution used in GAN.
760,3,\n\n- The first mention of partial observability can be moved to the introduction.
761,3," The authors use a Resnet-50 which is a\n  smaller and lesser performing model, they do mention that benefits are expected to be \n  complimentary to say larger model, but in general it becomes harder to improve strong models."
762,1," Arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics.\n4. For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation."
763,3,  What are some architectures that might be appropriate for different domains?
764,3, This can work if each layer expands the reachable region (the state) by one pixel if the pixel is not blocked.
765,1," \n\n3. Relatively minor: The writing of this paper is readable,"
766,1, The observation is explained well and substantiated by clear experimental evidence.
767,2,The sum total is frustration with what I can only call cavalier treatment of promising material.
768,3,"""The paper proposes a new neural network based method for recommendation.,"
769,3, The authors argue that the reason for those diminishing returns is that the agent is actually learning a trivial wall following strategy that doesn't benefit from more maps.
770,3, It seems to me this is a very general claim.
771,3, \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches.
772,1,"  Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation."
773,3," My best suggestions are to drop the \\Phis altogether and consider\nusing text subscripts rather than coming up with a new name for every variable,\nbut there are probably other things that will also help."
774,3,"To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution)."
775,1,"\n\n- Theorems 2.1, 2.2 and the observation (2) are nice!"
776,3,"""The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them."
777,3," \n\n# Summary of review\nI find the contribution to be incremental, and the validation weak."
778,3,"""In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR)."
779,3,"\"" Is this a known fact, or something that you observed empirically?"
780,3,"""This paper learns to construct masks and feature representations from an input image, in order to represent objects."
781,1,  Experiments and analysis are both presented clearly.
782,3,"\n\nThere are also quite a few correctness errors in the paper, and the polish of the plots and language needs work, as outlined below. "
783,3," The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform."
784,3,"""SUMMARY.\n\nThe paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting."
785,3,\n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.
786,1, \nThe model is evaluated on tasks that it was not trained on which indicate that this model learns generalizable latent representations.
787,3, See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability.
788,3, What type of distributions of X will be a good example?
789,3,"""This paper proposes a WGAN formulation for generating graphs based on random walks."
790,3,"\n\nI would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature, notably HTNs."
791,1," Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction."
792,3, What is the output (o_t) of the network?
793,3,"  For training, apply knowledge distilation for better training followed by fine tuning by reinforce."
794,3," For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1]."
795,3,which hyper parameters were tuned and with which values?
796,3," Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision."
797,3,"\n- How do those compare with CE and L_{5, 1} with the proposed method?"
798,3,"   However, at least in some key cases in the recent literature, parameter numbers *were* controlled (see e.g. Table 4 of Trabelsi (2017))."
799,3,\n\nAnother concern is that the authors' approach assumes that all parameters have\nthe same effect.
800,3," Would learning a mixture of HMMs (one per community) have similar performance?\n"""
801,3," The paper also states (Secs. 1 and 2) that the the network studied here is based on Hartono et al, 2015, with the main difference of the sigmoidal ouput layer being replaced by a softmax layer."
802,1," The authors spend so much effort developing their own algorithm! Also, in actual implementation, they only use a crude version of the inner algorithm for reasons of efficiency."
803,1,\n- Three new datasets of UI images and corresponding code\
804,3,". To improve the paper, it would be helpful to evaluate the method under various settings."
805,3,"""The authors propose to decompose reinforcement learning into a PATH function that can learn how to solve reusable sub-goals an agent might have in a specific environment and a GOAL function that chooses subgoals in order to solve a specific task in the environment using path segments."
806,1,"""The main strength of this paper, I think, is the theoretical result in Theorem 1"
807,3," In other words, the paper proposes a particular approach for the adversarial defence."
808,1, That said I agree this is an interesting avenue for future work.
809,3, It might be helpful to discuss more about the over-fitting of different graph properties.
810,3,More comments and further exploration on this results should be done.
811,3," The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure 2"
812,3," \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections."
813,1," In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps."
814,2,"So the paper, which I was initially excited to read, ended up just making me mad."
815,3,"\n\n2. In Lreconstruct, only min difference between A and A1 is considered."
816,3,\n\nThe interpretation of figure 2 is off.
817,3,"\n-\tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, \""A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming,\"" IEEE Journal of Selected Topics in Signal Processing. "
818,3,"\n\nThe approach is evaluated on three tasks, two synthetic and one real world."
819,3,"  In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu."
820,3," Does the \""knowledge of the current user utterance\"" include the word itself?"
821,3,"""This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems."
822,3, It also proposed to use budgeting techniques to overcome computational costs.
823,1,"\n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified."
824,3,"""The authors propose a method for reducing the computational burden when performing inference in deep neural networks."
825,3," \n\nDetailed Comments:\n\u201cIn the of NaaA\u201d => remove \u201cof\u201d?[[CNT], [null], [QSN], [MIN]]\n\u201cpassing its activation to the unit as cost\u201d => Unclear.[[CNT], [null], [CRT], [MAJ]] What does this mean?[[CNT], [null], [QSN], [MIN]]\n\u201cperformance decreases if we naively consider units as agents\u201d => Performance on what?"
826,3, Some of the GAN figures could also benefit from having captions.
827,2,STRENGTHS: none.
828,3,"\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured."
829,3," Different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the \""derivative\""."
830,3," It is interesting to see how DAs are used for conversational modeling,;"
831,3, The method is based on balance the class-priors to generalize well for rare classes.
832,3,"\n\n3. The authors demonstrated that, given the same resource budget, the wider networks with the proposed method are more efficient than the deeper networks due to the nature of GPU parallel mechanism."
833,3," As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable."
834,1," While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.\n\n"""
835,3,"  More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic."
836,3,"""The paper deals with \u201cfixing GANs at the computational level\u201d, in a similar sprit to f-GANs and WGANs. "
837,3," and in a conv net, just a convolution?"
838,2,"Did all 5 authors say,Yes, this is a piece of work I am proud to have my name on? - Twitter"
839,3,"Finally they find a \""a feature vector for each plate by summing up the output of the last recurrent layer overtime."
840,1," They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results."
841,3," While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm."
842,3,"""The authors present a method to enable robust generation of adversarial visual\ninputs for image classification."
843,3,"\n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax."
844,3," I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.\n"""
845,1, The application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy samples.
846,3,"""This paper induces latent dependency syntax in the source side for NMT."
847,3, Essentially the expert defines a new Q-value problem at every state \nfor the learner?
848,3, Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference.
849,3, \n\nA3C is known to be quite high variance.
850,1,"\n\nThe problem considered by the paper is interesting,"
851,3, Seems like warmup_time should also be an input to the algorithm.
852,3," \nIf the expert\u2019s first few transitions were easily approximable,\nthe learner would get local rewards that cause it to mimic expert behavior."
853,3, Is it not sufficient to have and discuss Fig 23?
854,3,\ and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates.
855,3, \n\n[1] A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning .
856,3,"\"" in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods."
857,3, The mask contains information about the presence/absence of objects in different pixel locations and the feature map contains information about object appearance.
858,3," Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning."
859,3,"\n- It would have been convincing to see an experiment showing actual use of the proposed method for navigating the face space, e.g. for finding criminals based on a description."
860,3,"\n- The empirical evaluation is done on intuitively related, superficially unrelated, and a real world\n  task."
861,3," They learn a distribution q(z | x, y) that randomly throw class logits."
862,2,"My disappointment when finding flawed analyses, conclusions, and terminology [..] was therefore substantial"
863,3,  Comparisons against these approaches would make the paper stronger.
864,1,\nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem.
865,3," So I suggest moving them to the Appendix and make the major focus more narrowed down."""
866,3," Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing."
867,3, I think it should be c_{ijk} * \\delta_{ijk} under the summations instead.
868,3,"""The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment."
869,3," The main difference\nappears to be that rather than using the classic GAN loss to shape the\naggregate posterior of an autoencoder to match a chosen, fixed distribution,\nthey instead employ a Wasserstein GAN loss (and associated weight magnitude\nconstraint, presumably enforced with projected gradient descent) on a system\nwhere the matched distribution is instead learned via a parameterized sampler\n(\""generator\"" in the GAN lingo)."
870,1,"  The engineering endeavor is impressive,"
871,1, The experiment shows good privacy and utility.
872,3, They also use SRM in combination with a neural network meta-modeling method and a hyperparameter optimization one and show that it can decrease the running time in these approaches to find the optimized parameters.
873,3,"""The paper applies tools from online learning to GANs. "
874,3,"\n\nIt is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input."
875,1, \n\nReview summary:\n\nI think this paper is interesting.
876,3,"\n\nI get it that maybe under different P(z) the difficulty of learning a good generator G can be different, and therefore one may wish to iterate between updating G (under the current P(z)) and updating P(z) (under the current G), and hopefully this process might converge to a better solution."
877,3, \n\nSignificance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.
878,3,high \n\nSummary:\nThis paper studies the problem of unsupervised learning of semantic mappings.
879,1, \n\nPros:\n- Investigating the ability of distributed representation in encoding input structured is in general interesting.
880,1," Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016)."
881,2,"In a nutshell, please cut out all the hype, show some integrity, and write a balanced paper."
882,1, The experiments with Baird's and CartPole were both convincing as preliminary evidence that this could be effective. 
883,3," \n\nThe paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''."
884,3,"""This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments."
885,3,"\n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n"""
886,3,".ther approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., \""Representation Learning on Graphs: Methods and Applications\"", William L. Hamilton, Rex Ying, Jure Leskovec, 2017."
887,3," To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space."
888,3,\n2. The last equation in pp. 3 defines the decision function f by an inner product.
889,1, but the paper has several strong points:\n\n* The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one.
890,2,"eviewer 1: 'I really enjoyed the introduction'.

Editor: 'In line with Reviewer 1, the introduction doesn't do a good job..."
891,3,. That this is the case should be made clear in the title and abstract
892,3," Publishing the code, as the authors mentioned, would certainly help with that."
893,3," In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?"
894,3, It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails.
895,3," \nAuthors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification."
896,3, \n\nTarget-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al.
897,1,":\n\n1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit)"
898,3,. Why does PSR truncate the indices in alpha?
899,3, It is interesting to see the impact of incorporating PCN into the training of OCN and encoder.
900,3, The objective function is a multi-class classification problem (using softmax loss) and with linear model.
901,3,\u201d Is there any empirical evidence for this?
902,3, \n\nSummary: \nA new stochastic method based on trust region (TR) is proposed.
903,3,"""The authors present 3 architectures for learning representations of programs from execution traces."
904,3,  You claim that downsampling the data to 15 minute time steps still captures the relevant dynamics of the data -- is it obvious from the data that variations in the measured variables are not significant over a 5 minute interval?
905,3," If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding)."
906,3,\n? p.7: What does \u201cfind the mid-point of the bin\u201d mean and should it not be 1018 instead of 1000 bins?
907,1, The experimental results on counting are promising.
908,3,"  Samples are shown for CIFAR 10, MNIST, and OMNIGLOT. "
909,3,"\n \n# Quality\n- Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games."
910,3,"  Further, typical program synthesis approaches don't explicitly learn to produce correct syntax."
911,3, At risk of being too reductionist:\nit looks as learning a set of filters on different coordinate systems given\nby the various powers of A.
912,3," \n- To make a fair comparison, the results in Table 1 should consider the amount of data used in pre-training the forward models."
913,3,"\n\nOn the practical side, the chosen baseline is very poor."
914,3,"""The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems"
915,3," Also, what is the epoch number, and why is this 1 for alpha=0?"
916,3,  Where is this method most applicable and where is it not applicable?
917,3,"\n\nMy biggest concerns that dampen my enthusiasm are some assumptions that may not be realistic in most controls settings:\n\n- First, the most concerning assumption is that of a symmetric LDS matrix A (and Lyapunov stability)."
918,1,"\n\n- The paper is reasonably clear,"
919,3," Furthermore,\nusing larger batches means fewer parameter updates per epoch, so training is\npotentially much faster."
920,3,  They show improved performance on a number of multi-task learning problems.
921,1, The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data.
922,1,\n\nThe finding of the effectiveness of idea (1) seems to be significant. 
923,2,Simply conducting the same analyses in a different data set does not equate novelty or impact to the...
924,3,\n-- What are the sizes of the train/test sets derived from the OpenSubtitles database?
925,3," The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e."
926,3," In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters"
927,3,\nPage 5:\n-\tSection 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5.
928,3," \n\nStill, the authors clearly utilise basic concepts (c.f. \""utilize eigenvector \nbasis of the graph Laplacian to do filtering in the Fourier domain\"") in ways\nthat do not seem to have any sensible interpretation whatsoever, even allowing\nfor the mis-understanding due to grammar."
929,3," Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies?"
930,3,\n\nI guess that networks including convolutional layers are covered by\ntheir analysis.
931,3,"""This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer."
932,3, It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case
933,3," The student model learns according to a standard stochastic gradient descent technique (Adam for MLP and CNN, Momentum-SGD for RNN), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model."
934,3," So, what is the advantage of the proposed \u201csc\u201d method compared to the \u201csc-seq\u201d method?"
935,1,\n\n\nClarity\n=====\n\nThe paper is clear and well-written.
936,3," One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those."
937,3,"""This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity."
938,3," We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.\n"""
939,1," I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method."""
940,1," In some cases the results are similar to pix-to-pix (also in the numerical evaluation) but the method allows for one-to-many image generation, which is a important contribution."
941,1,\n\nPros: \nOverall the paper is well-written
942,3," Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets."
943,3," They perform experiments on MNIST and CIFAR-10, doing classification by scaled regression."
944,3,\n\n-----------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper considers the use of natural gradients for learning.
945,3,"\n* The very recent paper by Krishnan et al. (posted to arXiv days before the ICLR deadline, although a workshop version was presented at the NIPS AABI workshop last year; http://approximateinference.org/2016/accepted/KrishnanHoffman2016.pdf) examines amortization error as a core cause of training failures in VAEs."
946,2,You need a comma here. Do they not have commas at your institution or do they just cost a lot?
947,1," \n\nThe paper is easy to read,"
948,3, The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.
949,2,"I am not sure why there is a full section about limitations, this in itself says a lot about the study"
950,3," The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model."
951,3, The fact that LastSeenValue\n  doesn't do as well as a linear model on TS alone would seem to\n  indicate that there are higher order autoregressive coefficients.
952,1, \n\nWhile the paper is reasonably clearly written and easy to read
953,1," For the task of prediction under the shift in\ndesign, shift-invariant representation learning (Shalit 2017) is biased even in\nthe inifite data limit."
954,3,\n\nThe description of the empirical setup could be more detailed.
955,1," Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system\u2019s latent dependency decisions."
956,3,"""Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks."
957,3," \n \nFor the experiments, other baselines should be included, particularly just regular Q-learning."
958,3,"\n\nComments:\n1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5)."
959,2,"Suggesting much but saying nothing of import, the sort of balderdash that is often compared to the waste of male ruminants."
960,3," I also think that a discussion of the \""attention is all you need\"" paper by Vaswani et al. is needed, as both articles seem strongly related."
961,3, Are the cells found interpretable?
962,3,"\n- Need to be more specific: \""use some channels to encode the id information\"". \n"""
963,2,[XX] is just as interesting to us as the new dress bought by Kim Kardashian
964,3,"\n\nOn the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2."
965,3," I would imagine the benefit of using ImageNet is just to bring a random, high-dimensional embedding."
966,3,\n\nCorrectness: There are minor flaws.
967,1,\n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation).
968,3,".\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens."
969,3," As a running example, they show how this can be be done for bubblesort."
970,3," Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017)."
971,3, The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements.
972,3, One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance.
973,3, Please expand on this.
974,3,\n\n(6) Below Definition 2.3: What is capital X?
975,1, (Although [particle] MCMC is probably a better choice if one wants extremely low bias.)
976,3,"\n\nSimilarly, the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer, and show the form of the resulting kernel functions."
977,3,"\n\nMoreover, the numerical experiments look to be realized in the context of targeted attack."
978,3," \n- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks."
979,1," With the current focus of the paper being the proposed system, it is interesting to the computer vision community."
980,2,There are many stylistics phrases to tidy which you can find yourself
981,3,\n4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when.
982,3, \n-   Equation (5)  should be - O(\\alpha^2)
983,3,  Soft unitary constraints also have been introduced in earlier work (citations are also in the paper).
984,3," The method uses a learnable character embedding to transform the data, but is an end-to-end approach"
985,1," Furthermore, while their approach is sold as a general sensor fusion technique,"
986,3,    \n\nThe authors limit what the car is able to do \u2013 for example it is not allowed to take actions that would get it off the highway.
987,1,  This is possibly one of the best papers on graph generation using GANs currently in the literature.
988,3," However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. "
989,3,"""In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner."
990,3,. This explicit duration modelling captures multiple scales of temporal resolution.
991,3, Otherwise in practice how to set these parameters to get better results is not obvious.
992,3," As mentioned by the authors in section 1, any relaxation in synchrony brings more noise and higher variance to the updates, and also may cause slow convergence or convergence to a poor solution."
993,1, The only real copyediting I noticed was in the conclusion: and be used \u2794 and can be used; that rely on \u2794 that relies on.
994,2,This article is on an interesting topic. Unfortunately there is no more positive to say about this manuscript.
995,1,\n\nThe main positive point is that the performance does not degrade too much.
996,1, The paper is revised and I saw NMS baseline is added.
997,3," The authors argue they could not reproduce Osband\u2019s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores."
998,3," I assume that it involves \\hat{M}, but it would be good to formally define this notation."""
999,2,I doubt that its worth publishing a handful of graphs that few would find surprising and that anyone...
1000,3," They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models."
1001,3," VI is a much older\tfield, and Kingma and Welling proposed a very special form of VI, namely amortized VI with inference networks."
1002,3,"""Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers."
1003,2,The paper could be considered for acceptance given a rewrite of the paper and change in the title and abstract.
1004,3,\n\nI will refrain from adding additional detailed commentary in this review because I am unable to judge this paper fairly with respect to other submissions owing to its large deviation from the suggested length limits.
1005,3," For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers."
1006,1, \n\nPros:\n+ The paper is well-written
1007,3," When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)"""
1008,3,\n- The papers lacks a more in-depth theoretical analysis.
1009,3, Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments
1010,3,"  Finally,  I'm not sure I understand the X axis in Figure 2, the (effective) number of examples is much higher than the size of the dataset."
1011,2,This paper is indicative of a degenerative research paradigm' h/
1012,3,")\n\nBecause the authors improved the manuscript, I upwardly revised my score to 'Ok but not good enough - rejection'."
1013,3,"  After all, the cluster membership is found based on the nearest target in the test stage."
1014,3," They evaluate that their method can learn to provide learning items in an efficient manner in two situations: (1) the same student model-type on a different part of the same data set, and (2) adapt the teaching model to teach a new model-type for a different data set."
1015,3," This way, finer discretization does not increase the dimension of \\x (nor its approximation), but rather improves the resolution."
1016,3,"""Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs"
1017,3,n\n- I am wondering about the effects of the temperature parameter t. Is that important for training?
1018,3,"  Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty."
1019,3, Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.
1020,3," As in, what should be the ground-truth answer against which the answers should such questions be evaluated."
1021,3,"""[Reviewed on January 12th]\n\nThis article applies the notion of \u201cconceptors\u201d -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks."
1022,3, Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.
1023,3," This seems rather limiting, can you comment on that?"
1024,3,"  Moreover, is this D_S same as the style classifier used in the metric?"
1025,1," \n\nI like this work,"
1026,3,\n\nTrust region methods are generally batch methods.
1027,3,"\n\n1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read."
1028,3,\n\n* I'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simulator.
1029,3, Motivation given in form of relevant applications and mention that it is relatively unstudied\n\t\u2022\t
1030,2,This review is without a doubt the single most difficult one I have had to write so far.
1031,1,.\n* The overall idea is interesting and has many potentials.
1032,3," For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. "
1033,3,\n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime.
1034,1,"n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). "
1035,3,"\n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn\u2019t fully utilize the deep net performance."
1036,3," I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients."
1037,1,. Authors formulate the active learning problem as core-set selection and present a novel strategy.
1038,3,"""The work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding."
1039,3," For example, simply applying kmeans to PCA features of the images on the MNIST data can get you pretty good performance."
1040,2,It is always rather pleasant to recommend that a paper be rejected.
1041,3,\n            (2) how was the masking done?
1042,3,\n\n3) The key of this paper is to approximate the dynamics using neural network (which is a continuous mapping) and take advantage of its gradient computation.
1043,3,\n\nI have a few questions on the motivation and the results.
1044,1, The paper is rigorous and ideas are clearly stated.
1045,3, but the significance of the performance difference in downstream ML tasks is unclear
1046,1,\n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs
1047,1,\n\nPro:  The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.
1048,1, It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear.
1049,3,"\n\nAlthough I believe there is work to be done in the current round of RL research using nearest neighbor policies, I don't believe this paper delves very far into pushing new ideas (even a simple adaptive distance metric could have provided some interesting results, nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains....),"
1050,3," It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs."
1051,3, Please do not use color as the only cue to identify a curve.
1052,1,\n\nMost of the originality comes from integrating time decay of purchases into the learning framework.
1053,3,"\n\niii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents? "
1054,1," There are a few things I think could be cleared up, but this seems like good work (although I'm not totally up to date on the very recent literature in this area)."
1055,1," It is evaluated on a corpus of 29M SLOC, which is a substantial strength of the paper."
1056,3,\n\nIt would be great to mention very briefly any helpful intuition as to why F_\\epsilon and H_\\epsilon have the forms they do.
1057,2,What do the authors mean by one standard error?
1058,3, Can you elaborate?
1059,3, What are the conditions under which the method is likely to perform well?
1060,3," I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality)."""
1061,3,"   The proposed network predicts a 2D mask image, where local maxima  correspond to object locations, and values of the maxima correspond to presence values."
1062,3,  Or at least have a good argument of why this is suboptimal compared to PLAID.
1063,1,\n\n\nPros:\n\nThe result highlights an interesting relationship between deep nets and Gaussian processes.
1064,2,The writing is often arrestingly pedestrian' :ArrestinglyPedestrian!
1065,3,\n\n6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph
1066,3,"   From the cited paper by Posner et al : \n\""The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems."
1067,1,  it'll interesting to see how the proposed approach gets around this issue.
1068,3,"Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835."""
1069,3," In this work, activation functions are considered random functions with a GP prior and are inferred from data."
1070,3,"\n\n\nOriginality\n=========\n\nThis idea is not new: if we search for \""Lipschitz constant estimation\"" in google scholar, we get for example\nWood, G. R., and B. P. Zhang. \""Estimation of the Lipschitz constant of a function.\"" (1996)\nwhich presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull)."
1071,3,"\n\nMost of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc."
1072,3,".\n- Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*.\n- Missing word in first sentence of S4.1: ... reason __ the ..."""
1073,3,"\n\n[1] Zoph, Barret, and Quoc V. Le. \""Neural architecture search with reinforcement learning.\"" ICLR (2017).\n[2] Baker, Bowen, et al. \""Designing Neural Network Architectures using Reinforcement Learning.\"" ICLR (2017).\n"""
1074,2,There is so much that is wrong with this paper that it is difficult to know where to start
1075,3," To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER."
1076,1,"  I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence."
1077,3,"""This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices."
1078,3," Since this is not my main area of research I cannot judge its originality in a completely fair way, but it is original AFAIK."
1079,3," In other words, what is supposed to be the take-away, and why should we care?"
1080,3,"""This paper adds source side dependency syntax trees to an NMT model without explicit supervision."
1081,3," However, y^* is also estimated by the encoder as shown in Eq. (2) and it varies depending on the input to the encoder."
1082,2,I can't possibly imagine what led the authors to believe that their paper was remotely interesting enough to submit for publication
1083,2,The most strikingâand troublingâfeature of this manuscript is that it contains so many exceedingly strange assumptions.
1084,2,Thats not how science is done.
1085,3," The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks."
1086,3, What insights do you gain by knowing these theorems etc.
1087,3,.\nThe approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.\
1088,3,"\n\nFinally, nowhere in the paper do you mention which nonlinearities you used or if you used any at all."
1089,3,"  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient."
1090,3,  With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor.
1091,3," The aim is to learn embeddings from supervised structured data, such as WordNet."
1092,3," For example, while the configuration the paper's baseline models are not given, the baseline accuracy of MNIST classification using MLP is 16.2%."
1093,3," And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other)."
1094,3," \n\nOverall, I think there is potential with this work but it feels preliminary."
1095,3,It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent.
1096,3,"""The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach"
1097,1,\n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design.
1098,1, The results from this paper is also promising that it showed convincing compression results.
1099,1,\n\nWhile the experimental results are interesting
1100,2,I am afraid this manuscript may contribute not so much towards the fields advancement as much as toward its eventual demise.
1101,3," Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow."
1102,3,? What makes this more challenging?
1103,3," Instead of using a fixed or Gaussian parametric filters, this work proposes to predict filter weights using a multi-layer perception."
1104,2,"it is impossible for the reader to keep track of the hundreds (thousands?) of acronyms in the paper. If there ever is an award for most acronyms in a paper, this one would win it hands down.  h/"
1105,2,This code sample cannot be adequately described without the use of strong language.
1106,3,"\n\nUnder the theme of sketched updates, they examine quantized and sparsified updates with the property that in expectation they are identical to the true updates."
1107,3, This might be worth mentioning.
1108,2,You have put in a lot of effort answering a question that should have never been asked -..
1109,1,"  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics)."
1110,3,
1111,3, I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.
1112,1,"""The paper is clearly written, with a good coverage of previous relevant literature."
1113,3," Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?"""
1114,3,\n\niii. Although the formal discussion is concerned with Markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the PPD) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used).
1115,3,"  In the paper, the author only got around 83 percent accuracy with SGD and 85 percent accuracy with TR."
1116,3, Or how to figure out a coordinate group?
1117,1, \n\nIt makes sense that the more flexible model proposed by this paper performs better than previous models.
1118,3,"  Expanding out, each term is a product of some of the r_i and some of the identities I."
1119,3," As far as I can tell, this paper has nothing to do with GANs."
1120,3,\n\n\n* Unclear relation with other papers:\nWhat part of the derivations of this work are novel?
1121,3, The authors should emphasize the first half of the method section are from existing works and should go into a separate background section.
1122,3,"\n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model)."
1123,3,"\n- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future."
1124,3, Evaluation metrics include repeated latency to the goal and comparison to the shortest route.
1125,3," Even though the proposed method works better, the prediction accuracies of S are still high."
1126,1,". For example, why JMVAE performs much better than the proposed model when all attributes are given."
1127,3,"""The authors propose a strategy for compressing RNN acoustic models in order to deploy them for embedded applications."
1128,1," \n\nWhile the experiments are compelling,"
1129,2,Maybe a scientific journal is the wrong outlet for your data
1130,3,".\nC. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words."
1131,2,"The accompanying simulation movie is neither interesting, nor particularly beautiful."
1132,3, Well within the remit of the conference.
1133,1,"\n\nThis paper merits acceptance on theoretical merits alone, because the FTRL analysis for convex-concave games is a very robust tool from theory (see also the more recent sequel [Syrgkanis et al. 2016 \""Fast convergence of regularized learning in games\""]) that is natural to employ to gain insight on the much more brittle GAN case."
1134,3," To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes."
1135,3,".\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E). "
1136,3,"""NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset?"
1137,3,"""The paper demonstrates the need and usage for flexible priors in the latent space alongside current priors used for the generator network."
1138,1,\n\n2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2.
1139,1,"""The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets."
1140,1,\n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.
1141,3,\n\nThere is a lot going on in this paper.
1142,3,"\n* In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion."
1143,3, Two literatures of note are intelligent tutoring and machine teaching in the computational learnability literature.
1144,3,  \n- How network behaves by introducing noise on vertices?
1145,3, What is a powerful algorithm?
1146,3, but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text.
1147,1,"\n- In general the topic is interesting, the solution presented is simple but needs more study"
1148,3, This figure only shows that training shallower networks is more effective than training the deeper networks on GPU.
1149,3," If I\u2019m correct, that\u2019s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = \u2026."
1150,3," The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks."
1151,3," On a conceptual level since S_c should be a PSD matrix it can be written as the square of some matrix, i.e. S_c = A_c^TA_c, then the Mahanalobis distance becomes (A_c z - A_c c)^T ( A_c z-A_c c), i.e. in addition to learning a projection as it is done in Snell et al, the authors now learn also a linear transformation matrix which is a function of the support points (i.e. the ones which give rise to the class prototypes)."
1152,1, The paper is very easy to follow.
1153,3," Agents working in this setting therefore, learn the language of the \""teacher\"" and efficiently ground words to their respective concepts in the environment."
1154,3, The authors simply show set of generated images.
1155,3, Why is this a reasonable assumption to make?
1156,3,"\n\nComments:\n\n-\tIn page 2, authors suggest that from that G\u00fcl\u00e7ehre, Bengio (2013) that for visual relations \u201cfailure of feed-forward networks [\u2026] reflects a poor choice of hyper parameters."
1157,3,"""This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up."
1158,3,"  In Advances in Neural Information Processing Systems 24, pages 2402\u20132410. 2011. """
1159,1, \n\nIt\u2019s hard to take issue with a paper that has such overwhelmingly convincing experimental results.
1160,2,"I am not sure why there is a full section about limitations, this in itself says a lot about the study"
1161,3," There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense."
1162,3, but you should be careful to distinguish this type of method from other approaches.
1163,3," \n(iii) In which way is a Gaussian prior uncorrelated, if there is just a scalar random variable? "
1164,3,"\n\nThe idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization. "
1165,3,".\n\nThe empirical results are only presented in table-of-numbers format (graphical comparisons would be easier to understand), and tables 5-8 are all zero, which doesn't make sense for these classification tasks."""
1166,3,"Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem."""
1167,3, More about conditions on eta_v would be illuminating. 
1168,3,"""Summary: \nBased on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network."
1169,2,the sthanthard of writing is impercable
1170,1,"""After reading rebuttals from the authors: The authors have addressed all of my concerns."
1171,1, It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance.
1172,3," Basics on mass transportation are briefly recalled in section 2, while section 3 formulate the GANs approach in the Wasserstein context."
1173,3,"\n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \""over a variety of viewpoints\""."
1174,3, The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one.
1175,3,  They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions.
1176,3, I wish to see how they can be used to improve binary networks.
1177,1,\n+ the tensor factorization set-up ensures that the embedding dimensions are aligned \n+ clustering by weights (4.1) is useful and seems coherent\n+ covariate-specific analogies are a creative analysis
1178,3,"\""\n\n> Table 1\n\nWhy is the second best method on CIFAR (\u201cHier. repr-n, random search (7000 samples)\u201d) never tested on ImageNet? "
1179,3,\n\n+ Several possible combinations between datasets and domains are considered to evaluate the network behaviour.
1180,3,"""The paper presents a method for feature projection which uses a two level neural network like structure to generate new features from the input features."
1181,3,\n- p.3: We use an appropriate encoder is repeated twice.
1182,3," For a reference, see Peters, Janzing, Scholkopf: Elements of Causal Inference: Foundations and Learning Algorithms (available as pdf), Definition 6.32."
1183,3, Results are reported on about 50 UCI datasets with different topologies.
1184,3,\n\n3. Clarification on the assumption (3).\nWhere is this assumption coming from? I can see that this makes the analysis go through but is this a reasonable assumption?  Does most of system satisfy this constraint? Is there any?
1185,1,\n\nClarity\nThe paper is clearly written.
1186,1,The method part is clear and well-written.
1187,3," However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results."
1188,3,The experiments are only limited to bAbI task which doesn\u2019t tell you much.
1189,3," The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input."
1190,2,It reads as if the author were giving a lecture and wandered off point to tell an interesting story.
1191,3, What are the motivations for this particular approach?
1192,3,"""This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function."
1193,3," The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory."
1194,3," Experiments are presented on MInst, Cifar10, and ImageNet."
1195,3,\n\nFigure 5: Looks to me like the baseline is actually doing much better than the proposed methods?
1196,3," As they write themselves \""there is no evidence showing that semantic meanings are fully linearly correlated."
1197,2,"In summary, the conclusions of the manuscript are on one side well known results achieved in a complicated way without demonstrating the advantage of proceeding so, and on the other side unfounded speculations based on simplistic reasoning and irrelevant setup."
1198,3," The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a \""reformer\"" (without seeing adversarial examples) to detect and correct adversarial examples."
1199,3,"\n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI."
1200,3, Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU.
1201,3," I would like the authors to have provided results on more than the current three datasets, as well as an explanation of how meaningful the MSEs are in each dataset (is a MSE of 0.2 meaningful for the Swimmer Dataset, for instance? the reader does not know apriori)."
1202,3, Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results.
1203,3,"\n- Page 2, line 12: \""prices."
1204,3,"""This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used."
1205,2,Reading this made my brain melt. Very true though.
1206,3,"\n\nThe paper acknowledges that complex networks are not new, and that the findings of previous authors is that complex networks perform less well than real-valued alternatives."
1207,3," It would be convincing if the authors could transfer an English paragraph into the style of a certain author, such as Shakespeare, which can be easily evaluated by a human instead of a trained classifier."""
1208,3,\n\nThe paper studies text embeddings through the lens of compressive sensing theory.
1209,3," For instance, what is s, r, t."
1210,2,That gives a ridiculous demonstration where authors forgot science and reinvent history (â¦) the...
1211,3,\n \n \nWhat is the relation between \\rho_jit and q_it ?
1212,1,"  That and other recent work have provided some systematic evaluations of complex-valued networks, and shown their utility in a number of cases. "
1213,3, I am puzzled why is the error is coming down before the boundary interaction?
1214,3,"""This paper presents some reviews on clustering methods with deep learning."
1215,3, The novelty would be improved with clearer differentiation from the Hazan 2017 paper.
1216,3,\ne)\tCan this approach be used with multiple i.i.d. graphs?
1217,3," As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy."
1218,3,  I thought the main selling point of the method is the computational gain.
1219,3, What is the accuracy drop after fine-tuning in both scenarios?
1220,1, \n\nPositive points:\nAuthors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning.
1221,3,"\n\nOverall, this paper is more suitable for the workshop track"
1222,3,\n\nMore generally I'd like to better understand what effect we'd expect this regularizer to have.
1223,3," This brings us back to the fact that features encoding the actual dynamics, potentially on many consecutive states (e.g. feature expectations used in IRL or occupancy probability used in Ho and Ermon 2016), are mandatory."
1224,1,  This is a standard result coming from the fact that the Fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective as (2).
1225,3,"\n\n- In Table 1 the linear SVM uniformly outperforms the RBF SVM, so why\n  use the RBF version?"
1226,3,\n\n- Page 4 p_i\u2019s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly).
1227,3, Was there any KL reweighting scheduling as done in the original BBB paper?
1228,3," Unfortunately, this was not done. I suspect that in this case, the results would be very similar. \n\n"""
1229,3, This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task.
1230,3, I\n\nThe model is inspired by the variational autoencoder.
1231,3,"  \n\nThe second part offers an exciting result:  If we learn policy pi_1 to satisfy objective phi_1 and policy pi_2 to satisfy objective phi_2, then it will be possible to switch between pi_1 and pi_2 in a way that satisfies phi_1 ^ phi_2."
1232,3, \n\n3. The result of the optimal mini-batch size depends on the training data size.
1233,3,\n- What is the error of the raw h-predictions?
1234,3,"\n\n[9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017"""
1235,3,"""The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets."
1236,3,\n\n(4) Please specify which version of the SQuAD leaderboard is used in Table 3.
1237,3,"\n4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  """
1238,3,  What's the exact challenge of training VAEs addressed by the convolution stochastic layer?
1239,3,"\n\nIt would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network."
1240,3," Here, the sharing is essentially meaningless, because the variance of the weights in this derivation shrinks to zero."
1241,3," I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. """
1242,3, -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer.
1243,3,". In the experiments section, the authors apply their pruning approach on a few representative problems and networks. "
1244,3,";\n- Gradient penalties work in all settings, but why is not completely clear;"
1245,1,"  The regularizer is specifically nice in this setting, as it suffices to have the Kronecker factors be unitary."
1246,3,  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.
1247,3," Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right."
1248,3,  This could be done by visually showing the partition constructed or seeing how the model learned to merge solutions..
1249,3, \n\nThe proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled. 
1250,3,  Can you explain better (e.g. in the supp mat)\nthe effect of this for different values of q_i?
1251,3, It shows that deep RNN is signficantly better than shallow one in this metric.
1252,2,"This is an outline of a paper, and not a fully-thought, well-organized, thoroughly-discussed paper. [..."
1253,1, Experiments on German/English and Chinese/English show gains over other reinforcement learning methods.
1254,3,"\n\nHowever, the originality and significance of this work is a significant drawback."
1255,3,\n5. One solution is to approximate the solution F^{-1} J using gradient descent.
1256,1,\n\nThe evaluation is extensive and mostly very good.
1257,3,"""In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network."
1258,3," This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets."
1259,3," I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-VAE and so on."
1260,3, Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable.
1261,3, I would enjoyed more digging in this direction.
1262,1," By the way still, SRM is interesting method if it can be trained once and then be used for different datasets without retraining. "
1263,3," The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level."
1264,3,"n4) Add a discussion on more structured sources of covariates (e.g., social networks)."
1265,3,"\n\n4.) \u201cDense pixel-level correspondences\u201d are discussed but not evaluated.\n"""
1266,3,"""This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation."
1267,3, But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior.
1268,3,"\n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global."
1269,3, however the technique is different since this submission uses PAC-Bayesian analysis.
1270,3,\n(4). What is Theorem 2 trying to convey?
1271,3," The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x."
1272,3," Moreover, authors argue that the \nquality of generated texts is not appropriately measured by perplexities,\nthus using another criterion of a diversity of generated n-grams as well as\nqualitative evaluations by examples and by humans."
1273,3," In any case, this figure should be corrected to reflect this."
1274,1, This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques
1275,3,"  Why? \n\n- Surely \""d_t = Dt\"" should be \""d_t = D v_t\"" in the \""Interaction Term (IT)\"" subsection?"
1276,3, Any defense can be *evaluated* against samples generated by any attacker strategy.
1277,3, if D(f||g) < gamma then the concept represented by f entails the concept represented by g.
1278,3, The clustered representations are the visual concepts.
1279,3,I think that would be a more useful measure for the learned eigenoptions.
1280,3,  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method
1281,1," Although there have been much previous work, this paper is along this line."
1282,1, The paper is easy to follow and the idea is interesting.
1283,2,I found the use of the evolutionary theory problematic. This is a highly contested theory and the authors do not attend to the major flaws
1284,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean
1285,1,"\n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments."
1286,3,"""This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives."
1287,1, Experiments are carried out very carefully.
1288,3," I\u2019m listing a small selection of relevant papers below, but I\u2019d encourage the authors to read a bit more broadly, and relate their work to the myriad of related older methods."
1289,3,\n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.
1290,3," Also, if there is an intrinsic 2D hidden structure in the data, then imposing a 2D representation can help (as a sort of a prior)."
1291,3, Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj.
1292,1," \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results."
1293,3," The paper does use skip connections, but the difference is that they are phased out over training."
1294,3, \n\nI have the following questions regarding the experiments:\n1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data?
1295,3,"""Neal (1994) showed that a one hidden layer Bayesian neural network, under certain conditions, converges to a Gaussian process as the number of hidden units approaches infinity."
1296,1,".\n\nThe work focuses on studying \""non-saturating\"" GANs, using the modified generator\nobjective function proposed by Goodfellow et al. in their seminal GAN paper, and\naims to show increased capabilities of this variant, compared to the \""standard\""\nminimax formulation."
1297,3, Being less grandiose would make the value of this article nicely on its own.\n*
1298,2,It feels a little bit like someone wanting to run a series of statistics.
1299,3," If yes, then it's not very surprising that adding the named entities to the vocabulary leads to overfitting."
1300,3,"\n\nIf the authors wish to push their dataset, it would help to first evaluate the quality of the dataset."
1301,1,\n\nOverall this is an interesting paper
1302,3," \n=======\n\nThis article examines the two sources of loose bounds in variational autoencoders, which the authors term \u201capproximation error\u201d (slack due to using a limited variational family) and \u201camortization error\u201d (slack due to the inference network not finding the optimal member of that family)."
1303,3,n\nThey evaluate the methods on several metrics.
1304,3," Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10.\n"""
1305,3,"\n\nThis paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, \u201cLearning to Navigate in Complex Environments\u201d) and one of the architectures (NavA3C+D1D2L) from that paper."
1306,3,Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. 
1307,3, This is not because the proposed model is technically difficult to understand.
1308,3," Instead, one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN? (This would also clarify the paper's contribution.)"
1309,3," Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?"""
1310,3,"\n\n6) Top of p.5: the sentence \""Since we need tilde{beta} to satisfy (...)\"" is currently awkwardly stated."
1311,3," More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights."
1312,3,"\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained."
1313,3,.\n- Some of the used metrics can detect mode collapse.
1314,3,"\n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May."
1315,3," As stated previously, the approach is validated with a large number of real Android projects\n4)"
1316,3," It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5."
1317,3,  What is the heuristic here?
1318,3,"""The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise."
1319,3,"\n- What is a \""sequence-level variant of CTC\""?"
1320,1," Furthermore, a clear description of your \u201cpull\u201d configuration (such as in Figure 1) i.e."
1321,3," Pixel saliency, in this case, is defined as the partial derivative of the output of the classification network with respect to each input pixel. "
1322,3," \n\nThe practical implementation section losses some of this clear organization, and could certainly be clarified each part tied into Algorithm 1, and this was itself made less high-level. But these are minor gripes overall."
1323,3,\n\n1. Why is this type of color channel modification relevant for real life vision?
1324,3,\n- \u201cthe gradient at s_{t+1} that will change the value the most\u201d  - This is too colloquial.
1325,1, The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow.
1326,3, I can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model.
1327,3, What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}?
1328,3," However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used)."
1329,1,  It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps. 
1330,3," Without a proper motivation, its difficult to appreciate the methods devised."
1331,3,"\n3.) Generalization to composite commands, where  a part of the command is never observed in sequence in the training set."
1332,3, This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017).
1333,3, \n\nThis paper takes this idea and applies it to deep neural networks.
1334,3,"\nThe basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c."
1335,3," On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines."
1336,1," By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels."
1337,3," Like several existing geometric CNNs, convolutions are performed on each point using nearest neighbors."
1338,3," A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens."
1339,3,"""This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution."
1340,3,"""The paper presents an extensive framework for complex-valued neural networks."
1341,3, Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic?
1342,1, \n\nThe quality and clarity of the paper can be improved in some sections
1343,3,"\n- What is the average number of sentences per document?[[CNT], [CNT], [QSN], [MIN]] It's hard to get an idea of how reasonable the chosen truncation thresholds are without this."
1344,2,"I have 3 main objections to this paper: it is self-contradictory, it's functionally obsolete, and it's been submitted to the wrong journal"
1345,3," The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word)."
1346,3,"\n\n[4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015"
1347,3," With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate."
1348,2,"Measurements were made, but why, besides a teaching exercise, remains obscure"
1349,3," In Section 3, a few methods from the literature are classified according to the proposed taxonomy."
1350,1,"\n\nI think this is good and potentially important work,"
1351,3," \n\nHowever, there exist several major issues which are listed as follows:"
1352,3," If we are serious about evaluating image realism and working towards passing the visual Turing test, we should report results without an artificial time limit."
1353,1,\n\nThe paper is well-organized and easy to follow.
1354,3, Have you run some experiments where you vary those parameters?
1355,3, It is then claimed that the generated images show that the network has learned good latent representations.
1356,2,"Let me expand, using an analogy. "
1357,3," Does it have some support?\n"""
1358,3,"""This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based)."
1359,1,\n\nReview:\nThe manuscript describes the proposed algorithm in great detail and the description is easy to follow.
1360,3, Now what do we do with this list?
1361,3," Note that exposition in Le and Zuidema (2015) discusses the pruned case as well, i.e., a compete parse forest."
1362,3, Also is the reference sentence for skateboard example typo-free?\
1363,1,  \n\nSignificance\n- Trust-PCL achieves overall competitive with state-of-the-art external implementations.
1364,3, The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy.
1365,3,"\nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved."
1366,3,"\n5. For the experiments on synthetic datasets, workers are randomly sampled with replacements. Were the scores reported based on average of multiple runs."
1367,3," Recently, it has been shown that averaging can be suboptimal for nonconvex problems, eg a better averaging scheme can be used in place [3]."
1368,2,"The quality of the data need to improve substantially. The data shown are not compelling, lack clarity, and do not support the conclusions."
1369,3, \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe).
1370,1,"Pros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\n"
1371,2,"The turn of phrase is just a vague rhetorical equivocation that is meant to add a semblance of an intellectual air to the text, a rhetorical attempt at being in fashion, but it dissolves only in empty intellectualism."
1372,3,"\n\n\nMinor Comments:\nSection 3.1, First Line. \u201df(ul(g(x),y))\u201d appears to be a mistake."
1373,3," For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices."
1374,1,"\n\nI increased rating for the paper,"
1375,3,"\n\n3. In section 5, the authors make a connection with the work of Ge et al. 2016."
1376,3," Further, the first two \""observations\"" in Section 2.2 would be more accurately described as \""intuitions\"" of the authors."
1377,3,"The experiments seem extensive, using many recently proposed RL methods, "
1378,3, The paper also introduces an attention method over chart cells.
1379,3," The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task."
1380,1," This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets."
1381,3," Also, several parts of the paper spend too much time on describing other work or on implementation details which could be moved to the appendix."
1382,2,Very very sloppy
1383,3," Specifically, the paper only parametrizes the Q function, and computes the policy gradient using the relation between the policy and Q function (Appendix A.1)."
1384,1, \nThe synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method.
1385,3, How many times did you run node2vec on each graph? \n
1386,3,  How does the performance change with the number of frames between checkpoints?
1387,3,"""This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network."
1388,1, I enjoyed reading the paper which is in general clearly written.
1389,3,\n\nThe experimental results show the results without the cyclic loss.
1390,3," However, I am not sure if the confusion is solely mine, or shared with other readers."
1391,3," For example: \n(1) The step number k is dynamically determined by a short line search as in Section 4 ``Dynamic Rollout\u2019\u2019, but later in the experiments (Section 6) the value of k is set to be 2 uniformly."
1392,3," The network is to be evaluated over a private input, so that only the final outcome of the computation-and nothing but that-is finally learned."
1393,3," The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example."
1394,3,"\n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3."
1395,2,A different paper would be a good paper
1396,1,"  Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible. "
1397,3, \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip.
1398,3," To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages."
1399,3,"  The implicit steps have the advantage that the choice of step-size is replaced by a choice of a proximity coefficient, which the advantage that while too large step-size can increase the objective, any value of the proximity coefficient yields a proximal mapping guaranteed to decrease the objective."
1400,3,"\n\n* \""Note here that, although we explicitly input an occupancy map to the master agent, the actual infor-\nmation of the whole system remains the same."
1401,3,   \n\nThirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1.
1402,3,For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials?
1403,1,\n\nStrengths:\nThe model and the mixed objective is well-motivated and clearly explained.\nNear state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard).
1404,3," The application/setting may be novel,"
1405,3, The structure allows for fast inference using a spectral approach.
1406,2,The paper is ill-informed and poorly argued. It is not suitable in my view for this or any other...
1407,3,"""This paper provides visualizations of different deep network loss surfaces using 2D contour plots, both at minima and along optimization trajectories."
1408,1,\n\nReview:\nQuality: The quality of the work is high.
1409,3, It would require very major editing to be fit for publication.
1410,1," However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands."
1411,1, The use of an auxiliary loss to control the number of state updates is interesting;
1412,2,I want to vomit; I cant believe this paper was submitted.
1413,1," Furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion."
1414,1,.\n2. The experiments are solid to demonstrate this method works very well
1415,3," By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility"
1416,3,"  On SQuAD dataset, their results show some small improvements using the proposed augmentation technique."
1417,3,"""The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks.Each word is represented  by a diagonal Gaussian and the KL divergence is used as a directional distance measure."
1418,3,\n \n \n6 Experiment\nsetare -> set are
1419,3," However: there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to \""lucky trial\""."
1420,3,"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper.pdf"""
1421,1,\n-\tThe generated examples are in some cases useful for interpretation and network understandin
1422,3,  d\n\nDefinition of u (eq. 3) => v?
1423,3, Could the authors comment on this potential limitation?
1424,1, The approach is thoroughly validated using two online behavioural experiments.
1425,1,"  The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). """
1426,3, Is it over multiple runs or within the same run?
1427,3,"\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results."
1428,1, The resulting DCN+ model achieved significant improvement over DCN.
1429,3,\n\nSome minor notes\n- 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question.
1430,1," The ERM for active learning has been investigated in the literature, such as \""Querying discriminative and representative samples for batch mode active learning\"" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper."
1431,2,This book has more mistakes than a hound has fleas
1432,3,"  I would like the authors to consider the following questions - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few)."
1433,3, Does it reduce the excess representational power compared to the LSTM cell that could result in better models?
1434,3," Instead, the second model could have been proposed directly, with the appropriate citation from the literature, since it isn't new."
1435,3,"  \n- Influence of the clustering method on VCs, eg k-means, gaussian, von-mises (the last one is proposed)?"
1436,3,"\n\n3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (\""it succeeds on 28% of the samples on MNIST;73% on CIFAR-10\"")."
1437,1,  The generated textual samples look good and offer strong support for the model.
1438,1," There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration."
1439,3," Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact."
1440,1," \n\nOverall, I think it is really an interesting direction and the proposed method sounds reasonable."
1441,2,"Since the manuscript is so lacking in all aspects, I wont bother going through it in detail."
1442,3, It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3).
1443,3," As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution."
1444,1," The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced)."
1445,3," Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition."
1446,3,The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.
1447,3," Indeed, this largely regulates the hard problems (i.e., controlling the low-level actions of the vehicle while avoiding collisions) to a separate controller."
1448,3,"""This paper presents simple but useful ideas for improving sentence embedding by drawing from more context. "
1449,3," Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first."
1450,3," Would different communities have different choices of T?[[CNT], [CNT], [QSN], [MIN]] \nh)\tAnd a related question, how well can the method generate the inter-community links?"
1451,3, The authors provide both theoretical and experimental validation of their idea.
1452,3, Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space? 
1453,3,\n\n2) Report results on well-known CF datasets.
1454,3," \n\nAdditionally, when I first read the paper I thought that the ZS1 experiments\nfeatured no QA training at all."
1455,3," \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing."
1456,3,"""The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations."
1457,3," For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI)"
1458,3,"""This paper presents a lifelong learning method for learning word embeddings."
1459,3," \n\nAfter rebuttal:\nThe current version of the paper still needs significant amount of work regarding the experimental part."""
1460,3, This paper proposes a method to uncover this structure from the filters of a trained ConvNet
1461,1," The idea is quite straightforward, and the paper is relatively easy to follow."
1462,2,So many electrons worked so very very hard on this paper
1463,3," The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017),"
1464,3, More details in the comments below.
1465,3,\n\t2.2) A synthetic dataset is only considered.
1466,1,"""This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs."
1467,3," Could the output grammar be extended to support joins, for instance? "
1468,1," The experiments, however, show clearly advantages of the approach "
1469,3,"\n\nTypos :\n- In Eq 3, the numerator has z_t. Should that be z_y ?"
1470,3,"  If so this seems like it may be a significant constraint that would shrink the application space and impact even further.\n"""
1471,1," \n\n---\nThe additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted. \n"""
1472,3," This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*."
1473,3, They both could be mini-batched similarly.
1474,3,. This is not a very challenging task.
1475,1, while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification.
1476,1,"  \n\nOverall, I think the paper proposes an interesting environment and task that is of interest to the community in general."
1477,3, A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.\n\n
1478,1," In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups."
1479,3, The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.
1480,3,". \nOnly whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task"
1481,3," \n\nFor the theory, there are a few steps that need clarification and further clarification on novelty."
1482,1,. It will be useful to also provide time to a certain accuracy that all of them get to e.g. the validation error of 0.1609 (reached by the 3 important cases).
1483,3,\n\n5. Proof of lemma 3.1.\nI found it's hard to keep track of which one is inside the expectation.
1484,3,"""I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision."
1485,3," The basic idea of the lambda return assumes TD targets are better than MC targets due to variance, which place more weight on shorter returns."
1486,3," \n\nTo understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines."
1487,3,"""This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization."
1488,3," Dead-reckoning (i.e., spatial localization from velocity inputs) is of critical ecological relevance for many animals."
1489,3," The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset."
1490,2,I feel the results are insufficiently counter-intuitive to warrant publication in [fancy journal]
1491,1,  The experiments are complete.
1492,3,"\n- the filtering of high-reward trajectories is what estimation of distribution algorithms [2] do as well, and they have a known failure mode of premature convergence because diversity/variance shrinks too fast."
1493,3,"  Instead, using only one set of test parameters, the authors compare their algorithm to a \u201cgreedy baseline\u201d policy that is specified a \u201calways try to change lanes to the right until the lane is correct\u201d then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front."
1494,3, The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.
1495,3," \n\n\nQuestions:\n- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor?"
1496,3,". Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released."
1497,3," Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection."
1498,3," When combining different constituents, an energy function is computed (equation 6) and the resulting energies are passed through a softmax."
1499,3," But if so, then these vectors aren't sparse at all\n  as most values are non-zero."
1500,3," In the second line, shouldn't the first term be \\tilde{\\mathcal{L}}(W_0 + \\Delta W) ?"
1501,3,\n\nThe authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*.
1502,3," \n - Page 2: \u201cthe neural network computes the probability for other examples not in the subset\u201d[[CNT], [null], [QSN], [MIN]] \n - Page 3: \u201cthe probability of all the examples conditioned on"
1503,3,\n\n+ This is more of a suggestion.
1504,1," They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models."
1505,3, It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy.
1506,1,"\n\nOverall the work is important, original, well-executed, and should open new directions for deep learning in program analysis."
1507,1,"\n\nThis paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission."
1508,3," To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images."
1509,3,"""The paper focuses on a very particular HMM structure which involves multiple, independent HMMs."
1510,3, The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations).
1511,3," The paper first argues that achieving privacy guarantees like differential privacy is hard, and then provides frameworks and algorithms that quantify the privacy loss via Signal-to-noise ratio."
1512,3," By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin)."
1513,3,    \n The final presence values though are sampled using Gumbell-softmax.
1514,3, The paper just told difference stories section by section.
1515,3, Is it because there is a lot randomness in the stochastic convolutional layer?
1516,3,". A neural network is already trained, and its weights are public."
1517,3, The covered framework is limited to regularization parameters.
1518,3,"\n\nMinor comments:\n- Sec. 3: \""Language is inherently tree structured\"" -- this is debatable..."
1519,3," For one, the problem is solvable with breadth first search in O(L^2) time, for an L x L maze."
1520,3," The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'."
1521,1," The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods."
1522,1," The approach is evaluated on an in-house dataset and a public NIH dataset, demonstrating good performance, and illustrative visual rationales are also given for MNIST."
1523,3,"\n(3). How do you choose the parameter \\lambda in Equation (2)?\n"""
1524,3, \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.
1525,3,"\n\nThe authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline."
1526,3,"""Typical recurrent neural networks suffer from over-paramterization."
1527,3, \n2.\tThere is another work that also considers the target-context interaction using interactive attention model.
1528,1,\n\nPositive aspects:\n+ The idea of using GANs for this goal is smart and interesting
1529,1,"\n \nTo conclude, while the general direction is interesting and the proposed method might work, the experimental evaluation is very poor, and the paper absolutely cannot be accepted for publication."""
1530,1, The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.
1531,3,"""This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions. "
1532,3,"\n\nComments:\n1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer?"
1533,3," Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided,"
1534,3," In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better."
1535,3,"""This paper analyzes the expressiveness and loss surface of deep CNN."
1536,3,", and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples."
1537,3, Why authors choose different set of benchmarks?
1538,1,\n\n+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization.
1539,3, I will detail concerns for the specific experiments below.\n\nSection 4.1:\n- How does held-out data fit into the plot?
1540,1,"\n\nWhile the idea is interesting and might be a good alternative to standard CNNs,"
1541,3," And even with a single block, does it matter what permutation you use?"
1542,3, It seems as a more natural way to do it.
1543,3,"\n\nIn Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three\npossible SGD schedules: * increasing batch size * decaying learning rate *\nhybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show\nthat across a range of SGD variants (+/- momentum, etc) these three schedules\nhave similar error vs. epoch curves."
1544,1,"The use of the proposed gamma distribution, as a simple alternative, overcomes this problem."
1545,3,"  Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \""A sensitivity analysis of (and practitioners guide to) CNNs...\"" Zhang and Wallace, 2015.)"
1546,1," \n\nClarity\n=====\nThe paper reads well,"
1547,3," \n\n4. The \""CVT, no noise\"" should be compared to \""CVT, random noise\"", then to \""CVT, adversarial noise\"". The current results show that the improvements are mostly from VAT, instead of CVT. \n\n\n"""
1548,3," however, I would like to see more experiments."
1549,1," The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training."
1550,1,"""# Summary and Assessment\n\nThe paper addresses an important issue\u2013that of making learning of recurrent networks tractable for sequence lengths well beyond 1\u2019000s of time steps."
1551,1,\n\n[Pros]\n- Interesting problem
1552,2,I wasn't sure which problem the author is solving &amp; vice verse it wasn't clear what problem the solution is intended to solve or explorer
1553,3, \n\nTheorem 4.1 seems to be implied by Theorem 4.2.
1554,3,"""This paper proposes to use 3D conditional GAN models to generate\nfMRI scans."
1555,3, \n\nOne challenge in assessing the experimental claims is that practical neural networks are nonsmooth; the quadratic model developed from the hessian is only valid very locally.
1556,1,\nI find the paper interesting
1557,3," But, in the experimental section results are shown only for a  single value, alpha_new=0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested."
1558,3,"\n\n--------------\nWeaknesses:\n--------------\n\n- Perhaps I'm missing something, but shouldn't the Single EN-DE/DE-EN results in Table 2 match the not pretrained EN-DE/DE-EN Multi30k Task 1 results? I understand that this is perhaps on a different data split into M1/2 but why is there such a drastic difference?"
1559,3,"""Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks."
1560,3, \n\n- How robust are the eigen options for the Atari experiments?
1561,3, The key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the training.
1562,3,\n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?
1563,3,"""This paper is an extension of the \u201cprototypical network\u201d which will be published in NIPS 2017."
1564,3,"\""; and \""[...] the action a_0 should be similar to actions sampled from pi_theta(a|s).\"" What do you mean \""should\""?"
1565,3, It would be interesting to show results on sensitivity to the number of updates (p).
1566,3,"\""\n\""experiments and theroy analysis are done\""\n"""
1567,3, The latter is important for the GAN-like training.
1568,3," Otherwise, r_1(.) for each point is 0, leading to a somewhat \""under-estimate\"" of the true LID of the normal points in the training set."
1569,1,\n\n - The paper shows that the results are plausible using a neat trick.
1570,3,"""The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid."
1571,3, \n(4) the idea of using surrogate labels to learn representation is also not new.
1572,3,\n\nA few remaining questions for the authors:\n* There is a parallel submission (presumably by different authors called \u201cResidual Connections Encourage Iterative Inference\u201d) which contains some related insights.
1573,3," As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable,"
1574,3,"\n\n[1] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. A semantic matching energy function for learning with multi- relational data. Machine Learning 1\u201327."
1575,1,"\n\nSignificance: although the application of L2S to RNN training is not new,"
1576,3, This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task.
1577,1,"  I agree with your pair of sentences in the conclusion: \""Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning."
1578,2,This paper is written.
1579,1," For the most paper, the paper is clearly-written, with each design decision justified and rigorously specified."
1580,3," Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them."
1581,3," \n\nFirst, the contributions need to be more clearly spelled out."
1582,1,"The paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network. "
1583,3,"  Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise."
1584,3, This makes me feel that there is large room to further advance the paper.
1585,3,\n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments?
1586,3, I didn't like that two types of experiment are now presented in parallel.
1587,1,"\n\nIn contrast to the theoretical part, the experiments seems very encouraging."
1588,3, So it is not clear if the proposed method can make a real difference on state of the art systems. 
1589,3, More iterations should be taken and the log-scale style figure is suggested.
1590,3, Or other component of DeepLab?
1591,3,"""The problem of interest is to train deep neural network models with few labelled training samples."
1592,3," The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case."
1593,3,"\n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation."
1594,3," More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper)."
1595,3, The paper is an extension of Kawaguchi'16.
1596,3,"  I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2"
1597,3,"""In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S."
1598,3,"\"" Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?\n\n* "
1599,3," Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions."
1600,2,"High was my expectation, and so much deeper was my disappointment"
1601,3," I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission.\n"""
1602,1,"Overall I think your intuitions and ideas are good,"
1603,1,\n\nPros and Cons\n============\n+ good results
1604,3,"\n3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?)"
1605,3,"\n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks."
1606,2,This work is only a maths game.
1607,3, This is particularly apparent in the empirical study.
1608,3,"""This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data."
1609,2,"The result does improve the state-of-the-art, but it is not strong enough for acceptance"
1610,3,"""**Summary**\nThe paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions."
1611,3, It seems that the validation error is still decreasing after 25 epochs?
1612,3," Such contribution can, in my opinion, be summarized  in a potential of the form\n\nwith\n\n$$\nR_BRE = a R_ME+ b R_AC = a \\sum_k  \\sum_i s_{ki}^2   +  b \\sum_{<k,l>} \\sum_i \\{ s_{ki} s_{li} \\}   \n$$\n(Note that my version of R_ME is different to the one proposed by the authors, but it could have the same effect)\n\nWhere a and b are parameters that weight the relative contribution of each term  (maybe computed as suggested in the paper)."
1613,3," \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. "
1614,2,there is no evidence to show this is the first paper to propose the idea
1615,3," \n\nThe expression \n\n   h_{aid} ( \\phi(0), v )   =   <  Wa,   [ \\phi(o) ; v ]   >   +   b\n\nmakes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions)."
1616,1,"\n\nThe task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained."
1617,3,"As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful."
1618,3,"\n\n- How the MDP M and options are defined, e.g. transition functions, are tochastic?"
1619,3, I first thought that performance would be better when the generator's encoder uses the unmasked sequence.
1620,3,\nPIB pursues the very natural intuition outlined in the information bottleneck literature: hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y.
1621,2,The take home message has to be extracted with significant labor from a punishing set of figures with multiple bar graphs
1622,3, Entire technique is explained in a short section-3.1 with many important details missing.
1623,1, \n\nClarity: The paper is well-written. 
1624,3," I.e. 1 or 3 feed-forward networks for age, zip code, and release dates?"
1625,3," The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient."
1626,3,"  The result, spikes having an exponentially decaying effect on the postsynaptic neuron, is similar to that observed in biological spiking neurons."
1627,2,"The question is not very clear, the analysis is not very thorough, and the conclusions are rather trivial or even self-contradictory"
1628,3,"\n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications."
1629,1, The following section (the part starting from 5.3) presents the key to the success of the proposed measure.
1630,3,\nLe et al. (2015) for instance perform a coarse grid search for each model.
1631,1,"\n\nThe paper has some interesting contributions and ideas, mainly from the point of view of applications, since the basic components (convnets, graph neural networks) are roughly similar to what is already proposed."
1632,3," Despite being a simple approach, the experimental results are quite promising."
1633,3, the approach shows that the proposed methods converge faster than existing methods.
1634,3," A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}."
1635,1, This opens nice perpectives for better and faster inference.
1636,3,\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN).
1637,3,"""The paper attempts to extend the predictive coding model to a multilayer network."
1638,1,"\n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text."
1639,3, \n\n- Only single runs for the results are shown in plots.
1640,3,"  The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art."
1641,3,"""This paper surveys models for collaborative filtering with user/item covariate."
1642,3,"\n\n1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a \""discrepancy\"" to me."
1643,3, Results are examined mainly by looking at the first two PCA components of the data.
1644,3, Did the authors try any values besides 0.5?
1645,3, The hard concrete distribution is a small but nice contribution on its own.
1646,3,\n\n- The results for the perceptual complexity experiment seem contradictory and inconclusive.
1647,3," A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure."
1648,2,(on a computer simulation paper) Did you use software?
1649,2,"I am sure that you would find what you expect, but I question its value as research"
1650,3,"\n\n- For experiments, they apply k-means clustering in the process so k is one parameter to tune. K needs to be tuned on validation set instead of testing set. "
1651,3, The reference to a GAN architecture seems very forced and out of the scope.
1652,1,"\n\n(3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train)"
1653,1,"""Overall I like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image, super-resolution, etc."
1654,3,  Combining policy gradient and Q-learning.
1655,3," \n\nOn the experimental side, to draw the conclusion, \""weighted sum\"" is enough for LSTM."
1656,3,"\n\nSection 3.2.2 shall be placed later on, and clarified.[[CNT], [CLA-NEU], [DIS], [MAJ]]\n\nDiscussion on mixing more than two sounds leads could be completed by associative properties, we think... ?\n"""
1657,3," \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?"
1658,3,"""() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. "
1659,3,. There exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed setting. 
1660,1,"\n\nOverall the paper is well-structured and related work covers the relevant papers,"
1661,2,"I stopped reading the subsequent data reports carefully, because I no longer had confidence that they would be accurate"
1662,3, Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks.
1663,3," The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017)."
1664,3, Why is paying to observe activations the one chosen here?
1665,3, \n* Formatting of figure 8 needs to be fixed.
1666,3,.  The other issue concerned the validation of the approach on databases other than MNIST.
1667,3,\n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class.
1668,3,\n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications?
1669,3," The previous SotA result on VGG16 was 5x acceleration with 1% accuracy drop, and here the reported result is 6.2x acceleration with 1.2% accuracy drop."
1670,1,"""The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption."
1671,3," ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention)."
1672,3,"""Thanks for all the explanations on my review and the other comments."
1673,1,"\n\nThe paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed."
1674,3,. How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network?
1675,3,"   The mistake is repeated for the following two terms. \n\n- Equations 3 and 4 suggest that despite their ordinal structure, sentiment labels are treated as unstructured at predict time."
1676,1, It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods.
1677,2,"Can you explain this part a bit further, but without going into detail."
1678,3," While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties."
1679,3, \n3-\tSo the main benefit of the approach seems to point towards the direction of what possibly happens in real brains.
1680,3,"""In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data."
1681,3, The question is not \u201cwhat\u2019s it take to get to 0 variance\u201d but \u201chow quickly can we approach 0 variance\u201d.
1682,3," But comparing to [1], this paper has limited contribution."
1683,2,"this study would have been really interesting 10-15 years ago, but not it seems quite out of date."
1684,3,"""This paper tried to analyze the subspaces of the adversarial examples neighborhood."
1685,1," The use of canonical coordinates is certainly a sensible choice (for the reason given above),"
1686,3,"\n[ ============================== END OF REVISION =====================================================]\n\nThis paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). "
1687,1,  \n\nQuality: The paper is good
1688,3, It would it make more sense if the comparison was between VGG and ResNet or other\ndifferent deep structures.
1689,3," It is unclear from the text if this is the case.[[CNT], [CLA-NEG], [CRT], [GEN]] \n                   The authors should do a better job explaining and comparing the overall experimental results."
1690,3,"However, it could benefit from additional details and a deeper analysis of the results."
1691,3,"""The paper is generally clear, and proposes to use a convolutional autoencoder based on 3D meshes."
1692,1," Performance on training set by the best model is close to perfect (99.5%), so the model is really learning the task."
1693,3, One reason is convexity in W of the problem (2). Any other?
1694,3, Layers alternate between convolutional and sigmoidal.
1695,3,\n\npros:\n(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples.
1696,3," Here are two articles, one that provides a long and thorough discussion that is a definitive start to the literature, and another that is most relevant to the current paper, on applying pedagogical teaching to inverse reinforcement learning (a talk at NIPS 2016)."
1697,3," The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq. 2)."
1698,3,"\"" Probably fairer/safer to say: did not report results on Atari games.\n"""
1699,3,  Figure 4 reports layer spectra for SN and WN.
1700,1,"""--------------\nSummary and Evaluation:\n--------------\nThis work present a novel multi-agent reference game designed to train monolingual agents to perform translation between their respective languages -- all without parallel corpora."
1701,1," \nUsing the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation."
1702,3,"  I am sure this idea has been tried before in the 90s but I am not familiar enough with all the literature to find it (A quick google search brings this up: Reinforcement Learning of Active Recognition Behaviors, with a chapter on nearest-neighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997-045/node3.html)."
1703,3,"  However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root)."
1704,3,"Detailed descriptions of all instantiations even parameters and comparison methods\n\t\u2022\tSystem specified\n\t\u2022\tValidation method specified\n\t\u2022\tData and repository,"
1705,3,\n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.
1706,3,"  Even then, the authors approach selects the highest number of\nexamples (figure 4). "
1707,1,n\n- The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs.
1708,3," but, in my opinion, a description of the learning framework should be given in the paper.[[CNT], [SUB-NEG], [DIS], [GEN]] Also, a summary of the hyperparameters used in the proposed system should be given."
1709,3,\n\nThe main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.
1710,3,\n- Why is the final Cross-Entropy Loss so high even though the accuracy is >99% for the MNIST experiments
1711,1, They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet.
1712,2,"There is a very interesting story to be conveyed in this paper, but it is hidden behind layers of mud, obscured by poor writing"
1713,3," \n\nComments:\n-- Since the authors are using a pre-trained VGG for to embed each image, I'm wondering to what extent they are actually doing one-shot learning here."
1714,3," This is a limitation, as the model space is not as flexible as one would desire in a discovery task."
1715,3," In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices;"
1716,1, \n\nStrengths:\n- The paper is very well written.
1717,3," It begins by observing that in time-limited domains, an agent unaware of the remaining time can experience state-aliasing."
1718,3," The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves."
1719,1," The auto-encoder performs fairly well,"
1720,3,"  Is each feature being normalized to be zero mean, unit variance, or is each training example being normalized?"
1721,3, This text generation task is traditionally done using recurrent neural networks.
1722,1,"  The authors define the \""good\"" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans."
1723,1,"\n\nTechnically, the paper is sound."
1724,1, \nPros: Good empirical results.
1725,1, While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK.
1726,3, Is there any guidelines to choose \\tau?
1727,3, I recommend the authors to perform either proper literature review or cite one or two papers on the time complexity and their weakness.
1728,3," For example, we\u2019re just told some values that hyperparameters were fixed at for both tasks - how were these chosen (including for the baselines)?"
1729,2,This work is stuck in the past. The referee would rather talk about the future - some of the senior co-authors were the future once!
1730,1, Some of the experimental results\ndo a good job of demonstrating the advantages of the models.
1731,3,  I would have appreciated more intuition behind these approaches.
1732,3,\n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5.
1733,3," \n\nOther concerns:\n1.\tIt seems that one needs to train at least three embedding matrices: A, C, D which represent input embeddings, output embeddings, and interactive embeddings, respectively."
1734,1,"\n\n## Quality/science of experiments\nThe experimental results have been updated, and the performance of the baseline now seems much more reasonable."
1735,1," Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work!"
1736,3," I wonder if this is due to the very small batch size used (\""a small batch size of 4 \"").\n\n\n"""
1737,3,"\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE."
1738,3,"\n\nOther comments:\n - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a \""topic sentence\"" at the beginning."
1739,1, It has a very nice introduction and literature review of Prioritized experience replay
1740,3,"The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks."
1741,3,\n\n A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class
1742,3,"\n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \""ensemble voting\"" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?)"
1743,1," Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data."
1744,3, The paper could be more focused around a single scientific question: does the PATH function as formulated help?
1745,1,The authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching.
1746,3," The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce."
1747,3,"\n\n- Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me.[[CNT], [CNT], [CRT], [MIN]] It would be nice to expand this[[CNT], [null], [DIS], [MIN]].\n\n- Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right?"
1748,3, The second step also shrinks the number of targets over time to achieve clustering.
1749,3," Some questions are not clear from section 3.4:\n1.[[CNT], [CNT], [CRT], [MIN]] Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it?"
1750,2,The main problem is that it does not fit the intuition about what should work.
1751,3, It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.
1752,3, (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.)
1753,3, The authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new RL algorithms.
1754,3, They all seem to be equally capable.
1755,3, Am I missing sth?
1756,2,I suggest you consult a competent statistical advisor.
1757,3, \n\nIt is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text.
1758,3,"\n\n[Other comments]\n\n* \""Given this state of affairs, perhaps it is time for us to start practicing\n  what we preach and learn how to learn\""\n\nThis is in my opinion too casual for a scientific publication..."
1759,3,"""This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation."
1760,3," but it makes the model considerably simpler than any practical setting.\n\n"""
1761,2,You make the reader feel uneducated
1762,3,"\"".\n\n\"", disagreement emerge\"" -> \"", disagreements emerge\""?[[CNT], [CLA-NEU], [QSN], [MIN]]\n\nThe paper needs to include SOME definition of robustness, even if it just informal."
1763,3,"  \nBut, given that the method is solving a formulation that leverages second order information, it would seem reasonable to compare with existing techniques that leverage second order information to learn neural networks, namely BFGS, which has been studied for deep learning (see the references to Li and Fukushima (2001) and Ngiam et al (2011) below)."
1764,3, They additionally use their embeddings to decrease the search time for the Sarfgen program repair system.
1765,3,\n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance.
1766,3,"\n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating."
1767,3," For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w\u2019 \\geq w^{k/k\u2019}."
1768,3, Your work falls under a similar category.
1769,3," \n\n2. The approach:\n(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3."
1770,3, This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.
1771,3, One candidate for this is the image feature space learned by a deep network.
1772,3,"\nMoreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...\n\n"""
1773,1," In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper."
1774,3," See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has \u201cprivate\u201d latent variables for vision similar to this work (this is relevant to Sec. 3.2.)"
1775,3, The small step is in the direction of gradient when top class activation is taken as the objective.
1776,3," They conduct a complete experimentation, testing different popular RNN architectures, as well as parameter and hyperparameters values."
1777,3,"\n\nThe main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem\nby starting with a small set of examples, solving a constraint problem to get a hypothesis program,\nthen looking for \""counterexamples\"" where the hypothesis program is incorrect."
1778,3,"Currently, the authors mention in the conclusion that, as is known to often be the case with GANS, that the results were indeed sensitive."
1779,3," The model consists of three modules: encoder, interaction module and elimination module."
1780,1,\n\nOverall the paper is well written and polished.
1781,2,Too mathematically inclined.
1782,1," Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier."
1783,3, Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function.
1784,3," \n\nGiven the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable. "
1785,3,  All of their evaluation is based on the accuracy of defogging.
1786,3," And it is concluded in the final three sentences of the paper that the presented network \""can infer effective latent representations for images of other objects\"" (i.e., of objects that have not been used for training); and further, that \""in this regards, the network is better than most existing algorithms [...]\""."
1787,3, I am guessing the random walk transitions are the ellipsoids?
1788,2,I think the audience will eat him alive. But I want to be there to hear it.
1789,3,"\nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked."
1790,1," However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds,"
1791,3," . \n\n2) - Have you tried resnets generators and discriminators  at various depths , with padding so that the spatial resolution is preserved?"
1792,3,"\n4. Defining the loss w.r.t. the state, and taking the derivative of the state w.r.t. to theta is a bit odd."
1793,3, I suggest you remove them everywhere.
1794,3, The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work.
1795,3,"\n\n\nComments:\n\n- Why use a model-free technique like Q-learning especially when one knows the model of the car in autonomous driving setting and can simply run model-predictive control (MPC) (convolve forward the model to get candidate trajectories of certain reasonable horizon, evaluate and pick the best trajectory, execute selected trajectory for a few time-steps and then rinse-and-repeat."
1796,3," Unfortunately, all tables with the experimental results are left to the appendix."
1797,3," Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space."
1798,1, Can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work section.
1799,3," Specifically, it poses just little constraints and presents no stochasticity (options result in stochastic outcomes)."
1800,3,\n\n=========================\nUpdate after author rebuttal:\n=========================\nI have read the author's response and have looked at the changes to the manuscript.
1801,1, The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion.
1802,3, The paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work.
1803,3," In addition, if this was challenging to optimize, it'd be useful to include lessons for how the authors manage to train their model successfully."
1804,3, The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.
1805,3,"""The paper presents a reinforcement learning-based approach for program synthesis."
1806,1," I found the discussion about rank to be very intuitive,"
1807,3,"""Summary: This paper introduces a model that combines the rotation matrices with the LSTMs."
1808,3,"  The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning."
1809,1, I agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learning.
1810,3, So I gather our policy takes the form of a composed function and the chain rule gives close to their expression in 2.2
1811,3,". I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model."
1812,3, The main technique is to use UCB (upper confidence bound) to speedup exploration.
1813,3," While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs."
1814,1,"\n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions."
1815,1," \n\nOverall, I like this paper and think the underlying group-wise posterior construction trick is worth exploring further."
1816,3," In my opinion, selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it."
1817,1," Statistical tests are performed for many of the experimental results, which is solid."
1818,1,\n\nThis paper is clearly written
1819,3," The setup is slightly different from the original theorem in Hazan et. el., 2017 including the noise model, so I strongly recommend to include the original theorem in the appendix, and include the full proof in the appendix."
1820,3,\n  \n- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7.
1821,3,  \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA.
1822,3," \n\n[1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016"
1823,2,At first I thought this was a practical joke h/
1824,3, \u00bb: should \u00ab unless \u00bb be replaced by \u00ab if \u00bb?
1825,3, Does this create any instability in learning?
1826,1,"\n\n* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted."
1827,3,"\n- some of the other papers might use data augmentation, which would increase noise robustness (did not check, but this might explain some of the results in table 4)"
1828,1, I appreciated the set-up of the introduction with the two questions.
1829,3,"   You state that they are extracted from  64 beam candidates, are they unique N-best lists?"
1830,1,"\n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified."
1831,3,"\n\nThe point is, \n\na) The second term will introduce  low correlation in saturated vectors, then the will be informative."
1832,3, Experimental results are reported in a self generated data set.
1833,3,\n\n-------------------------\nI read the response and the new experimental results regarding WGAN.
1834,3,.\n\nThe selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries?
1835,3, The authors further propose model-pruning optimizations which are aware of the persistent implementation.
1836,2,"I cannot figure out what point you are making and why you have written this article. This version cannot be saved, so start again with a much clearer vision of what you are trying to say, and the point you are making."
1837,3, I could imagine a network learning to ignore features of objects that tend to wander over time.
1838,3,\n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them.
1839,3," The paper then discusses how SGD can be seen as an algorithmic way of finding minima with large \""evidence\"" --- the \""noise\"" in the gradient estimation helps the model avoid \""sharp\"" minima, while the gradient helps the model find \""deep\"" minima. "
1840,2,"That omission is a standard feature of articles from the fourth author's lab. Although it normally doesn't bite him, it does in this case."
1841,3,\n\n2) p 3 center -- this seems to be reinventing non-maximum suppression
1842,3," In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels)."
1843,3," Although by looking some features maps, this rule might be hold as shown in Fig.5."
1844,3,It may be interesting to explain the meanings of individual components.
1845,3," The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels."
1846,3,"  How much of the ideas offered in this paper would then generalize to non-resnet settings?\n\n"""
1847,1,\n\nThe experimental results shown in this paper are clearly compelling in exposing the weaknesses of current seq2seq RNN models.
1848,1,"\n\nOverall I think the trick needs to be motivated better, and the experiments improved to really show the import of the d-independence of the KL."
1849,2,The proposal is also poorly written and unfocused with only brief moments of meritorious thinking.
1850,1,\n\n* This is quite an interesting paper with a sensible goal.
1851,3," The basic idea is to count word pairs which co-occur with a preposition, rather than single words which co-occur, as in standard word vector models such as word2vec."
1852,1, The analysis with the toy network is interesting and helps illustrate the method.
1853,3,\u201d\n[f] \u201cwe are the first to evaluate any DRL-based navigation method on maps with unseen structures
1854,3,The original one or the modified one?
1855,3," More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance)."
1856,3," Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions."
1857,1," Moreover, I find the proposed algorithmic approach interesting."
1858,3, Please add information about how this critical value was generated.
1859,3,"\n\nAfter observing an experience ( S,A,R,S\u2019 ) we use Bellman Error as a loss function to optimize Qp for parameter p."
1860,1," \n\nPros:\n1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community. "
1861,1, The notation also make it easy to understand the differences between different models.
1862,3," It proposes a new dataset based on an artist's work, and compares existing methods in terms of the realism of the synthetic faces they can create."
1863,3,\nI am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist
1864,1,\n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure.
1865,3,"  The approach is validated using programming assignments from an online course, and compared against syntax based approaches as a baseline."
1866,3, The authors claimed to obtain efficiency improvement and better numerical stability.
1867,3,"""Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization."
1868,3, Could the proposed approach be adapted to such cases.
1869,3," The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free"
1870,2,I am personally offended that the authors believed that this study had a reasonable chance of being accepted to a serious scientific journal.
1871,1,\n- The visualizations of universal perturbations as they change during AT are nice.
1872,1,\n\nReview Summary:\nThe proposed technique is interesting and the experiments indicate its superior performance over existing techniques. 
1873,3," The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June)."
1874,1," I found it interesting to see what it does with the mixed signals of the word \""but\"": on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence."
1875,3, \n\n2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset.
1876,3, Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture?
1877,3,". Furthermore, it becomes clear, that without CNN structure no really good performance is achieved neither on CIFAR nor on ImageNet "
1878,3,  It avoids learning parameterized policies.
1879,3, It would be interesting to think about what further might be done here.
1880,3, \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf .
1881,1,\n\nHaving the luxury of some supervised episodes is of course useful.
1882,3,"\n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others?"
1883,2,I hope the authors can learn from this exercise on what is expected to craft a publishable paper. -Edito
1884,3,   It might help to show the distance between the actual model parameters that those algorithms converge to.
1885,3, This is unfortunately often the case when dealing with combinatorial search/optimization.
1886,2,"The study rationale is unclear, the exact research question to be addressed poorly delineated and the overall motivation for the study poorly linked to the literature summary and aims of the study"
1887,3,"\n\""\\pi_\\theta described in the previous sections\"", notation \\pi_\\theta appears the first time here...\n3x3 -> 3 \\times 3\nofB\nV_{\\theta^t w^t}[[CNT], [CLA-NEG], [DIS], [MIN]]\np.6:\nthe the\nFig.2's caption:\nWhat does \""both cases\"" refer to? They are three models."
1888,3," Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015)."
1889,3," Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer?"
1890,3," I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n"""
1891,3,"  The paper states the purpose of the meta learning is \""to learn a general word context similarity from the first m domains\"", but I was never sure what this meant."
1892,3, Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis.
1893,3, Is there some central limit theorem explanation?
1894,3, Is the neighborhood predefined?
1895,3,  In practical terms this is a linear combination of these graph\nconvolutional layers.
1896,2,This submission looks more like an advertising booklet rather than a research paper.
1897,2,â¦this paper is extremely lengthy and tedious with respect to its importance and relevance.
1898,3," In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method."
1899,3,"\n\n[1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio"
1900,3," However, in the implementation, it is restricted to skip connection by addition."
1901,3,"\n3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from?"
1902,1, \n\nThe proposed technique sounds.
1903,1," The theoretical analysis is carried-out very nicely, and the theory is supported by experiments on two-dimensional toy examples, and Fourier-spectra of the iterates of SGD."
1904,3," Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required."
1905,1,\n\nClarity:\n\nThe paper is clearly written and easy to follow and understand.
1906,3,\n- Another nice thing about differential privacy and cryptography is that they are impervious to different algorithms because it is statistically hard or computationally hard to reveal sensitive information.
1907,3,. What is the main benefit of the proposed mechanism compared to the existing ones?
1908,3," To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells."
1909,3," Using Bayesian Optimization, search over this space can yield decodings with targeted properties."
1910,3, Earlier in the work (e.g. before experiments and discussion) would allow the comparison with MMD to inform the context of the introduction.
1911,2,This is all science done by wishful thinking. -..
1912,3,"If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution)"
1913,2,So much trash belonging to the worst school of Bedlam literature â since Mr. Melville seems not so much unable to learn as disdainful of learning the craft of an artist. [original review of Moby Dick
1914,3," However, it would also be useful to do an ablation study of the \u201cfactorization\u201d of action values. "
1915,3, Why training a network to predict 2D spatial location from velocity inputs?
1916,3, This average is then fed to a softmax layer for answer prediction.
1917,3," This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described."
1918,3," To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments."
1919,3,"  \nThis especially true for adversial perturbations, which have been used as test cases in this work."
1920,3,Could go either way since the network\n   has to allocate resources to learn other games too.
1921,3,"\n\nCons:\n\u2022\tHard to replicate experiments without the deep computational pockets of DeepMind.\n"""
1922,3," Also, other FDA approaches for operator learning should be discussed and compared to the proposed approach.\n"""
1923,3," However, such batch approaches for tensor factorization are not new and I am quite skeptical about their correctness (see above)."
1924,3,\n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X.
1925,3,". Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions"
1926,3,\n\nMy concern is that one-bit system is already complicated to implement.
1927,3,"This is\ncryptic, just show us that this is the case.[[CNT], [null], [DIS], [GEN]]\n\nRegarding the experiments there needs to be more discussion about how the\ndifferent model parameters were determined."
1928,1," From the analysis in the main paper, I believe the theoretical contribution is correct and sound."
1929,3, The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).
1930,3," Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well."
1931,3,\n2. The authors made an essential assumption that all target samples have the same style embedding y*.
1932,3,"  The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1.\n"""
1933,3," \n\nThe mutation is a method to sample individual independence of the objective function, which is very different with the gradient step."
1934,1,\n4. This would be a very useful tool for the community if open sourced.
1935,3," If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples."
1936,3, The first baseline is a sanity check to ensure that you are not observing some random effect.
1937,1,"\n\n\nComments:\n\n-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect"
1938,3," \n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}."
1939,3," \n\n On one hand, I would suggest that this work would be better placed in an engineering venue focused on fluid dynamics."
1940,1," \n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin."
1941,1," The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that \u201cDQNs  are susceptible to periodically repeating mistakes\u201d."
1942,3," \nFor relating properly to the literatue, the experiment for speeding up Hyperband should also mention previous methods for speeding up Hyperband by a model (I only know one by the authors' reference Klein et al (2017))."
1943,1,\n\nThe paper is clear to follow and the objective employed appears to be sound.
1944,2,I would advise the authors to go back to the drawing board and consider exactly what this paper is to do and do this well
1945,3,"\n  As such, I think it's a reach to claim the model is learning interpretable topics."
1946,3," For example:\n\n1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning."
1947,3, The \u201clooks linear\u201d initialization proposed in \u201cThe shattered gradients problem\u201d (Balduzzi et al) implies that alpha=0 may work better.
1948,3,"\n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously."
1949,3,"  If such were present, I'd rate this paper significantly higher."
1950,3, This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack.
1951,3,\n \nwhat is the std for CartPole in table 1
1952,3,"""Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two."
1953,3,"  Journal of Machine Learning Research, 17(187): 1-25, 2016."""
1954,3," but would hope another reviewer is more familiar with the specific application domain than I am."""
1955,3,"""This paper proposes to improve time complexity of factorization machine."
1956,3," It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions."
1957,3, However the description of the reinforcement learning step could have been made a bit more clear.
1958,2,The authors should at least try to read some of the previous work in the field before attempting to solve our problems.
1959,3, This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1).
1960,3,"""\nThis work proposes to study the generalization of learning neural networks via the Fourier-based method."
1961,3," Even assuming this is a meaningful task, surely the natural baseline would be to treat these phrasal verbs as non-compositional (e.g. extend the vocab with words like \u201csparked_off\u201d)  and train Word2Vec."
1962,3," Furthermore, such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation (3)."
1963,3," theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used)."
1964,3,"  This expression is used to develop a new layer called a \u201cwarp layer\u201d which essentially tries to compute several layers of the residual network using the Taylor expansion expression \u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with."
1965,3,"""This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses."
1966,3," Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled."
1967,3,\n- Sec 4.6: The explanation for why the accuracy drops for all models is not clear.
1968,3,"\n\n5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits?"
1969,3,"\n\nThe paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies."
1970,3, Method description is clear.
1971,3,"\n\n \""4/ We discuss how to use these insights to improve the design of WGANs more generally."
1972,3,"  This paper is more about classifying Tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional state. \n\n\n\n\n\n\n\n\n\n\n\n"""
1973,3," but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage?"
1974,3, This projected input is then used to produce the classification probabilities.
1975,3,  But is there a pattern?
1976,3, but I think the paper would be a lot stronger and more convincing with some additional work.
1977,3,\n\nThe paper is quite well written aside from some grammatical issues.
1978,1,   Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations.
1979,3," This leads to a natural, testable notion of generalisation."
1980,1," \n\n** DETAILED REVIEW **\n\nOverall, this is a good paper."
1981,3,"\n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models."
1982,3,"\n\nRemarks\n------------\n\nThe main claim of the paper is that RNN are over-parametrized and take a long time to train (which I both agree with), but you didn't convinced me that your parametrization solve any of those problems."
1983,3,"""In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework."
1984,1,\n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance)
1985,3," Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data."
1986,3,"  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy."
1987,2,Where is research?
1988,3,"\n\n- Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused?"
1989,3,"\n\nUsing MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8]."
1990,1,"\n\nIn summary, the idealized model gives a good demonstration of the problem itself."
1991,3,"\n4. Under the experiments, different variations of Majority Vote, EM and Oracle correction were used as baselines."
1992,1, But I do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it.
1993,3,How many runs for evaluation?
1994,3, The authors discuss how this architecture can solve the problem exactly.
1995,3, What are these mixing conditions?
1996,3, Are you assuming isotropic diffusion? Is that realistic? 
1997,1, The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved.
1998,3," My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation."
1999,2,Appears to be on the perimeter of meaningful investigation.
2000,2,"Why dont you just send copies of this to the two people in the world who care about it, and forget the publication route?"
2001,3,"""This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits."
2002,1," The studied problem is interesting, and the paper is well-written."
2003,1, \n- Very interesting dataset to be released.
2004,3,"""Summary: This paper tackles the issue of combining TD learning methods with function approximation."
2005,3, What guarantees can we hope to achieve?
2006,3," A method for learning this is presented, and fine tuned with an actor-critic method."
2007,3," Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model."
2008,3,"\n\n========================================================================\n\nThis paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance."
2009,2,Was the white noise random?
2010,3,  The idea is to explicitly model interactions between aspects and words expressing sentiment about them.
2011,1,\n\nThe idea of applying the Fourier-based method to generalization is interesting.
2012,1,\n\nThe results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper.
2013,3,\n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks.
2014,3," Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be.[[CNT], [CNT], [DFT], [MIN]] A simple explanation in the introduction would improve the writing."
2015,3," In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem). "
2016,3," \n\n\nReferences\n[1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside\nconvolutional networks: Visualising image classification\nmodels and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n[2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581\n"""
2017,3," The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2."
2018,3, The parameters of the model are an embedding for each word and a local context unit. 
2019,1,"\nLikewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability,"
2020,3, \n \nAnother concern is that the novelty.
2021,3,"  If the emphasis of this conclusion is on  the number of images needed, then it will be good to show more analysis on Figure 6: e.g. Why does the accuracy curve drops to zero before going up?"
2022,3,\n\nExperiments\n\nYou should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training).
2023,3,"""This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree."
2024,3,"\""\nWhen approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored."
2025,2,"It is difficult to imagine any paper overtaking this one for lack of imagination, logic, or dataâit is beyond redemption"
2026,3,\n2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error.
2027,1," \n\nThis paper touches on many interesting issues -- deep/recurrent models of time series, privacy-respecting ML, adaptation from simulated to real-world domains."
2028,2,The manuscript shall be rejected in its current form if submitted for a journal
2029,3,"\n\nMy concerns are as follows:\n1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task."
2030,3, Do they (partially) share the incentive or may have completely arbitrary rewards?
2031,3," Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots."""
2032,1,"\n\nOverall this paper examines interesting structured and randomized low communication updates for distributed FL,"
2033,2,I find this submission confusingly written. The aims and objectives seem rather muddled. References are rather sparse.
2034,3,"""The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem."
2035,3,"\nHow about min, max ?"
2036,3, \nThe experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames.
2037,1, It seems like the method could be more informative than the other methods.
2038,1,"\n\nOverall, the paper seems to have both a novel contribution and strong technical merit."
2039,3, Is it due to the network morphing that preserve equality?
2040,3,\n\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization.
2041,3,\n4. Does the  Hungarian algorithm used for matching scales to much larger datasets?
2042,3,. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case
2043,3,"""The authors present a new variation of autoencoder, in which they jointly train (1) a discrete-space autoencoder to minimize reconstuction loss, and (2) a simpler continuous-space generator function to learn a distribution for the codes, and (3) a GAN formulation to constrain the distributions in the latent space to be similar."
2044,3,\n\nTable 2: It would be good to see standard errors on these numbers; they may be quite high given that they\u2019re only evaluated on 100 examples.
2045,3," Furthermore, in Section 4 a new method is proposed, that is to combine the best parts of the already existing models in the literature."
2046,3,"  \n- Since filter weight prediction forms the central contribution of this work, I would expect some ablation studies on the MLP (network architecture, placement, weight sharing etc.) that predicts filter weights."
2047,3," The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance."
2048,3, \n\nThe method is also very close to the simplest IRL method possible which consists in placing positive rewards on every state the expert visited.
2049,3,? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily.
2050,1,"\n- There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network."
2051,1, I am satisfied with the improvements to the paper and have changed my review to 'accept'.
2052,3," Therefore\ncompared with pure in-between-agent communications, MS-MARL is more efficient in reasoning\nand planning once trained. [...] "
2053,3,\n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?
2054,3," In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n"""
2055,3, What could explain this phenomenon?
2056,1,"\n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n"""
2057,1,"\n\nThe experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome."
2058,1," However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting."""
2059,3," From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration."
2060,3," b) which action to perform given the previous action and the world state,"
2061,3, Also large batches are used mainly during training where memory is generally not a huge issue.
2062,3,\n\nThe paper presents comparisons with baseline methods.
2063,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
2064,1, The ideas build up on each other in an intuitive way.
2065,1," \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest."
2066,2,this is where the real problems start
2067,3,"""\n* In the \""flat vs sharp\"" dilemma, the experiments display that the dilemma, if any, is subtle."
2068,1, I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering.
2069,3,"\n\nI'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC."""
2070,3," Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?"
2071,2,this pieceâ¦shows no real understanding of history at all
2072,1,\n3. Experimental performance is convincing.
2073,1,\n\nThis paper targets at a potentially very useful application of neural networks that can have real world impacts.
2074,3, Not a realistic setting.
2075,3," However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models."
2076,1,"\n\nThe decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs."
2077,2,Nothing new or ground breaking is discussed in this tutorial
2078,3,"\n-- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015.\n-- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016."
2079,3, What about other potential reward definitions?
2080,3, Some examples are:\n- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
2081,1,\n\nI think the methodology presented in this paper is neat and the experimental results are encouraging.
2082,1," The work is still great,"
2083,3,\n-- How is test set accuracy defined in section 5.3?
2084,3," \nRather than the \""baseline\"" epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy."
2085,3,"""This study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a 3-way factorization thereby identifying spatial and temporal modules as well as their signed activation."
2086,1," The technique sounds, the presentation is clear and I have not seen similar paper elsewhere"
2087,2,You're really funny.
2088,3,"\n\nSection 4.2:\n- For the second embedding, what exactly was the algorithm trained on?"
2089,3," Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity."
2090,3," The motivation in the appendix is very informal and no clear derivation is provided.[[CNT], [CLA-NEG], [CRT], [MIN]] The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states."
2091,3,"\n\nGiven labeled samples from a source domain and unlabeled samples from a target\ndomain, this paper proposes to minimize the risk on the target domain by \njointly learning the shift-invariant representation and the re-weighting \nfunction for the induced representations."
2092,3, This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference.
2093,3," To generate point clouds, they sample a latent code and pass it to the decoder."
2094,3, You optimize the coefficients of a polynomial. Did you try anything else?
2095,1,\n- Fairly good experimental results
2096,1, The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting.
2097,2,"This is a misleading paper, badly executed and negligently written"
2098,3,"  Given that this is n-best rescoring, how are the N-best lists generated?"
2099,3," \n\n- \""Table 4.1 compares these log likelihoods, with VHE achieving state-of-the-art."
2100,3," - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution?"
2101,1," I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures."
2102,3,\n3. trains variatns of a forward model f on the hidden states of the various learned agents.
2103,2,Presented paper has 13 pages and 26 adequate references. The paper seems to be very interesting.
2104,3, The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).
2105,3," The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets."
2106,3,"\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to\nnetworks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83\u201398, 2013."""
2107,2,You haven't reflect all relevant studies.
2108,3,The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior.
2109,3,"""This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension."
2110,1," This is not necessarily a bad thing, since the\nextensive experiments (both \""toy\"" and \""real\"") are well-designed, convincing and\ncomprehensible"
2111,3," The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed."
2112,1," Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms."
2113,3,"  Does the occupancy grid account for sensing limitations (e.g., occlusions)?\n\n"""
2114,2,I dont know what to do with this.
2115,3,  Would the result be more inconsistent if the hypercolumns had smaller receptive field?
2116,3," It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done."
2117,3,The mechanism of partial pulling is very simple (just let SGD proceed after pulling a partial parameter block instead of the whole block).
2118,3,"\n - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)?"
2119,1,"""This paper is well written and it was easy to follow"
2120,2,The authors seem to be reinventing the wheel and a flat tire to go along with it
2121,3," Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown."
2122,3,"  When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those."
2123,1, Some of the results are better
2124,1,"\n\nAlthough the results are promising,"
2125,2,"X and Y are both tools that knotheads can use to move 
science backwards"
2126,2,"I am generally very happy to provide extensive comments on manuscripts, but this submission was an absolute waste of my time."
2127,3,"\n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223\u2013242, April 2001,"
2128,3,\n\n3) Controlling the amount of distortion.
2129,3, In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.
2130,3, This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets.
2131,3," I would be interested to know what happens if the causal graph is not known, and even worse cannot be completely identified from data (so there is an equivalence class of possible graphs), or potentially is influenced by latent factors."
2132,3," Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this?"
2133,3, That these steps are general?
2134,3,"At last, they also establish the sample complexity for recovery."
2135,2,To put it bluntly: The last thing I want is a reviewer rejecting my papers with reference to an ideological paper like this one
2136,3," As far as I can glean, the topology of the neural network is constructed using the chart of a CKY parser."
2137,3, Fair comparison of the data is a serious concern.
2138,3,"\nQuality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done,"
2139,3,\n\n- Stopping condition.
2140,1, The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below).
2141,1, \n- interesting idea but I think it's more theoretical than practical.
2142,1, Experiments results are good for given synthetic scenarios
2143,3, It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism.
2144,3,\n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help?
2145,1,"\nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN."
2146,1,. I do think that it is a solid contribution with thorough experiments.
2147,3,"\n\n-multiagent deep RL has been very active last 1-2 years. E.g., see other papers by Foerster, Sukhbataar, Omidshafiei"
2148,3," So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree)."
2149,3," In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83). "
2150,3, Could you perform ablation without contextualized embedding (CoVe)?
2151,1," \n\n-- While the authors have convinced me that data augmentation indeed significantly improves the performance in the domains considered (based on the results in Table 1 and Figure 5a),"
2152,1,"  While ICLR is not focused on neuroscientific studies, this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation."
2153,1,\n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.
2154,3,\n - Can the results be applied to some practical task? Why are the results interesting and/or useful?
2155,3," If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).  \n"""
2156,3," but should be\nfurther verified by multiple runs.\n"""
2157,3, The discriminator then needs to decide whether this is a real corrupted measurement or a generated one.
2158,3, Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters.
2159,3,"\n(ii) While it is commonly done, it would be nice to get some insights on why a Gaussian approx. is a good assumption."
2160,3,\n\nIn reviewing the paper the following questions come to mind:\n1) Is the false positive rate too high to be practical?
2161,1,  The proposed approach is very interesting.
2162,3," I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do."
2163,3, Does it suggest the cosine similarity is not effective in measuring the state similarity?
2164,3," Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM? "
2165,3," Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner."
2166,3," Is the latter a proxy for the former? How are they related?"""
2167,3,"""This paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and document."
2168,3," I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise."
2169,3, N-grams are by definition contiguous sequences... The authors may want to consider alternatives.
2170,1,"\n\nThis paper has many strengths:\n1) The writing is clear, and the paper is well-motivated\n2)"
2171,1,\n\nThe main weaknesses of the paper are in the soundness of some of its qualitative analyses and claims.
2172,3, A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged.
2173,3,"""The paper studies a combination of model-based and model-free RL."
2174,1," \n\nOverall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that."
2175,3, Are the test molecules somehow generated in a directed or undirected fashion?
2176,3, The authors do not provide any analysis about what can be learned from this learning procedure.
2177,2,"Reviewer : The conclusions are supported by the results and discussion.
Reviewer : The conclusions are not supported by the result"
2178,3," However, is there any theoretical guarantee or empirical evidence\nto show the proposed method does not suffer from the drawback of high variance?"
2179,3, The architecture is trained through n-step Q-learning with reward prediction loss.
2180,3," Hence, the apparent semantic coherence in what the authors call \""topics\""."
2181,3, At least the concept of stochastic predictions should be discussed\n* The rule-based baselines are not described in detail.
2182,3," \nIn my understanding, in experiment 2 the network, while learning to recognize the numbers, it also learns to\nbe invariant to color thanks to a tailored data augmentation and the good final results are expected."
2183,3,\n\n** PAPER SUMMARY **\n\nThe author proposes to combine siamase networks with an SVM for pair classification.
2184,3,\n\nThe authors also demonstrate that using multiple agents with different policies can be used to collect training examples for the PATH function that improve its utility over training examples collected by a single agent policy.
2185,2,This is (Im sorry) utter nonsense.
2186,3,"""The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space)."
2187,3," Is performance maintained only on the last 2 tasks, or all previously seen tasks?"
2188,3,"\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important."
2189,3," For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task."
2190,1,"""Pros: \nThe paper is clearly written and studies an interesting problem."
2191,3,"\nIn section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers."
2192,1,\n3. Efficient CUDA implementation (not experimentally verified)
2193,3, Have I missed the reference to the proof of Thm 2?
2194,3,  There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf.
2195,3,"  With the low level controller off, collisions became possible."
2196,3," \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results)."
2197,3,"\n\nThe authors take a binary-circuit approach: they represent numbers via a fixed point binary representation, and construct circuits of secure adders and multipliers, based on homomorphic encryption as a building block for secure gates."
2198,1,"   \n\nIn addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet."
2199,1," \n\nGranted, being able to infer hidden states is of course an important problem,"
2200,1," Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation."
2201,3,"""The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space)"
2202,2,The paper is neither exciting nor harmful.
2203,1,  Reasonable baselines.
2204,3,"""This paper proposes using long term memory to solve combinatorial optimization problems with binary variables."
2205,3,"   \n\nTo estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016."
2206,3,"\n- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art."
2207,3, Is it experiments from figure 3.
2208,1," The idea proposed in the paper is just a stack of \""better\"" experiments."
2209,1," Empirical results on ResNet50 on CIFAR show promising results for simulations with slow workers and servers, with the proposed approach."
2210,3," For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better."
2211,3, \n(b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution\n
2212,3,  Please comment in the rebuttal and I would appreciate if the details of the synthetic PIR values on the training set could be explained.
2213,3, The authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to VGG16.
2214,3,"  Similarly for the continuous control with sparse rewards environments \u2013 if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning."
2215,2,I can see that the manuscript has archival value
2216,3,"""The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels."
2217,3,\n\nPro:\n1. Challenging and relevant problem solved better than other approaches.
2218,3,"\n    -- \""if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \""."
2219,3," The model is based on the SWAN architecture which is previously proposed, and an additional \""local reordering\"" layer to reshuffle source information to adjust those positions to the target sentence."
2220,1,"Overall, I think this paper would be a better fit in a recsys, applied ML or information retrieval journal."
2221,3, It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear.
2222,3,\n\nThe authors provide a clean variational inference algorithm to learn their model.
2223,3," How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)? "
2224,1," In any case, these methods are useful to document, but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the paper."
2225,3,\nThis paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound).
2226,3,"""This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks."
2227,2,This is a terrible sentence. It has so many different things in it and not enough punctuation
2228,3, Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.
2229,3,"  If I understand correctly, f_p is a representation of *all images* of a specific layer *k* at/around pixel \""p\"", (According to last line of page 3)."
2230,3, It then derives the idea of eigen options from the successor representation as a mechanism for option discovery.
2231,3," For practical usage, especially in computer vision, GPU speedup is needed to show an impact"
2232,1,"""The authors present a novel evolution scheme applied to neural network architecture search."
2233,3,"\n\nTwo propositions are made, (although I would argue that their derivations are trivially the consequence\nof the model structure and inference scheme defined), and experiments are run which compare the approach to maximum likelihood estimation for 'Y' using an equivalent stochastic network architecture."
2234,1,n\nThe proposed model can maintain performance of single-task models and in some cases show slight improvements.
2235,3, The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset).
2236,3,n2) When empirical isolated saddle points are close to true isolated saddle points
2237,3,"""UPDATE: Following the author's response I've increased my score from 5 to 6."
2238,3,\n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD.
2239,3," \n\nSection 4.1 is the second most important section of the paper, where properties of VCs are discussed."
2240,3,. It is interesting to add the memory cost per channel into the optimization process. 
2241,1,"\n\nThe idea of using task decomposition to create intrinsic rewards seems really interesting,"
2242,3, Can you apply this method to other multi-agent problems?
2243,3, In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.
2244,3,"\n\nThe later point would normally make me attribute a score of \""6: Marginally above acceptance threshold\"" by current DL community standards,"
2245,1, While the RWA was an interesting idea
2246,3," Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results."
2247,3,"  To improve the understanding of the CCC-based operation, it would further be worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance."
2248,3," Also the data problem is extremely simple, and it is not clear the didactic benefit of using it."
2249,1,\n- Thought-provoking approach
2250,1," \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit)"
2251,3, Goldberg and Nivre just adapted it to transition-based dependency parsing.
2252,3,
2253,3," By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers."
2254,1,"\n-- Section 4, importance of large datasets."
2255,3," It shouldn't be, but the authors should spend a little on this."
2256,2,"The paper is grossly over referenced, and reads a little like a student trying to impress a supervisor that a lot has been read"
2257,3,"  Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation."
2258,3," So, I'd really urge the authors to extend this evaluation."
2259,1,\nThe invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG.
2260,3, (ii) \u201cStochastic Variational Deep Kernel Learning\u201d (NIPS 2016);
2261,3,\n+ Simple/directly applicable approach that seems to work experimentally;
2262,3, What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4.
2263,3," Since w_j^t can be defined iteratively and recursively (as a dynamic program), it\u2019s probably worth writing both out, for expository clarity."
2264,3," But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted."
2265,3,"""This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip."
2266,3, This seems inconsistent with the previous definition of \\psi\n\n- p.
2267,1,In the context here this didn\u2019t seem a particularly relevant addition to the paper. 
2268,3,  (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?
2269,3,"\n\n* The trade-off parameter \\gamma is a \""fiddle factor\"" -- how was this set for the lung image and MNIST examples?"
2270,3,"""This paper aims at robust image classification against adversarial domain shifts."
2271,3,and (iii) the significant risks associated with training with a physical vehicle;
2272,3, The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss.
2273,2,"There is potentially an interesting essay to be written about [x], but this one isnt it"
2274,3,"The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014)."
2275,1,\n- A wide range of experiments are conducted to demonstrate performance of the proposed method.
2276,3, It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious.
2277,1,"\n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving"
2278,3," For the convolutional layers of VGG19, they observe that the sum of IDs for each feature map is roughly equal to the ID of the matrix formed by concatenating the vectors over all feature maps."
2279,1," In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful."
2280,1, The results presented are state-of-the art.
2281,3, Is e_o fixed for the non-structured knowledge?
2282,3,  This can then be fed into a classifier.
2283,3," The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients."
2284,3," The authors run 700 epochs and even 1400 epochs with path-wise training on CIFAR, while the baselines only have 160~400 epochs for training."
2285,3,"  The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space. "
2286,3," Therefore, the motivation of the paper may make more sense if the proposed method is applied to a different NLP task."
2287,3,  Can this be formalized?
2288,3, Why is Theorem 1 not a function of L?
2289,2,Im just bored as a reader.
2290,3," What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation?"
2291,3, Do you run the narrator model with all possible obfuscations and pick the best choice?
2292,3,"""This paper introduce a times-series prediction model that works in two phases."
2293,3,\n5. What is the memory consumption for different solvers?
2294,3," Besides, the eigenoptions seem to help for exploration (as a uniform policy was used) as indicated by plot 3(d), but could they help for other tasks (e.g., learn to play Atari games faster or better)? "
2295,1, This paper has some interesting insights and a few ideas of how to validate an evaluation method.
2296,3, Should these parameters be take out of the n-step advantage function A?
2297,1, \n\nPros:\n* Important problem
2298,3, In the PPD the only (jointly) winning move is not to play.
2299,3," In this model, the neurons must pay to observe the activation of neurons upstream."
2300,3,\n\nThe experiments supported the hypothesis that the RNNs are able to \n\n- generalize zero-shot to new commands.
2301,3, Does it indicate the number of  iterations over the same dataset?
2302,1,"  The illustration about the problem is clear, as well as the explanation for the formulations."
2303,3," They also observe that the gap persists at test time, although it does not examine how it relates to approximation error."
2304,3,"  As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified."
2305,1, Only on CIFAR100 the proposed approach is much better than other approaches.
2306,3," More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4)."
2307,1,\n\nAn important argument in favour of using complex-valued networks is said to be the propagation of phase information.
2308,3,"""This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem."
2309,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
2310,1, The authors' technique may let us do this data-generation easily.
2311,3,"""This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation."
2312,3," \n* Section 3. As said before, a general description of the learning framework should be given."
2313,3,\n\nThis is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.
2314,1, My feeling from reading the paper is that it is rather incremental over Cai et al.
2315,3, This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches.
2316,3,"""This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015). "
2317,3," Shortly\n  thereafter we are told that the generator quickly learns to produce norm 1\n  outputs as evidence that it is matching the encoder's distribution, but this\n  is something that could have just as easily have been built-in, and is a\n  trivial sort of \""distribution matching"
2318,3, CNN is known to be usually unable to capture long-range correlations in natural language (unless enhanced with attentions).
2319,3, The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed.
2320,3,"?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces"
2321,3,"""The paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making."
2322,1, \n\n# Clarity\nThe paper is overall clear and easy-to-follow except for the following.
2323,3, The authors suggest computing a similarity matrix amongst the tasks.
2324,3,"\n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training)."""
2325,3, It should be a proper control.
2326,3,"  The would help understanding the proof, and possibly reuse the same idea in different context."
2327,3," A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames."
2328,3,"\n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context."
2329,3,"""The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference."
2330,1,"\nare clearly of great interest, especially when compared to state-of-the-art assimilation strategies\nsuch as the one of B\u00e9r\u00e9ziat."
2331,1," This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n"""
2332,3," However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations?"
2333,3," The work builds on previous approaches, such as Vilnis and McCallum's Word2Gauss and Vendrov's Order Embeddings, to establish a partial order over probability densities via encapsulation, which allows it to model hierarchical information."
2334,3," The presentation of the paper, however, should be improved significantly before publication."
2335,3,"\n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n"""
2336,3," In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers?"
2337,3,"\n\nIn the same section, the expression for \\alpha_{ij} seems to assume that \\delta_{ijk} = \\dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions)."
2338,3,  \n5. How many layers is the DenseNet-BC used in this paper?
2339,3,"\n- The authors should have compared their approach to the \""base\"" approach of Natarajan et al."
2340,3,"""The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric."
2341,1, The main conclusion is intuitive.
2342,1,"\n\nThere are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately Gaussian."
2343,3," The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is)."
2344,3," To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix."
2345,3,\n\nIt seems that there could be more things to show in the experiments part.
2346,1," \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric."
2347,3,  I think there is\nsome subtle but important discussion needed on how this framework fits into\nmodern distributed systems for SGD.
2348,3," but unfortunately the efficacy is not well justified in theory.[[CNT], [CNT], [CRT], [MAJ]] The empirical study is not always convincing, and did not compare with many state-of-the-art baselines."
2349,3,"\n\nThe theoretical analysis in the paper is straightforward, in some sense following from the definition."
2350,3, Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.
2351,3,"\n\n- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?"
2352,3, The proposed model variations (which replaces the \u201ccontent update\u201d that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary.
2353,3," \n\ncons/suggestions: \n- the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail."
2354,3," Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer."
2355,3,The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).
2356,3, For example: What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later?
2357,3,  This method is based on convnets that map raw pixels to a mask and feature map.
2358,3,"\nBased on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer."
2359,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
2360,3," A kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in M- : why not using those terms instead of ai <di,dt>?"
2361,3," So, how can the given observations be used to explain more recent works?"
2362,3, I  would like to see more careful architecture search and ablation studies.
2363,3,"\n\n5. I don't quite agree with the asserted \""multi-modal structure\"" in Figure 2."
2364,3,"\n\nIn section 5.2 the reference to Table 5.2 should be Table 1.\n"""
2365,3,"The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network. "
2366,3,"  Toward this end, the authors adopt a memory-network based approach."
2367,1," But, the chosen benchmarks and datasets seem to be not very standard for evaluating geometric CNNs."
2368,3," It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo."
2369,1,\n\nI like this paper.
2370,3,"\n4) about section 3.2, again I didn't get whether the model needs RL for training."
2371,3,"If the deep network match the shallow one , this can be understood as a form of \u201cdistilled content \u201c loss? and the disc of the deep one will take care of the texture , style content? is this intuition correct? \n\n- original cyclic reconstruction constraint is in the pixel space using l1 norm usually, the regularizer introduced matches in a feature space , which is known to produce better results as a \u201cperceptual loss\u201d, can the author comment on this? is this what is really happening here, moving from cyclic constraint on pixels to a  cyclic constraint in a feature space  (shallow network)?\n\n-  *"
2372,3,"\n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory."
2373,1,". The datasets considered in the experiments are also large, another plus."
2374,3,"\n- you say there is no \""complexity\"" incrase when using \""logadd\"" - how do you measure this? number of operations? is there an implementation of \""logadd\"" that is (absolutely) as fast as \""add\""?"
2375,3," It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function."
2376,3,\n\nExperimental results are not convincing.
2377,3,"""The paper proposes a simple dilated convolutional network as drop-in replacements for recurrent networks in reading comprehension tasks."
2378,1," \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn\u2019t require assumptions on parameter dimensions and data matrices."
2379,3," The authors should include the exact model specification, including for the HRED model."
2380,1,  The empirical evaluation demonstrates the effectiveness of the approach.
2381,3,"""The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update."
2382,3," For example, Snooper which uses a RandomForest  (https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-016-3281-2) and hence would be of interest as another machine learning framework."
2383,3,  Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets.
2384,3," Their \""related work section\"" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations."
2385,1,"\n\nThe idea of explicitly accounting for the boundedness of clinical scores is interesting,"
2386,1,\n3.\tThe experiments and analysis presented in the paper are insightful.
2387,2,"Statistical analysis. It is a bit strange for me that authors have used Python for statistical analysis instead of using SPSS or MATLAB as usual in the field. Please, explain."
2388,1,"\n21. Appendix A to E are not necessary, since they are from the literature.[[CNT], [PNF-NEG], [CRT], [MIN]]\n22. sct 3.1, par 2: \""is approximately unitary.\"" -> \""is approximately unitary (cf Appendix F).\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n23. sct 4, par 1: \""and backward operations.\"" -> \""and backward operations (cf Appendix G and H).\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nPros\n------\n\n1. Nice Idea that allows to decouple the hidden size with the number of hidden-to-hidden parameters."
2389,3,"  This seems crucial for scaling up this approach""?"
2390,2,"Finally, I have substantial criticisms, in addition to the disappointment of seeing hoary old..."
2391,1,"Clearly the authors were able to make it work, with good results."
2392,3," As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda."
2393,3,  And why limit to 50 epochs then?
2394,1," The survey part is nice,"
2395,3,"\n\nClarity: The use of the term \""adversarial\"" is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label (e.g. from \""church\"" to \""tower\"" or vice-versa)"
2396,3, Same goes for using a running average of phi(s) and the correct tau(s) in final policies.
2397,3, These limitation should be discussed more explicitly and more thoroughly.
2398,3, What do you mean by re-initializing and retraining the narrator? Isn\u2019t it costly to reinitialize the network and retrain it for every turn?
2399,3,"\n\nThe paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior)."
2400,3, This can be improved to help readers understand better.
2401,3,"\n\nReference\n[a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017"""
2402,3,\n\nii. the equilibrium concepts being considered i.e. does the paper consider Markov perfect equilibria.
2403,3, The authors state that all units playing NOOP is an equilibrium.
2404,3,"\n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose \u201cstochastic lazy attributes\u201d, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid."
2405,1,\n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting.
2406,3," Baring theoretical results, you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directly."
2407,3," A rich lists of the possible components of the neural network-based clustering methods are given, that include the different neural network architectures, feature to use for clustering, loss functions used and more."
2408,3," The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks."
2409,3,"\n+During CIFAR-10 experiments when r=1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels?"
2410,3,  Is it the optimal solution (x_t)?
2411,3, Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes.
2412,1,\nGood baselines.
2413,3,"   Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient"
2414,3,"\n\nThe proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units."
2415,3,".\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works."
2416,1,"\n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions."
2417,3," Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure."
2418,3,\n6) Fig 5. What does good performance look like in this domain.
2419,3,"""The key contribution of the paper is a new method for nonlinear dimensionality reduction."
2420,1," However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator."
2421,3, The framework is represented by a feed-forward deep architecture analogous to a residual network.
2422,3," As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick."
2423,3, It would be helpful to discuss the conditions where we can benefit from the proposed method.
2424,3," \n\nMy final decision is dependent on the author's response to my questions. """
2425,3,\nWhat are the context vectors in Figure 1? 
2426,1, The proposed model achieved STOA performance on Stanford Question Asnwering Dataset
2427,3,"\n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks?"
2428,3, any comment on this?
2429,3,"\n[2] Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson. A la Carte \u2014 Learning Fast Kernels. AISTATS 2015."""
2430,2,"Publishable, but why?"
2431,1,\n\nThe paper is well-written and easy to follow.
2432,3," For example, the authors mentioned that \""there has been no complete implementation of established deep learning approaches\"" in the abstract, however, the authors did not define what is \""complete\""."
2433,1, \n\nThe main results include:\n(1) In a rollout update a mix of MC and TD update (i.e. a rollout of > 1 and < horizon) outperforms either extreme. 
2434,3," A variant on this, which also incorporates the ranks of the imposters sorted by a metric such as edit distance or BLEU metric with respect to the ground truth is also introduced."
2435,1, This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.
2436,3," What I thought was especially interesting is how their analysis can be extended to other graph problems; while their analysis was specific to the problem of maze solving, they offer an approach -- e.g. that of finding \""bugs\"" when dealing with graph objects -- that can extend to other problems."
2437,3," \"" What else does it depend on?"
2438,3, Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map.
2439,3,"""The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces."
2440,3,"  Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable)."
2441,3, \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results?
2442,1,"  The methods presented here could be of interest to those training language models for use in specific systems, and the paper reads reasonably clearly"
2443,3," However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture."
2444,3,"\n- Section 4:  How is \""as soon as possible\"" encoded in this objective?"
2445,3,"\n\n- Section 5.2, this was nice and contributed to my favorable opinion about the work.[[CNT], [CNT], [APC], [MAJ]] However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero."
2446,1, I found the paper relatively creative and generally well-founded and well-argued.
2447,3, \n- How much data was actually used to learn the Path function in each case?
2448,3,n3) Provide a common code framework with all methods\
2449,3," The mechanism is called \""bias-corrected moment estimate\"" in the Adam paper, arXiv:1412.6980."
2450,3," In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?"
2451,1,"\n\nPros:\nThe paper is well-written and clear, if a bit verbose."
2452,3, Experimental results are given on different multi-task instances.
2453,3," although more of a discussion of the relationship to the technique of Genevay et al. would be useful: how does your approach compare to the full-dual, continuous case of that paper if you simply replace their ball of RKHS functions with your class of deep networks?"
2454,3,"\n\nThe submission's (counter-)claims are served by example (cf. Figure 2, or Figure\n3 description, last sentence), and mostly relate to statements made in the WGAN\npaper (Arjovsky et al., 2017)."
2455,1,\n- One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit.
2456,3,\n[r2] LCNN: Lookup-based Convolutional Neural Network.
2457,3,\n\n(2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter \\alpha (although only fixed value is used in the experiments) to control the negative saturation zone.
2458,2,This needs some rephrasingâits loaded with the assumption that there is a real world
2459,3," It is more appropriate to refer to encoding a combination of the current value and the increment as a version of predictive coding in signal processing rather than the proportional derivative scheme in control theory because the objective here is encoding, not control."
2460,3,"\n\n6) I do not see any novel contribution in the analysis of the batch normalization (end of section 5): bn has been \npreviously used for  domain adaptation\nRevisiting Batch Normalization For Practical Domain Adaptation, arXiv:1603.04779\nAutoDIAL: Automatic DomaIn Alignment Layers, ICCV 2017"
2461,3,"\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers."
2462,3,"\nUnlike the most common form of imitation learning or behavioral cloning, the authors \nformulate their solution in the case where the expert\u2019s state trajectory is observable, \nbut the expert\u2019s actions are not."
2463,3, The performance is investigated in terms of wall clock time. 
2464,1,"  Nothing about the proposed method (e.g. the neural net setup) is specific to images, so this seems quite readily doable."
2465,3,\n\nPlease refer to below for more comments and questions.
2466,1, The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!
2467,3,\n\n\n===============================================================\n\nRevising my review following the rebuttal period and also the (ongoing) revisions to the paper.
2468,3,"\n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads."
2469,3," On geometry images, in particular it would be possible to apply standard convolutional architectures without any special processing."
2470,1,Con:\n-            The paper is generally well-written 
2471,3,.\n\nI updated my scores based on the reviewers responses.
2472,3, (b) To find the value that best matches a key at the decoder stage?
2473,3,"For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE."
2474,3, I wonder though if such a transformation could not be learned by the vanilla prototypical networks simply by learning now a projection matrix A_z as a function of the query point z.
2475,3," For instance, does the same phenomenon happen for different datasets?"
2476,3,\n\nThe paper  comes with a series of experiments to empirically \u00ab\u00a0demonstrate\u00a0\u00bb the conjectures. 
2477,1,"\n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \""regret minimization\"" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives."
2478,3,  But this cannot be true when the index is a weighted sum of the constituent assets.  
2479,1," While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics."
2480,1,"  This eliminates the need to restart training when making an architectural change, and drastically speeds the search."
2481,3," However, is this statement enough to cross the acceptance threshold of ICLR?"
2482,3,"\n\n[1] Easy Questions First? A Case Study on Curriculum Learning for Question Answering. Sachan et-al.\n[2] Learning Word Vectors for Sentiment Analysis. Maas et-al."""
2483,3, The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations.
2484,3," Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation."
2485,3,  Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images?
2486,3,"  I would like to see to the top k nearest neighbors of each of those words.\n"""
2487,3, The concern is that the contribution is quite incremental from the theoretical side
2488,1, \n\nThe paper is overall quite interesting and the study is pretty thorough: no major cons come to mind.
2489,2,The stimulus is artificially produced and therefore irrelevant.
2490,2,Is this not the most frightening finding of all?
2491,3," In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced."
2492,3," When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}."
2493,3,"\n\n- For results in Table 1 and Table 2, how are the confidence intervals computed?"
2494,1, \n2. The discussion comparing the related work/baseline methods is insightful.
2495,3," Of course, the moving-out is biased but the replacing is unbiased."
2496,3, \nOn a related point: What would Figure 2 look like for the constand uncertainty setting?
2497,3, The main modeling challenge here to to define a  good directional measure that can be suitable for lexical entailment.
2498,3,  The authors perturb input images and create explanations using different methods.
2499,3," In the videos, it seems that the people and chairs are always in the same place."
2500,1,\n\n* But it improves on each noise type when it is trained on that noise type.
2501,3," Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings."
2502,1," The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards."
2503,3," Lastly, they incorporate attention for large visual inputs."
2504,3," Oh - now I get it.\nBecause it takes an extended n-step transition and generates an action.  \n\n\n\n\n\n"""
2505,3," Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections."
2506,3, The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results.
2507,3," but the paper has little scientific value.[[CNT], [CNT], [CRT], [MAJ]]  Below are a few suggestions to make the paper stronger."
2508,3,"\n \nOn the theoretical side, the discussion could be improved. Namely, Section 3 about \""limitation of domain adversarial training\"" correctly explained that \""domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity\"". "
2509,3,  Are there any\n  surprising long-range dependencies? 
2510,3,  The next utterance is then generated based on this dialog act.
2511,3,"""The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \""style memory\"", which would presumably capture non-class information."
2512,3," The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle."
2513,3, 2. human subjects driving cars in the simulator.
2514,1, The ideas are illustrated through several well-worked micro-world experiments.
2515,3,"\n2. Only one task / No real-world task, such as Excel Flashfill.[[CNT], [SUB-NEG], [DFT], [MIN]]\n\n[1]: \""Neural Program Meta-Induction\"", Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli"""
2516,3," More importantly, MagNet is able to defend the adversarial examples very well (almost 100% success) no matter the adversarial examples are close to the information manifold or not."
2517,1, \n - The paper is well written and the contribution is clear.
2518,3," Similarly, since the KL loss is the same as XENT, why give it a new name?"
2519,3,"""The authors study the effect of label noise on classification tasks."
2520,1,"\n\n* The noise model experiment in Appendix D is commendable, "
2521,2,Line 306. The sentence follows a bit of a Yoda-esque grammar
2522,1,"\n\n4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W."
2523,3,\n\n- Are there plans to release the dataset?
2524,3,"\n\n* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016)."
2525,3," \n2. The authors should do a better job at explaining the details of the state definition, especially the student model features and the combination of data and current learner model."
2526,3," Rather, removing the distributional and cycle constraints changes the overall objective being optimized."
2527,1, \n\nThis paper is well-written and easy to follow.
2528,2,"The color rainbow is pretty, but largely useless."
2529,3,  By choosing the hierarchy level and type of filter the results of the GAN differ.
2530,1,\n  - extensive experimental comparison and good experimental results.
2531,3, The writing could be improved.
2532,1, The paper presents a good work and is well articulated. 
2533,3,"\n- As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments."
2534,3,"""The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features."
2535,3," In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size."
2536,3," So I think the idea is OK, but not a breakthrough."
2537,3,\n\nMajor comments:\n1) Spatial resolution. What spatial resolution is the model generating images at?
2538,2,The arguments in the paper are compelling but not convincing
2539,3, but the repeated references to these tables suggest that these experimental results are crucial for the authors\u2019 overall points.
2540,3,"\n\n[1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017."
2541,3, A batch or an epoch or other?
2542,3,"\n\nThe main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'."
2543,3," The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem."
2544,3," Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters."
2545,3, It seems to imply that VAN has skip connections.
2546,3, Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework?
2547,3,\n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions.
2548,1,"?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition"
2549,1," Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better."
2550,3, The GAN training objective function utilizing 3 conditional classifier.
2551,3," The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results."
2552,3,  How is the terminal time known a priori?
2553,3,The time-aware agent shows improved performance in a time-limited gridworld and several control domains.
2554,3, This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks.
2555,1,"  Overall, the paper is mathematically sound,"
2556,1," Although the performance on automated metrics is encouraging,"
2557,3, The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.
2558,3, \n\nIf good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes.
2559,3,  This is true even for random perturbations.
2560,3, Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.
2561,3,\n5. In Table 2 and Figure 3 the results are reported with percentage of using the learning curve. 
2562,3," Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points."
2563,3,\n\n2- How sensitive is the results to the number of -1 in the diagonal matrix?
2564,1,. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally
2565,1, The comparison of the dual critic to the true Wasserstein distance is very interesting.
2566,3,"\n\n-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as\n\nM."
2567,3,"""Paper summary:\nThis paper proposes a technique to generalize deconvolution operations used in standard CNN architectures."
2568,1,  \nThe authors clearly performed an impressive amount of sensitivity experiments.
2569,3,"\n- Sec 1: \""abilities not its representation\"" -> comma before \""not\""."
2570,3,\nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components.
2571,1, The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.
2572,3, Even RL methods with linear function approximations use abstractions.
2573,3,\nThe idea of re-using a pretrained agent has both pros and cons.
2574,3," If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales."
2575,3," When positive initializations are enforced, the network can more or less mimic the BFS behavior, but never when initializations can be negative."
2576,2,Lots of hand waving in this Discussion
2577,3," I have a few questions about the\nevaluation, but most of my comments are about presentation."
2578,2,"It is unclear how this would advance the field beyond providing additional, previously unknown information"
2579,3," Additionally, in the 5-shot non-distractor setting on tiered ImageNet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset?"
2580,3," What do you do when you go back to the task that doesn't have the input, feed 0?"
2581,3," This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A."
2582,3,"\n\nReferences:\n- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit."
2583,3, What kind of clusters are discovered?
2584,3,"  Is this assuming that the \""initial training set\"" which is used to obtain the \""pre-trained DNN\"" free of adversarial examples?"
2585,3,\n- how is this an end-to-end approach if you are using an n-gram language model for decoding?
2586,3," At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables."
2587,3, How does this affect the paper statements and results?\n-
2588,3,"""The idea of using cross-task transfer performance to do task clustering is not new."
2589,1," The \nempirical validation, while being limited in some aspects, is largely convincing."
2590,3," However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem."
2591,3, The authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the parameters.
2592,1,"\n\n\""from the first m domain corpus\"" -> \""from the first m domains\""?[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""may not helpful\"" -> \""may not be helpful\""\n\n\""vocabularie\"" -> \""vocabulary\""\n\n\""system first retrieval\"" -> \""system first retrieves\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nCOMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines."
2593,3,"""his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA."
2594,3," Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting."
2595,3," In general some more nuanced statistical analysis of these results\n  would be worthwhile, especially where they concern human ratings.\n- The dataaset fractions chosen for the semi-supervised experience seem\n  completely arbitrary."
2596,1,"  In the runs that reach the goal, the proposed method is about 20% faster than the simple baseline, though it does not reach the goal every time."
2597,3," The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution."
2598,3, Should the risk bound only depends on the dimensions of the matrix W?
2599,3, The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions.
2600,3," Also, in the bottom right panel in Figure 2, GrandNorm and equal weighting decrease test errors effectively even after 15000 steps but uncertainty weighting seems to reach a plateau. Discussions on this would be useful."
2601,3, Is there a way to fix it besides using ASG?
2602,3,\n\np3. What does 'normalized' mean?
2603,3," The Greedy schedules are far inferior to hand-tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be \""seen\"" with a small lookahead."
2604,3, For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1?
2605,3," The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\"
2606,3, It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial.
2607,3,  Or is this work focused on improving the performance of existing methods?
2608,3," This is important in order to evaluate the complexities involved in computing its Hessian.\n"""
2609,3, The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective.
2610,3,\nA few comments:\n1. How do the authors propose to deal with multimodal true latent factors?
2611,3, It can be further improved as mentioned in the comments.
2612,1, \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel.
2613,1,\n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance
2614,3,This paper should at least cite those papers and qualitatively compare against those approaches.
2615,3,n\nIt looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct? 
2616,3,"\n\nThe model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements."
2617,3,"\n\nIf I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues.[[CNT], [EMP-NEG], [CRT], [MIN]]\n\n+++ ResNet scaling +++\n\nThere is a crucial difference between VANs and ResNets."
2618,3,"\n\n\nWeaknesses:\n\n1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch."
2619,3,\nThey present results on MNIST aiming to demonstrate that using PIBs improves generalization and training speed.
2620,3, This is especially true as a more challenging benchmark\ncould be created very easily by simply scaling up the image.
2621,1," When combining multiple types of augmentation the results are better,"
2622,3,"""the paper presents a way to encode discrete distributions which is a challenging problem."
2623,3,\n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks.
2624,1,"\n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising"
2625,3, Do you have an idea why a low threshold hurts the performances?
2626,3,\n4) in section 2.2 why is the behavior policy random instead of epsilon greedy?
2627,3," \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal."
2628,3,"\u201c This is not true; rather, TD minimizes the mean-squared project Bellman error."
2629,3, Do the \\rho values\nsomewhere treat these two quantities differently?
2630,1,- Model is novel and interesting
2631,1,"\n\nOverall, I am inclined to accept this paper on the basis of its experimental results."
2632,3,"   \n\nWhy is it important to maximise I(X_l, Y) for every layer? "
2633,3, In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL.
2634,3,"\n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks."
2635,3,"\n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way"
2636,3," \n\nSecondly, both structured update and sketched update methods adopted by this paper are some standard techniques which have been widely used in existing works."
2637,3," While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to."
2638,1,  Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases.
2639,3,"\u00a0\u00bb \n\n\u00ab\u00a0a predictive state is defined as\u2026 , where\u2026  is a vector of features of future observations and ...  is a vector of\nfeatures of historical observations."
2640,3, Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?
2641,3,"It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on."
2642,1," The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently)."
2643,3," In figure 8, my interpretation of (a) is\nthat the initial model has learned to be invariant to color and this remains true even if the fine-tuning data\ndo not contain any negative data."
2644,2,"Im really sorry about this reviewer. If youd like, I can get you a new one. - Edito"
2645,1," This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning."
2646,1," The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally."
2647,2,"The manuscript is poorly written although it can be follow with lots of typos, and a review by a native English speaker is clearly needed"
2648,3,"  \n\nIn complex environments learning the PATH network is far from easy.[[CNT], [null], [DIS], [MIN]]  I.e. random walks will not expose the model to most states of the environment (and dynamics).[[CNT], [null], [DIS], [MIN]]  Curiosity-driven RL can be quite inefficient at exploring the space. "
2649,3," Given the advancement in gradient based inference for HB the last couple of years (e.g. variational, nested laplace , expectation propagation etc) for explicit models, could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?"
2650,3, How limiting is the learning of a?
2651,1,"\n\nNovelty:\nPrevious papers like \""beta-VAE\"" (Higgins et al. 2017) and \""Bayesian Representation Learning With Oracle Constraints\"" by Karaletsos et al (ICLR 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does."
2652,3," What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs."
2653,3,"  \n2) Considering this is an unsupervised domain adaption problem, how do you set the hyper-parameters lambda and the kernel width?"
2654,3, The discussion here can be useful for other researchers.
2655,3,"""Summary:\nThe contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics."
2656,1,.\n\nQuality and significance:\nThe paper proposes an interesting direction for optimizing the computational cost of training and inference using neural networks
2657,3," (This is in part, addressed in the CIFAR80 20 experiments in the appendices)."
2658,1,.\n\u2022\tIt\u2019s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.
2659,1,"""\n This was an interesting read."
2660,3,\n- Authors claim that they operate directly on the gradients inside the network.
2661,3,\n\n3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief.
2662,3," In their generative approach they say they find outputs by \""nearest neighbor search.\"""
2663,1,"\n3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters."
2664,3, This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.
2665,3," If the authors believe that the 50% case is not necessary, please feel free to explain why."
2666,3, \n\n* The network takes as input a 2.5m (this is large) occupancy grid representation of the local environment.
2667,3, The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence).
2668,3,"  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is."
2669,3,  The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle.
2670,3," On this graph, one can perform interventions and get a different distribution of labels from the original causal graph (e.g. a distribution of labels in which women have the same probability as men of having moustaches)."
2671,3, Does no batching (in section 3.2) means a batch size of one utterance?
2672,3," If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior."
2673,3,"\nSpecifically, it would be nice if train, test and validation URLs would be operated chronologically. I.e. all train url precede the validation and test urls."
2674,3,"\n\n\""2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence."
2675,3,"\n\nBeyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach."
2676,1," \n\nAnother reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data."
2677,1, This is exciting as it could significantly push the state of the art in sketch understanding and generation.
2678,2,"This study, an original work and repetition of earlier feature of these studies is similar. Writing language is quite a lot of errors"
2679,3, It will be fair/interesting to see the result for CPU time where small batch maybe favored more.
2680,3," However, I have some concerns about the specific implementation and model discussed here."
2681,3,"""Summary: This paper studied the conditional image generation with two-stream generative adversarial networks."
2682,3," The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices. "
2683,3," \n\nIn Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem."
2684,3,\n\n4. The sequence ordering is important.
2685,1,\n\nClarity:\n\nThe paper is clearly written.
2686,3,"\n\nThe paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs."
2687,1,\n\nI enjoyed reading this paper.
2688,3," To make a stronger case for this research being relevant to the real autonomous driving problem, the authors would need to compare their algorithm to a real algorithm and prove that it is more \u201cdata efficient."
2689,3,"""Quick summary:\nThis paper shows how to train a GAN in the case where the dataset is corrupted by some measurement noise process."
2690,3,\n \n\u201cAnother one is credit assignment.
2691,3,\n- Skipping behavior can be controlled via an auxiliary loss term
2692,3,"""=======\nUpdate:\n\nThe new version addresses some of my concerns."
2693,2,"papers such as this, lacking any knowledge of astronomy and physics, should never be published in any scientific journal with referees"
2694,1,\nThe proposed approach does as well or better than competing approaches.
2695,3, Something is wrong unless the authors are using different beam widths in the two settings.
2696,3," If you say something like \""they did not estimate a true posterior\"" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior."
2697,3,"\nWhile PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features."
2698,3," In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper. "
2699,3,\nThe baseline is random forests and feature engineering.
2700,3,"  There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences."
2701,1,  The authors then provide several evaluations of complex-valued networks on some standard ML benchmark tasks.
2702,3, Hence inferring a structured latent space is a challenge.
2703,3,"\n\n[as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix]"
2704,2,"They have addressed most of the reviewer comments, although their responses to a few of them remind..."
2705,3," Hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer, which predicts the word that would make the reader fail if the word is obfuscated.."
2706,3," As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery."
2707,3," In particular, it classifies existing methods into four different categories, according to the representation of the interactions of users, items and attributes."
2708,3," Then, based on an arbitrary prefix of epochs y_{1:t}, a model can be learned to predict y_T."
2709,3," Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two)."
2710,1,"\n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR."
2711,3," De et al (AISTATS 2017) have a method for gradually increasing batch\nsizes, as do Friedlander and Schmidt (2012)."
2712,3,\n- Best to clarify what the weights in the weighted sum of Natarajan are. 
2713,3,\nIt should be noted that the limitations of the IB for deep learning are currently under heavy discussion on OpenReview.
2714,3,"The nets, in principle, could learn to recognize objects based on shape only, and the shape remains stable when the color channels are changed."
2715,3," They give results on the quality of the approximation using these operator networks, and show how to build neural network layers that are able to take into account topological information from data."
2716,3, The authors claim that their use in the latent space makes it more practival
2717,3,"""This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization."
2718,3,\n\nQuestions: \n- How would the model need to change to account for example difficulty?
2719,3,. These questions should really be answered in [1]
2720,2,"If this topic were not dear to my heart, I would perhaps have struggled to follow your logic."
2721,3, but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice.
2722,3, Why is TreeQN's approach better than VPN's approach?
2723,3, They choose the alignment of the kernel to data as the objective function to optimize.
2724,1,"\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication."
2725,3, A few typos.\n\nOriginality\n\nThe approach is a straightforward extension of the MCMCP approach using generative models.
2726,2,"hD project paper submitted one year after successful defense: 

Student should get a new PhD project"
2727,3," Like the anonymous commenter, I also initially thought that the proposed \""spectral normalization \"" is basically the same as \""spectral norm regularization\"", but given the authors' feedback on this I think the differences should be made more explicit in the paper."
2728,1," Overall,  the study is interesting and contains some new idea."
2729,3," It would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted HALS which admit negative values in the optimization, see also the review by N. Giles\nhttps://arxiv.org/abs/1401.5226\nas well as for instance:\nNielsen, S\u00f8ren F\u00f8ns Vind, and Morten M\u00f8rup."
2730,3," That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n"""
2731,1,\n\nMinor notes:\n - The paper was very well written/edited.
2732,3,is there a replay memory? How are the samples gathered?). 
2733,3,"""This paper presents a pixel-matching based approach to synthesizing RGB images from input edge or normal maps."
2734,3, How good are the low-precision models trained by the authors at transferring to other tasks?
2735,3,. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. 
2736,3,"""This paper proposes an idea to do faster RNN inference via skip RNN state updates."
2737,3," \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage?"
2738,3, This would enable a better test of the generalization capabilities in what is essentially a continuously changing environment.
2739,3, One offshoot of it has its own society with conferences and a journal devoted to it (The International Artificial intelligence in Education Society: http://iaied.org/about/).
2740,1,\n\nEvaluation: \nSignificance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value.
2741,3,\nThe hidden representation obtained by the encoder should already capture information about the relation.
2742,3," For example, if I increase the number of importance samples, even if I'm overfitting in Fig 3(b), wouldn't the green line move towards the red simply because my estimator depends less on a poor q?"
2743,3," In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective."
2744,3,"\n- It would be worth explaining, in a sentence, the approach in Shen et al for\n  those who are not familiar with it, seeing as it is used as a baseline."
2745,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
2746,3," However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function."
2747,3, \nThe method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer. 
2748,3," This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers."
2749,3,\n* The found hyperparameter of the grid-search would also be interesting to know.
2750,3,. The word pairs are extracted using Stanford parser
2751,3,"""The paper takes a closer look at the analysis of SGD as variational inference, first proposed by Duvenaud et al. 2016\nand Mandt et al. 2016."
2752,2,"Moreover, it is very difficult to see the actual contribution this manuscript will have"
2753,3, How larger values of T to better model inter-community links?
2754,3, \n\nEquation 3: please define H.
2755,1, The derivations look correct to me.
2756,3,\nCon:\n1. Not entirely convincing that it should work better than already existing methods.
2757,1," The fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models, adds to the quality of the experiment."
2758,3," In particular, the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi companies."
2759,3,"\n\nCotterell et al., EACL 2017 \""Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis\"": This paper also derives a tensor factorization based approach for learning word embeddings for different covariates."
2760,1," However, I wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other tasks."
2761,3," Considering that the DrQA is a better system on both SQuAD and TriviaQA, the speedup on DrQA is thus more important."
2762,3,\n\nFinally -- is it expected that the ordering of the factorization in Eq. 3 does not count much (results in Table 3)?
2763,3," The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context."
2764,3, All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*
2765,3, It is not clear the better performance comes from reservoir sampling or other differences.
2766,1,The main theoretical contribution stimulates and motivates much needed further research in the area.
2767,3, the contribution is not significant as the paper misses an important explanation for the phenomenon.
2768,3," \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach."
2769,3,"\nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n"""
2770,1,\n\nSignificance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings.
2771,1,\n- The basic observation wrt the behavior of AT is clearly communicated.
2772,3,"""The paper presents a series of empirical studies of the generalization ability of convolutional neural networks (CNNs) applied to image recognition tasks related to shape images. "
2773,3,  The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine.
2774,3, The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015).
2775,3," The interesting issues are referred to future works.\n"""
2776,3,"""This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix."
2777,3,"   For reference,  please see papers from Saon et al., Seide et al, Povey et al, Yajie Miao et al in various ICASSP, Interspeech and arXiv papers. Comparisons with weak baselines can significantly color the conclusions."
2778,3, The paper does an insufficient job describing why deep RL is the right way to formulate this problem.
2779,3,"""In this paper, the authors show how a Deep Learning model for sea surface temperature prediction can be designed to incorporate the classical advection diffusion model."
2780,3,. It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately
2781,2,I am sympathetic to what the author is trying to do here - Reviewer 
2782,3,\n\n3) Some parts of the paper are hard to read. Sections 3 and 4 are not easy to understand.
2783,1,"\n\nThe paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures."
2784,3,"\nI thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition."
2785,3, The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large.
2786,3, Is it because of the residual connection?
2787,3," However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0."
2788,3," The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form. \n"""
2789,3,"\n \n\u201cit uses \\epsilon-greedy as a policy, \u2026\u201d Do you mean exploration policy?"
2790,3,"""A large margin , end to end language model that uses a discriminative objective function is proposed."
2791,1," Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings."
2792,1, As such I'd rate it as borderline; though perhaps interesting enough to be worth presenting and discussing.
2793,3," \n\nConculsion\n- can you show how your approach is not so computationally expensive as RNN based approaches? either in terms of FLOPS or measured times\n"""
2794,1,"\n\nRead the rebuttal and revision and slightly increased my rating."""
2795,1,"""Quality\n\nThis is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging."
2796,3," If yes, please update 6.1.1 to make this distinction more clear."
2797,3," As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples."
2798,3,"""[====================================REVISION ======================================================]\nOk so the paper underwent major remodel, which significantly improved the clarity."
2799,3,". For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet."
2800,1,"\n- Strong potential impact, especially on constrained power environments (but not limited to them)"
2801,3,"\n\nIn the text, k suggests to be identical to the number of entities in the image."
2802,1,\n\nPros:\nThe regularization of the Q-values w.r.t. the policy of another agent is interesting\n\n
2803,3, (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?)
2804,3,"""The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space."
2805,3," The algorithm is closely related to boosting and MKL, while there is no such comparison."
2806,3, \nIn the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets.
2807,3, This section could be improved by demonstrating the approach on more datasets.
2808,3, the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. 
2809,3, For example: \n(1) The definition of \\bar{A} in Section 4 is broken.
2810,3," Previous approaches to training data mixing are (1) from random classes, or (2) from the same class."
2811,3,"\n\nIn the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy."
2812,1," although the idea of the extensive experimental comparison is good.\n\n\n"""
2813,3,"\n\n*Originality and significance*\n\nAs far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1."
2814,1,\n\nQuality\nThe paper makes relevant comparisons and is overall well-motivated.
2815,3, It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms.
2816,1,\n\nThis paper is reasonably readable.
2817,3,\n\n4. Is trade-off between 1 to 2 bits really important? 
2818,3, This might also help the readers better understand where the polytopes in Figure 1 come from.
2819,3," By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n"
2820,3,  The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation.
2821,3,"Clearly if the agent receives the maximal possible reward for a well designed navigation task it must, by definition, be doing perfect SLAM & path planning."
2822,1,\n\nPros\n- Greatly improves the data efficiency of recursive NPI.
2823,2,There are so many things wrong with this manuscript that I do not know where to begin
2824,3,Some notations in the LSTM section could be better explained for readers who are unfamiliar with LSTMs.
2825,1," IMO, it is a very nice, clean, and useful approach of combining causality and the expressive power of neural networks."
2826,3," First off, how is this (constant) width of the predictive region chosen?"
2827,2," found the use of the evolutionary theory problematic. This is a highly contested theory, with major flaws"
2828,3, It\nwould be great to compare this model to a non-stochastic one which just has the\nmultiplicative effects applied in a deterministic way (during both training and\ntesting).
2829,3, Is it possible to communicate\nthe intuitions behind what is going on?
2830,3, The idea being to construct a superposition of Taylor approximations of the individual monomials.
2831,3," Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not."
2832,3, Here the covariates are morphological tags such as part-of-speech tags of the words.
2833,3," The authors evaluate on several synthetic tasks, as well as an ICU timeseries data task."
2834,2,"ntire review, not an excerpt: I do not trust the data or the underlying thesis."
2835,1,". In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance]"
2836,1," The experimental results are encouraging,"
2837,3,"""This manuscript proposes a method to improve the performance of a generic learning method by generating \""in between class\"" (BC) training samples."
2838,1," \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work."
2839,3,"""This paper proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear)."
2840,3," It seems highly problematic to simply remove a whole part of the gradient, to reduce effect on the next state."
2841,3,\n\nQuestion:\n- Which layer mean and variance are reported in Figure 2? 
2842,3,"  Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.\n\n"
2843,1,\n2) faster gradient update than Vorontsov et al. (2017)
2844,2,"We invited 18 reviewers and after quite a long time, only one reviewer had agreed. That review is now..."
2845,3,\n\n2. Why is the adversary called narrator network?
2846,2,"The authors showed the differences at scale 3, and so what?"
2847,3,  \n- It is not clear how authors uses PCA to reconstruct faces in the test set.
2848,3," The DTP algorithm is slightly adapted to make it more biologically plausible, by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation (leading to the variant SDTP), and by making the optimisation of both involved losses parallel."
2849,3,"""\n- Paper summary\n\nThe paper proposes a label-conditional GAN generator architecture and a GAN training objective for the image modeling task."
2850,3,"\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011)."
2851,3, There should be a single definition for p(F|X).
2852,3,"\nHow would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing."""
2853,3,"""This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage."
2854,2,My greatest criticism of the paper is the tendency of the authors to make an argument and then immediately contradict themselves
2855,3," \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?"
2856,3, Common batch sizes range from 64 to 1K (typically >= 128).
2857,3,". As far as I know, even though modern CNNs have reduced convolution\u2019s computation complexity,"
2858,3," For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs."
2859,3," \n\nRe: \u201cone creates additional samples by modifying\u2026\u201d be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf\u2019s, he called it \u201cvirtual examples\u201d and I\u2019m pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied)."
2860,3," Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014\n\nMo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors\n\nto name a few via quick googling."
2861,3," Consider calling k a time index instead of t_k a time interval (\""subject x experiences cause m occurs [sic] in a time interval t_k\"")\n- Line after eq 8: do you mean accuracy term?"
2862,3, I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?
2863,2,"The authors merely used somewhat
bigger guns than previous studies and generated nothing but more smoke."
2864,2,To improve this you need to be more robust on all fronts etc.
2865,3,"\n\nIs the \""Karel DSL\"" in your experiments the full Karel language, or a subset designed for the paper?"
2866,1," \n\nTheorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify)."
2867,2,The manuscript is a collection of fragmented and disconnected descriptive observations.
2868,3,\n\nThe paper also presents several methods for negative samplings and according to table 4 there is a lot of performance variability based on the method that is used for selecting negative sampling.
2869,2,"Also, the manuscript is very tedious and reads like an unedited thesis chapter."
2870,1,"\nOverall, fine work, well organized, decomposed, and its rationale clearly explained."
2871,3," If yes, it would be good to show them pairwise.\n- figure 5: please explain why the saliency map is relevant."
2872,2,I have followed the work of this group for the last few years and it is usually well polished. That is not the case for the present paper.
2873,3," And on a related note, how were the number of sampled"
2874,3,"""Summary:\n\nThis paper presents a new network architecture for learning a regression of probability distributions."
2875,1,  The paper also provides numerical results to support their theoretical findings.
2876,1,\n\nThe authors wrote a clear paper with great references and clear descriptions.
2877,3,". Hopefully, the suggested studies will improve the quality of the paper in the future submission."
2878,3, How could the authors\nget 16% on MNIST with an MLP of any kind?
2879,1, \n\nI like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boosting.
2880,3,  Results show\nthat the approach is a bit faster than CEGIS in a synthetic drawing domain.
2881,3," For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?"
2882,3,"""This paper proposes a variant of neural architecture search."
2883,3," There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others)."
2884,3, Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?
2885,3, Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature.
2886,3," \n\nAlso, a related article: One article testing rational pedagogy in more ML contexts and using it to train ML models that is\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016)."
2887,3,\n\nCons:\nThe idea is rather incremental compared to FractalNet.
2888,3, It is quite easy\nto probe the test set to get best performance on these benchmarks.
2889,3," All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems."
2890,3,. The proposed approach will be useful when the new domain does not have enough data available.
2891,3,"That being said, this particular use of deep learning in this context might be novel."
2892,1,"  This will be an interesting paper to have at the conference and will spur more ideas and follow-on work."""
2893,3, At each iteration the teacher generates  examples based on the students current concept.
2894,3," This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity."
2895,3, So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset).
2896,3,"\n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\"
2897,3," However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau."
2898,3," Is this for compressing the networks (for which there are alternate procedures), or anything else."
2899,3, but it shows significant gains from it.
2900,2,â¦defining general populations for studies might help the reader and help the authors avoid their meaningless generalities.
2901,3," In particular, what are the state space and transitions?"
2902,1,\n\nCLARITY: The paper is very well written and is easy to follow.
2903,2,This looks like a work of pure fantasy.
2904,1, \n\nThe paper is written in a clear way.
2905,3, \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model? 
2906,3, How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results?
2907,3,"  First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update."
2908,3,"\nIn terms of experiments, it is shown that the system is more effective than others but not so much *how* it achieves this efficiency."
2909,1," I think that having\nless number of parameters is a good thing in this setting as the data is scarce,\nhowever I would like to see a more in-depth comparison with respect to the number\nof features produced by the model itself."
2910,3,\n- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood.
2911,3,"\n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning."
2912,3, The second one is a cycle consistency loss to make sure that the the original sentence can be recovered from the content of the transferred sentence and the style of the original sentence.
2913,3, Could the authors discuss this aspect?
2914,3," Moreover, is CIFAR 10 experiments conclusive enough. """
2915,3, The work is very light on references.
2916,3, \n\nIssues that I wish were addressed in the paper: \na)\tHow is the method learning a generator from a single graph?
2917,3, The method is tested on four simple UCI datasets.
2918,3,"  From my own experience, even if I spend several hours copy-pasting from project gutenberg, it is not enough for even good matrix factorization embeddings, much less tensor embeddings."
2919,3,". \n\nQuality: The mathematical formulas describing basic complex analysis ideas (e.g.  derivatives of complex functions, definitions of complex versions of standard activation functions) seem reasonable to me"
2920,3," You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)?"
2921,3," Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order."
2922,3, This method is a combination of the online variational inference for streaming environment with Monte Carlo method.
2923,3,\n\nThere is a more fundamental question for which I was not able to find an explicit answer in the manuscript.
2924,2,Details of how important these effects are are missing. Ref.75 is entertaining but inadequate in this respect.
2925,3,"How do different agents perceive their time, is it synchronized or not?"
2926,3," Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity."
2927,1," \n\nWhile the results in this paper look good,"
2928,3,   It should at least be evaluated.
2929,3,"\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and J\u00fcrgen Schmidhuber."
2930,1,\n\nThe submission has following PROS:\n\n+ The proposed visual Turing test provides a novel solution to evaluate the generation quality.
2931,3,"""This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples."
2932,1,\n\nPros: \n - the crashed cars dataset is interesting.
2933,3, Did each points are sampled from same travelling distance or according to the same time interval? 
2934,1,"\n\n[Summary]\n\nI think this is a good paper which integrates vision, language, and actions in a virtual environment."
2935,1, The MNIST explanations help a lot.
2936,3, Is it meaningful to perform ADD or SUBSTRACT on the leaned code?
2937,3," Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important."
2938,3,\n\nHow sensitive are the results to hyperparameters?
2939,3, The generalization bound depends on the spectral norm of the layers and the Frobenius norm of the weights.
2940,3, \u201cour experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than 0.1%.
2941,3,"""This paper introduces a number of different techniques for improving exploration in deep Q learning."
2942,3," The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems."
2943,3,"\n\nAdditionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated."
2944,3,"- Which multi-label classifier is used to classify images in attributes?"""
2945,3," The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x."
2946,3,"""Summary of the paper: \n\nThis paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), "
2947,3,\n\n2) Predicting the noise has no guarantee that the data items are better clustered in the latent space.
2948,1,\n\n- The paper is well written overall
2949,3,"  As an example of time-travel to the past, the authors talk\nabout RBMs and stacks of auto-encoders as if that was the deep learning\nstate-of-the-art."
2950,3," Seem to be rather orthogonal.\n"""
2951,3,  This was totally unclear until fairly deep into Section 3.
2952,1, 2-Using multiple models for short and long term memory. 
2953,3," In AAAI, Austin Texas, USA, Jan. 25-30 2015. \u2028\n2. Y. Engel, S. Mannor, and R. Meir."
2954,1,  The suggested method seems to work well on several document classification tasks.
2955,3,\n\nWhat cost is used for generative modeling on MNIST?
2956,3, The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations.
2957,2,There are FAR too many analyses and results. The reader is swamped.  Its simply not possible to take it all in. It needs to be pruned.
2958,1," \n\nThe paper is well written,"
2959,3,"\n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. "
2960,3, Section 3 makes connections to previous work on\nfinding optimal batch sizes to close the generaization gap. 
2961,3, the problem also persists here as the gan is discriminating between a continuous and a discrete distribution. 
2962,1,"""The paper is motivated with building robots that learn in an open-ended way, which is really interesting."
2963,3,", but the abstract and introduction make very strong claims about outperforming \""state-of-the-art supervised approaches\"""
2964,3,"\n* Finally, not being very familiar with multigrid methods from the numerical methods literature \u2014 I would have liked to hear about whether there are deeper connections to these methods.\n\n\n"""
2965,1,\n\nOverall the paper is well written.
2966,3," Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all."
2967,1,"""This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements:"
2968,1,"  It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent."
2969,1,\n- The proposed memory architecture is new. 
2970,1,"\nAs far as I am aware, this is a novel approach to the problem."
2971,3,\n1. Figure 1 could be improved using a concrete example like in Figure 6.
2972,3,"""The paper extends the idea of eigenoptions, recently proposed by Machado et al. to domains with stochastic transitions and where state features are learned."
2973,3,\n\nThe presentation of the paper can be improved a bit.
2974,3,"""In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. "
2975,3,.\n\nWhen it comes to the experiments only one real-world experiment is present
2976,2,"I cannot make out signs of independent thinking, work beyond the state of the art, or anything groundbreaking"
2977,3,"""This paper argues about limitations of RNNs to learn models than exhibit a human-like compositional operation that facilitates generalization to unseen data, ex. zero-shot or one-shot applications."
2978,3,  Such kernels are very related to neural networks (for instance PNG kernels with linear rectifier nonlinearities correspond to random layers in NNs with ReLU) and in the NN context are much more interesting that radial basis function or in general shift-invariant kernels.
2979,3,"   A trained \""narrator\"" could learn to actually change the correct answer."""
2980,3," Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale."
2981,3,"""In this paper, the authors present an adaptation of space-by-time non-negative matrix factorization (SbT-NMF) that can rigorously account for the pre-stimulus baseline activity. "
2982,3, Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to\nany of the known categories.
2983,3,"\n\n2) I have the questions as follows:\ni) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots."
2984,3,"""This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input."
2985,3," To speed up the search, they focus on finding cells instead of an entire network."
2986,3, Surely one could\ndevise situations where VAE outperforms WAE.
2987,3,\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks.
2988,1," The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections."
2989,3," \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%."
2990,3, They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.
2991,3,"  The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it."
2992,1,\n\n =Quality=\nThe authors seem to be experts in their field.
2993,1,\n\n(3) I like the idea of data augmentation with paraphrasing.
2994,1,"\nThe paper is well-structured, and the proposed method is clearly described."
2995,3, \nThe goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance.
2996,3," However, the bounds also depend polynomially on L, and as far as I can tell, L can be polynomial in T for certain systems if we want to achieve a good overall cost."
2997,3," For instance, \""its variants\"" is confusing because there is only other variant to VProp."
2998,3,"\n\nSecond, the title of the submission, \u201cDo Deep Reinforcement Learning Algorithms Really Learn to Navigate\u201d makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper)."
2999,3," This identity connection acts as a \u201csurrogate memory\u201d component, preserving hidden activations over time steps."
3000,2,The sentence is not only overly complex but is a string of theory-jargon that purports to be a dog but is rather a dog skeleton with half the bones missing h/
3001,3,\n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100?
3002,1," The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work."
3003,3,  Multilingual Distributed Representations without Word Alignment. ICLR 2014.
3004,3, What happens if 0 has semantics ? 
3005,3, This leads to generalization for 2-layer networks with appropriate bounded size.
3006,3,"\n\n\nAdditional comments:\nThe method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN)."
3007,3,I think this only contains a single round of optimization.
3008,3,"\n\nYou use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs?"
3009,3,"""The paper presents a multi-task, multi-domain model based on deep neural networks."
3010,2,Proposition 7 was so fundamentally wrong that I saw no point in reading beyond it
3011,2,What were you thinking?
3012,3," Broadly, the approach is to have a generator produce the \""full\"" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples."
3013,3, I was wondering where this assumption is used?
3014,3," For example, in Section 3.3, how each s_(i, j, k) is sampled from S?"
3015,1,\n\nDespite of this limitation I think the paper's idea is OK and the result is worth to be published
3016,3,  It uses established work on network morphisms as a basis for defining a search space.
3017,3,"\"" experiment which trains with knowledge and then tests without knowledge."
3018,1, Some experimental settings\nare well defined to shed light on few generalization aspects of the networks.
3019,3,\n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.
3020,3," Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful."
3021,3, Previous work in using recursion to solve problems (Cai 2017) used explicit supervision to learn how to split and recurse.
3022,1,"\n\nThis paper is well written, easy to understand and presents a simple and apparently effective method of detecting out of distribution samples."
3023,3, \n1) Use the unifying notation to discuss strengths and weaknesses of current approaches (ideally with insights about possible future approaches).\
3024,1," \n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised."""
3025,3," For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks."
3026,3,"""This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model)."
3027,3,\n\n3) I have the same questions about Partial Episode bootstrapping: Is there a task in which we find our RL agents learning in time-limited settings and then evaluated in unlimited ones?
3028,3, They then go on to explore the sparsity of the latent space
3029,3, The authors also propose a way to determine the target rank of each layer given the target overall acceleration.
3030,3," I could imagine a line of experiments that investigate the idea of selectively stopping episodes when the agent is no longer experiencing useful transitions, and then showing that the partial episode bootstrapping can save on overall sample complexity compared to an agent that must experience the entirety of every episode.  """
3031,1,\n\nClarity: The mechanism of generating the text samples using the proposed methodology has been described clearly.
3032,3," The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work."""
3033,3, Only a tiny subset of words have intrinsic and context-free affect.
3034,2,The conclusion is something of a shaggy dog.
3035,3," Additionally, they propose an extension JCP-S, for n-order tensor decompositions."
3036,3,"\n\nRelated work: many related work in robotics community on the topic of task and motion planning (checkout papers in RSS, ICRA, IJRR, etc.) should also be discussed."""
3037,3,"  In particular, they look at the problem of maze testing, where, given a grid of black and white pixels, the goal is to answer whether there is a path from a designated starting point to an ending point."
3038,2,"There are numerous problems with this paper, starting with the title."
3039,3,"\n\nPage 4: \""we apply the technique of variational inference Wainwright et al. (2008)\""."
3040,2,This proposal left me cold - In response to a grant proposa
3041,3,\nThe results are only limited to single toy task.
3042,3, You say that test set also contains obfuscated documents.
3043,3,"\n\nOverall, the paper provides limited technical novelty."
3044,3,\n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities.
3045,3, Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?
3046,1,\n\nThe paper is generally well written and most details for reproducibility are seem enough
3047,3," For instance, one can find SOTA results for CIFAR100 around 16% and for CIFAR10 around 3%. Similarly, one can find SOTA results for IWSLT2015 around 28 BLEU"
3048,1, and literature review is sufficient.
3049,3, What is the bases for these parameter choices?
3050,3," To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks."""
3051,3," In particular, the authors point out that in general, SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise."
3052,3,\n\n4. How is the model trained?
3053,3,"The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016),"
3054,3," The key finding is that MC appears to be more robust than TD in a number of ways, and in particular the authors link this to domains with greater perceptual challenges."
3055,3,"  Maybe providing rather original headings is better?[[CNT], [PNF-NEU], [QSN], [MIN]]  It's a style issue that is up to tastes anyway so, again, it is minor.[[CNT], [PNF-NEU], [DIS], [MIN]]\n\n- \""However, sharing latent variables across an entire class reduces the encoding cost per element is significantly\"": typo.[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n- \""Figure ?? illustrates\"".\n"""
3056,3,"  Thus, the authors insist on context modeling to obtain a relevant analysis of a word's meaning."
3057,3," Note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations (which then leads to an approximation of desirable quantities, e.g. the gradient, in expectation) and the current approach where subtensors are considered as batches."
3058,3, Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere).
3059,3," \n\n3. In Section 4, it would be great to have more in-depth simulations (e.g., multi-task learning in various settings)."
3060,1,\n\nThis work shows few interesting results
3061,3, What\u2019s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function?
3062,1," \n- The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al."
3063,3, I would like to see a plot of the sample energy as a function of the number of data points.
3064,1,"n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance)."
3065,1,"\n\nOriginality\nThe theoretical results are original, and the SGD approach is a priori original as well."
3066,1," Even better: add the expression \""P(male = 1 | mustache = 1) = 1\""."
3067,3,\nThis paper is much related not to representation learning but to user-interface.
3068,1,"\""gives good performance\""\n- \""Recent works\"", \""several works\"", \""most works\"", etc.-> "
3069,3, Why are the Concorde models faster than unigrams and bigrams?
3070,3,"  Is the method giving us a picture of this data set?"""
3071,1,"\n - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them."
3072,3, \n\nEquation 3 sparsifies (i.e. prunes the edges) of a graph -- namely $re_{G}$.
3073,1,"""This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder."
3074,3," Further, is the auto-encoder trained separately or jointly with the training of the one-shot learning classifier?"
3075,3,  I am not an expert in this area.
3076,1, The draft is well written with convincing experiments.
3077,3,\n- The work is less about benefiting from demonstration data and more about using off-policy data.
3078,3, A deep neural network with this integral feedforward is called a deep function machine. 
3079,3, \n\nThe paper is chiefly concerned with analysing these local minima by expanding the cost function about them.
3080,3," They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments."
3081,3, Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research.
3082,3,\n\n4. A wall-time experiment is needed to justify the speedup.
3083,3, Should ALS be applied repeated (each round solves d problems) until convergence?
3084,1,\n\n\n- Pros of this work\n\nThe paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN.
3085,3, What problem does the guide actor solve?
3086,3," The authors argue that it is not a data augmentation technique, but rather a learning method."
3087,3,"\nIn any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point."
3088,3, This process is applied iteratively to different groups of layers.
3089,3,\n- Figure 6 / sect 4.2: which model-free agent is used?
3090,3, This effect is not combated by batch normalization.
3091,3,\n- the labels selected by the authors for the icu example are to forecast the next 15 minutes and whether a critical value is reached.
3092,3,\n\n3) How should we set the parameter lambda?
3093,2,It (paper) has a kind of self-help quality to it
3094,3,They also analyze the effect of various span-identification steps and preprocessing steps on the performance.
3095,3," Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet."
3096,1,\n\n# Novelty and Significance\nThe proposed idea is novel in general.
3097,3," \n\nThe crossover operator is the policy mixing method employed in game context (e.g., Deep Reinforcement Learning from Self-Play in Imperfect-Information Games, https://arxiv.org/abs/1603.01121 )."
3098,3,3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset.
3099,3," Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function."""
3100,3, Results on this data might be more convincing.
3101,3, The authors essentially develop an attack targeted to the region cls defense.
3102,3,\n* Detailed analysis on different ensemble fusion methods on both training time and testing time.
3103,3," One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3."
3104,3," Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly."
3105,3," Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used."
3106,3, Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy?
3107,3,\nIt should be stated more clearly how the results from Figure 4 were obtained.
3108,1,"""I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP)."
3109,3," However, it does not mean training the wider networks is more efficient than training the deeper ones."
3110,3," If that's not the motivation, then what is it?"
3111,1,"\n\u20282- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense."
3112,3, I would suggest to \nput the two together since they both demonstrate that the network can learn to be invariant to a certain\ndomain aspect as far as data augmentation is used to cover that aspect for at least a part of the observed\ncategories.
3113,1,\n\nPros:\n- Interesting formulation.
3114,3,"""The paper studies the expressive power provided by \""overlap\"" in convolution layers of DNNs. "
3115,3,"\nIn my opinion, this result is not too surprising given the existing power of deep learning to fit large datasets and generalize well to test sets."
3116,3,"""This paper presented a Generative entity networks (GEN)."
3117,3,  Is there a particular reason for this?
3118,3,"""This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples (for few-shot classification) or action-reward pairs (for reinforcement learning) in order to take the appropriate action."
3119,3," \n\u2028(TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader."
3120,3,"""This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used."
3121,2,"The research questions are vague, which makes them uninteresting"
3122,1,\nThe proposed method has better robustess in different tasks and different batch size setting.
3123,3, A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.\n\n-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice.
3124,3," In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper."
3125,3, It would be necessary to see the tensor reconstruction error during the following 2 scenarios:\nWe apply the CP decomposition to a pretrained network
3126,1,\n\nReview:\nThe paper is well written. 
3127,1," Is this the number of elements, or the number of rows/columns?[[CNT], [null], [QSN], [MIN]]\n\nLemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)?"
3128,2,Your abstract wouldnt have made me want to read it had I not been a reviewer.
3129,3, This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established.
3130,3,"  \n\nSome minor comments: \n\n1.\tIn page 2, 6th line after eq (1), \u201c\u2026 these two problems\u201d --> \u201c\u2026 these three problems\u201d \n2."
3131,2,"'Putting it unfairly perhaps, and I am sure this is not what is meant, but one ungenerous reading of the proposal is..."
3132,3," It would be interesting to consider also a single construction, instead of the composition of two constructions.."
3133,1,"""The paper proposes a novel workflow for acceleration and compression of CNNs."
3134,3," In my point 4, I was suggesting that in order to have clustering performance, one might alternatively work on the softmax outputs instead of the inputs."
3135,3," As usual, the label information was added as the input of generator and discriminator as well."
3136,1,\n\nIt is shown that intra-decoder attention decoder improves performance on longer sentences.
3137,3,  The analysis answers:\n\n1) When empirical gradients are close to true gradients\
3138,3,"\n\nHowever, I think this paper has limited contribution and novelty,"
3139,3," Pixel GAN rather then only giving a global score for the whole image? """
3140,3," \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"""
3141,3, The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work.
3142,2,This was one of the least interesting papers that I have read in quite some time.
3143,3,"""The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs."
3144,3,"\nNevertheless, the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN.\n\n"""
3145,3, The main framework is based on the Dawid-Skene model.
3146,1, And using the procedure to test and improve current version of GANs. It demonstrated some interesting stuff.
3147,1, These are pros of the approach.
3148,1,"  .\n\nThough experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline."
3149,1,\n\n+ Paper is well written and easy to follow.
3150,3,\n\n3. The paper wants to find a good trade-off on speed and accuracy.
3151,3,"""This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image."
3152,3," After obtaining this operator, this paper substitutes this operator to the optimal control problem and solve the optimal control problem to estimate the optimal control input, and show that the gap between the true optimal cost and the cost from applying estimated optimal control input is small with high probability."
3153,1,"  \n\nDespite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.\n"""
3154,1," While the motivation of the paper makes sense, the model is not properly justified, and I learned very little after reading the paper."
3155,3," Because of different benchmarks, it is not clear whether the performance improvements are due to technical improvements or sub-optimal parameters/training for the baseline methods."
3156,3,\n* On implementation - the authors mention using Tensorflow\u2019s auto-differentiation.
3157,3," It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work."
3158,3, Several experiments demonstrate the quality of the prediction and the uncertainty over dropout. 
3159,2,This paper is desperate. Please reject it completely and then block the author's email ID so they can't use the online system in the future.
3160,3," To do something similar with GPs, we would need to learn the kernel."
3161,2,"The following discussion seems to ignore this major flaw, which turns mere arm-waving into Olympic-level calisthenics"
3162,1,\n\n- paper is overall well written/clear
3163,3,\n\nThe experiments range over 4 video datasets. 
3164,3," The author also addressed this point, and I changed my scores accordingly. """
3165,3," How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL."
3166,1,"\n\nOverall the paper is a clearly written, well described report of several experiments."
3167,3," If it is from the bot response, then it is known which words are named entities therefore embedding can be constructed directly."
3168,3, The fact that the paper does not come with substantial theoretical contributions/justification still stands out.
3169,3,  A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable.
3170,3,"\n\nFurthermore, I would like to see two additional analysis."
3171,1,"\n\nOverall, this work is providing useful experimental insights, clearly motivating\nfurther study.\n"""
3172,3," Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target."
3173,1,. That is an advantage.
3174,3," For example, the experimental result on structured QA task (section 3.1), where it states that the performance different between models of With-NE-Table and W/O-NE-Table is positioned on the OOV NEs not present in the training subset."
3175,3," Moreover, in Sec. 4.3, it is said that agents can see around them +10 spaces away; however, experiments are run in 7x7 and 10x10 grid worlds, meaning that the agents are able to observe the grid completely."
3176,3," In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \""expert\"" trajectories can be generated in unlimited quantities."
3177,3, As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction.
3178,3,"\n\nThis appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods)."
3179,1,"\n\n\nPros:\n- results\n- novelty of idea\n- crossover visualization, analysis\n- scalability;"
3180,1," \n\nThe theory part seems to be technical enough and interesting,"
3181,3,. \nThe paper compared the vanilla version of BiVCCA but not the one with factorized representation version
3182,3," I think there\u2019s an easier way to make this argument:\n\nGiven an unbiased estimator \\hat{Z} of Z, by Jensen\u2019s inequality E[log \\hat{Z}] \u2264 log Z, with equality iff the variance of \\hat{Z} = 0."
3183,3,"""This paper proposes a model for solving the WikiSQL dataset that was released recently."
3184,3," ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)"
3185,3, \n\nAside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters
3186,1," Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas."
3187,3, What does DeltaVar mean in eq (2)?
3188,3,"\nIf there is a gain to learn a shared embedding manifold, which is plausible, this gain should be evaluated between a baseline, that learns separately the games, and an algorithm, that learns incrementally the games."
3189,3,  The authors use an energy function based approach.
3190,3," Also it would be very useful to say that a physician was consulted and that the critical values were \""clinically\"" useful."
3191,3," It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening. "
3192,3,\n\n- Are the checkpointing modules designed to only detect adversarial examples?
3193,2,Please reject it completely and then block the authors email ID so they cant use the online system in the future.
3194,1, The authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from Wikipedia and IMDB.
3195,1, The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield.
3196,3, The proposed method is then tested on two image data sets.\n\n
3197,1,  This work has a well-established motivation: traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predictions.
3198,3, Could you provide a large sample of the data at an anonymized link?
3199,3,"\nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images."
3200,3,"""This paper proposes to use RGANs and RCGANS to generate synthetic sequences of actual data."
3201,3," The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models."
3202,3, This I do not think is a reference to a facial alignment method bu t rather a set of general purpose linear algebra methods.
3203,3," In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing."
3204,3,"""This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks."
3205,1," In general, questioning the reliability of the visualization techniques is interesting."
3206,2,The writing and presentation are so bad that I had to go home early and spend time wondering what life is about.
3207,3, The method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human drivers.
3208,3," Are the results still valid for another measure of discrepancy based for instance on another measure, e.g. Wasserstein?"
3209,1,\n\nThis work is well-written and cites previous work appropriately.
3210,3," \n-- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016."
3211,2,"Your content is stellar and your writing is excellent. However, consider hiring an editor for subsequent revisions - The entire revie"
3212,3, I could not imagine how VAE and T are trained simultaneously.
3213,1, \n\nThe idea is simple and easily explained in a few minutes
3214,3, A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for.
3215,1,"\n- This gives a more unified way of understanding, and implementing the methods."
3216,1,  This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results.
3217,3," Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation."
3218,3," Given the recent interest in (deep) reinforcement learning (combined with the lack of theoretical guarantees in this space), this is a very timely problem to study."
3219,3, Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.
3220,3,"  One simple one would be what if they only used the f and draw z samples for N(0,1)?"
3221,3,"\n\nHowever, as discussed in the introduction, the reason an efficient\nsampling method might be interesting would be to provide insight\non the components of perception."
3222,3,"  That\u2019s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation. In the discussion, it says. "
3223,3,"""The paper proposes an additional transform in the recurrent neural network units."
3224,3,"\n4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y).[[CNT], [PNF-NEG], [DFT], [MIN]]\n5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10."
3225,3, Have you estimated the probability for positive vs. negative gradient values for  K=10?
3226,3," According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why?"
3227,1," But it seems that the extent of these guarantees are comparable to those of several other generative models, including WGANs, the Sinkhorn-based models of Genevay et al. (2017, https://arxiv.org/abs/1706.00292/), or e.g. with a different loss function the MMD-based models of Li, Swersky, and Zemel (ICML 2015) / Dziugaite, Roy, and Ghahramani (UAI 2015)."
3228,2,"To be frank, it was boring to read once one got past the beginning. This is rather ironic given the paper is about humour."
3229,3, What is the motivation to just average the inverse covariance matrices to compute S_C? 
3230,3, \n\nPros: the intuition and geometry is rather clearly presented.
3231,3,"\n4. Is the error in Table 2 averaged over multiple runs? If yes, how many?"
3232,3,"  For instance, learning via RL + demonstrations was already studied into papers by Farahmand et al (APID, @NIPS 2013), Piot et al (RLED, @ ECML 2014) or Chemali & Lazaric (DPID, @IJCAI 2015) before Hester et al (DQfD @AAAI 2018)."
3233,1,\n\nThe paper is well written and the method reasonably well explained
3234,3," Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).33.07"
3235,3,"  Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4?"
3236,3, They apply the rotations before the final tanh activation of the LSTM and before applying the output gate.
3237,3," This is the main contribution of the paper, since swan already exists."
3238,3," Thus, I would say original."
3239,3,"\n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case."
3240,3,"\n\nMy main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?)."
3241,3, What input is injecting?
3242,3, The experiments are conducted on the Shapeworld dataset.
3243,3, 1) The arguments for using clusters instead of single sentences are questionable.  
3244,3,"\n\n- It's odd to write \""we do not suggest a specific neural network architecture for the\nmiddle layers, one should seelect whichever architecture that is appropriate for the domain at\nhand."
3245,3,"""This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs)."
3246,3," As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set."
3247,3," It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked)."""
3248,3,"""This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy."
3249,3,"  So some regions are more important than others, and the top half may be more important than an equally spaced global view."
3250,3," However, there are a couple additional experiments that would be quite nice:\n\u2022\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X)."
3251,3,"""This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models."
3252,3, \n-- How to interpret the isolated components condition in Theorem 4?
3253,3,"""This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks."
3254,3,\n to quickly sample examples with large inner products with the current parameter vector \\theta.
3255,2,"The authors are permitted to believe what they want to, but the data did not support important implications."
3256,3," However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here."
3257,1,\n\nQuality: SeaRnn is a well rooted and successful application of the L2S strategy to the RNN training that combines at the same time global optimization and scalable complexity.
3258,3,  \nThe suggested method follow an iterative process in which the student and teacher are interchangeably used.
3259,3, I would perhaps even call this a distillation network rather than a crossover network.
3260,3," But more importantly, I think that the proposed method has its limitation about what kind of physical systems it can model."
3261,3, \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP).
3262,3," A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points."
3263,3,"""Summary:\n\nThis paper proposes generative models for point clouds."
3264,3, This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body.\
3265,1, The experimental results show the authors have found a way to use second order methods without making performance *worse*.
3266,1,"""The paper is clear and well written."
3267,1," The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models."
3268,2,This is starting to feel like a book report
3269,3," If TS does not select certain actions, the Q-function would not be updated for these actions."
3270,3," For example, it is difficult to discover residual network denovo."
3271,3," To the best of my knowledge, Mirowski et al. never made such a bold claim (despite the title of their paper)."
3272,3,. I would suggest keeping the main results in the main body and move extended results to an appendix.
3273,3, This is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced above.
3274,2,The lead author of this study has an apparent history of convincing otherwise well-respected scholars to be unwitting co-authors on his poor excuses for academic papers
3275,3,\n\nWord2gauss also evaluates on similarity and relatedness datasets.
3276,3," In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks."
3277,3,"""This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks."
3278,3,"\n\n[7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015"
3279,1,\n\nOriginality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings. 
3280,3, The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.
3281,3," For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks?"
3282,3," Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance."
3283,3,An further external reference \ncould be used to give an idea of what would be the experimental result at least in the supervised case.
3284,3, And there should be a single definition for L_pred.
3285,3," This would be very interesting to have arguments on why being better than the \""Dirac estimation\"" in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation)."
3286,1, but it is to the best of my knowledge the first explicit application of this idea in neural architecture search.
3287,3," The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations."
3288,3,"""The paper present online algorithms for learning multiple sequential problems."
3289,3,"\n3. Why the authors choose Persian Cat, Container Ship, and Volcano in the experiments?"
3290,3,"\n\nGenerally, I find a jarring mis-fit between the motivation (deep learning\nfor driving, presumably involving millions or billions of parameters) and\nthe actual reach of the methods proposed (hundreds of parameters)."
3291,2,"Sprinkled here and there are some things, also taken from his readings, that are more or less correct. But it is all very confused."
3292,2,This paper is to science as astrology is to astronomy h/
3293,3,"  If not, what is the difference between s_0:m and s_1:m?"
3294,3, What if I only have pure text document without these HTML structure information?
3295,3,\n- The definitions of the statistics and features (state and observation features) look highly elaborated.
3296,3,\nThis is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision.
3297,1, but there seems many other things to play with.
3298,3,  I apologize if some of\nthese comments have already been addressed in replies to other reviewers.
3299,3, Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)?
3300,1, \n\nAnother task you could try is to learn to perform the same task in two different environments.
3301,3,"\n\nFor the newly added case of VAN(lambda=0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id=Sywh5KYex)."
3302,3,"""EDIT: The rating has been changed. See thread below for explanation / further comments."
3303,1," Overall, I think this is interesting work,"
3304,1," The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers."
3305,3, This result is formally stated and proved in the paper.
3306,1," Figure 3 shows that the proposed method learns faster than DQN,"
3307,3, How many classes and examples does the testing problems have?
3308,3,"""In this work, the objective is to analyze the robustness of a neural network to any sort of attack."
3309,1, The paper is very well motivated and tackles an important problem.
3310,1," While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference. "
3311,3," Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc."
3312,3," The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels."
3313,3," They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure."
3314,1,\n\nIt is very positive that the figures are very helpful for delivering the information.
3315,3,"""Quality\n\nThe authors introduce a deep network for predictive coding."
3316,3, The authors state significant improvements in classification using generated data.
3317,1," To the best of my knowledge, the method proposed by the authors is novel, and differs from traditional sentence generation (as an example) models because it is intended to produce continuous domain outputs."
3318,3," There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies."
3319,3," \n- Why are (1, 0) and (1, 1) not useful pairs?"
3320,3,"""An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning."
3321,3," The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks."
3322,3,\n\nI will adjust my score based on the answer to these questions.
3323,3," c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space."
3324,2,This is a long and tedious manuscript 
3325,3," The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables."
3326,3,  Do the authors think researchers never tried to do this task before then?
3327,1," Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach."
3328,3,"""Summary: the paper proposes a tree2tree architecture for NLP tasks."
3329,3,"\n\nOverall, the paper proposes an interesting improvement to this area of synchronous training, however it is unable to validate the impact of this proposal."""
3330,3,"  Identifying some weakness in the formulation, the authors propose 5 solutions."
3331,3,"""This paper introduces a convolutional autoencoder for irregular graphs, specifically surfaces in the form of discrete meshes in 3D."
3332,3," If yes, what is its value?"
3333,3," During evaluation, what is a step?"
3334,3, So I think that the motivation behind introducing this specific difference should be clear.
3335,3," From the methodology point of view, such extensions are relatively straightforward."
3336,3,\n\n\n- References\n\n[1] Edward Snelson and Zoubin Ghahramani.
3337,2,"In the interest of being helpful, my suggestion is that the authors go back and review what is involved in the scientific method"
3338,3,"\n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one."
3339,2,This paper could be considered for acceptance given a rewrite of the paper and a change of the title and abstract.
3340,3, The authors should try to give their opinion about the design obtained.
3341,3," To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack."
3342,3," This has already been successfully applied in multiple domains eg. in computer vision (Krizhevsky et al, NIPS 2011), NLP (Bahdanau et al 2014), image retrieval (Krizhevsky et al. ESANN 2011) etc, and also studied comprehensively in autoencoding literature."
3343,1," As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution."
3344,3,"  I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic."
3345,3," It's only baseline is a GAN model that isn't even very convincing (GANs are finicky to train, so is this a badly tuned GAN model?"
3346,3," Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution."
3347,1,"  \n\nOverall, I liked the paper."
3348,3," They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation."
3349,3, These two methods seem to be closely related and should be thoroughly compared.
3350,3,\n\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.
3351,3,   An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.
3352,3," Because the layer-wise decoupling, it can easily be applied for distributed training of the model."
3353,1," Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks."
3354,3,"""The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise."
3355,3,"\n2) The scaling rules described in section 5 could help practitioners use much larger batch sizes during training, by simultaneously increasing the learning rate, the training set size, and/or the momentum parameter."
3356,3,"  My scores were changed accrodingly.\n"""
3357,2,"This paper reads like a womans diary, not like a scientific piece of work"
3358,3, Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement?
3359,3,"2) It would be easier for discussion if the authors could assign numbers to every equation."""
3360,3,"\n\nIs the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)? "
3361,3, \n\non the positive side:
3362,3, The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples.
3363,3,"  To solve this problem, the authors proposed five formulations in the final prediction layer."
3364,3,"  \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 \u2014> see Fig.1\npage 4just before equation 8: the the"""
3365,1,"""--------------------\nReview updates:\nRating 6 -> 7\nConfidence 2 -> 4\n\nThe rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I\u2019ve increased my score.\n-----"
3366,1, They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.
3367,3, Optimal with respect to what task?
3368,2,I recommend the publication even if I am not impressed
3369,3,"\n\nIf I may summarize the key takeaways from Sections 5.4 and 6, they are:\n- GAN training remains difficult and good results are not guaranteed (2nd bullet\n  point)"
3370,3," The new derivation of the method yields a much simpler interpretation, although the relation to the natural gradient remains weak (see below)."
3371,1,"  Experiments are conducted on the task of image classification for a couple well-known DNN architectures (VGG and Resnet) to show a speedup of runtime in testing, significant compression of the network, and minimal degradation in performance."
3372,2,The presentation is of a standard that I would reject from an undergraduate student
3373,1, I also like that they try to design experiments to understand the role of specific parts of the proposed architecture.
3374,3," Until that has been shown, the impact of this paper seems somewhat limited to me."
3375,3,"""This paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs."
3376,3,"  \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size."
3377,1,\n- The structure of the network is clearly justified in section 4.
3378,1,\n\nThis is an intriguing idea
3379,3,"""This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN)."
3380,3," The \""parallel ordering\"" terminology also seems to be arbitrary..."
3381,3,"""This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network."
3382,3, \n\n\nMinor remarks\n- Table 1: the reported results of the Matching Network are different from the results in the paper of Vinyals (2016).
3383,3, \nSo the two can be presented together in the same section.
3384,3,"\n\nThe authors conjecture that several of the above observations can be explained by the fact that the training target in MC methods is \""ground truth\"" and do not rely on bootstrapping from the current estimates as is done in a TD rollout."
3385,3,\n\nCons:\n(1) No direct comparisons with other methods are provided.
3386,3," Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks."
3387,1,"\n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task,"
3388,3,"\n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2."
3389,3," Again parameter tuning would potentially change all of\nthese figures significantly, as would e.g. a change in hardware."
3390,3,\n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.
3391,3," - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem)."
3392,3," No Bayesian elements, like prior or likelihood appears here."
3393,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -Edito"
3394,3, I would suppose that flatness tends to increase the variability captured by leading eigenvectors ?
3395,3," The experiments show that this approach can be used to successfully detect adversaries for several datasets, including MNIST, CIFAR10, and a small subset of ImageNet, and also investigates the robustness across different variations of attack.\"
3396,3, It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.
3397,3,"\n- Section 1, second paragraph: senstence\n- Section 3.1, first paragraph: thorugh\n- Section 5: architetures"""
3398,3, \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?
3399,1," \n\nThe introduction is fairly strong, but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy \""1) train S on random exs; 2) train T to pick exs for S\"" makes sense."
3400,3," By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment."
3401,3,"  Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities."
3402,3," This limitation is from Hazan et. el., 2017, where the authors wants to predict the output."
3403,3,"""This paper extends the framework of neural networks for finite-dimension to the case of infinite-dimension setting, called deep function machines."
3404,3,. It can be fine to not be SOTA as long as it is acknowledged and discussed appropriately.
3405,1, It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions.
3406,3,"""In this paper, the authors proposed a method to transfer the text style to a specific target style."
3407,3, Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance.
3408,3, Or is there some form of supervision involved?
3409,3," \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?"
3410,3,  The best human designs outperform the evolved networks.
3411,3, -- finding a more efficient search path would be an important next step.
3412,3,\n\n4. Are you willing to release your code to reproduce the results?\
3413,3,"  To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?"
3414,2,This is from a methods perspective rather unacceptable in these days of voodoo science.
3415,3," The paragraph on p. 2 is now written without do-notation (\""intervening Mustache = 1 would not change the distribution\"").[[CNT], [PNF-NEG], [CRT], [MIN]]  But this way, the statements are at least very confusing (which one is \""the distribution\""?).[[CNT], [CLA-NEG], [CRT], [MIN]] \n- I would get rid of the concept of CiGM.[[CNT], [null], [DIS], [MIN]]  To me, it seems that this is a causal model with a neural network (NN) modeling the functions that appear in the SCM."
3416,3, In what we call the Pong Player\u2019s Dilemma (PPD) when an agent \nscores they gain a reward of 1 but the partner receives a reward of \u22122.
3417,3, However I would encourage authors to rephrase their claim of emergent translation (the title is misleading) as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages (bridged by the image) and hence there is no autonomous emergence of translation out of need.
3418,3,   This papers experimentation sections sets a positive example by exploring a comparatively large space of standard model architectures on the problem it proposes.
3419,3," \nSentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?"
3420,3,"   Regardless, more discussion regarding what the Ax_i embeddings are meant to capture (in contrast to the Cx_i vectors) would be appreciated."
3421,3,"""This paper investigates the impact of character-level noise on various flavours of neural machine translation."
3422,3," Also, tasks may have hierarchical correlation structures."
3423,1,\n\nThe introduction was fine.
3424,3," From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role. "
3425,3,"\n5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser?"
3426,1,  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets.
3427,3," The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications."
3428,3," \n\nIn related work, I would cite co-training approaches."
3429,3,"""The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations."
3430,3, The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference).
3431,3,"\n\nThe list goes on...\n\nFor such a complex architecture, the authors must try to analyze separate modules as much as possible."
3432,1," However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. "
3433,3,"""The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem."
3434,3,"   \nSection 5.2 was nice and so was 5.3.[[CNT], [null], [APC], [MAJ]] However, for the covariate specific analogies (5.3.) the authors could also analyze word similarities without the analogy component and probably see similar qualitative results."
3435,1,\n+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks
3436,3, This is used for learning initial feature representation of the student model.
3437,3, That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples.
3438,3,"\n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point."
3439,3,\n\nMinor things:\n-Missing propto in Eq 7
3440,3," Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice; multi-step updates, regularized against MC returns, stochastic mirror descent."
3441,1, Each extension to the Dual Actor-Critic is well motivated and clear in context.
3442,2,The introduction seems pointless. It offers some odd views
3443,3, There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one.
3444,1,"\nAppendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation[[CNT], [CNT], [DIS], [MIN]]\n\nConclusion:\nThe paper follows an interesting approach,"
3445,3," In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps."
3446,2,The peaceful atmosphere between Christmas and New Year was transiently disrupted by reading this...
3447,3,"  Essentially, a normalization vector is maintained an updated separately."
3448,3, Just a horizontal line in blue and red?
3449,1,\n\nThe experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods.
3450,2,Publishing this manuscript would disembowel the credibility of this journal
3451,3,"\n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}?"
3452,3," To get a better impression\nof what approaches perform well, their parameters should be tuned to the\nparticular benchmark."
3453,2,"If the results are correct they cannot be new, if they are new they cannot possibly be correct. It is hard to say which is the case"
3454,1," Due to the simplicity of the method, this paper could be a useful baseline for future work."
3455,3, The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers.
3456,3," Also, comparison needed to fitting GloVe on the entire corpus (without covariates) and existing methods Rudolph et al. 2017 and Cotterell et al. 2017."
3457,3, Though the proof of Lemma 2 only appears to be using the 1-Lipschitzness property of phi as well as phi(0) =0. (Unless they can generalize further; I also suggest that they explicitly state in the (interesting) Lemma 2 that it is for the ReLU activations (like they did in Theorem 1)).
3458,1,  Great to see that the method is fast---it seems fast enough to use in practice in a real IDE.
3459,3," They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation."
3460,3," Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well."""
3461,3," Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice."
3462,3,"""The authors introduce the task of \""defogging\"", by which they mean attempting to infer the contents of areas in the game StarCraft hidden by \""the fog of war\""."
3463,3,"""One of the main problems with imitation learning in general is the expense of expert demonstration."
3464,3," While the first three tasks are smaller proof of concept, the last task could have been\n  more convincing if near state-of-the-art methods were used."
3465,3,\n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.
3466,3,\n \nThere are possible future directions to be developed.
3467,3," Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity."
3468,3," However, in [4] the authors manage to avoid the additional Q(A) approximation that breaks the variational bound."
3469,1," \nThe paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth."
3470,1,  The results support the method's utility.
3471,3,"\n\n------------(Original review below) -----------------------\n\nThe authors present an enhancement to the attention mechanism called \""multi-level fusion\"" that they then incorporate into a reading comprehension system."
3472,3,"""Authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information."
3473,3, Thus the learned weak learner at this round will make different mistakes.
3474,3, These details are useful for judging technical correctness.
3475,3," However, the authors do not explain how this architecture can be used to do the domain adaptation."
3476,1," The overview of the literature is also very well done. """
3477,3,"  In fact, such works even not that lambda should be related to confidence."
3478,3, The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions.
3479,2,I would advise the authors to go back to the drawing board and consider exactly what this paper is trying to do and do this well
3480,3,"""This paper aims to provide a continuous variant of CNN."
3481,3,"""The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy)."
3482,3,". All data are then labelled as \""original\"" or \""transformed by ...(specific transformation)\"". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels."
3483,2,It is always rather pleasant to recommend that a paper be rejected.
3484,3,"""The below review addresses the first revision of the paper"
3485,3," Firstly, it shows experimentally that the same effects appear even for simple models such as linear regression. "
3486,3,"- Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017."
3487,3," It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547)."
3488,3," This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images."
3489,1,.\n\nI think this paper should be accepted as it is interesting and novel
3490,3,"\n\nDetailed answers:\n\n1) Indeed, I was not aware that the paper only focuses on one dimensional functions."
3491,3, \n\n2. It will be great if the author could provide some discussions with respect to the analysis of information bottleneck [3] which also discuss the generalization ability of the model.
3492,3,\n\n3. The paper could benefit greatly from better integration with the existing literature.
3493,3, \n- Comparison to VPN on Atari is not much convincing.
3494,3," This year strong improvements over state-of-the-art have been achieved using attention for translation (\""Attention is All You Need\"") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition)."
3495,3," I.e. first doing the expansion keeping the beta terms and Frobenius norm sum, and then going directly to the current O(...) term."
3496,2,"The paper descends into nonsense, never to return, on line 44."
3497,3," If one can initialize lambda arbitrarily and have this method find the optimal lambda, that is more impressive than a method that works simply because of a fortunate initialization."
3498,3," \n\nIt is nice that the authors tried to extend the \""noise as target\"" to the clustering problem, and proposed the simple \""delete-and-copy\"" technique to group different data points into clusters."
3499,3,"""This paper investigates the effect of adversarial training."
3500,3," But section 4 and 6 need a lot of details. [[CNT], [CNT], [DFT], [GEN]]\n\n2) My comments are as follows:\ni) this paper claims that this is a general sentence embedding method, however, from what has been described in section 3, I think this dependency is only defined in HTML format document."
3501,3,"  For example, if it is due to overfitting, is there a correlation between performance and size of FNNs? "
3502,3," \n\nOn permuted MNIST, Table 2 could include results from [1-4]."
3503,3, Is it for space? for speed? for expressivity of hypothesis spaces?
3504,3,\n\nWas the edge-importance reported in Section 2.3 checked against various measures of edge importance such as edge betweenness?
3505,3, The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed.
3506,3,\n\nI understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring.
3507,3, The question this leaves open is whether f-Hyperband would reach the same performance when continued or not. 
3508,3,"\n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}."
3509,3, Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label).
3510,3," While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer."
3511,2,"I cannot, for the life of me, figure out why this paper was written."
3512,3, Is it data dependent?
3513,3,"\n\nOriginality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work."
3514,1, \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision.
3515,3," In this model, self-organising is a property of the hidden neurons' activation (eq. 1-3), and the training procedure is entirely supervised."
3516,3," For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward."
3517,2,I do not have the background to assess the accuracy or to detect errors in the equations but I do not agree with this
3518,3,\n3) When the empirical risk is close to the true risk.
3519,3," Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs."
3520,3," \n[2] G. D. Konidaris, S. Niekum, and P. S. Thomas. TD\u03b3: Re-evaluating complex backups in temporal difference learning."
3521,3, An auxiliary \u201clabel difference cost\u201d was further introduced to encourage class information captured by the foreground generator.
3522,3," While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers. "
3523,1," \n\nThe contribution in terms of the theory of using random features to perform kernel ridge regression is novel, and interesting."
3524,3, It would strengthen the paper significantly if the experiments presented on WSJ in the first section were also conducted on the \u201cinternal\u201d task with more data.
3525,2,I don't see much science in this manuscript.
3526,3,"\"" It also proposes \""Generalized Binarization Transformation\"" for the first layer of a neural network."
3527,3," As a result, in my view, the paper has limited novelty and originality."
3528,3,"""This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors."
3529,1," As is, while results are promising,"
3530,3,"""The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning."
3531,3,"\n- The most influential deep learning paper here might be Seide, Li, Yu Interspeech 2011 on CD-DNN-HMMs, rather than overview articles"
3532,3,"""This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets."
3533,3, We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge?
3534,3, Why not adding (non Bayesian) context (not label) to the task will not work as well?
3535,3," The FGSM method (Goodfellow et al, 2015) is basically 1 inference operation and 1 backward operation."
3536,3," \n- In Section 7, it's less useful to spend time describing what happens when the visualization is done wrong (i.e. projecting along random directions rather than PCA vectors) -  this can be put in the Appendix."
3537,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
3538,3,  This could be a way to decrease the number of CG iterations that must done at each step.
3539,3," b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters."
3540,3, This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches.
3541,3, The authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model.
3542,3,\n- 1D CNNs would be TDNNs
3543,3," Sens-FGSM outperforms the adversarial training defenses tuned for the \u201cwrong\u201d iteration, but it does not appear to perform particularly well with error rates well above 20%."
3544,3," The contributions here are: 1) experiments testing the DTP algorithm on more difficult datasets,"
3545,1, The results are moderately convincing in favor of the proposed approach.
3546,3," The authors do not comment on this.[[CNT], [CNT], [DFT], [MIN]] It would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases (e.g. cases when the low-rank decomposition leads to a significant decrease in performance)."
3547,3, Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs.
3548,2,"It is not clear how the of data presented in Figures 1, 2, 4, 5, 6, 7, 8, S1, and S2 was quantified and analyzed"
3549,3,"\n\nIt appears that the output for kennen-o is a discrete probability vector for each attribute, where each entry corresponds to a possibility (for example, for \""batch-size\"" it is a length 3 vector where the first entry corresponds to 64, the second 128, and the third 256)"
3550,3,Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student?
3551,3," This article focuses on the calculation of gradient for write network, and provides some mathematical clues for that."
3552,3,"""This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016)."
3553,1,"\n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores)."
3554,3, Empirical results are presented for several benchmarks.
3555,3,"""The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations."
3556,2,The authors conclusions not only contradict their own data but also the laws of thermodynamics
3557,3, I would like to see results from a few different random seeds.
3558,3," The focus of the paper is confusing: while the introduction discusses about works on CNN model interpretability, the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier, without analyzing anything in relation to interpretability."
3559,1,. I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing
3560,3,  An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym.
3561,3,"n\n- The paper could possibly be clearer by integrating more of the \""background\"" section into later sections."
3562,3, This can be used to determine which model should be trained (further).
3563,3,"\"" -> The existing network architecture is used to provide a variational inference framework for I(Z,Y)."
3564,3,The present paper proposes to obtain mixed strategy through an online learning approach.
3565,3, \n\nThe function class Zonotope is a composition of two parts.
3566,2,My recommendation for the authors is therefore to shelve the manuscript
3567,3," On the other hand, if no communication with the central station is allowed, then positions of other agents may be also partial observable."
3568,3,  The second problem of what tasks to evaluate on is a general problem with comparing RNNs.
3569,3," \n\nLast, show the sensitive results of the proposed method by tuning alpha and beta."
3570,3," This is different from the sum of the original losses, which seems to be the one used to train the \u201cequal weight\u201d baseline."
3571,3," A rich model is required in order to minimize the discrepancy between the distributions of the domains, while a  not too complex model is necessary to avoid mappings that are not \u00ab\u00a0meaningful\u00a0\u00bb.\n To this aim, the authors  introduce a new notion of function complexity which can be seen as a proxy of Kolmogorov complexity."
3572,3," Therefore, the authors derived the update formulate based on the analytical continuation technique."
3573,3, I personally find this exciting/refreshing and will be useful in the future of machine learning.
3574,2,The overall tenor is disturbingly glossy and pretentious given the gravity of the topic
3575,1, The author also reports state-of-art results.
3576,1,"\nHowever, performance results seem to be competitive and that's the reader may\nbe eager for insights."
3577,3, One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.
3578,3,  Is 8 good for architectures A through E?\n3.
3579,3," Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed."
3580,3,  These scaling rules are confirmed experimentally (DNN trained on MNIST).
3581,3," The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network."
3582,3," Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization."
3583,3," The authors present some observations of the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors (GloVe in this work), and test the refined vectors on 80 TOEFL questions and 50 ESL questions."
3584,3,"\n\n- Related to the above, it should be clarified what is meant by dropping a\n  class."
3585,3," Your a(.) function seems to be the policy here, which is invariable denoted \\pi in the RL literature."
3586,3," The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over)."
3587,3,\n-How does the distribution of Q values look like during different phases of learning?
3588,3," This allows them to perform the vector products needed per layer; two's complement representation also allows for an \""easy\"" implementation of the ReLU activation function, by \""checking\"" (multiplying by) the complement of the sign bit."
3589,3,.\nUPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf\nEfficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf\nA bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf\n\n4.
3590,3," The following is the differences that I found from this paper, so it would be important to discuss why such differences are important."
3591,3, These solutions achieve zero squared-loss.
3592,3, Consider expanding on that in section 4.5.
3593,3,\n\nThe proposed method is evaluated on just one dataset.
3594,3," One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. "
3595,3," This is a general comment over this kind of approach, but I think it should be addressed. \n"""
3596,3,"  Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains."
3597,3," However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space."
3598,2,This code sample cannot be adequately described without the use of strong language.
3599,3," See, for example: https://arxiv.org/pdf/1711.00489.pdf.]\n3. Hybrid method is even better."
3600,3,"""This paper proposes to automatically recognize domain names as malicious or benign by deep networks (convnets and RNNs) trained to directly classify the character sequence as such."
3601,3, More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion.
3602,1,\n\nClarity:\nThe paper is clearly written
3603,3,"""Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network."
3604,3, A neural network that finds a naturalistic solution for the production of muscle activity.
3605,1," \n\n= Clarity = \n\nOverall, the exposition regarding the method is good."
3606,3," Finally, they show that singular perturbations can be easily detected."
3607,3," Once it is train it will give away the decoder and keep the encoder for sending information.\n\n"""
3608,3,"  There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. \""universal schema\"" [Riedel et al, 2014]."
3609,3," The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or \u201cstyle\u201d features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy."
3610,3, I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations.
3611,1," Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis"
3612,3,"\n\nRegarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this."
3613,3,\nI ignore the complexities of n-step learning and discount factors for clarity.
3614,1,"""This paper presents an simple and interesting idea to improve the performance for neural nets."
3615,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
3616,1,"\n\nSection 3 could use some more signposting.[[CNT], [SUB-NEU], [SUG], [MIN]] Especially for 3.3 it would be good to explain (either at the beginning of section 3, or the beginning of section 3.3) why these measures matter and what is going to be done with them.[[CNT], [SUB-NEU], [DIS], [MIN]]\n\nIt's good that LEAR is mentioned and compared against, even though it was very recently published."
3617,3,MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.
3618,3," As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed."
3619,3," First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD."
3620,3, Here it would be interesting to see how CCC fares (in all combinations with cooperators and defectors).
3621,3,\n\nThe main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principle.
3622,1," This itself can be a small, nice contribution."
3623,3," After reading through both, my review score remains unchanged."
3624,3, The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class.
3625,3," Within this specific setting the authors differentiate their approach from others \nby developing a solution that does NOT estimate an explicit dynamics model ( e.g.,  P( S\u2019 | S, A ) )."
3626,1,"\nThe paper is well written (up to a few misprints), the introduction and the biological background very accurate (although a bit technical for the broader audience) and the bibliography reasonably complete."
3627,1," I Firstly, the paper introduces a large sketch dataset that future papers can rely on."
3628,2,"Not all that compelling an idea to me, regardless of whether it is true"
3629,3," Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words?[[CNT], [PNF-NEU], [SUG], [MIN]] And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away?"
3630,3,\n\n- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images.
3631,3, Ideally further insights could be derived from these results.\
3632,3,"Notably, the question of transfer\nand generalization is of high relevance here."
3633,3,\n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers.
3634,1,\n\nSignificance:\nThis is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumstances.
3635,3,"""Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches."
3636,3, Will the method discover more classes when 100 unknown classes are used?
3637,1," I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion."
3638,3,"""The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly."
3639,1,\n - Table 2: These results are interestingly different.
3640,3, The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.
3641,3, The agent is tasked with driving the ego-car down the n-lane highway and stopping at \u201cthe exit\u201d in the right hand lane D km from the start position.
3642,1, Logic flows naturally within the paper.\n\n
3643,1,"The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. "
3644,3,"\n\nEvaluation: The link prediction task is too easy, as links are missing at random."
3645,1, One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M)
3646,3,"\n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points."
3647,3," Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change."
3648,3,\n\nThe main idea of  using upper bound (as opposed to lower bound) is reasonable.
3649,1,"\n\n-\tThe authors bring a good point on the limitations of the SVRT dataset \u2013 mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations, which make it difficult to quantify the effect of image variability on the task."
3650,3,"\n\nComments:\n\t\n1-\tThe results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach."
3651,3, The proposed algorithm is validated on several image classification datasets.
3652,3," Then, a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribution."
3653,3,"\n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples."
3654,3," First, the model is *not* trained on the true distribution which is unknown."
3655,2,This technique will never work.
3656,3, Does this sacrifice exploration for exploitation in some quantifiable way?
3657,3,"\n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\n"
3658,1," \n\nPerhaps the authors have just done a good job of laying the groundwork, but the dual-based approach proposed in section 3.1 seems quite natural."
3659,3," Secondly, the paper introduces the model for generating sketch drawings."
3660,3,"The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing."
3661,3,"""\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL."
3662,1, The presented algorithm sketch-rnn seems novel and significantly different from prior work.
3663,3," It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable."
3664,3, Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity.
3665,3, Algorithm 1 is also stated from that thinking and it is a well-known optimization algorithm.
3666,3,"""Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and"
3667,1, First the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line version.
3668,3,""".\n\n\nMinor comments:\n- page 1: The reference list could also include  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00934 and  https://arxiv.org/abs/1510.02777\n- "
3669,3, I would be excited to see similar analysis of other toy problems involving graphs.
3670,3," The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3)."
3671,1,\nii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings.
3672,1, \n\nOverall I think this is a good paper.
3673,3," Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state."
3674,1,\n\nThe experiment section definitely demonstrate the effort put into this work.
3675,3, \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though.
3676,3, I would like to see the same spectra included.
3677,3," Because NAT is a fundamental starting point of the work, it will be nice to elaborate the NAT method to be more understandable."
3678,1," Judging the content of the paper alone, it should be accepted."
3679,3,\n7. What's the reward used in the experiments
3680,3," To validate such theoretical result, a non-deep-learning model should be adopted."
3681,3,  Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time.
3682,3, Why is using a permutation of each pixels' color a good idea?
3683,1,"\n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read."
3684,1,\n\n\nCons:\n\nThis is mostly an application of an existing method to a new domain -
3685,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
3686,3, The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach.
3687,3," Perhaps the title should be \""Rotation Operation in Long Short-Term Memory\""?"
3688,3,"  Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported."
3689,3, The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients.
3690,1, It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper.
3691,3, Building GAN's that operate above-and-beyond moderate spatial resolution is an open research topic.
3692,3," Intuitively, if the repetition issue is prominent to having decent summarization performance, it might affect our judgement on the significance of using intra-attention or combined RL signal."
3693,1," The reported likelihood results are very impressive though, and would be reason for acceptance if correct."
3694,3,\n\n(8) Below Theorem 2.4: Why is Phi now nk x T instead of nk x nT as in Definition 2.3?
3695,2,Find your inner nerdâit must be a big part of youâbind and gap it and then dump it in the ocean tied to a large rock.
3696,3, \n\n2) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al.
3697,2,I am very enthusiastic about the topic of the article and the applied research design to answer the research questions.
3698,3,\n\n* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s?
3699,3, This incorporates information from more distant vertices in one propagation step.
3700,3,"\nThe paper first presents the mathematical form of the proposed activation function (ISRLU), and then shows the similarities to ELU graphically."
3701,1," The idea is interesting and the result looks promising,"
3702,3, We can also see from Table 1 that if we replace KL by its symmetrical variant we get similar results.
3703,2,The writing and presentation are so bad that I had to go home early and spend time wondering what life is about
3704,3,\n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants.
3705,3," Again, since the main contribution is to solve a specific problem, it would be worthy to compare with a more extensive benchmark, including state of the art algorithms used for this problem (e.g., heuristics and metaheuristics)."
3706,3," In high-dimensional state space, the authors propose to approximate that matrix with a convolutional neural network (CNN)."
3707,3, Deep reinforcement learning that matters.
3708,3,\n\n\nQUALITY\n\nThe symbol d_{rew} is never defined \u2014 what does \u201cnew\u201d stand for?
3709,3," Therefore I have modified my rating accordingly."""
3710,3," The idea of functional PCA is to view \\x as a function is some appropriate Hilbert space, and expands it in some appropriate basis."
3711,3," \n- Footnote 1, line 2: \""an exchange\""."
3712,3, The authors also study the analytic form of critical points of a single-hidden layer ReLU network.
3713,3," Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal."
3714,3,\n\n\nGeneral Comments:\n\nThis paper proposes to use the rotation matrices with LSTMs. 
3715,3," Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution."
3716,1,"""This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space."
3717,3," While this may be statistically significant, it is a very small gain nonetheless."
3718,3," I suggest the authors try this experiment as well and I hope the answer is no, as separated input embeddings for gate functions was verified to be useful in some \""old\"" works with syntactic features as gate values, like \""Semantic frame identification with distributed word representations\"" and \""Learning composition models for phrase embeddings\"" etc."
3719,3," However, it is not clear why the method is tested only on a single data set: MNIST."
3720,3, Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations.
3721,3,\n\nOne question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options).
3722,3,"  They are also fairly specific, for example \u201csurprise\u201d is sudden reaction to something unexpected, which is it exactly the same as seeing a flower on your car and expressing \u201cwhat a nice surprise."
3723,3,"\n- The unconditional generative model *only* relies on the \""injected randomness\"" for generating drawings, as the initial state is initialised to 0."
3724,3,\n\nThe writing is ok.
3725,3,  This is secondary to the proposed framework.
3726,1," \n\nAll in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset."""
3727,3, The authors use multiple data sets to study different aspects of the proposed approach
3728,3, The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions.
3729,3, Numerical experiments show that such sparse networks can have similar performance to fully connected ones.
3730,3, (2016).\nIt took me until section 3.5 to figure it out.
3731,3, Is this finetuned during training?
3732,3," The first three results are on small datasets/tasks, O(10) feature dimensions, and number of\n  tasks and O(1000) images; (i) distinguish two MNIST digits, (ii) 10 UCI tasks with feature sizes\n  4--30 and number of classes 2--10, (iii) 50 different character recognition on Omniglot dataset."
3733,1,"\nThe paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below)."
3734,3,\n- What does it mean by \u2018unofficial dataset\u2019 (page-4)
3735,3, I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).
3736,3, I believe the primary difference (other using an LSTM instead of a convnet) is to replace max-pooling with softmax-pooling. Do these two architectural changes matter?
3737,3, The CW-SC kernel can be regarded as a redundant version of interleaved group convolutions [1].
3738,1,\n- The experimental setting is very non-trivial and novel.
3739,3," The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition."
3740,3," eg. I am not sure what\noccupancy values, or inducing points are.[[CNT], [null], [DIS], [MIN]] \n\n* Supposingly that the authors properly consider computation in RKHS, then \\Sigma_i\nshould be definite positive right?"
3741,3, The authors further propose to maintain a coreset which consists of representative data points from the past tasks.
3742,3,"\n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise."
3743,1,\n\nThe proposed algorithm is new and writing is clear.
3744,3,\n- One major bottleneck of the model is that the proposals are not jointly finetuned.
3745,3, I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.
3746,1, I enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sentence.
3747,3,\n\nThere exist a number of \u201cdrag and drop\u201d style UI design products (at least for HTML) that would seem to accomplish the same basic goal as the proposed system in a more reliable way.
3748,3, Maybe you could comment on this.
3749,3,\u201d  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.
3750,3, However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks.
3751,3,  Should one expect a certain trend in these sequences?
3752,3," In particular, perhaps the deviations from Brownian motion could also be due to the discrete\tnature of SGD (i.e. that the continuous-time formalism is only an approximation of a discrete process)."
3753,3," For example, you could imagine that in a morphologically rich language, this method would work well to learn the representation of certain morphemes such as case endings or verbal conjugation."""
3754,1,"                                                            \n                                                                     \nOverall, the paper presents a novel idea, which is well motivated and clearly presented."
3755,1, The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off. 
3756,3, \n\nCons:\n- I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others.
3757,1,"""This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization."
3758,2,"I am, frankly, underwhelmed by the revisions. Most of the responses sound smooth, but really just written to avoid serious additional work"
3759,3, People have also specifically tried to train a siamese network and use its features as input to the SVM.
3760,3,"""The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space."
3761,3,"\n\nQuestions/other:\n- What is meant by \""implicit\"" models?"
3762,3,   \n\n    3. What is u is equation 3?
3763,3," Same comment in Atari, but there it\u2019s not really obvious that the proposed architecture is helping."
3764,3,\n\nI would be less concerned about the above points if I found the experiments compelling.
3765,3,"\nFrom the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly."
3766,1, It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks.
3767,3,"\n\nThe authors say of the proposed TwinStream dataset that it \""may not be\nrepresentative of real use-cases\"". It seems odd to propose something that is\nentirely artificial."
3768,3, How do they impact for instance changing the time range in the definition of $\\Phi$) in the performance of the meta-learner?
3769,1, The paper is interesting and well-written.
3770,3, The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem.
3771,3," In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to \u2018negative interference\u2019 between tasks [1, 2]."
3772,3," They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing \""bugs\"" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths."
3773,3," This regularization, while helping exploration, do changes the original objective."
3774,3,  Please state precise results using mathematical formulation.
3775,1,  The paper is generally pretty well written.
3776,1,\n\nSignificance: Average.
3777,3, \n6. Extra models like Deep Networks with/without matrix factorization could be added.
3778,1,"""1) This paper proposes a new dataset for Reading Comprehension (RC)."
3779,3,"\n\nThe method proposed in the paper already exists for classical function, they only transpose it to neural networks."
3780,3,"  It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them."
3781,1,"  Some interesting results are obtained, such as \""enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures\"" ."
3782,3, Therefore I does not seem pertinent here.
3783,1, The theorem stated in the paper seems to provide an interesting link between SR and the Laplacian.
3784,1," The idea is interesting, and overall introduction is clear."
3785,3," So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough)."
3786,1,"""\n\nThis paper presented interesting ideas to reduce the redundancy in convolution kernels."
3787,1,"\n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context."
3788,2,"If philosophy of science was not taught in the authors doctoral program, (s)he needs to go back and dope slap his/her major professor"
3789,3,"\n\nThis aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset."
3790,3,"            Sec 4.1, p5: Last equation: Perhaps useful to explain the term $log(\\phi_j^* | \\theta)$ and why it is not in subroutine 4 ."
3791,2,The paper suffers from its desire to be accessible and directly impactful
3792,2,"The fact that the question of this paper has never been asked should, on balance, count against the paper"
3793,3,"""1. Paper summary \n\nThis paper describes a technique using 3 neural networks to privatize data and make predictions: a feature extraction network, an image classification network, and an image reconstruction network."
3794,1," The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs."
3795,1,\n+ The results seem interesting too
3796,3,"\n- In addition to the above point, I guess the expectation is needed as the original formulation of GAN."
3797,3, Superior performance to recent baselines (e.g. EWC) is reported in several cases.
3798,3,\n\nIf the authors add FSGM to the batch of experiments (especially section 4.1) and address some of the objections I will consider updating my score.
3799,3,\n\nThis paper highlights this problem as a fundamental issue limiting meta-optimization approaches.
3800,3,It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score.
3801,3," The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha."
3802,1," According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task."
3803,1,\nWriting clarity - the paper is very well written and clear.
3804,2,"The word asses should read assess 
"
3805,2,The chapter is too scholarly and too casual
3806,3," Further the claims all over the paper, comparing with the existing works. are over the top and not justified."
3807,1, \n\n*Quality* \nThe problem addressed is surely relevant in general terms.
3808,3, Though the grounding or attention is performed for each word at each location of the visual map.
3809,3," Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs."
3810,3," However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features."
3811,3,"""This paper proposes a new method for estimating optimal transport plans and maps among continuous distributions, or discrete distributions with large support size."
3812,3," I think it will eventually become a good paper, but it is not ready yet."
3813,1," Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples."
3814,3," As a result, I suggest the authors add more ablation tests regarding (1) replacing the outer-fusion with simple concatenation (it should work for two attention layers); (2) removing the inner-fusion layer and only use the final layer's output, and using residual connections (like many NLP papers did) instead of the more complicated GRU stuff."
3815,3," Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero."
3816,1,\n- Quantitative improvement with respect to the state the art.
3817,3,n\nIt would be better to provide deeper analysis in Subsection 6.1.
3818,1,"""The paper addresses an important problem in multitask learning."
3819,3,\n\nCons:\n\nThe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods
3820,3,"""\nSummary: the paper proposes an idea for multi-task learning where tasks have shared dependencies between subtasks as task graph."
3821,1,\n\nThe experiments are convincing and solid.
3822,3," However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section."""
3823,1,"\n\nThe paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al). "
3824,1, While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.
3825,3,It needs to but \n                  that was not explicit in the paper.
3826,3," I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it."
3827,3, \nPseudo labelling can be obtained by transformations of original input data
3828,3, It's fine if that pushes the paper somewhat over the 8th page.
3829,3,\n- the generalization ability of RNNs on longer SCAN commands
3830,1,"\n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract."
3831,3, \n\nThe paper follows with some fun experiments implementing these new game theory notions.
3832,3," \n\n[1] Wang & Manning, Fast Dropout Training."
3833,1,\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound.
3834,1, \n(3) TD methods can outperform pure MC methods when the return is mostly dominated by the reward in the terminal state.
3835,3, Can you make this more precise?
3836,3,"\tIn page 2, the line before the last line, \u201c\u2026 resolbing problem\u201d --> \u201c\u2026 resolving problem\u201d\n"""
3837,1," The paper is well written, the method is easy to implement, and the algorithm seems to have clear positive impact on the presented experiments."
3838,3, Is there any justification for such a claim?
3839,3,"\nSpecifically the analysis involves three datasets and two visual domains for each dataset: besides the original version\nof each image a new version is created by inverting its colors, i.e. simply rescaling the color channels in [0,1] and then\napplying (1-pixel_value)."
3840,3," However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG."
3841,3,\n\nIt would be great if the above could be addressed.
3842,3,"""This paper studies a toy problem: a random binary image is generated, and treated as a maze (1=wall, 0=freely moveable space)."
3843,1,"\n\n- The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods."
3844,1,\n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability)
3845,3, They show results on CIFAR-100 and ImageNet (as well as mini ImageNet).
3846,1,\n\nQuality\n======\nThe approach seems sound
3847,3, but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds.
3848,3," In a response to a question I posed, you mentioned that we you meant was \""we use about 40% memory for the gradient computation and storage\""."
3849,3, The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance.
3850,3, \n- Proof of Proposition 5 (cf. page 13): this is a sketch of proof to me.
3851,3," Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task."
3852,3," It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves."
3853,1,\n* elegant and simple solution
3854,3,  This leads to the question of cheating in the learning process.
3855,1," \n\nOverall, I think the idea presented in the paper has merit,"
3856,3,Are these really \u201cadversaries\u201d?
3857,1,\n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.
3858,1,"  Overall, the paper is interesting and promising;"
3859,3, But it would be good to know whether the approach works well across games and is competitive against other stronger baselines.
3860,1, \n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences.
3861,3, Thus this space may constitute a better candidate for performing data augmentation by small perturbations or by nearest neighbour search around the given vector since 1) the augmented data is more likely to correspond to features of similar images as the original provided image and 2) it is more likely to thoroughly capture the intra-class variability in the augmented data.
3862,1,"\n\nAlbeit the above caveats, we iterate the paper offers a nice construction for an important problem."
3863,3,"\""\nAs our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that \""doing xxx should improve things\"" without actually trying it."
3864,3," Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results."
3865,3, I prefer to have discussions on experimental results with both datasets.
3866,1, The authors also introduce a specific control variate technique based on the so-called Stein\u2019s identity.
3867,3,                    \u201c distributions has\u201d ->  \u201c distributions have\u201d.
3868,3," To my knowledge, copy task could be solved easily for super long sequence through RNN model."
3869,1, Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference.
3870,3," E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS)."
3871,3," \nThese are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art)."
3872,3," For the Office experiment, the LCO appears to be trained on ImageNet data."
3873,3,"\n\nNegatives: \n- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation)."
3874,3,"\n- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)?"
3875,3, Further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the performance.
3876,3,"\nGiven this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix."
3877,3," but with dense connections (Huang et al., 2017) and with a classifier at each layer."
3878,3,"  Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. """
3879,3,"""This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric,"
3880,3,"\n\n1. Apply the sum-of-squares (SOS) method.\nThe paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy."
3881,3,"""General-purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcome\u2014this paper included."
3882,3, It also have some visualization about how the model decay the weights.
3883,1,"  The paper is well written, easy to follow, and have good experimental study."
3884,3," \n\n3. For clarity, the authors should express equation 5 in terms of Y_1, Y_2, Y_3, and Y_4."
3885,3,Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems.
3886,1," The proofs are clear, and seem correct on a superficial readthrough; I have not carefully verified them."
3887,3," To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case."
3888,2,The language is so inaccessible that I can't make up my mind whether they're trying to hide something or actually think this is good writing
3889,3," \n\nOne way to greatly improve the impact of the paper would be to take the observations made from the simulated data experiments (e.g., MNIST) and use them to make changes to how CNN training is done on another real task (e.g., ImageNet) and show improvements in performance."""
3890,3, It seems to me the variances \\nu^2_i shall be directly estimated from \\hat{z} as is.
3891,3," The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach?"
3892,3,  The paper argues that the recurrent state should be high-dimensional (in order to be able to encode the input and extract predictive information) but the recurrent dynamic should be realized by a low-capacity model.
3893,3,"  In fact, it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation, or, frankly, the way in which the experimental results are described."
3894,1," \n\nTo summarize, the first part is interesting and nice,"
3895,3, Do you have an hyper-parameter that sets the amplitude of the constraint?
3896,3,\n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most).
3897,3," In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation."
3898,3, How can the reader conclude any functioning from these images?
3899,3,   The blue points seem to consistently become more dense from (a) to (c).
3900,3,"\n\n- Since the model gets information from the AP and HP before doing any\n  iterations, why not go on and use that to help select candidates?"
3901,3,\n-Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper.
3902,3, Theorem 3 provides conditions under which the multiplication X*Y is a locally open map.
3903,3," It seems that effectively, the  introduce their additional softsign in the process."
3904,3,"\n\nThere are 5 terms in the proposed objective function. There are also several other parameters associated with them: for example, the label temperature of z_2\u2019\u2019 and and parameter alpha in the second last term etc."
3905,3,"""This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm."
3906,3,\n\nMinor:\n- It\u2019d be helpful to add the formulation of gated linear units and residual layers.
3907,3,\n- What data do you use for the distillation?
3908,3,Is that Fig 8 @0%?
3909,3, Could the authors explain their choices?
3910,3," In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?"""
3911,3,"   See e.g. Section 2.2 of [Collins et al. JMLR 2008] (this is for the more general multiclass logistic regression problem, but (2) is just the binary special case of equation (4) in the [Collins ... ] reference)."
3912,3,"  I suggest TR be replaced by \""Stochastic TR\"" everywhere."
3913,1,"\n\nOn the overall, while the idea may be of interested,"
3914,1,"""The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly."
3915,1,"\n\nPros:\n\n- It is interesting that the latent space models are most successful, including the relatively simple GMM-based model."
3916,3," \n\n To demonstrate the \""mesh/graph generation\"" capability truly, the authors need to experiment on novel topology generation."
3917,3,"  Indeed, a skeptical reviewer may suspect that the authors needed to add perceptual realism to the evaluation because that\u2019s the only thing that justifies the adversarial loss."
3918,3," \n\nIn 3.1, at point (2), the authors mention that DSC filters are learned from the\ndata whereas GC uses a constant matrix."
3919,3, Interested readers can then work through the math if they want to.
3920,2,It is safe to say that the authors want to see their paper published more than I really want to...
3921,3, Did you consider doing that here too?
3922,3," That is, even if there is no way to reach a reward state in 32 steps, an MC value function approximation with horizon 32 can extrapolate from similar looking observed states that have a short path to a rewarding state, enough to be better than a blind random walk."
3923,2,I would very much have liked to read the article promised in the abstract.
3924,3, Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size?.
3925,3," I would have appreciated a comparison to other methods for guiding discriminator representation capacity, e.g. autoencoding (I'd also imagine that learning an inference network (e.g. BiGAN) might serve as a useful auxiliary task?)."
3926,1, Both these contributions are important in the effectiveness of the overall algorithm.
3927,1,"\n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN."
3928,3,"  As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large."
3929,3,"\n\n[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. \u201cUnifying Visual-Semantic Embeddings with Multimodal Neural Language Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.\n"""
3930,3,"""Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions."
3931,3, However the MSE obtained when not using regularization is the same (or even smaller) than when using it.
3932,2,Figure 3: I haven't been this confused since Cardi B's 2019 Met Gala outfit
3933,3," There seems to be a gap between the first paragraph and the second paragraph.[[CNT], [PNF-NEG], [CRT], [MIN]]  The authors mentioned that \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system."
3934,1,"\nFinally, extensive experiments are conducted and they are in accordance with the theory."
3935,3,\n\nORIGINAL REVIEW BELOW\n\nThe paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections).
3936,3," The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons."
3937,3,". Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results."
3938,3," The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work."
3939,3," If we are just showing images, then they are evaluating image synthesis, which do not necessarily contain the desired properties in videos such as temporal coherence."
3940,3,"\n- Eq 1: \n  *label \""y_i\"" has two different semantics (L_ocn it is the class label, while in L_pcn it is the label of an image pair being from the same class or not)"
3941,1,"\n\n* Experimental Results\nFrom what I understood from the experiments, it seems that using the \u201ctwo-pass decomposition\u201d (i.e. projected gradient descent) is better than CP-ALS (gradient descent ended with a single projection step)."
3942,3," Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation."
3943,3," \n\nThe authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem."
3944,3,"    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data. "
3945,3,"   \n\n\n[1] Balles, Lukas, Javier Romero, and Philipp Hennig. \""Coupling Adaptive Batch Sizes with Learning Rates.\"" arXiv preprint arXiv:1612.05086 (2016).."
3946,3,  The grid tensors are matricized and the ranks of the resultant matrices are compared. 
3947,2,The peaceful atmosphere between Christmas and New Year was transiently disrupted by reading this...
3948,3,\n\n3) Why the different initializations for the recurrent weights for the hexagonal vs other environments?
3949,3, Updates are made to the parameter representing the # of bits via the sign of its gradient.
3950,3,\n\n- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator? 
3951,3," where you hide some known entries during model training, and evaluate on these entries during test? """
3952,3,"""This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime."
3953,3," My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme."
3954,2,Unfortunately I was hoping for more.
3955,3," In the data collection stage, how were the points lists generated from pen strokes? "
3956,3," so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map."
3957,1, The experiments show the proposed approach is effective on 14 UCI datasets
3958,3, This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.
3959,3,"\"" Psychometrika 23.2 (1958): 111-136.\n[4] Zhang, Cheng, Hedvig Kjellstr\u00f6m, and Carl Henrik Ek. \""Inter-battery topic representation learning.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n\n"""
3960,1,\n\nThe results presented by this paper shows improvement over the baseline.
3961,3,"\nAs a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point."
3962,3," Some of the figures do not have the notations of curvey, making people hard to compare."
3963,3, The authors train their system with the testdata included which leads to very different visualizations.
3964,3," Also, near-optimality would refer to some parameters being chosen in the best possible way."
3965,1,"\n\nOverall, I found this paper quite interesting."
3966,3, \n\nEquation (4) shows that the proposed approach would weight Q different GC\nlayers.
3967,1," \n\nThe proposed approach is interesting,"
3968,3," \nWhat are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)?"
3969,3, It would be interesting to investigate the limits of this statement: what\nwould happen by augmenting only 8 or 7 or 6 or 5... categories instead of 9?
3970,3," Finally, while this method introduces a principled way to remove mean baseline activity from the sensory-driven response, this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response (Destexhe et al., Nature Reviews Neuroscience 4, 2003; Gutnisky DA et al., Cerebral Cortex 27, 2017)."""
3971,3,"""This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector."
3972,1,"\n\n\n#######################################################\n\nPost-rebuttal review\n---------------------------\n\nGiven the details the authors provided to my review, I decided to adjust my score."
3973,1, Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant.
3974,3, In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials.
3975,3,  What is the conclusion from Figure 4(b)?
3976,3,"\n\nAs long as the analysis is experimental, the state of the art should be considered."
3977,3,"  You need to add \u00ab\u00a0with probability $1-\\rho$ as in Avron\u2019s paper.[[CNT], [EMP-NEU], [DIS], [MIN]] \n- p12: the derivation of Eq (10) from Eq (9) needs to be detailed."
3978,3, Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.
3979,3, At least local minima issues could be assessed using multiple random initializations.
3980,3, \n\nThe paper revisits mostly familiar ideas.
3981,2,editor] I will make a final determination without ... review (which will assuredly be positive unless you go all Trump on someone).
3982,3, They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs.
3983,3," It could provide more insights to practical problems if more experiments were done: e.g. can this technique help domain adaptation?\n\n"""
3984,3, \n\npage 14:\n- Proposition 1: the proof could be shorten by simply stating in the proposition that f and g are distribution...
3985,1,"\n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant."
3986,2,I am not convinced if any clear real value of this research.
3987,3,\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)?
3988,3, Is there a connection to subtasks rewards in earlier HRL papers?
3989,3, how did you decide on the words to mask? was this at random?
3990,3, This widely accepted definition of navigation does not preclude being limited to known environments only.
3991,3," Also, DQN does not optimize the Bellman residual, it\u2019s a TD update. \n"""
3992,3,"""The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks."
3993,2,"While the authors have no legal obligation to cite these unpublished results, they are probably morally obliged to consider them"
3994,1, \n  Positives:\n- the output kernel update is well justified
3995,3," Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%)."""
3996,2,The investigator is in the top 50% of his field
3997,3, The author is suggested to do a careful format check.
3998,3,  The explanation maps mirror for the considered methods the model's reaction to the input.
3999,3,\n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs. 
4000,1," The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked)."
4001,3,".  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value."
4002,1, \n\nI thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds.
4003,3,"  To the reviewer, It seems \u201clife-long learning\u201d is the same as \u201conline learning\u201d.[[CNT], [CLA-NEG], [CRT], [MIN]]  However, the whole paper does not define what \u201clife-long learning\u201d is.[[CNT], [CLA-NEG], [CRT], [MIN]]\nThe limited budget scheme is well established in the literature.[[CNT], [CNT], [APC], [MAJ]] \n1. J. Hu, H. Yang, I. King, M. R. Lyu, and A. M.-C. So. Kernelized online imbalanced learning with fixed budgets."
4004,1,"\nThis is an interesting extension of the open-world paradigm, where at test time, the classifier has to identify images beloning to the C seen classes during training, but also identify (reject) images which were previously unseen."
4005,3,"\n5. Present results on larger-scale applications (Text8, Teaching Machines to Read and Comprehend, 3 layers LSTM speech recognition setup on TIMIT, DRAW, Machine Translation, ...), especially because your method is really easy to plug in any existing code available online."
4006,3, Collecting the user specific models and averaging them to form the next global model.
4007,3,  I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters.
4008,1, See more comments below.\n\n= Originality / Significance = \n\nThe paper presents a clever idea that could help make SPENs more practical.
4009,3," In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE."
4010,3,"""The authors present a model for text classification."
4011,3,"\n\n\nDetailed comments and questions:\n\n\nThe distribution of connections in \""windows\"" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image."
4012,3,"     The results over the IMDB dataset in the original paper [2] are higher than the ones reported here, using a simple model (BoW)."
4013,3," However, the previous settings can be reinterpreted in the authors setting."
4014,2,"The underlying science here is quite interesting, but the presentation does its best to disguise it"
4015,3,"""This paper proposes an iterative inference scheme for latent variable models that use inference networks."
4016,3,"  They compare this\nbound with a similar recent bound proved by Bartlett, et al."
4017,3," In the videos, it seems like the robot could get a slightly better view if it took another couple of steps."
4018,1, \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score.
4019,1, The paper rightly mentions that existing reading comprehension datasets (e.g. SQuAD) where the current methods are already performing at the human level largely due to large lexical overlap between question and document.
4020,3,"\n\n5) It would be nice to more quantitatively map out the relation between speed tuning, direction tuning, and spatial tuning (illustrated in Fig. 3)."
4021,2,I have reviewed many manuscripts in my career and I have never seen so many repeated mistakes of this...
4022,1,   My intuition is that this simpler approach would work better.
4023,3, The authors define a Wasserstein Distance Network to find  a suitable affine transformation that reduces the nuisance factor.
4024,3," One could argue that TreeQN is learning an \""abstract\"" planning rather than \""grounded\"" planning."
4025,3, and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters.
4026,3,\n\nAn analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation.
4027,3, The use of a straight-through estimator allows the model to be trained with standard backpropagation.
4028,3," Empirically, they show the performances are better than random feature and the LKRF."
4029,3," Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting."
4030,3, It would be much more helpful (and easier to read) if it were enlarged to take the full page width.
4031,3,\n\nThe authors should consider adding equation numbers.
4032,3," Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently."
4033,1,\n4) The performance of the technique is reasonable enough to actually be used.
4034,3," With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5)."
4035,3, The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration.
4036,3," Currently, Section 3 is called \""Background\"" which does not say much.[[CNT], [null], [DIS], [MIN]] Section 4 contains CIGMs, Section 5 Causal GANs, 5.1. Causal Controller, 5.2. CausalGAN, 5.2.1. Architecture (which the causal controller is part of) etc."
4037,1,"\n\nThough novel,"
4038,3, They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer.
4039,2,The manuscript in the present form is not a review article but is rather a number of research papers stapled together.
4040,1,"\n\n- The DA results are shown with a linear classifier, for the comparison to the baselines to be fair, which I appreciate."
4041,3,   The paper uses such object representation to inform state representation for reinforcement learning.
4042,1,\n+ interesting idea of using the algorithm for RLfD
4043,3, \n\nThe paper evaluates popular GAN evaluation metrics to better understand their properties.
4044,2,There are not enough headings.
4045,3,"   Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined. "
4046,3, The recent paper by Chen et al (2017) would be relevant here.
4047,3,\n\n\nEVALUATION:\n\nI think exploring and understanding entropy-regularized RL algorithm is important.
4048,1,"""The paper presents interesting algorithms for minimizing softmax with many classes."
4049,1, The results are visually compelling
4050,1,"""This is a very clearly written paper, and a pleasure to read."
4051,1,"""The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs."
4052,2,'The only redeeming aspect of this manuscript is that it is so poorly written that it fails to convey the incorrect conclusions drawn here' [from the most recent episode of  on the review process
4053,2,I would suggest that you do some homework and redirect this work to an actual new and novel and mechanistic work and test it against data
4054,1,  \n\n1. The addition of auxiliary layers improves Sequence Tagging results marginally.
4055,3, A fully connected layer is used to fuse the multimodal information.
4056,3," In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem."
4057,3, They do this by adapting a technique from computational cognitive science called rational pedagogy. 
4058,1, \n--I liked the uncertainty analysis.
4059,2,"I also added back in the two lines on p.16 that youd inexplicably deleted. If theres a reason for their deletion, let me know.

-Journal editor, after telling me to reduce the word coun"
4060,3,"The key distinction being that in this gamified setting, the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth."
4061,3,  It could be removed to focus more extensively on the continuous case (right example).
4062,3, \n\nExample 2 from intro when comparing with other results on page 2:\nThe authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity).
4063,3,"\n\nThe empirical work is somewhat compelling,[[CNT], [EMP-POS], [APC], [MAJ]] though I am not an expert in this\ntask domain."
4064,3," From the machine learning perspective, the proposed \""attacking\"" method is standard without any technical novelty."
4065,1," However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout"
4066,3,  A short discussion of this result would make the paper stronger.
4067,1,\n\nOverall the math looks reasonable.
4068,1,"\n\nThe paper is mostly clear and well-presented,"
4069,3,"\n\n\""However in practice the operations were incredibly slow, taking up to 30 minutes in some cases.\"" It is unclear what operations are referred to here."
4070,2,This manuscript is only theoretical in the sense that the methods are proposed but not adequately tested
4071,1,"\n\nThe presentation of the core idea is solid,"
4072,3,"  BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y)."
4073,3, It would be interesting to see the results of the new activation function on LSTM.
4074,3,\n- I am not an expert in this area.
4075,3," The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads."
4076,2,"If this was taken from a successfully defended thesis, as it appears to have been, then he should not have been awarded a PhD"
4077,3," The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized."
4078,3, \n\nI am genuinely curious and would love the authors' comments on this.
4079,3," Of course, that survey paper correctly attributes the origin to [1]."
4080,3,"\n\n2. According to the description, referring to an existing named entity must be done by \""generating a key to match the keys in the NE table and then retrieve the corresponding value and use it\""."
4081,3,\n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b].
4082,3, \n- It is necessary to put Table 5 in the main paper instead of Appendix.
4083,3, Can the authors clarify how does the decoder learns to generate these words.
4084,1,\n\nMinor comments:\n\n- Theorems seemed reasonable and I have no reason to doubt their accuracy
4085,3," \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?"
4086,3, b) perturbed decoded image;
4087,1,"  However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR."
4088,1," All the statements are very clear, the structure is transparent and easy to follow."
4089,3, \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.
4090,2,2. [Paper Strengths]: None
4091,3, The authors suggest that training a GAN provides a speed benefit with respect to other attack techniques.
4092,3, I would thus suggest that the authors update the paper accordingly.
4093,3," This also seems to subsume the first condition, s\\geq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9."
4094,1,\n(c) generation of high quality samples even with higher perplexity on ground truth set.
4095,3," It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side."
4096,1,\n\t\u2022\tExplanations are exceptionally well done: terms that might not be familiar to the reader are explained.
4097,3,\n(2) how is the level of uncertainty related to performance?
4098,3," However, the fact that reward prediction loss is used to train TreeQN significantly weakens this claim, and there is no such an evidence in the paper."
4099,1," \n\n-\tThe point brought about CNN\u2019s failing to generalize on same-ness relationships on sort-of-CLEVR is interesting, but it would be good to know why PSVRT provides better generalization."
4100,3,"\n\n\""and is hardly reusable\"" -> \""and are hardly reusable\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""Skill composition is the idea of constructing new skills with existing skills (\"" -> \""Skill composition is the idea of constructing \nnew skills out of existing skills (\"".\n\n\""to synthesis\"" -> \""to synthesize\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""set of skills are\"" -> \""set of skills is\"".\n\n\""automatons\"" -> \""automata\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""with low-level controllers can\"" -> \""with low-level controllers that can\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""the options policy \u03c0 o is followed until \u03b2(s) > threshold\"": I don't think that's how options were originally defined... beta is generally defined as a termination probability.[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""The translation from TLTL formula FSA to\"" -> \""The translation from TLTL formula to FSA\""?"
4101,3,"""Eigenoption Discovery Through the Deep Successor Representation"
4102,3,"\n\nWith these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved,;"
4103,1, For these reasons I \nbelieve the paper has sufficient merits to be published at ICLR. 
4104,3,"""The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor."
4105,3,"""- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations."
4106,1,\n\nPros:\n- De-duplication modules of inter and intra object edges are interesting.
4107,3,\n- quantitative results (Table 1) too little detail:\n        * why is this metric appropriate
4108,1,.\n\nPros\n------\n- Interesting idea
4109,3, The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme.
4110,1,\n\nI found it is very interesting that the emergence of these representations was contingent on some regularization constraint.
4111,3," In the paper, the author argued \""we propose and evaluate the minimal changes...\"" but I think the these type of analysis also been covered by [1], Figure 5."
4112,1,\nShows a number of qualitative and quantitative results
4113,3,"n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017"""
4114,2,The orgnization and writing of the paper need to improve. There are some grammar errors need to correct.
4115,3," While on MNIST and CIFAR, DTP and SDTP performed as well as backprop, they perform worse on ImageNet"
4116,3, An iterative algorithm is developed for model optimization.
4117,3,"\n\nWhile basically the approach seems plausible, the issue is that the result is\nnot compared to ordinary LSTM-based baselines."
4118,3, The paper provides several advances:\n- \\epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training.
4119,3,  The key technical limitation is the dependency of the local minima on the weight parameters
4120,1,\n\n# Paper discussion:\nIn general I like the idea of looking further into the effect of adding network structure on the original\ninformation bottleneck results (empirical and theoretical).
4121,3, The proposed methods are evaluated on two car datasets.
4122,3, Making Neural Programming Architectures Generalize via Recursion.
4123,3,"\n- when an agent is trained on fixed number of random map, its performance on random unseen maps doesn't increase with the number of training maps beyond ~100 maps. (figure 5)."
4124,1," experimental validation is extensive, making it a worthy contribution in my opinion."
4125,2,This abstract is based on using intelligent transportation systems (whatever that is). This aim is not fulfilled by the paper
4126,3,"\n\n6. If there is a section on INFOGAIN exploration, why not mention it in the main text?"""
4127,1, This iterative projection tends to perform better than iteratively optimizing f(W) and then applying the projection step only once at the very end of the optimization (assumedly the CP-ALS method that is used for comparison).
4128,3,(ii) how to make use of the episodic memory when deciding what action to take.
4129,3," Finally, the introduction paragraph of Section 5 is rather bold, \""resembles the learning process of human beings\""?[[CNT], [EMP-NEU], [QSN], [MIN]] Not so sure that is true, and it is not supported by a reference (or an experiment)."
4130,3," Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead."
4131,3,"""This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program."
4132,3,\n\nA more detailed review follows.
4133,3, If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead.
4134,3," It can rather be seen as an interesting empirical study, with \""negative result\"
4135,1,"""The authors propose a new network architecture for RL that contains some relevant inductive biases about planning."
4136,3," Specifically, I would quantify the cells' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons."
4137,1," I liked the originality of this paper. """
4138,2,This paper may sink without trace' h/
4139,3, Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled.
4140,3,  Some points have been clarified but other still raise issues.
4141,3,"""In Bayesian neural networks, a deterministic or parametric activation is typically used."
4142,3,"\u201d In this case, these are just different interventions on possibly the same object."
4143,3, The targets\u2019 update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation).
4144,3," The authors claim that this approach is better at avoiding \""narrow\"" local optima, and therefore will tend to generalize better than minibatched SGD."
4145,3,"\nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?"
4146,3,"""The paper introduces a generative model for graphs."
4147,1,\n\nThe tested networks seem to perform reasonably well on the task.
4148,2,The analyses are too statistic - submissio
4149,3,"\n- Regarding the evaluation, you wrote:\""In order to evaluate the interpolation capability of the autoencoder, we split the dataset in training and test samples in the ratio of 1:9."
4150,3,"""In this paper the authors consider learning directly Fourier representations of shift/translation invariant kernels for machine learning applications."
4151,1, The general methodology also makes sense to me.
4152,3," Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy)."
4153,2,I am impatient with this vague assertion.
4154,3, \n- It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2.
4155,3,\n- can you also compare the training times?
4156,3,"  For instance, Borsa et al 2017 doesn't do inverse RL (as said in the related work section) but learn to perform a task only from the extrinsic reward provided by the environment (as said in the introduction)."
4157,1, I have not seen this particular setup for processing meshes with neural networks in an autoencoder setting.
4158,3, According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet.
4159,3,"\n - do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains?"
4160,3, Or am I missing something?
4161,3, \n\n- It is hard to map System-{ABCD} to the underlying proposed methods described in Table 2.
4162,3," Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened."
4163,2,"The so-called XX test is incompetent, irrelevant, immaterial and without any foundation whatsoever in the established literature"
4164,2,So in summary: the paper is oblique to the entire current literature and it fails to relate to relevant investigations
4165,3,".\n* The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners."
4166,2,The manuscript embarrassingly fails in addressing the declared aims.
4167,1,\n- good compression with little loss of accuracy on best strategy\
4168,3, I am quite convinced that any somewhat correctly setup vanilla deep RL algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box.
4169,2,This paper introduces tools to answer questions which it does not seem many people are interested in
4170,2,I concur with the reviewers that the methods are insufficient and thus the conclusion are totally...
4171,1," The writing is clear, concise and easy to follow."
4172,3, Posterior weights are sampled to select actions during execution (Thompson Sampling style).
4173,3,\n\n- The structure of Section 3 needs to be improved.
4174,3,\n5) It would be useful to discuss the average reward setting and how it relates to your work.
4175,3,"\n- This is visible on universal perturbations, which become less effective as more AT is applied."
4176,1,"\n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims."
4177,3," \n\nThe previous paper (Mirowski et al. 2016) introduced a deep RL agent with auxiliary losses that facilitates learning in navigation environments, where the tasks were to go from a location to another in a first person viewed fixed 3d maze, with the starting and goal locations being either fixed or random."
4178,2,The writing and data presentation are so bad that I had to leave work and go home early and then spend time to wonder what life is about.
4179,3," I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper."
4180,1,  This paper should be accepted.
4181,3,"   In the conclusions the authors admit that: \""Many theoretical questions remain, such as accelerating the search for Fourier peaks\""."
4182,3," If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7)."
4183,3, Maybe bring this detail forward)?
4184,3," Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation."
4185,3,", \nno other approach is considered besides the prothotipical network and its variants. "
4186,1,\n\nPros:\n-- SDR characterisation of the convolutional filters is interesting
4187,3,"""The paper introduces a method for learning graph representations (i.e., vector representations for graphs)."
4188,3, What data augmentation was used for the CIFAR-10 dataset?
4189,3,"\n\nAs mentioned in earlier comments, please reword / clarify your use of \""activation function\""."
4190,3,\n* The much better performance of ConvE is worrying there.
4191,3, These datasets should show whether using more general features (YOLO-9k) helps.
4192,3,"""The paper proposes to add a rotation operation in long short-term memory (LSTM) cells."
4193,2,all I can say is that the author is blissfully unaware of what a standard is â in linguistic terms â and does not have the linguistic competence to describe it
4194,3, When does it not work?
4195,3," To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays)"
4196,3,\n1b. Why should we keep increasing the regularization constant beyond a limit?
4197,3,Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time.
4198,3, The local dimensionality of this paper is the SVD dimensionality on the augmented images of the same class images which classified by the neural network as high probability.
4199,3,\n* One thing I\u2019m unclear on is how convergence was assessed\u2026 my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn\u2019t this also depend on the depth in some way?
4200,3," Furthermore, the paper is easy to read with good organization."
4201,3," However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here."
4202,3," And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%."
4203,3," However, I did notice that the majority of these changes were superficial re-orderings of the original text."
4204,3,"""The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations."
4205,3, This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network.
4206,3,"\n\n* \""...self-consistent equations are highly non-linear and still too abstract to be used for many...\"", presumably what was implied was that the original solution to the information bottleneck as expressed by Tishby et al is non-analytic for most practical cases of interest?"
4207,3," Realizing that the linear mapping is the derivative of network output w.r.t. the input (the Jacobian), the authors proposed to use the reconstruction loss defined in (8)."
4208,3, Did the baseline (original model) reported here also use 50k? 
4209,3," As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate)."
4210,3," \n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies."
4211,1,"\n\nOverall, the paper presents an interesting approach"
4212,3," Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable."
4213,3," The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5])."
4214,1," This is a big advantage when training is performed on hardware with computational limitations, in comparison to \""post-hoc\"" sparsification methods, that compress the network after training."
4215,3, Much explanation is needed in the author reply in order to clear these questions.
4216,3,"""The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training."
4217,3,". The paper shows results on the MNIST, SVHN, and Celebrity Faces datasets."
4218,1, \n3) the robot task is impressive.
4219,1, \n\nThis is an important problem and the paper attempts to tackle it in a computationally efficient way.
4220,3,\n\n2. I think the discussions around Eq. (1) are not well grounded.
4221,3,"""This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks."
4222,3,"  Indeed, the discovered network diagrams (Figures 4 and 5) fall in this space."
4223,1,"""The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study."
4224,1," But, it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment."
4225,3,"and b) how exactly the nuisance variation is reduced."""
4226,1,\n- The experimental results are impressive in their outperformance.
4227,1,"  In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why."
4228,3, Some parts of the paper are hard to read.
4229,3,"""\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning."
4230,3, \nPage 6:\n-\tHow is equation 7) optimized?
4231,3,"""This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem."
4232,3, The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets.
4233,3, The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K.
4234,3,"\n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain."
4235,3,"4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper. "
4236,3,"""Paper Summary:\n\nThis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network."
4237,3,\n\nSome roughly chronological comments follow:\n\nIn the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.
4238,3," Specifically,\na.\tThe author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d."
4239,1," \n\nOverall, I think the paper proposes some interesting ideas,"
4240,3," i.e., are they randomly permuted between layers so that the blocks mix?"
4241,3, The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).
4242,1," The authors cited the no requirement of \""a predefined number of clusters\"" as one of the contributions, but the tuning of alpha seems more concerning."
4243,3," Then an algorithm is given to convert a generic plan into a Monge map, a deterministic function from one domain to the other, following the barycenter of the plan."
4244,3,"\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs."
4245,3," The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers."
4246,3,"""This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention."
4247,3," Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n"""
4248,3, DDQN wasn\u2019t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods).
4249,1," A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization."""
4250,3,(like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this. 
4251,1,"\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods."
4252,3," With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful."
4253,3, What components precisely does an element of U have?
4254,3, It is also unclear to me what are the sources of the uncertainties captured.\
4255,3,"\n\nAnother broader question I have is in the distinction between lower and upper layers (those referred to as \""feature extracting\"" and \""classification\"" in this paper)."
4256,3,\nThese subgoals are linear functions of the expert\u2019s change in state (or change in state features).
4257,3,"\n- In Eq 4, |C_i || y_j| seems a strange notation for union."
4258,3,\n\nOTHER COMMENTS:\n\n- the list of contributions is a bit strange
4259,3, How important is # of layers and residual connections?
4260,3, A better generative model for cycles and trees could help.
4261,3," Because we already have algorithms which better solve this domain, why is your method advantageous?"
4262,3,"  In the introduction, it would also improve the paper to outline clear points of methodological novelty."
4263,1," The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details."
4264,1,"""The paper is relatively clear to follow, and implement."
4265,3, Then a linear classifier is applied on top of the sum of the region embeddings. 
4266,1, Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants.
4267,3, \n\n- Can this approach be applied to semi-supervised learning?
4268,3, The authors show that this effect is caused by the noise in the obective function.
4269,3, \u201cGen Image Encoder\u201d and repeat the rest of the proposed pipeline.
4270,1," Otherwise, the paper is well-structured and easy to follow.\"
4271,3,\n\nThe paper has a technical focus.
4272,1, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance.
4273,2,We note that you may well have completed an interesting study but the manuscript in its current form does not convey that
4274,1,"""Strengths:\n* Very simple approach, amounting to coupled training of \""e\"" identical copies  of a chosen net architecture, whose predictions are fused during training. "
4275,3,"\n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity."
4276,3," To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem."
4277,3,"""Make SVM great again with Siamese kernel for few-shot learning "
4278,3," The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way."
4279,3," The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient."
4280,3,\n[r1] CNNpack: packing convolutional neural networks in the frequency domain.
4281,3," Or introducing noise \""on-line\"" as part of the training? "
4282,3,"""This paper proposes an unsupervised method, called Parallel Checkpointing Learners (PCL), to detect and defend adversarial examples."
4283,1," Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods."
4284,1,. A discussion of this relationship seems really important.\
4285,3,\nThe paper is also light on discussion of related work other than Makhzani et al
4286,3," The main idea is to combine the approaches in (Han et al., 2015) and (Ullrich et al., 2017) to get a loss value constrained k-means encoding method for network compression."
4287,3,\n\nI have no major criticisms.
4288,3," \n\n2. Could the authors comment on the relative training time of the models with the trace-norm regularizer, L2-regularizer and the unconstrained model in terms of convergence time."
4289,3,"If there's a gap, it suggests that your NN approximation might have not been that accurate."
4290,3, The results in the paper are mostly qualitative and only on MNIST.
4291,3," \n\nIt would've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al. (2017), since this is mentioned favorably in the introduction."
4292,3,"\n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \""in which there is further sharing of masks between gates\"" and the one with \""independent noise for the gates,\"" as described in the footnote."
4293,3,"""This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible \nstrategies."
4294,3,  The authors evaluate their proposed methods on one toy problem and two real-world problems.
4295,1," The results are fine when the idea is applied to the BiDAF model,"
4296,3,"  If we\nscale up to batch sizes of ~ N/10, we can only get 10x speedups in\nparallelization (in terms of number of parameter updates)."
4297,3," Also, on a related note, in Figure 2, does each point correspond to a specific choice of these regularization parameters?"
4298,3,".\n\nIt appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. The authors should explore this behavior in greater detail."""
4299,3,Different observability modes\n\t\u2022\tEvaluation against most compatible methods from other sources \n\t\u2022\t
4300,2,Find your inner nerdâit must be a big part of youâbind and gap it and then dump it in the ocean tied to a large rock.
4301,1,\n\nQuality: The method appears to be technically correct.
4302,3," The approach of using BN after non-linearity is termed \""standardization layer\"" (https://arxiv.org/pdf/1301.4083.pdf)."
4303,3,"""\n# Summary of paper\nThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method."
4304,1,"\n\n- The experimental section is very strong; regarding the partial observability experiments, assuming actions are here factored as well, I could see four baselines \n(two choices for whether the baseline has access to the goal location or not, and two choices for whether the baseline has access to the vector $a_{-i}$)."
4305,3, I got confused at several points because it was not clear what was exactly being estimated with the CNN.
4306,3,\n2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression).
4307,1, The paper is well written and provides excellent insights. 
4308,3," \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and"
4309,1,"\n4. However, this paper introduces two different types of networks for \""parametrization\"" and \""physical behavior\"" mapping, which is interesting, can be very useful as surrogate models for CFD simulations."
4310,3,"\n\nMinor comments:\n- Figure 1: what is the difference between \""cost-sensitive loss\"" and just \""loss\""?"
4311,1,"\n\n(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs."
4312,3,TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations. 
4313,3," Crucially, the teacher model will also rely on these learned features."
4314,3,"""This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural."
4315,3, Why would that result in zero training error?
4316,3, \n\nI would like to see comparison experiments with voxel based approaches in the next update for the paper.
4317,3,\n\nThis paper leans heavily on Hazan 2017 paper (https://arxiv.org/pdf/1711.00946.pdf). 
4318,3,  What will happen if we restrict the copy mechanism to only copy from SQL table.
4319,1, The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster.
4320,3,"n\nAssuming that the authors can address these clarity issues, I would in principle be happy for the paper to appear. """
4321,2,"Im sorry, this topic is just not very interesting."
4322,3,"""This paper explores the idea of adding parameter space noise in service of exploration."
4323,1,"\n\nI appreciate that the authors developed extensions of the core method to more complex group structures,"
4324,3, The authors demonstrate\nthe method on a corpus of books by various authors and on a corpus of subreddits.
4325,2,The regression analysis is rubbish. Let's see what happens when you do this properly.
4326,3, Are the negative images messing up the training when the number is between 10^1 and 10^2?
4327,2,Figure 1. What does Min in panel c stand for?
4328,3, Authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zeros.
4329,3, Is it possible to combine them together?
4330,3,"If fixed, what sizes were tried?"
4331,3,"""This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum."
4332,3, Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers.
4333,3,"""This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words."
4334,3," However, \\alpha* is actually 0, and this choice would catapult w far away from w*."
4335,3,"""(Last minute reviewer brought in as a replacement).\n\nThis paper proposed \""Bayesian Deep Q-Network\"" as an approach for exploration via Thompson sampling in deep RL."
4336,3, This is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen.
4337,3, What it actually investigates is the performance of existing image classifiers and object detectors.
4338,1,\n\nClarity:\nThe paper is generally well-written and easy to read.
4339,3," The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information."
4340,1,\n\n\nEXPERIMENTS:\n\nLavaworld: authors show that pretraining the PATH function on longer 7-11 step policies leads to better performance\nwhen given a specific Lava world problem to solve.
4341,3,"""The authors adapts stochastic natural gradient methods for variational inference with structured inference networks."
4342,3," \nIt employs a previously proposed system, Open Classification Network, for classifying instances into known classes or rejecting as belonging to an unseen class, and applies hierarchical clustering to the rejected instances to identify unseen classes."
4343,2,"This paper was authored by a keen blogger and his colleagues. The paper perhaps reflects this informal approach, to its detriment"
4344,1,"\n\nThe paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading."
4345,3, \nThis algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.
4346,3, \n\nThe robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right?
4347,1,\niii) The model generalizes well in terms of link and node classification
4348,3, An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance.
4349,3, This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map.
4350,1,\n\t\u2022\tExcellently structured and presented paper\n\u00a0\n\t\u2022\t
4351,3,  For few-shot classification then the cross-entropy classification loss is used on the node.
4352,1, The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies.
4353,3, The authors propose to train a forward and backward RNN in parallel.
4354,1,nIt would be nice to see empirically how much of computation the proposed approach takes during training. 
4355,3,.\nThe paper states three conjectures (predictions in the paper):
4356,3,"""This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm."
4357,3,"""This paper present a method for detecting adversarial examples in a deep learning classification setting."
4358,1,\n\nThis seems to be a nice treatment of distribution to distribution regression with neural networks.
4359,3,\n\nIs your equation 1 correct? I understand that your logits are reciprocal of mean squared error.
4360,3," As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data)."
4361,1,\n\nRELATED WORK:\n\nGood contrast to hierarchical learning: we don\u2019t have switching regimes here between high-level options
4362,3," However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better. "
4363,1,\n\nOverall I think this method is inventive and shows promise for probing invariances.
4364,1,"""This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages."
4365,3," Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \""general English\""."
4366,3," \n\nThe paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification."
4367,3, \n\nThey then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i).
4368,3,"The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2."
4369,3,"\n- the language at the end of section 3 is very vague and noncommittal -- maybe just state what you did, and separately give future work suggestions?"
4370,1,\n\npros:\n(1) The idea is introduced clearly and rather straightforward.
4371,1, The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution.
4372,3," However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs."
4373,3,"  This manuscript formulates a convex program of optimal control without the separate system ID step, resulting in provably optimality guarantees and efficient algorithms (in terms of the sample complexity)."
4374,3," My intuition is that if the learner algorithm is convex, then ultimately they will all get to the same accuracy level, so the task is just to get there quicker."
4375,3," For example, in standard vision systems, low level filters \""V1\"" learn edge detectors (gabor filters) and higher level filters learn angle detectors [1]."
4376,3,"\n\n(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer."
4377,3,"""This paper analyzes the loss function and properties of CNNs with one \""wide\"" layer, i.e., a layer with number of neurons greater than the train sample size."
4378,3," I think it will be helpful to state the algorithms explicitly in the main paper."""
4379,3,"""The paper addresses the problem of learning mappings between different domains without any supervision."
4380,2,It reads like papers often do when they are written in LaTeX. Reject.
4381,1," Also, the combination of Kronecker product and soft unitary constraint is really interesting."
4382,2,Based on theoretical considerations I dont see a reason to perform these experiments.
4383,1, Results look like significant improvements over standard learning setups.
4384,1,"\n\n7. The experiments in Section 3.2.3 were interesting, since they demonstrate that the model can be warm-started from a model that hasn\u2019t fully converged."
4385,3,"Further, how many tau\u2019s one should evaluate?"
4386,3,. The acronym pkeep in later Tables should be clarified.
4387,1," \n\nPros:\n1, The paper is well presented and is easy to follow."
4388,2,I dont believe in simulations
4389,1, The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN.
4390,3,"""This paper proposes a new way of sampling data for updates in deep-Q networks."
4391,1, They are the best in terms of precision.
4392,3,  It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions. 
4393,1,It also adopts an interesting multicodebook approach for encoding than binary embeddings.
4394,3,"""This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time."
4395,2,This is an area ripe for future research. Recommendation: Rejec
4396,1, The method is clear
4397,3," From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods. "
4398,1,"Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea."
4399,3,"\nUsing leaky neurons for encoding and decoding is standard, see e.g.:\nBharioke, Arjun, and Dmitri B. Chklovskii. \""Automatic adaptation to fast input changes in a time-invariant neural circuit."
4400,3,  Could you elaborate on this?
4401,3," Specifically, this comprises a standard word embedding an accompanying local context embedding."
4402,3," The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though."
4403,3,\nOur main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims.
4404,3,"\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs."
4405,3,""")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \""center\"" ?"
4406,3,"The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention"
4407,3," What is \""performance\"" measured in? In general the Table captions need to be clearer and more descriptive"
4408,3, It belongs to the recent family of papers based on GANs
4409,3,"\""\n\nExplanations of better generalization properties of TR over SGD are important."
4410,1,n- in the experiments they trained the network on 64 x 64 patches and achieved convincing results
4411,1, All these seems very sound and interesting.
4412,3," But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima."
4413,1,\n-Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits
4414,3," \n\n== Other comments\n\n1) Note that d(A, B'_theta) is *equal* to min_alpha max_w  (...)  above equation (2) (it is not just an upper bound)."
4415,1, These theoretical results justify the objective function\nshown in Equation 8.
4416,3,\n\n== Detailed comments ==\nThe proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region.
4417,3, And I write this as a card-carrying computational linguist.
4418,1,"\n\nI find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below."
4419,3," Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition."
4420,3, The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations.
4421,3, Is it the case that the model learns to attend to last sentences for all the questions?
4422,3," Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking."
4423,3," Some Theory and Empirics\"" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images."
4424,3, This ensures that VAN starts out as a regular ResNet.
4425,3," it looks like finer scales are progressively dropped in successive blocks,"
4426,3, Is it to learn a continual learning solution?
4427,3,"\n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method."
4428,1," In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training."
4429,1,"\n2) I believe the control experiments are encouraging,"
4430,3,\n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation.
4431,3,"\n5) I would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale, as this would have practical implications.."
4432,1," In all three cases, the proposed solution outperforms the baselines on larger problem instances. """
4433,3," The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task."
4434,3,"\n\nThe significance of this work would still be quite strong if, as the author's suggest, these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable parameters. "
4435,3,"""This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks."
4436,1,"\nThe authors claim that the source of objective mismatch comes from n-step Q-learning, and their method is well-justified in 1-step Q-learning"
4437,3," While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet."
4438,3,"""This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting."
4439,1, \n\nPros: \n-The results on StarCraft are encouraging and present state of the art performance if reproducible.
4440,3," There also exists prior work on optimizing neural nets via GA (Leung, Frank Hung-Fat et al., IEEE Transactions on Neural networks 2003)."
4441,3, What happens if predicting the polar origin is not trivial and prone to errors?
4442,3,"\nMoreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...\n\n"""
4443,1,n\nThe method seems to work well based on the samples and ROC curves presented.
4444,3,\nThey show that it can be included as additional term in cost functions to train generic models.
4445,1, Removing baseline is a reasonable step and the paper includes analysis of several spike-train datasets.
4446,3, The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet)
4447,1,"\n\nThere are many good ideas and experiments in this paper and I would strongly encourage the authors to resubmit this work to a future conference, making sure to reorganize the paper to adhere to the relevant formatting guidelines."""
4448,3,"\n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines."
4449,1, \n\nLet me state the main interesting results before going into criticisms:\n1. TR method seems to generalize better than mini-batch SGD.
4450,3, It seems to me that a human subject experiment to somehow compare the two methods is required.
4451,1, (It would be excellent if an outcome of this paper was that commercial MT providers answered it\u2019s call to provide more realistic noise by actually providing examples.)
4452,3, The model is trained on an empirical distribution whose points are sampled from the true distribution.
4453,3,\\nI think the value in this paper comes from a practical/simple way to do policy randomization in deep RL.
4454,2,Find your inner nerdâit must be a big part of youâbind and gap it and then dump it in the ocean tied to a large rock. -Referee 
4455,3,"\n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems."""
4456,3, In that report \\alpha_l is a scalar instead of a vector.
4457,3, It would be desirable to sum up the overall procedure in an algorithm.
4458,3," The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm."
4459,3, \u201cCustomers\u201d randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.
4460,1,"  The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range)."
4461,1, I feel that the first two ideas are particularly interesting. 
4462,3,"  Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps."
4463,3, What is the meaning of success rate in here?
4464,3, Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse.
4465,3,\n\nCons:\n* The assumption that the measurement process *and* parameters are known is quite a strong one.
4466,3, The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.
4467,3," Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding."
4468,3, They also show that they can train deep residual network architecture on CIFAR without the use of BN.
4469,2,"I was really looking forward to reading this manuscript, however this enthusiasm soon waned."
4470,3," It might be the case that the considered scenarios indeed happen in computer vision related problems, but I am not an expert in that regard."
4471,3," If I optimized a method using this synthetic data, I would still need to assess the result on real data."
4472,3," It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet."
4473,3,"""This work shows that a simple non-parametric approach of storing state embeddings with the associated Monte Carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards (reacher, half-cheetah, double pendulum, cart pole) (due to the need to threshold a return the algorithms work less well with dense rewards, but with the introduction of a hyper-parameter is capable of solving several tasks there)."
4474,3,"  Like, what does it mean when we say, \u201cenvironment that the multi-agent system itself touches\u201d?"
4475,3,"Conclusion is short and a few directions for future research would have been useful."""
4476,3, however it is simply a classification problem
4477,1,\nI especially like the idea of \u201csoft unitary constraint\u201d which can be applied very efficiently in this factorized setup.
4478,1, A nice application\nof the separable principle to GCN.
4479,3, I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters.
4480,3,"""The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space."
4481,3,  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.
4482,3," Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea."
4483,3,"""This paper presents a method for image classification given test-time computational budgeting constraints."
4484,1, \n - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model.
4485,1,"n- Experiments show improvements over placement by human \""experts"
4486,3,  There can be many other approaches to mitigate the impact of angle bias.
4487,3, \nOverall the process by which GPNs are made tractable to train leverages many recent and not so recent techniques.
4488,1, \n* The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions.
4489,1, but its application for learning discrete networks is to my best knowledge novel and interesting.
4490,3, Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance.
4491,3,"""The authors address two important issues: semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image."
4492,3," To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters."
4493,3, So I guess it can be thought of as a kind of hierarchical RL.
4494,3,\n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches.
4495,3," This is the problem where given just one labeled image of a class, the aim is to correctly identify other images as belonging to that class as well."
4496,3," From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations."
4497,3,"\n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older)."
4498,3,". It also needs a lot of more studies on related work."""
4499,3," What happens when the encoder is optimized separately for each data point during training as well as testing?"""
4500,3, Authors are suggested to involve more datasets to validate the effectiveness of the proposed method.
4501,3,"""The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter."
4502,3,"\""\n\n7) Typo below (6): citetbarlett2017...\n\n8) Last paragraph p.5: \""Recalling that W_i is *at most* a hxh matrix\"" (as your result do not require constant size layers and covers the rectangular case). \n"""
4503,3,"  If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples."
4504,3,\n\nI think this paper would improve by demonstrating how time-aware policies can help in domains of interest (which are usually not time-limited).
4505,3," There are many related works concerning adaptive batch sizes, such as [1] (a summary in section 3.2 of [2])."
4506,3," However, it is not clear to me what contribute to the massive improvement of speed."
4507,1,n- it would be interesting to apply the approach to graphs with discrete and continuous labels
4508,3," \n\n- Generally, the quantitative impact of the adversarial loss never comes together. The only statistically significant improvement is on perceptual image realism."
4509,3, I think at least Machine Translation and other classification results should be added.
4510,3," Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN."
4511,3," I wonder if you could put this in the context of \""training with input noise\"", which has been studied in Neural Network for a while (at least since the 1990s)."
4512,3," \n\nFor the mixed decoding objective, how is the mixing weight chosen and what is its effect on performance?"
4513,2,My argument against it as a full paper is that it makes an incremental contribution to the state of the the science.
4514,3,  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.
4515,3, I think that it would make the paper stronger.
4516,1,"""The paper is generally well-written and the intuition is very clear. "
4517,3, but are minor contributions.
4518,3, My main concerns are on the usage of the given observations.
4519,3, \n- The notation changes between E and N and Z for the noises.
4520,3,"\n\n   Q( s,a,g ) = g(S) Wa S + Ba\n\nThe subgoal is the same as the first part, namely a linear transform of the expected expert direction in \nstates similar to state S.\n\n    g(S) =  Wv   SUMj  < Wk S, Wk Ej >  ( Ej - Ej\u2019 ) \n\nSo in some sense, the Q function is really just a function of S, as g is calculated from S."
4521,3,"\n* In Section 5.2, what is \""strong inference\""?[[CNT], [null], [QSN], [MIN]] This is not defined previously.[[CNT], [null], [DIS], [MIN]] \n* Have you evaluated on a larger dataset such as CIFAR?"
4522,3, Could authors please clarify this?
4523,3, But this does not matter since you already use the labels in the softmax loss.
4524,3," In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations."
4525,3," \n3.\tIt is better to provide results in terms of accuracy for both datasets, as previous methods usually use accuracy for comparison."
4526,2,A blizzard of extraneous data external information should be culled.
4527,3," This problem is basically active learning for\nprogramming by example, but the considerations are slightly different than in standard active\nlearning."
4528,3,\n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.
4529,3," More specifically, the paper proposed to generate images from attribute and latent code as high-level representation."
4530,3, The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image. 
4531,3," Although point 2 seems novel, I am not convinced that it is significant enough for ICLR."
4532,1," \n- As the proposed method was successful for the QA task,"
4533,3," Instead of using 1 long paths, one can simply use 2, 3, 4 etc."
4534,2,Nobody in their right mind would ever suggest such a model
4535,3," I'm usually quite happy to see connections being made to other fields,"
4536,3,\n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.
4537,3," While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible."
4538,1, \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.
4539,3," As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15)."
4540,3,". However, what is unique to RNNs is their ability to model long term dependencies across time."
4541,3,  The probability of what?
4542,3,\nI also have some concerns regarding the claim that \u201cWe confirm that optimization with the framework of NaaA leads to better performance of RL\u201d.
4543,3, There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2.
4544,3,"  Graph structure is determined by program abstract syntax tree (AST) and next-token edges, as well as variable/function name identity, assignment and other deterministic semantic relations."
4545,3," In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore."
4546,3,"""This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization."
4547,3, Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data.
4548,3,  How is the conditional entropy term estimated?
4549,1,"\n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original."
4550,3,\n\nThe main issue I am having is what are the applicable insight from the analysis:
4551,3," It seems to just subtract a constant 1 from the regularization term."""
4552,3,"  Further on you cite that L_PIB is intractable due to the high dimensionality of the bottleneck variables, I imagine that this still yields a high var MC estimator in your approximation (in practice)?"
4553,3,  There are two levels of sampling: of the latent X and of the observed value given the latent.
4554,1,\n\nThe paper is well written overall and relatively easy to understand.
4555,1," The authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations."
4556,3," VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution."
4557,3," Of course, the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized."""
4558,3,"""After reading the revision:\n\nThe authors addressed my detailed questions on experiments."
4559,3, They key innovation is a convolutional architecture that represents the invariance around the target base.
4560,3,\n\nAnother point relates to the fishing game.
4561,1,\n--The experiment section is thorough.
4562,3,They evaluate the suggested model on synthetic data and outperform the current state of the art in terms of accuracy.
4563,3," A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way."
4564,2,"This will never work: Negative reviews of famous, ground-breaking papers...."
4565,3, The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter.
4566,3, Applying the proposed method on the strong baselines would highlight the author's claims more strongly than just applying on the average performing chosen baselines.
4567,1," The application domain is interesting,"
4568,3,\n* Authors seem to only consider deterministic defogging models.
4569,3, How realistic is this set-up and in what application is it expected that this will show up?
4570,2,I did not attempt to understand Figure 2 because there was no motivation given for it
4571,3," In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options."
4572,3," They first take co-occurrence counts counts of pairs of words in a local window of each preposition, and then factorize the matrix to find low dimensional word representations."
4573,3,"\n\nSecond, all experiments were conducted on the MNIST dataset."
4574,3,\n[3] Clustered multi-task learning: A convex formulation (NIPS)
4575,1,\nThe authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices.
4576,3,"""The proposed approach, GTI, has many free parameters: number of layers L, number of communities in each layer, number of non-overlapping subgraphs M, number of nodes in each subgraph k, etc."
4577,3, It is quite unclear how one could generalize the Theorem 1 to arbitrary activation functions phi given the crucial use of the homogeneity of the ReLU at the beginning of p.4.
4578,3," At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017."
4579,3,\n\nI have two main concerns with the presentation.
4580,3,"\n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper."
4581,3," \n\niv. In part 1 of the proof of the main theorem, it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability (equation at the top of the page)."
4582,3," For example, Figure 5-7 show variable sizes of the generated outputs."
4583,3, Finally the third technique learns to weight the samples according to their distance to the original prototypes.
4584,3," The authors design an architecture that works on image captioning, image classification, machine translation and parsing.\"
4585,2,This paper still reads in places more like alternative fan-fiction than scholarship.
4586,1," This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense."
4587,1,\n- Reads well
4588,3,"   I can't point to exactly what would have to be different to make things \""work\"", because it's really hard to do that ahead of actually trying to do the work. "
4589,3," Or conversely, GPO without high-reward filtering during crossover."
4590,3, Is q(t|x) in Fig 1 a typo?
4591,3," The authors provide yet another instantiation of such an approach, but this time with an LSTM."
4592,3,  How is this achieved in practice?
4593,3, There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable.
4594,1, applying it to architecture search is novel.
4595,3, It then argues that speeding up the activation function may be important since the convolution operations in CNNs are becoming heavily optimized and may form a lesser fraction of the overall computation.
4596,3,"\n\nBased on the arguments above, I think this paper is valuable at least\nconceptually, but doubt if it is actually usable in place of ordinary LSTM\n(or RNN)-based generation."
4597,1,. \n\nThe paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper)
4598,3," but unfortunately the efficacy is not well justified in theory.[[CNT], [CNT], [CRT], [MAJ]] The empirical study is not always convincing, and did not compare with many state-of-the-art baselines."
4599,3, Do the authors think that this distinguishability can lead to a defense that uses these statistics?
4600,3,"  Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?"""
4601,1, The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach.
4602,3," Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression."
4603,3, \n\nOne major question.
4604,3,"  It feels to me that these tend to be sparse,"
4605,3, (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.
4606,3,\n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?
4607,1," The experiments show that the generation component is quite effective,"
4608,1," \n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality."
4609,1,"\n\nA particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation."
4610,3, \n- Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE).
4611,1,"\n\nI found the paper quite interesting, but meanwhile I have the following comments and questions:"
4612,3,"""Summary: \n\nThis paper proposes a multi-agent communication task where the agents learn to translate as a side-product to solving the communication task. "
4613,1, It seems to me that this paper is a significant contribution to the field of question answering systems.
4614,3,"Again, Tamar et al. 2016 deals with this 3 points."
4615,3,"""This paper presents a method for choosing a subset of examples on which to run a constraint solver\nin order to solve program synthesis problems."
4616,3,"\n* The term \""AI-related task\"" sounds a bit too broad[[CNT], [PNF-NEU], [SUG], [MIN]]\n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection."
4617,3, This needs to be explored.
4618,3,"""In this paper a neural-network based method for multi-frame video prediction is proposed."
4619,1," The idea of counting the number of regions exactly by solving a linear program is interesting,"
4620,1, Dict is much more useful than spelling here.
4621,3,"""The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal."
4622,3,". If these are better than random and the perturbations are more successful it would be a much more compelling story. """
4623,3,"\n\nSection 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments. h is never discussed after section 2.2."
4624,1, \n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family).
4625,3,\n\nQuality\nThe authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in Danihelka et al. (2016).
4626,3," Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al."
4627,1, The experimental results also look promising.
4628,2,"The text is overly expansive, desultory, and often diaphanous, so that the raison d'Ãªtre of an overarching theoretical structure is neither pellucid nor convincing."
4629,2,"This paper is baffling.[â¦] In particular, I have not looked at all at section 2 of the paper"
4630,3," Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points."
4631,3,\n2. Clipping is supposed to help with the exploding gradients problem.
4632,3," Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance."""
4633,1, Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently.
4634,3,  \n\nThe plots could also use a bit of help.
4635,1,"\n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis."
4636,3,"\n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive."
4637,3, Examples of missing details are: how are the high-reward trajectories filtered?
4638,3,\n\n+ Insights on how different modalities affect the prediction results.
4639,2,"This manuscript uses half of the available pages, and fails to explain what has been done, how and..."
4640,3,"""Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation."
4641,1,"  Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems."
4642,3,"        (c) from sort-of-CLEVR \""objects\"" to PSVRT bit patterns"
4643,3,"\n\n- The idea of sample reweighting within the MMD was in fact already used for DA, e.g., Huang et al., NIPS 2007, Gong et al., ICML 2013."
4644,1," However, the results show that the deeper self-attention layers are indeed useful (but still not improving a lot, about 0.7-0.8%)."
4645,3,"  Compared to prior work (hierarchical latent tree model), this work introduces skip-paths."
4646,3," The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence."
4647,1, This is where I learned something new.
4648,1, though it's not clear from the paper that the approach is a substantial improvement over previous work.
4649,3, The task is performed on tabletop videos of a robotic arm manipulator interacting with various small objects.
4650,3,\n\nI am confident that point 1 has been used in several previous works.
4651,3,\nThere are a couple of highly related work with multi-view VAE tracking similar problem have been proposed in the past.
4652,3," For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results."
4653,3,how does this change depending on the dimensionality of the latent codes?
4654,3,"""Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games."
4655,1,\n+ The idea is clearly explained and well motivated.
4656,1,"\n\nIn summary, I feel that while there are some issues with the paper, it presents\ninteresting results and can be accepted."""
4657,3, \n\nExperiments -- why/how would you have distorted test data?
4658,3,"""Summary:\nThis paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion"
4659,3,"\n\""To enjoy the best of both worlds, we also introduce a \u201chybrid\u201d method in the Figure 3, that is, first run TR method for several epochs to get coarse solution and then run SGD for a while until fully converge."
4660,3,  \n\nTaxi: the authors train the PATH problem on reachability and then show that it works for TAXI.
4661,1," While this might be (known) intuition that people rely on when learning deep generative models, it is important to be able to test it, as this paper does."
4662,1,"  Additionally, this approach allows a significant reduction of training time it seems.\n\n"""
4663,3, Is that correct?
4664,3,"   Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters."
4665,3," Specifically, a simple method such as \""background subtraction\"" can easily infer the mask (the outlying pixels which correspond to moving objects)  while simple tracking methods (see a huge literature over decades on computer vision) can allow to track these objects across frames. "
4666,3,. This makes it even likely that the same subject with the same disguise appears in the training and test set.
4667,3, Is it due to the simple hill climbing procedure (basically evolution that only preserve the elite)?
4668,3," \n\nSection 4, while interesting, appears to be somewhat disconnected from the rest of the paper..[[CNT], [EMP-NEG], [CRT], [MIN]] \n\nIn Theorem 2.3. explain why the two layer case is limited to n=1.."
4669,3,"""The paper adopts the concept of saliency to explain how the deep model makes decisions with adversarial perturbations. "
4670,3," Moreover, I would be very curious about ways to better integrate causality and generative models, that don\u2019t focus only on the label space."
4671,3, It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better)
4672,3, The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a \u201cgate\u201d to the information flow
4673,3, The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).
4674,3,"\n\nIn section 2.2, it says \""observation that the double centering...\""."
4675,2,"Words are used inappropriatelyâI count, for example, 13 instances of 'unique', but it is used correctly only once."
4676,1,g \n-\tThe method enables creation of adversarial examples for block box classifiers
4677,3,". \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank,"
4678,1, The motivation of the work is clearly explained and supported with relevant related work.
4679,3," Basically, it means that B(P_X) should be a small constant."
4680,3,  What about comparison to other batch normalization methods in biology such as SEURAT? 
4681,3, This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method.
4682,1," This is an interesting result, and useful in its own right."
4683,3," Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels."
4684,1," In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer."
4685,2,"â¦somewhat anachronisticâthis study would have been really interesting 10-15 years ago, but not it seems quite out of date."
4686,3, Lots of people have tried pre-training a neural network on auxiliary task(s) and using the features from it as input to the final SVM classifier.
4687,1,"\nThe result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable."
4688,3,"\n\n\n[1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux."
4689,1,"""Update:\nOn further consideration (and reading the other reviews), I'm bumping my rating up to a 7."
4690,3,"\n\nI'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?"
4691,3, It can be trained with source sentences having various styles and it can produce sentences in a different style without changing the content much.
4692,1," \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem,"
4693,3," One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added"
4694,1,"\  It first shows that a simple weakly regularized (linear) logistic regression model over 200 dimensional data can perfectly memorize a random training set with 200 points, while also generalizing well when the class labels are not random (eg, when a simple linear model explains the class labels); this provides a much simpler example of a model generalizing well in spite of high capacity, relative to the experiments presented by Zhang et al (2017). "
4695,3,\nCons\n-\tThe idea implementation is basic
4696,2,It appears that publication in any form would be premature at this time.
4697,3,"""The authors define a deep learning model composed of four components:  a student model, a teacher model, a loss function, and a data set."
4698,3,The authors go on to compare their baseline-corrected (BC) method with several established methods for dimensionality reduction of spike train data.\n\n
4699,3, Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity?
4700,3," They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem."
4701,3,"  Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling."
4702,2,The English language ranks this manuscript among the top 5 worst manuscripts I have ever reviewed
4703,3,"\"" Why then try to link \""meanings\"" to basis vectors for the rows of A?"
4704,3, Because Gumbel Softmax doesn\u2019t scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experiments.
4705,3," Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature."
4706,3," The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible."
4707,3,"\n- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper."
4708,3,"""This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints."
4709,3," Or a simpler genetic algorithm that just preserves the kills off the worst members of the population, and replaces them by (mutated) clones of better ones, etc."
4710,3,\n\nComments:\n- Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity?
4711,1, The authors derive relevant sample complexity results for their approach.
4712,1," The work is original, but I would say incremental, and the relevant literature is cited."
4713,3,\n\n* Please clarify whether the objective value shown in the plots is wrt the training\n  set or the test set.
4714,3, \n\n- Some figures and tables are overlapping in the experiments. Just keep one is enough.
4715,3," I believe the conclusions made by the authors are mostly valid.[[CNT], [null], [APC], [MAJ]] \n\nI would firstly like to point out that measuring generalization is not standard practice in RL."
4716,3, This choice could be motivated from the success of residual networks.
4717,3,"""The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN."
4718,3,\n\nDoes Figure 8 show an example input after the extractive stage or before?
4719,3, I believe the following things would be interesting points to discuss / follow up:
4720,3," The paper's main claim is that this model architecture enables strong\ngeneralization: it allows the model to succeed at the instruction following task\neven when given words it has only seen in QA contexts, and vice-versa."
4721,2,I don't think that means what you think it means
4722,1,"\n5. While the algorithm empirically improve over k-means,"
4723,1, The paper is very well written.
4724,1,   The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper).
4725,3, Most of the training takes place without the skip connection.
4726,1,\n\nOverall the paper was well written.
4727,3," The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly."
4728,1,"""This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images."
4729,3,"\n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems."
4730,1," Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community."
4731,3,"\n\n2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage."
4732,3,"  It may very well end up that your very simple model does as well as a well estimated model, but that is something to establish in your paper, not assume."
4733,3,"""This paper proposes a direct way to learn low-bit neural nets."
4734,3,\n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets.
4735,3," The model does not assume an explicit prior over the underlying dynamical systems, instead only uncertainty over observation noise is explicitly considered."
4736,1, \n\nI like the way they handle the nonconvexity component of the model.
4737,2,This paper is too difficult for a journal on Astrophysical Fluid Dynamics
4738,3,"""In this paper, the authors define a simulated, multi-agent \u201ctaxi pickup\u201d task in a GridWorld environment."
4739,3, The GAN is 1 forward operation.
4740,3, \n\nMinor points: Section 5.3 Square map: how much more reward will the agent gain by taking the optimal path?
4741,3, \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.
4742,3," The most difficult part is the lack of precision on the maths, it is hard to figure out what the authors contribution indeed are."
4743,3, Currently most domains are terminated by failure/success conditions rather than time.
4744,3,\n- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder)
4745,1,"\n\nCOMMENTS\n\nThe introduction is well-written, clear, and concise."
4746,1," (So, the *interpretation* of the message is learned). I like this idea,"
4747,3,\n- 'Non-parametric filter' may not be right word as this work also uses a parametric neural network to estimate filter weights?
4748,3,"\n\n- page 3: \""(cite a couple of them)\"" should be replaced by some actual references :)"
4749,3,  The model is trained on a new dataset collected from Tumblr.
4750,1,"\n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)"
4751,3, The idea is to jointly train
4752,3," To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations."
4753,3," I am a weak reject"""
4754,3," All examples in the dataset are considered \""normal\"" to start with."
4755,1,  It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.
4756,3," I suggested that the author should reform the structure, ignore some unrelated content and make the clear claims about the contributions on the introduction part. "
4757,3,\n\nA deep architechture is proposed to solve the problem: see fig 1.
4758,3," I guess the most accurate opposite would have been \""The service is quick but not good\""... )"
4759,3, It would be nice to understand/visualize what information have been extracted in the representation learning phase.
4760,3, Can it be made crisper?
4761,3, or even how reliable they are since the effectiveness has not been evaluated.
4762,3,  A minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to one.
4763,3," \n\nThe paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop."
4764,3," \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc?"
4765,3," To be consistent, notation like \\odot and \\bigodot might be a bit clearer."
4766,3,\n- why cite Anonymous (2018) instead of Appendix...?
4767,3,\n- The authors showed benefits compared to a continuous relaxation baseline.
4768,3," Therefore, the novelty of the proposed method is somewhat weak."
4769,3, These functions are trained to maximize expected squared jump distance.
4770,1," In MNIST experiment, for example, better numbers are reported for larger LSTMs."
4771,2,This manuscript was neither enjoyable nor informative to read
4772,3,"\n\nHow can variational inference be attributed to (again) a survey paper on the topic from 2008, when for instance [2] appeared in 2003?[[CNT], [CNT], [QSN], [MIN]]\n\n\n- Correctness of the approach"
4773,1, \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated
4774,3,\n\nMy major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments.
4775,3," When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds)."
4776,1," The formulation of the model is straight-forward,"
4777,3, \n\t\nStrengths:\n\n1.\tThe data collection process is interesting.
4778,1,"\n\nThe results of MOS is very good,"
4779,3, \n\nExperiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples).
4780,3, \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples).
4781,3,"In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand? "
4782,3,"  \n\n(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale?"
4783,3,"\n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better."
4784,3,\n\nQuestions\n--------------\n\n1. Section 3: You say that you can vary 'pf' and 'qf' to set the trade-off between computational budget and performances.
4785,3,\n- The extensive experimentation shows that the proposed network is better than previous approaches under different regimes.
4786,1,"  \n- This work collected a new dataset for 3D face expression representation, which is great (the state of 3D face databases which are available to researchers is very limited, so this is a step in the right direction)."
4787,3,"\nSpecifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy."
4788,3, Now it's calculated as the average of the last layer's features
4789,3,"""\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee."
4790,1, The Related work part of this paper makes a good description of both topics.
4791,3,"\n- define V in Thm. 1.\n- in eq. (4) it may be clearer to denote g_k(w). Likewise in eq. (6) \\hat{g}_\\hat{A}(w), and in eq. (14) \\tilde{g}_{\\cal{A}}(w)."
4792,1, I would say that the topic is an important one.
4793,3," If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still \""work\"" and the distribution of X can also change without affecting the accuracy of the predictions."
4794,3," For example, I can't find the definition of D.[[CNT], [PNF-NEG], [CRT], [MIN]] From the context it seems to be the number of layers in the network (I shouldn't need to guess)."
4795,3, An existing node embedding method is used to learn vector representations for the nodes
4796,3," Based on my understanding, the claims are 1) the new formalization of the goal of GAN training and 2) using this test to evaluate the success of GAN algorithms empirically? "
4797,1," \n\n- The analysis of section 4.1 is interesting, it was insightful and to the best of my knowledge novel."
4798,3,"\n\nFor active learning, the proposed method seems to be specific to the case of obtaining a single label."
4799,3," \nComparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q."
4800,3,"\n \nThere is a big gap between the last two paragraphs of section 3.[[CNT], [PNF-NEG], [CRT], [MIN]]\n \n4. Neuro as an agent[[CNT], [CNT], [CNT], [GEN]]\n \n\u201cWe add the following assumption for characteristics of the v_i\u201d -> assumptions for characterizing v_i[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n\u201cto maximize toward maximizing its own return\u201d -> to maximize its own return[[CNT], [CLA-NEG], [CRT], [MIN]]\n \nWe construct the framework of NaaA from the assumptions -> from these assumptions\n \n\u201cindicates that the unit gives additional value to the obtained data.[[CNT], [CLA-NEG], [CRT], [MIN]] \u2026\u201d I am not sure what this sentence means, given that \\rho_ijt is not clearly defined.[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n5. Optimization\n \n\u201cNaaA assumes that all agents are not cooperative but selfish\u201d Why?"
4801,3, They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks.
4802,1, \n\nOriginality:\nI'm pretty sure this is the first paper to tackle this problem directly in general.
