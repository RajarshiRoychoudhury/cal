,Class Index,Description
0,1,". However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs)."
1,1," I think it will be helpful to state the algorithms explicitly in the main paper."""
2,1," I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?"
3,1," Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity?"
4,1, Why not use the more standard 1/t decay?
5,1," Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful."
6,1,"\n5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating."
7,1," The authors compare their method, named Sequential Regression Models (SRM) in the paper, to previously proposed methods such as BNN, LCE and LastSeenValue and claim that their method has higher accuracy and less time complexity than the others."
8,1,\n- The paper is clearly written.
9,1, It would be interesting to investigate the limits of this statement: what\nwould happen by augmenting only 8 or 7 or 6 or 5... categories instead of 9?
10,1,"""This paper proposes to use neural network and gradient descent to automatically design for engineering tasks."
11,1,"""The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time."
12,1,I think the same method can be applied to GRUs or LSTMs
13,1, The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.
14,1, The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks.
15,1, The optimisation method appears close to brute force and is limited to 2 layers.
16,1, \n\nI understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better.
17,1," but the empirical evaluations are not convincing. \n\n \n\n \n\n """
18,1," \nLet < Wk S, Wk Ej > be a weighted similarity between learner state features S and expert state features Ej\nand Ej\u2019 be the successor state features encountered by the expert."
19,1,"\n-\tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, \""A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming,\"" IEEE Journal of Selected Topics in Signal Processing. "
20,1,"\n\nIt would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error. "
21,1,"""The authors present a model for text classification."
22,1," \n\nFor componentwise multiplication, you have been using \\circ, but then for the iterated component wise product, \\prod is used."
23,1,"\n\n- In Table 4, I find it surprising that there is an actual speedup for the model with larger width."
24,1,"\n\nOn the whole, I think this paper does paper does a good job of motivating the\nproposed modeling decisions."
25,1,\n- The paper is mostly clear and well written.
26,1,"  \n\nThis approach is original and significant,"
27,1," Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization."
28,1," yet not SOTA, and come at a price of more complex training protocols (see below)"
29,1," Unfortunately, I do not see any novel concept in the experimental setup."
30,1,"  Again, I strongly suggest the authors to provide a complete review of the literature."
31,1,"\n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering."
32,1," -- it's true that in Figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem;"
33,1, It may be useful to compare to the uncertainty produced by a GP with suitable kernels.
34,1,\n\nFigure 5:\na) What was the rational for stopping training of CommNet after 100 epochs?
35,1," The experiments, however, show clearly advantages of the approach "
36,1," Since there is some related work, it may be also worth to compare with it, or use the same datasets."
37,1,  It is a simple extension of [1] without considering and solving problems in [1].
38,1," \n(c) If the paper length is limited, a supplementary material about those details would be preferred."
39,1, All these seems very sound and interesting.
40,1,"\n\n* I don't see the difference between what is described in Section 2.2\n  (\""learning which model to learn\"") and usual machine learning (searching for\nthe best hypothesis in a hypothesis class)."
41,1," \n\nAuthors claim that the proposed model can generate \""fluent, coherent\"" output, however, no evaluation has been conducted to justify this claim."
42,1," \n\nIn experiments, why the off-policy version of TRPO is not compared."
43,1, I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making.
44,1, In Section 2 the authors review conceptors.
45,1, \n    Also it is really hard to understand how could they obtain such impressive result by doing an unsupervised training on a dataset containing 3353 samples taking into account the high capacity of the models they are using.
46,1,\n\n1) Sec 2.2 introduces the C&W attack.
47,1," The selection of the inputs happens through gate weights, which are sampled at train time."
48,1," In particular, the advantages and disadvantages of different categories are not systematically compared, and hence the readers cannot get insightful comments and suggestions from this survey.\"
49,1, The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss.
50,1, The paper is very well motivated and tackles an important problem.
51,1,\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound.
52,1, \n\nClarity: The paper is written well and the issues raised are well motivated and proper background is given.
53,1," This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods."
54,1,"\n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \""In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S\u00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1)."
55,1," I think the paper is borderline, leaning towards accept."
56,1," This type of study is important to give perspective to non-standardized performance scores reported across separate publications,"
57,1,"In particular, the authors assume that the function admits a representation as a sparse and low-degree polynomial"
58,1," However, all of the other grapheme-based end-to-end systems enjoy the same benefit as CTC and ASG."
59,1,\nPage 7:\n-\tFigure 7 is not very informative.
60,1,\n- The paper proposes a new objective function to measure joint sensitivity
61,1," Put in this light, the proposed paper does not contribute much."
62,1, Stochastic gradient Riemannian Langevin dynamics on the probability simplex.
63,1," However, I think some of the counter-intuitive claims on the proposal learning are overly strong, and not supported well enough by the experiments."
64,1," For instance, in the decoder, the paper mentions for the first time that there are variables \u201cz\u201d, but does not mention in the encoder how the variables \u201cz\u201d were obtained in the first place (Sec. 3.1)."
65,1,"\n\nGiven that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons."
66,1,  I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.
67,1," \n\nThe major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy."
68,1,"\n2. The definition of v and delta_TD w.r.t. to v seems unnecessary, since you only use Q."
69,1, \n\nOrdering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth.
70,1,\nOptimizing the PIB objective is intractable and the authors propose an approximation that applies to binary valued stochastic networks.
71,1," In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector."
72,1, Can you explain why this is possible?
73,1," \n\nI am very happy to see experimental evaluations on real robots, and even in two different application domains."
74,1," \nIt is unfortunate but understandable that the GPN model experiments are confined to another paper."""
75,1, \n \nAnother concern is that the novelty.
76,1,". In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements"
77,1,"\nI think this could explain the better experimental results\n\nIn the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper."
78,1,"\n- Something feels wrong/hacky/incomplete about just doing \""ensemble\"" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \""random initialization + SGD/Adam + specific network architecture\"" to maintain this idea of uncertainty."
79,1," The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames."
80,1," If the authors want to make the second claim, training the network with adversarial examples coming from OptMargin is missing."
81,1,"\n- \""simply use the weighted error function\"": I don't think this is correct, AdaBoost loss function is an exponential loss."
82,1," Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks."
83,1,"\n\nIn table 3, it would be very helpful to display the English source."
84,1,\n\nThe authors do not compare with variational approaches to Bayesian learning\n(Lipton et al. 2016).
85,1," However, the statement \u201crecently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework\u201d is incorrect."
86,1,   This idea seems interesting but very difficult to evaluate.
87,1," However, there is not a comparative evalution with these methods."
88,1,\n\n5. Figure 4: the authors should show the same plot for more convolutional layers at varying depth from both VGG and ResNet.
89,1," In the VAN initial state (alpha = 0.5), both the residual path and the skip path are multiplied by 0.5 whereas for ResNet, neither is multiplied by 0.5."
90,1, So first they are separately learnt and then fine-tuned together.
91,1, Interested readers can then work through the math if they want to.
92,1, The related work section mentions Generative Adversarial Nets.
93,1," The key finding is that MC appears to be more robust than TD in a number of ways, and in particular the authors link this to domains with greater perceptual challenges."
94,1,"\n- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)"""
95,1," It is misleading to claim any unsupervised or semi-supervised learning based on the *self-organising part* of, for example, eq. 14, which is merely a result of applying chain rule through the hidden neurons' activation."
96,1," Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear."
97,1,\n- What does it mean by \u2018unofficial dataset\u2019 (page-4)
98,1,"\nAs follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem."
99,1, How long is a typical game? 
100,1," In that case the soft ordering could\n  actually learn the optimal depth as well, repeating identity layer beyond the option number of\n  layers."
101,1, To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units).
102,1, During the rebuttal I would ask the authors to extend f-Hyperband all the way to the right in Figure 6 (left) and particularly in Figure 6 (right).
103,1,"""This paper is well written and easy to follow."
104,1, \n- Section 4.2 - \\hat{Q} is just Q in many places.
105,1,"Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit. "
106,1,"""This is a very clearly written paper, and a pleasure to read."
107,1, A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work.
108,1,"\n\n    Q( S,a ) = g(S) Wa S + Ba \n\nSo this allows the Q-function more flexibility to capture each subgoal in a different linear space?"
109,1,\n\nPROS \n- A new approach to analyzing the behavior of weight matrices during learning
110,1,\n\nPros:\n\u2022\tWell written and clear
111,1," The authors propose a way to merge/discard sensors, and there is no comparison with other ways of doing it (apart from the trivial sensor concatenation)."
112,1," While this paper makes a contribution to representation learning in suggesting a good way to learn a representation for prepositions, it does not make any contributions to methods of representation learning."
113,1," For example, the policy gradient in (8), which is further decomposed in Appendix A.1 as (15) and (16) and simplified, appears to be Eq. (50) of [Schulman et al., 2017] (https://arxiv.org/abs/1704.06440)."
114,1," \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy."
115,1, I think this more counts as careful engineering of the SAN model rather than a main innovation.
116,1,"? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another"
117,1," The topology is already known, the upsampling just predicts a function on this topology."
118,1,"\n\nFor me the novelty of this paper was in its application to the realm of emotion theory, but I do not feel there is a contribution here."
119,1," However, such contributions are quite minor, and technically heuristic."
120,1," In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned."
121,1," In any case, competing with an active leader board would be much more compelling."""
122,1,  The open-world related tasks have been defined in many previous works.
123,1,"\n  - the novelty of the approach is somewhat limited,"
124,1, The authors should compare to such methods as a baseline.
125,1,\n2) The authors reject the technique of 'Deep compression' as being impractical.
126,1,\n\nClarity:\nThe paper is very clear.
127,1, Is it due to the simple hill climbing procedure (basically evolution that only preserve the elite)?
128,1,\n+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks
129,1," Could you please explain why such phenomenon happens?"""
130,1," but ultimately takes the wrong view point:\nThe authors try to attribute fragility to explaining methods, which visualize/measure the reaction of the model to the perturbed inputs."
131,1,"  As such, it ought to be evaluated on other types of problems to demonstrate this generality."
132,1," The authors spend so much effort developing their own algorithm! Also, in actual implementation, they only use a crude version of the inner algorithm for reasons of efficiency."
133,1," The basic idea is to count word pairs which co-occur with a preposition, rather than single words which co-occur, as in standard word vector models such as word2vec."
134,1,   A language model could probably be used to generate a compelling distractor.
135,1," I agree the computational budget makes sense for cross data transfer,"
136,1,"\n \n# Quality\n- Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games."
137,1," \n\nIn section 3.1, it would be helpful to cite a reference to support the form of dual problem."
138,1," That\u2019s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair."
139,1, I would say the idea is well justified in several ways.
140,1,\n\nMinor\n- write in the description for table 1 what task the accuracies correspond.
141,1," For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper."
142,1, \n\nI was expecting the authors to mention \u201cgoal emulation\u201d and \u201cdistal teacher learning\u201d in their related work.
143,1," Since the efficiency is one of the main contributions, I suggest authors add this comparison.\n\n"
144,1, \n\nQuestions:\nHave you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study)
145,1,"For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum."
146,1, It's fine if that pushes the paper somewhat over the 8th page.
147,1,"\nExperiments show that on the navigation task, the proposed approach outperforms\na variety of baselines under both a normal data condition and one requiring\nstrong generalization."
148,1,"\n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL."
149,1,\n- Few-Shot Learning Through an Information Retrieval Lens (Triantafillou et al.)
150,1," Also, if the claim is that there are not deep learning survival analyses, please see, e.g. Jing and Smola."
151,1,  The coreset construction requires one to construct a set of  points which can cover the entire dataset. 
152,1,
153,1,\n-            I do not think the empirical results provide enough evidence that it is a useful/robust method.
154,1," They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating."
155,1,"""Summary: \nThe paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem"
156,1,"\nThe proposed criteria are not compared with previously proposed ones - equivariance, invariance and equivalence [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]."
157,1," \n\nComments:\n-- Since the authors are using a pre-trained VGG for to embed each image, I'm wondering to what extent they are actually doing one-shot learning here."
158,1," Indeed, Neal himself says that in the multi-output settings, all the outputs become uncorrelated."
159,1,"""Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights. "
160,1, How does num required epochs get impacted as we increase this class space?
161,1,"""This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis."
162,1," More worryingly, when observing that the method of (Mirowski et al, 2017) may not generalize to unseen environments in claim [c], the authors of this submission seem to confuse navigation, cartography and SLAM, and attribute to that work claims that were never made in the first place, using a straw man argument."
163,1," This is a poor quality paper, is unclear in the results (what metric is even reported in Table 6), and has little significance (though this may highlight the opportunity to revisit the encoding scheme for characters)."""
164,1,"I therefore have no option but to vote for reject of this paper, based on my educated guess."
165,1," Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound?"
166,1,"""This paper considers distributed synchronous SGD, and proposes to use \""partial pulling\"" to alleviate the problem with slow servers."
167,1,"\nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD."
168,1,"\n\nReporting per-token accuracies, such as those in Table 2, is problematic."
169,1," While none of the contributions are especially novel,"
170,1," One could imagine coding missing data in various ways (e.g. asking the generator to produce a missingness pattern as well as a time series and allowing the discriminator to access only the masked time series, or explicitly building a latent variable model) and some sort of principled approach to missing data seems crucial for meaningful results on this application. """
171,1," \n\nSummary of the review: \n\nThe paper is well written, clear, tackles an interesting problem."
172,1,   Then is there anything new in FAME on the gray images?
173,1,"\n\nI apologise for short and late review: I got access to the paper only after the original review deadline."""
174,1,\n\n4. The proposed model merely showed two-layer generation results.
175,1, \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.
176,1,"""The paper proposes a method for few-shot learning using a new image representation called visual concept embedding."
177,1,"""The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning."
178,1,\n  - lack of comparison to other tensor approaches to the word embedding problem (i.e. other algorithms for the tensor decomposition subproblem);
179,1, but the experiment section failed to convince me (see details below).
180,1, \nThese are temporary and are later made redundant.
181,1,"  Its practical utility is questionable.  It\u2019s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments."
182,1,"\n\n[1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015\n[2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016\n[3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016 \n[4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017\n[5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017\n"""
183,1,\n2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.\
184,1, It also analyses how adversarial training affects detection.
185,1,\n\nThe paper uses much space to show how to compute gradients in the proposed\narchitecture: there is obviously no need for this in a day and age where\ngradients are automatically derived by software.
186,1," If that is the case,\nthe paper does not have any original contribution."
187,1, Authors also propose a paraphrasing based data augmentation method which helps in improving the performance.
188,1, What I do not understand are then the high classification errors reported in Tab. 1.
189,1," One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y."
190,1," Finally, in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work, this is not the case eventually, as it is acknowledged at least in the case of MIXER."
191,1, The citations are in non-standard format (section 1.2: Kalman (1960)).
192,1," From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta-learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level\n3."
193,1,"""The paper is clear and well written."
194,1," The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks."
195,1,\nOnly a single synthetic task is reported.\n\n
196,1," Here is an example where this algorithm fails: consider the point (x,y,w,\\alpha,\\lambda) = (100, \\sigma(100), 1.0001, 1, 1)."
197,1, This idea of training with a standard loss (conditional log lik.) and then amortizing inference post-hoc was not explicitly introduced as an alternative to the saddle point objective you put forth earlier in the paper.
198,1,\n\nPros:\nUses a simple parametrization of the rotation matrices.
199,1,"\n\nIt should also be noted that I was asked to review another ICLR submission entitled \""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\"" which amazingly introduced the same \""Pong Player\u2019s Dilemma\"" game as in this paper."
200,1,"""The below review addresses the first revision of the paper"
201,1,"\n\nSome comments:\n\n* Section 4: I found this argument extremely interesting.[[CNT], [null], [APC], [MAJ]] However, it\u2019s worth noting that your argument implies that you could get an O(1) SNR by averaging K noisy estimates of I_K."
202,1, It would be nice to see how the behavior and boundaries look in these cases. 
203,1, \n\nClarity:  The paper is clearly presented and easy to follow.
204,1,"  \nHowever, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this."
205,1, They are train of the same machine or cluster in a controlled manner by the person that would use the system.
206,1,\n\n\n2. High level paper\n\n- I believe the writing is a bit sloppy.
207,1,"\n\nSome minor grammatical mistakes/typos (nitpicking):\n- \""gives a good performance\"" -> "
208,1, The point of showing your algorithm not solve Baird\u2019s counter example is unclear.
209,1,"\n\nIf the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal."
210,1, but the ideas are the same.
211,1,"  To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations."
212,1,\n\nThe author\u2019s articulate a specific solution that provides heuristic guidance rewards that cause the \nlearner to favor actions that achieve subgoals calculated from expert behavior\nand refactors the representation of the Q function so that it \nhas a component that is a function of the subgoal extracted from the expert.
213,1, I disagree. Let's look at an example. Consider ResNet first.
214,1," It is evaluated on a corpus of 29M SLOC, which is a substantial strength of the paper."
215,1,\n\nBy interpreting the NCE in terms of matrix factorization allows the\nauthors to better explain this learning criterion and more\nspecifically the self-normalizing mechanism.
216,1," \nFurthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets."
217,1,  it is hard to judge the significance of the technical contribution made by the paper.
218,1,\n \nHow should one choose tau_theta?
219,1,"  As a side note, I would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on \""modified Brownian motion\u201d."
220,1," In the first line, shouldn't the first term not contain \\Delta W ?"
221,1, Some experimental settings\nare well defined to shed light on few generalization aspects of the networks.
222,1, I assume this is an artifact of the way the goal recognizer is trained.
223,1," Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach."
224,1," They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features."
225,1,"  The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches."
226,1, although I doubt how much improvement one can get from SQDML.
227,1, there are two major issues of this work: (i) limited  novelty - the idea of unsupervised manifold projection method has been proposed in the previous work;
228,1," The main difference is that, instead of using an auto-encoder, they learn features phi(s) such that the next state s' can be recovered from it (they argue that this way psi(s) will retain information about aspects of the environment the agent has control over)."
229,1,"""This paper introduces a skip-connection based design of fully connected networks, which is loosely based on learning latent variable tree structure learning via mutual information criteria. "
230,1," However, an ablation study of the latent defender is missing: Specifcially, it is not clear a) how much does stage 2 (model refinement with clusters)  contribute to the accuracy (how does the model do without it? "
231,1, Physical meanings of Theorem 1 are not well represented.
232,1, This also explains why the authors have just tested the performance on extremely small sized data. It will not scale.
233,1, they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders.
234,1,"\nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n"""
235,1," \nSpecifically, for two folds: \n(1) in general, some more work in investigating the task space would be nice."
236,1,"\n\nRemarks\n------------\n\nThe main claim of the paper is that RNN are over-parametrized and take a long time to train (which I both agree with), but you didn't convinced me that your parametrization solve any of those problems."
237,1,"  In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer."
238,1, My main concerns about this work are have two aspects: \n(a)\tNovelty\n1.\tThe idea is a good one and is great incremental research building on the top of previous ideas.
239,1,Comments:\nTensorflow citation is missing.\n
240,1,"\n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper.\"
241,1,"  Therefore, we address the following questions.[[CNT], [null], [DIS], [GEN]]  Will reinforcement learning work even if we consider each unit as an autonomous agent \u201d\nIs there any citation for the claim \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system\u201d ? "
242,1,  In International Conference on Machine Learning (pp. 2171-2180).\n\nwhich the authors fail to cite.
243,1,       (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset\n 
244,1,\n\nHow much can change between the goal images and the environment before the system fails?
245,1," Thus I think this paper is below the acceptance threshold."""
246,1," The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making."
247,1, This organization would make the writing much clearer.
248,1, Can you comment more on how to pick that value in real-world settings? Just saying sufficiently many (Section 4.2) is not sufficient.
249,1, What would happen if the learning rate is not reset at the beginning of each cycle?
250,1," In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms."
251,1,"""This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time."
252,1," While effective, such a classifier might not be as powerful as multi-layer architectures that are used as discriminators. "
253,1, \n\nIt is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text.
254,1," However, I am not sure if the paper is a good fit for ICLR since it is purely theoretical in nature and has no experiments."
255,1," \n\nThe paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD."
256,1, Maybe adding plots to the paper can help.
257,1,\n[3] Clustered multi-task learning: A convex formulation (NIPS)
258,1," Since FSGM is known to be robust to small random perturbations, I would be surprised that for a majority of random directions, the adversarial examples are brought back to the original class."
259,1, \n\npros:\n--I liked the posterior sharpening idea.
260,1," \n\nThis paper offers a promising direction for language modeling research,"
261,1,"""\n\nThis paper proposes to use deep reinforcement learning to solve a multiagent coordination task."
262,1, but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.
263,1, Isn't that a good way to help avoid local optima?
264,1, but not convincing enough.
265,1," It is a still kind of soft-attention, except that is performed for each word in a sentence."
266,1,"""This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU."
267,1,"  \nOverall, this paper presents an overly simplified game simulation with a weak experimental result.\n"""
268,1,"The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other \""governing\"" sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc."
269,1," As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image."
270,1," Indeed judging from the large variance in the accuracy of predicting S in Table 1a-c for single adversaries, I suspect one of the main advantage of the current MARS method comes from variance reduction."
271,1,  How would they respond to similar reductions?
272,1,"\n\nOverall the paper is a clearly written, well described report of several experiments."
273,1,"\nThe paper is well written (up to a few misprints), the introduction and the biological background very accurate (although a bit technical for the broader audience) and the bibliography reasonably complete."
274,1,"""This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences."
275,1," In other words, how structured is this prior and does it really give us some of the interesting properties of deep neural networks, or is it mostly a cute mathematical trick?"
276,1,"  The rotation of the filters (as 2D images or images with depth) seem to be quite different from \""rotating\"" a general N-dim vectors in an abstract Euclidean space."
277,1,"\n* In Eqs. 4 & 5, the momentum parameter $m$ is not yet defined (it will be defined in Eqs. 6 & 7 below)."
278,1," For example, the update equations in Adam were specifically designed to correct for this effect."
279,1," The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description."
280,1," To be accepted to ICLR, either outstanding performance or truly novel design principles are required."""
281,1," Compared with the n-gram replacement used in the paper, which one is better?"
282,1," \n\nTheir method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model."
283,1,"\"" In an ideal world I would like the assumptions required for this to hold true to be a fleshed out a little here."
284,1,"  While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy."
285,1,"n\n5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive. "
286,1," \n2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets,"
287,1,\n\nThis paper leans heavily on Hazan 2017 paper (https://arxiv.org/pdf/1711.00946.pdf). 
288,1," As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc)."
289,1," What is missing is a discussion of the differences regarding the later numerical experiments, and a clear delimitation to Hartono et al., 2015, when Eqn. 15 is discussed."
290,1, (or perhaps does it not matter due to the convolutional structure?)
291,1,"""This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model)."
292,1,"\n\nFinally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning."
293,1,\n+ Simple/directly applicable approach that seems to work experimentally;
294,1,  This should be explained.
295,1,"\n\nCurrent approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval."
296,1," This is the main contribution of the paper, since swan already exists."
297,1, While the authors use a set of very novel generative models
298,1, The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017).
299,1, I would like to see results on more datasets and more discussion on the data augmentation technique.
300,1,  the empirical evaluation is on the weak side and the rich properties of the model are not really shown off.
301,1," (It would seem that *more* robustness is better than less, but the text says that lower values are \nchosen.)"
302,1," In Figure 5, k seems to be larger than the number of entities."
303,1," How was the train/val/test split done, etc."
304,1," The authors formulate the learning problem as a minimax problem which tries to choose diverse example and \""hard\"" examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function."
305,1," In principle, the idea seems to be clear,"
306,1,The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before.
307,1," While not disallowed by theory, doing both is redundant because they enforce the same mechanism."
308,1, the pain point is knowing when tasks are begin and end.
309,1,"""Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples)."
310,1," For instance, it is also not clear if the paper is modeling variable length sequences in a similar manner to Eslami, 2016 or not, and if this work also has a latent variable [z, z_pres] at every timestep which is used in a similar manner to Eqn. 2 in Eslami, 2016."
311,1,\n\n(4) Performance is worse than the best hand-designed baselines.
312,1,"\n\nAlso, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning."
313,1, The paper proposes a Bayesian framework to address it with a Gaussian mixture model.
314,1," I However, the proposed method departs from the theory that justifies the variational autoencoder."
315,1,  These CARs are used in the A3C algorithm.
316,1, The value for a bin is the (normalized) number of nodes falling into the corresponding region.
317,1," Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems."
318,1," Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected."
319,1," Is this the number of elements, or the number of rows/columns?[[CNT], [null], [QSN], [MIN]]\n\nLemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)?"
320,1,"\n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014."
321,1,"""`The papers aims to provide a quality measure/test for GANs. "
322,1," A kind of matching between the context and the target is already considered in the definition of alpha -with metric learning in M- : why not using those terms instead of ai <di,dt>?"
323,1," but there is a real lack of theoretical guarantees motivating the penalty form chosen, such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular."""
324,1,"\n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss."
325,1, \n\nQuestions with respect to dynamical systems point of view: Eq. 4 assumes small value of h.
326,1,"\n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments."
327,1, The authors suggest that such a model could help with disentangling factors of variation in data.
328,1, The experiments are also very weak.
329,1," Inference networks, in general, have two sources of approximation errors."
330,1, The work would be stronger if the authors can extend this to higher dimensional time series.
331,1,"\n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis."
332,1," Although \\tau goes to 0, there is still a gap between Q and Q'."
333,1,"\n\nIt would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.   "
334,1," As noted in the paper, the generation of syntactically and semantically valid data is still an open problem."
335,1," Moreover, the whole model architecture is only evaluated on the SQuAD dataset."
336,1,"""This paper proposes a simple problem to demonstrate the short-horizon bias of the learning rate meta-optimization."
337,1,"  In fact, such works even not that lambda should be related to confidence."
338,1," The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2])."
339,1," What is the take-home message of the paper?"""
340,1,It would be better to present accuracy as for the other tables/experiments
341,1," However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm."
342,1, Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic).
343,1," However, the experimental results are not clearly presented. "
344,1,"  The parameters of the FFNN are updated via a genetic algorithm with a fitness function defined as the error on the downstream classification, on a held-out set."
345,1,\n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them.
346,1," the paper never says exactly how, not even if you read the supplementary material)."
347,1," \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. """
348,1," Moreover, the motivations behind the proposed methods for different components are missing."
349,1, Any sort of remotely convincing experiment is left to 'future work'.
350,1,  I suggest that all these are fully clarified as parts of Algorithm 1 itself.
351,1," Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition."
352,1,"Specifically, compare the accuracy of a classifier trained on a noise-perturbed version of the real dataset to that of a classifier trained on a mix of real data and synthetic data generated by the model being evaluated."
353,1,"""\nIn this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together. "
354,1,"""This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling."
355,1, \n\n1) It's not clear how frequently RL agents will encounter time-limited domains of interest.
356,1," I generally liked the paper and the approach, here are some more detailed comments."
357,1," On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original."
358,1," I hope it is indeed the former, else I have a major concern with the efficacy of the algorithm as ultimately, we care about the test performance of the compressed models in comparison to uncompressed model."
359,1, It would be interesting to see if the additional auto-encoder part help address the issue.
360,1, The paper also raises good questions for future works.
361,1,"\""Bayesian recurrent neural networks\"" already exist and is rather vague for what is being described in this paper."
362,1, \nc) This plot is disconcerting.
363,1,  The paper is generally pretty well written.
364,1,  Or the actual optimization?
365,1," \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling."
366,1," Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?"
367,1, What specific optimization is being depicted?
368,1, They then go on to explore the sparsity of the latent space
369,1, The plot does not explicitly account for the distillation phase.
370,1,\n3. There is a lack comparison to other methods such as Shaham et al. (2017).
371,1," \"" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet."
372,1," But, the LTMN significantly outperforms the baseline solver even in the training set."
373,1, \n\nOriginality\n=========\nThe application seems original.
374,1,\nThe results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.
375,1," Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures."
376,1,  \n\nTrust region subproblem (TRS) has been analyzed and developed so much in the optimization literature.
377,1,\n(c) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples.
378,1,"\n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment."
379,1, The author is suggested to do a careful format check.
380,1, The network is basically learning to discard the sensor when the local standard deviation is high.
381,1,"\n\npage 8, line 7 in section 4.3: \""the the\"" (unintended repetition)[[CNT], [PNF-NEG], [SUG], [MIN]]\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?"
382,1," They won\u2019t look as flattering, but will properly report our progress on this journey."
383,1," however, this paper needs a revision in various aspects, which I simply list in the following:;"
384,1,"\nI suggest the authors make clear if they used the split training, validation, test."
385,1,"\n\nIf my understanding of the method is correct, the novelty is limited."
386,1,\n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks.
387,1,  but the method of analysing the failure modes is interesting.
388,1," Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others."
389,1,"\n\n- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model."
390,1,"\n\nIs the proposed contribution of this paper just to add the double ELBO or does it also include the AESMC (that is, should this paper subsume the anonymized pre-print mentioned in the intro)? "
391,1," Using features sur as time-series (TS), Architecture Parameters (AP) and Hyperparameters (HP) is appropriate, and the study of the effect of these features on the performance has some value."
392,1,\n\nPros:\n-Theoretical results on the convergence of OT/Monge maps\n-Regularized formulation compatible with SGD
393,1,"\n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper."
394,1," \n\nUtilizing a trained GAN, the authors propose the following defense at inference time."
395,1," But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted."
396,1,  The right figure would be nice to see with time on the x-axis as well.
397,1," In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation."
398,1," In addition, more analysis may better support existing claims."
399,1," \nOverall, this is a good paper,"
400,1, The authors compared Dauto with several baseline methods on several datasets and showed improvement.
401,1," This must be mentioned in section 4.2 \""does parameter space noise explore efficiently\"" because the answer you seem to imply is \""yes\"" ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D"
402,1,"However, this is where I have issues with the work:\n\n1) In all quantitative results, Chekhov GAN do not significantly beat unrolled GAN."
403,1," Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related \""sentences\"" could be encoded, including document title, section title, footnotes, hyperlinked sentences.This is a valid good idea and indeed improves results."
404,1,"""The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space."
405,1,". Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution."""
406,1, \n\nMy primary concern about this paper is the lack of interpretation on permuting the layers.
407,1," Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly."
408,1, An adversary could choose any policy.
409,1," \n\nFor the computational learnability literature, complexity analysis for teaching has a subliterature devoted to it (analogous to the learning literature)."
410,1, This equation is then discussed and the SOM-like updates and the differences to previous pure SOMs are highlighted.
411,1," The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great."
412,1, The writing of the paper is excellent.
413,1,. Why does PSR truncate the indices in alpha?
414,1," \n\nIf we trust the authors, then the paper seems good because it is fairly unusual."
415,1," It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses."
416,1, The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.
417,1, The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off. 
418,1,"""The paper proposes to use 2-d image representation techniques as a means of learning representations of graphs via their adjacency matrices."
419,1, They perform various experiments to analyze how these quantities depend on modeling decisions and data sets.
420,1,"\n\nThe paper is well written,"
421,1,"\n- How large are the training/test sets used in these experiments (for test sets, number of words, for training sets, amount of data in hours (is this ~10,000hrs), whether any data augmentation such as multi-style training was done, etc.)"
422,1,"  One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible."
423,1," Otherwise, r_1(.) for each point is 0, leading to a somewhat \""under-estimate\"" of the true LID of the normal points in the training set."
424,1," \n\nA further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not."
425,1," A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works."
426,1," Here's the relevant phrase: \""the cost function is typically non-stationary due to the interactions between multiple devices\"""
427,1,"\n\nAssuming the experimental setting is correct, it is not clear to me the reason of having the representation of r (one-hot-vector of the relation) also in the decoding/generation part."
428,1, \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.
429,1, A random starting point is generated.
430,1,"\n\n- It seems that the model benefits from at least two separate effects - one is\n  the attention mechanism provided by the sigmoids, and the other is the\nstochasticity during training."
431,1,"\nMulti-Batch Experience Replay for Fast Convergence of Continuous Action Control (Han and Sung, 2017)"
432,1, \n\nThe improved lower bound given in Theorem 6 is very modest but neat.
433,1, \n\nHave you considered approximations to Wasserstein distance? E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs:
434,1, \n\nThe SOTA claim might not hold if baselines are given the same capacity.
435,1,"\n- Figure 4 shows \""most interpretable mixture components\""."
436,1,"\n\nIn Tamar's work, a mapping from observation to reward is learned."
437,1,"""1. Paper summary \n\nThis paper describes a technique using 3 neural networks to privatize data and make predictions: a feature extraction network, an image classification network, and an image reconstruction network."
438,1,  \n- What does the PIR with the model in Section 3 stand for?
439,1," Thus, I would say original."
440,1,\n[r2] LCNN: Lookup-based Convolutional Neural Network.
441,1,"\n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers."
442,1,"  Here, this is replaced by an implicit regularizer."
443,1, \n4. There is no report of results on huge datasets like big Imagenet which takes a lot of time for deep training and we need automatic advance stopping algorithms to tune the hyper parameters of our model on it.
444,1,\n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.
445,1,\n\nThe originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper.
446,1, The battery can be \u201crecharged\u201d by moving to a \u201ccharge\u201d tile.
447,1," yet lack exploration of the model, at least to draw conclusions like \""we address the challenge of understanding the internal visual cues of CNNs\""."
448,1," If it can be integrated into the training end-to-end, it might be better."
449,1," These are the crucial questions related with fair comparisons, which I would like to know specific answers to make my final decisions."
450,1, But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages).
451,1,"\n\nIn order to do so, an extensive framework is proposed, consisting of 3 ConvNet architectures, followed by a hierarchical clustering approach."
452,1, Is it not sufficient to have and discuss Fig 23?
453,1,"  Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952)."""
454,1,"\"" Not only is it impossible to reproduce a paper without any architectural details, but the\nresult is then that Fig 3 essentially says inputs -> \""magic\"" -> outputs."
455,1, Morever this fine-tuning experiment needs more details: is it based only on parameter initialization\nor there are some fully frozen network layers?
456,1,"""The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable."
457,1," The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems."
458,1,. The author should also compare this extension of VAECCA.
459,1, This is an improvement on earlier proposals (e.g. Revisiting Synchronous SGD) that allow for slow workers.
460,1,"\n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques."
461,1, This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns.
462,1,I think that would be a more useful measure for the learned eigenoptions.
463,1," Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored."
464,1, Did you do a grid search to find the optimal level of sparsity at each level?
465,1," \n\n2. As well know, VGG16 with well training strategy (learning rate decay) could achieve at least 92 percent accuracy."
466,1, What is the actual contribution of the paper w.r.t. to this body of work?
467,1, However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method.
468,1," With this setup, and assuming the Jacobian decomposes, they prove that the GSC increases exponentially (Proposition 5)."
469,1,...\n\n- The mathematical writing should be more rigorous;
470,1," This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word\u2019s soft head (called Shared Attention in the paper); and (3) they gate when attention is used."
471,1, Are these patchs a dense coverage of the image? Sparsely sampled? Densely sampled with overlaps?
472,1, please can you describe the distributions in a more precise way?
473,1," The depth of the network is only 5, even with many layers listed in table 5."
474,1,"\n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples."
475,1,Some notations in the LSTM section could be better explained for readers who are unfamiliar with LSTMs.
476,1,\n\nThe definition of F_\\epsilon is made unnecessarily confusing by the omission of x and y as arguments.
477,1, Are the differences actually significant?
478,1," With the increasing focus on applying RL methods to continuous control problems and RTS type games, this is an important problem and this technique seems like an important addition to the RL toolbox."
479,1," Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets."
480,1,  Further I found it not surprising that the PIR changes when a highly parameterised model is trained for this task.
481,1,\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks.
482,1,".\n\nConclusion:\n- Based on the comments above, I recommend weak reject."
483,1,":\n\n1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit)"
484,1,"\n\nOverall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking."
485,1, and the experiments also need to be improved
486,1," I did not see support in this paper for the claim in the abstract that special architectures make complex networks work better, or that they are well suited to particular data sets"
487,1," In particular, the paper would benefit greatly from being made much shorter, and having more of the important details or proof outlines for the various propositions in the main text."
488,1, The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture.
489,1," \n- In the case of images, what is a dominant vs recessive pattern?"
490,1," \n\nSome visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.\n"""
491,1," (That is, are there counter examples for relaxations of this assumption?)"
492,1,  It'd\nbe stronger if the approach could also be demonstrated in another domain.
493,1, Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs.
494,1, Also the well-known original DBN paper has MNIST as main example (and main selling point) with close to 1% error.
495,1, I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation
496,1,\n\nPros:\n - clear definitions of terms\n - overall outline of paper is good\n - novel metric.
497,1," \n\n- All the comparisons are based on \""epochs\"", but the competing algorithms are quite different and can have very different running time for each epoch."
498,1,"\n-\tFigure 4, b and c: it is not clear what the (x,y,z) measurements plotted in these 3D drawings are (what are the axis)."
499,1," Only reason 1 mentions the gradient vanishing problem, even though the title of this section is \""Relation to Vanishing Gradient Problem\"". \n"""
500,1," However, I have several concerns about the algorithms proposed in this paper:"
501,1,\n- a final classifier performing the desired classification task.
502,1,"""This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data."
503,1,"\n\nIn section 5.2 the reference to Table 5.2 should be Table 1.\n"""
504,1,\n\n(2) The experiment is minimal and even the given experiment is not described well.
505,1," The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task."
506,1,"""The paper compares some recently proposed method for validation of properties\nof piece-wise linear neural networks and claims to propose a novel method for\nthe same."
507,1," The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline."
508,1,"""This paper presents a reading comprehension model using convolutions and attention."
509,1,"  That\u2019s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation. In the discussion, it says. "
510,1,"\n\nIt would be interesting to see the relation between the \""unified\"" gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework."
511,1, The main modeling challenge here to to define a  good directional measure that can be suitable for lexical entailment.
512,1, The first one is a style discrepancy loss to enforce that the discrepancy between the learnt style representation for a source sentence and the target style representation is consistent with a pre-trained discriminator.
513,1, It would be intersting to present some analysis regarding the gradient w.r.t. W.
514,1,\n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.
515,1, Are the KL divergence small enough to guarantee similar predictive rewards?
516,1,  The domain is video games.
517,1, In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model.
518,1,".\n- As the evaluation data is not available, it is really hard to assess the quality of the models."
519,1," This seems interesting[[CNT], [CNT], [APC], [MIN]] but I didn't find any result on that in the paper."
520,1, There is no quantitative evaluation and comparisons.
521,1," If so, how?"
522,1,"Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art."
523,1, The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product.
524,1," \n- Why are Joulin 2016, Krause 2016 not relevant?"
525,1, and indeed the results here are interesting as they favour relatively simpler structures.
526,1,\n\nIt is admirable that the authors use an interesting (and to my knowledge novel) data set.
527,1," Therefore, the uncertainties produced by the model appear to be a black-box."
528,1,"\n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000."
529,1," E.g., what is the purpose of the fully connected layers, why do the CNNs split and what is the difference in the two blocks (i.e. what is the reason for the addition small CNN block in one of the two)"
530,1,"\"" \n\nIt is clear that regularization should play a significant role in shaping the decision boundaries."
531,1, \n\nPros: the intuition and geometry is rather clearly presented.
532,1,"\n\nDetailed comments:\n* Authors should at very least cite (Vinyals et al, 2017) and explain why the environment and the dataset released for Starcraft 2 is less suited than the one provided by Lin et al."
533,1," Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first."
534,1," Spatio-temporal video autoencoder with differentiable memory, arxiv 2017\n\nSince this is prior state-of-the-art and directly applicable to the problem, a comparison is a must. "
535,1, How many times is this procedure repeated?
536,1," My best suggestions are to drop the \\Phis altogether and consider\nusing text subscripts rather than coming up with a new name for every variable,\nbut there are probably other things that will also help."
537,1," I understand this design choice was made to focus more on the image side, rather than the decoding (language) side, but I find it to be very limiting."
538,1,\n\nThe authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.
539,1,  The paper also proposes a removal strategy that bounds the number of support vectors in the kernel machine.
540,1," Interestingly, the\nfilters defined in the Hilbert space  have parameters that are learnable."
541,1, 2014 (they used this method to perform data imputation).
542,1," Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is \u201cquite a reasonable baseline\u201d."
543,1," Here's one reference (others should be easy to find): \""SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs\"" by Boehm et al, IEEE Data Engineering Bulletin, 2014."
544,1, Why not\nexamine a more thorough range of values?
545,1,"\n\n2. The authors should try more complicated datasets, like CIFAR-10."
546,1, \n* Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs.
547,1, The use of genetic algorithms has also been considered.
548,1, In this sense the results cannot by any means by reproduced.
549,1,"\n\nFor MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized."
550,1,"""The work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding."
551,1," In particular, the principal contribution of the sequence tagging experiments seems top be different than what is advertised earlier on in the paper. "
552,1, This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches.
553,1,"\n\n+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain."
554,1," Related work is clearly cited, and the novelty of the paper well explained."
555,1,"  However, the results in Tables 3 and 4 show this not to be the case."
556,1," Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order."
557,1, And the comparisons to other papers seems not fair since they all operate on different search space.
558,1," A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs."
559,1," This point is murkier, because the paper doesn't discuss possible increases in *training time* (due to increased number of iterations) in much detail."
560,1,"\n\nThere are also some typos: For example, the dimension of a is inconsistent."
561,1," If so, how is this method specific to batch normalization?"
562,1," Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more."
563,1,\n* The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach).
564,1," I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful."
565,1,.\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.
566,1," Even though no conclusive section is provided,"
567,1,  The authors propose a first modeling called ASC.
568,1," No Bayesian elements, like prior or likelihood appears here."
569,1," Since the paper does not provide significant theoretical or algorithmic contribution, at least more realistic and diverse experiments should be performed. """
570,1," In particular, they use a CNN encoder-decoder to learn a motion field, and a warping function from the last component to provide forecasting."
571,1," \n- The difference between Figures 1, 4, and 6 could be clarified. "
572,1,\n\n+ effort to compare to previous experimental setups
573,1,"\"" Again, this is ambiguous.[[CNT], [null], [DIS], [MIN]] To me, this sentence implies that you extend the benefit of avoiding vanishing and exploding gradients to fully-connected networks without skip connections.[[CNT], [null], [DIS], [MIN]] However, nowhere in your paper do you show that trained VANs have less exploding / vanishing gradients than fully-connected networks trained the old-fashioned way. Again, please reword or include evidence.\n- \""where the proposed method is shown to outperform many architectures without skip-connections\"" Again, this sentence makes no sense to me."
574,1,"This provides an interesting and, as far as I can tell, novel view on MAML."
575,1, Taking away a large part of the Gaussian distribution.
576,1, although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).
577,1,"\n\nPros:\n\nThe intuition is that the ReLU network output is locally linear for each input, and one can use the conjugate mapping (which is also linear) for reconstructing the inputs, as in PCA."
578,1,. The experiments show improvement over baselines.
579,1," \n\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments."
580,1," What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. "
581,1," That is, the authors learn all the previous layers\nby finding point estimates."
582,1," While there have been numerous GAN-like approaches for language understanding, very few, if any, have shown worthy results."
583,1, \n\nThe robustness to navigation commands in Section 4.5 is very forced and incorrect -- randomly inserting unseen words at crucial points might lead to totally different original navigation commands right?
584,1,\n\nIn my opinion there are several weak points:\n\n1) The approach to obtain the image-like representation is not well motivated
585,1, \n\nThe reviewer would expect papers submitted for review to be of publishable quality.
586,1, Other do not add a significant novelty\nand their contribution is not clear.
587,1,"\n\u2013\u00a0What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult?"
588,1,"\n\n----------------------\n\n\n\nIn this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch-size, activation function, no. layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully."
589,1," Moreover, some databases could be used for different tasks, such as WSJ or ImageNet."
590,1," Moreover, the context selection variable \""a\"" should be considered part of the dataset, and as such the paper should report how \""a\"" was selected."
591,1," According to the visualization, the model could attend the right region of the image for finishing a navigation and QA task."
592,1, It is not clear how this issue is addressed in this paper.
593,1,\n\nUnfortunately there is minimal quantitative evaluation (visualizing 264 MNIST samples is not enough).
594,1," Nature neuroscience, 18(7), 1025-1033."
595,1,\n- An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards.
596,1," Finally, the introduction paragraph of Section 5 is rather bold, \""resembles the learning process of human beings\""?[[CNT], [EMP-NEU], [QSN], [MIN]] Not so sure that is true, and it is not supported by a reference (or an experiment)."
597,1,"""\nThis work proposes to study the generalization of learning neural networks via the Fourier-based method."
598,1," Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people."""
599,1,\n\nThe authors call their findings theory.
600,1," However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them."
601,1," This would demonstrate the real tradeoffs between bias, variance, and computation."
602,1,"\n\nIn brief: quality is high, clarity is high, originality is high, and significance is medium."
603,1," The choice of using noise-free data only is a limiting constraint (in [Chen et al. 2016], Info-GAN is applied to real-world data)."
604,1,"  In that work a \""distractor sentence\"" is manually added to a passage to superficially, but not logically, support an incorrect answer."
605,1,  To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently.
606,1,\n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper 
607,1,\n\nWhat I like about this paper is that:\n\n1) The experiments are very carefully designed and thorough.
608,1," Intuitively, one can see why this may be advantageous as one gets some information from the past."
609,1, The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps)
610,1," But section 4 and 6 need a lot of details. [[CNT], [CNT], [DFT], [GEN]]\n\n2) My comments are as follows:\ni) this paper claims that this is a general sentence embedding method, however, from what has been described in section 3, I think this dependency is only defined in HTML format document."
611,1,"For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE."
612,1,\n- The experimental results seem to be complete for the most part.
613,1, It would be better if the authors could explain this more?
614,1,"\n- In Figure 3b, it is not clear to me what the difference between the red and blue curves is."
615,1,"\n\nFor the presentation, the motivation in introduction is fine,"
616,1," They therefore propose using a more-biased but lower-variance bound to train the inference parameters, and the more-accurate bound to train the generative model."
617,1, Perhaps not that much?
618,1,"""The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version."
619,1,"""This paper considers a dichitomy between ML and RL based methods for sequence generation."
620,1,The authors should improve\nthis paragraph.
621,1,"  Wether balls are in L2 or L_\\infty, or another norm makes a big difference in defense and attacks, given that they are only equivalent to a multiplicative factor of sqrt(d) where d is the dimension of the space, and we are dealing with very high dimensional problems."
622,1,"\n\nIn this formulation:\n\nThen R_ME has a high response if the node has saturated responses -1\u2019s or 1``s, as one desire such saturated responses, a should be negative."
623,1,The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks.
624,1,"  I think the idea here is that the expression \nTrans\u2019(  (s,s\u2019) , a ) represents the n-step transition function and \u2018a' represents the first action?"
625,1,"  One example work is \""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\"". The authors didn't compare their method with this one."""
626,1,"  \n- There are different references to the \""Appendix\"", \""Suppl. Material\"", or \""Sec. 8\"" -- please be consistent (and try to avoid ambiguity by being more specific -- the appendix contains ~20 pages)."
627,1,\n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals.
628,1," They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.)"
629,1," Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case"
630,1, \n\n2  The contribution of this paper is not clear.
631,1,"""There is a vast literature on structure learning for constructing neural networks (topologies, layers, learning rates, etc.) in an automatic fashion."
632,1," For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work"
633,1," I was also wondering about when 2 or more layers are block sparse, do these blocks overlap?"
634,1," For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph."
635,1,"""The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise."
636,1," \n\nTwo ways I could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build \""defoggers\"" for other domains (and spelling out more explicitly what domains the authors expect their insights to generalize to), or doubling down on the StarCraft application specifically and showing that the defogger helps to win games."
637,1,"\n\nOverall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response."
638,1," While reducing the number of updates allows for faster training if a large number of parallel hardware instances are available, the total cost of training is still governed by the number of training epochs."
639,1,"  The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons."
640,1," Based on this, outputs of Gc should not change much when flipping the input labels."
641,1,\n\n2. One intuitive approach to task balancing would be to weight each task objective based on the variance of each task. 
642,1,  Or at least have a good argument of why this is suboptimal compared to PLAID.
643,1," The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \""crossover\"" network is presumably time-consuming."
644,1," Or where it attends varies across examples?\n\n10. Are you willing to release the code for reproducing the results?\n\nMinor comments:\n\nPage 1, \u201cexploit his own decision\u201d should be \u201cexploit its own decision"
645,1," A strict editor would be helpful, because the underlying content is good\n - odd that your definition of generalization in GANs appears immediately preceding the section titled \""Generalisation in GANs\""\n - the paragraph at the end of the \""Generalisation in GANs\"" section is confusing."
646,1,"\n(3). How do you choose the parameter \\lambda in Equation (2)?\n"""
647,1," Thus, they are essentially learning the skip connections while using a human-selected model."
648,1,""")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \""center\"" ?"
649,1,\n\nIt is shown that intra-decoder attention decoder improves performance on longer sentences.
650,1, In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set. 
651,1, The observation is explained well and substantiated by clear experimental evidence.
652,1,"The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance."
653,1, The method is based on balance the class-priors to generalize well for rare classes.
654,1," During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation."
655,1,\nThis result is related to the third conjecture of the paper that is :\n3. the number of the number of mappings which preserve a degree of discrepancy  is small.
656,1,"""The paper discusses the problems of meta optimization with small look-ahead: do small runs bias the results of tuning?"
657,1," The idea is interesting to me, but I think this paper still needs some improvements."
658,1, I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA.
659,1," The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables."
660,1, The main technique is to use UCB (upper confidence bound) to speedup exploration.
661,1," The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis."
662,1,"This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5."
663,1,"\n* Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20."
664,1, It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. 
665,1," In the same vein, it would be silly to dismiss the present work simply because it lacks spikes."
666,1,The mechanism of partial pulling is very simple (just let SGD proceed after pulling a partial parameter block instead of the whole block).
667,1, More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion.
668,1, \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.
669,1,"""This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances."
670,1,  The best human designs outperform the evolved networks.
671,1," However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place,"
672,1,"""This paper satisfies the following necessary conditions for\nacceptance."
673,1,"\n- The reason that Deep Speech 2 is better on noisy test sets is not only the fact they trained on more data, but they also trained on \""noisy\"" (matched) data"
674,1," \n\nIt is interesting that although L2 regularization does not lead to low \\nu parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization."
675,1," Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of \""embedding\"" an \""imaginary trajectory\"" remains completely opaque to me."
676,1,"\n\nRegarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns."
677,1, It might be helpful to discuss more about the over-fitting of different graph properties.
678,1,"  Right now, lots of this information is just\nprovided in text, so it's not easy to make head-to-head comparisons."
679,1, Does it indicate the number of  iterations over the same dataset?
680,1,"""The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position."
681,1," As shown in\nFigure 7 (a)(b), by only allowed an independently thinking master agent and communication among\nagents, our model already outperforms the plain CommNet model which only supports broadcast-\ning communication of the sum of the signals."
682,1," \n\nStill, the authors clearly utilise basic concepts (c.f. \""utilize eigenvector \nbasis of the graph Laplacian to do filtering in the Fourier domain\"") in ways\nthat do not seem to have any sensible interpretation whatsoever, even allowing\nfor the mis-understanding due to grammar."
683,1,"\n\n- Section 5.1, 2nd paragraph, did you swap tau_theta and tau, or am I just confused?"
684,1," Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks."
685,1,\n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates?
686,1,\n- The main idea of the proposed method is clear.
687,1,"\n\nOriginality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real-world problems."
688,1," The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work."
689,1,\n\nSignificance:\n\nThe paper's contributions are significant.
690,1,  Results show\nthat the approach is a bit faster than CEGIS in a synthetic drawing domain.
691,1," For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier"
692,1," In particular, the LSTM baseline seems to weak compared to other works."""
693,1,"""The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state."
694,1,"--but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, \""The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule\"")."
695,1,"\n\n\nComments:\n\n-The work is interesting, for an important problemin representation learning, while in machine learning in general with the unsupervised aspect"
696,1,\n\nWhy multitask learning help the model perform better is still unclear.
697,1," Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper. "
698,1,"""\nThe paper was fairly easy to follow,"
699,1,"""The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones)."
700,1," Also, the authors do not give a conclusive analysis under what condition it may happen."
701,1,"\n\nAbstract:\n\n- \""Part of the success of ResNets has been attributed to improvements in the conditioning of the optimization problem (e.g., avoiding vanishing and shattered gradients)."
702,1,"\n\nThe submission's (counter-)claims are served by example (cf. Figure 2, or Figure\n3 description, last sentence), and mostly relate to statements made in the WGAN\npaper (Arjovsky et al., 2017)."
703,1,\n\nOverall the paper is well written and polished.
704,1,"\n* In Eq. 3, it is also not clear to me why the third term has a different constant weight than the first two."
705,1,  All of their evaluation is based on the accuracy of defogging.
706,1,"""The proposed approach, GTI, has many free parameters: number of layers L, number of communities in each layer, number of non-overlapping subgraphs M, number of nodes in each subgraph k, etc."
707,1,\n5. The comparison against previous work is missing some assurances I'd like to see.
708,1," The experiments show that the generation component is quite effective,"
709,1,"\n3.) Generalization to composite commands, where  a part of the command is never observed in sequence in the training set."
710,1, Seems thin for a ICLR paper.
711,1,. They did not provide the comparison of running time which I believe is important here as the efficiency is emphasized a lot throughout the paper
712,1,"\n\nPaper is mostly very clearly written,"
713,1," Indeed, a CP decomposition of a tensor can not be reconstructed from CP decompositions of its subtensors."
714,1,"\n4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively. "
715,1," That only happens if you perfectly sample each particle from the true posterior, conditioned on all future information."
716,1," Therefore, the following sentence seems sensational without substance: \""Therefore, on a full meta-modeling experiment involving thousands of neural network configurations, our method could be faster by several orders of magnitude as compared to LCE based on current implementations."
717,1," Last but not least, it would be more convincing to show the convergence speed of the proposed method."
718,1, \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak.
719,1,"\n\n12 pages is very long, 8 pages was the suggested page limit. It\u2019s understandable if the page limit is extend by one page, but 4 pages is over stretching."
720,1,"Indeed, of the examples in Table 2, the first 3 look bad, the fourth isn't generally  true but valid in certain contexts, the 5th is again wrong "
721,1," (That is, how many experimental runs were averaged or was the experiment run once?)."
722,1," If this is correct, how many Progs are consistent with a typical Y?"
723,1, The main issue I have is with the part about momentum.
724,1,"""The paper deals with \u201cfixing GANs at the computational level\u201d, in a similar sprit to f-GANs and WGANs. "
725,1,\n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution.
726,1," \n\nMy primary suggestion is that I would like to know a lot more (even qualitatively, does not need to be extensively documented runs) about how sensitive the results were--- and in what ways were they sensitive--- to various hyperparameters."
727,1," However, they only explore this relationship under identical objects."
728,1,\n* There are quite a lot of typos.
729,1,\n\n3. The paper could benefit greatly from better integration with the existing literature.
730,1, The authors might want to comment on the relative merits between GP-SSMs and DE-RNNs.
731,1,"\n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets."
732,1, Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM)
733,1, One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.
734,1," I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2)."
735,1,"        (c) from sort-of-CLEVR \""objects\"" to PSVRT bit patterns"
736,1, The 2D example seem to work very well and the convergence curves are far better with the proposed regularization.
737,1,\n\n- In the last section authors mention the intent to do future work on atari and other env.
738,1, This one is a more recent development.
739,1,"\"" experiment which trains with knowledge and then tests without knowledge."
740,1," On the other hand, in this semantic space, by construction, we are guaranteed that similar concepts lie near by each other."
741,1," I have the impression that we use a jackhammer to break a small brick here (no offence). But maybe that I\u2019m missing something here.\n- Several works consider the size and the depth of the network as hyper-parameters to optimize, and this is not new."
742,1,"""The paper evaluates one proposed Deep RL-based model (Mirowski et al. 2016) on its ability to generally navigate."
743,1," This is a very nice contribution.\n"""
744,1, Is it because it does not achieve SOTA?
745,1," The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold."
746,1,"\n3. Fix notation on page 3.[[CNT], [PNF-NEG], [SUG], [MIN]] Dot is used on the right hand side to indicate an argument but not left hand side for equation after \""with respect to \\lambda\""."
747,1,"\n\n- For measuring performance, authors employ keystroke saving rate."
748,1,\n\nThe experimental results are also not explained thoroughly enough.
749,1,  \n\n1. The addition of auxiliary layers improves Sequence Tagging results marginally.
750,1,"\nOne suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case."
751,1," The paper is well explained, and it\u2019s also nice that the runtime is shown for each of the algorithm blocks."
752,1, It is not clear from the paper why the authors do not test their systems resiliency to this form of attack.
753,1," There are plenty of works on this topic [R1, R2, R3]. "
754,1, All we have a besides text is a small figure (figure 1).
755,1, I enjoyed the investigation of the effect of L_2 regularization on qualitative optimization behavior.
756,1,"\n  - lack of details on how experimental results were obtained, in particular, lack of the details on the values of the free parameters in the proposed algorithm;"
757,1, At the very least a mention of how the convergence proof would follow other common proofs in RL.
758,1," it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times"
759,1," But we can\u2019t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect?"
760,1, Why is the mutual information in Figure 3 so low?
761,1," Specifically, I would quantify the cells' direction tuning using the circular variance methods that people use for studying retinal direction selective neurons."
762,1,"\nHyperparameter tuning is another aspect that is not sufficiently precise in the experimental setup: it seems that the parameters are tuned on test (for all methods), which is not fair since target label information will not be available from a practical standpoint."
763,1,"  It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques."
764,1,"  \""separated\"" would be much closer. "
765,1,"""\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count."
766,1,\n\nThe legend in the middle of Fig 4 obscures the plot (admittedly not substantially).
767,1," However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,\u201chorse\u201d) may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse)."
768,1,\n\nThe descriptions of the different architectures compared are overly verbose -- they are all simple standard convnet / RNN architectures.
769,1,"\n\nWonja et al, The Devil is in the Decoder, In BMVC, 2017\n"""
770,1, \n\nComments:\n- It is weird to use both a discount factor \\gamma *and* a per-step penalty.
771,1," This is a big advantage when training is performed on hardware with computational limitations, in comparison to \""post-hoc\"" sparsification methods, that compress the network after training."
772,1,"  Instead of real interactions, the submission proposes to maximise the activations of hidden units in a separate neural network."
773,1,"In summary, while the paper contains some good ideas, I certainly think it needs more work to make a clear case for this method. \n"""
774,1," Unfortunately, they are not able to show any types of synthetic noise helping address natural noise."
775,1, AdvGAN is able to generate adversarial samples by running a forward pass on generator.
776,1,"\n\n- The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection)."
777,1,\n\nThe paper is well written. 
778,1, \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.
779,1,\n\nI find the basic idea quite compelling.
780,1, What is the total computation time of the different variants and baselines
781,1," Despite of its simplicity, there are no discussions/empirical comparisons with other approaches, and simple ablative analysis such as performance of detector with and without saliency maps.   \n"""
782,1," And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference."
783,1,"\n\nAnother issue is the fact that, on my humble opinion, the main text looks like a long proof."
784,1,  This is secondary to the proposed framework.
785,1, Will the method discover more classes when 100 unknown classes are used?
786,1, Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm!
787,1,"\n\n(clarity) While Section 3.3 clearly defines the explicit form of the algorithm (where data and labels are essentially pre-processed via a copula transform), details regarding the \u201cimplicit form\u201d are very scarce."
788,1,"\n\nOverall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental."
789,1,\n\n6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph
790,1, My main concerns are on the usage of the given observations.
791,1,"\n\nCons:\n\n- Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing.[[CNT], [CNT], [CRT], [MAJ]]\n\n- Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent."
792,1, The results obtained on the considered problem
793,1, This is crucial to understand the advantages of the BC technique.
794,1, At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped.
795,1, They extend the scheme to allow the use of different scaling parameters and to m-bit quantization.
796,1," This article focuses on the calculation of gradient for write network, and provides some mathematical clues for that."
797,1,"\""(P6)\n\nSome minor issues:\n\""Zhu et al.(2011) discuss heterogeneous transfer learning where in they use...\""(P3)\n\""Each label vector (a tuple of label, label-probability pairs).\"" (incomplete sentence?P5)"""
798,1, \n\n- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V.
799,1," I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough."
800,1,\n\nThe paper does not write mathematically rigorous.
801,1,\n\nDetails:\nhave been successfully in anomaly detection --> have been successfully used in anomaly detectionP
802,1,"\n - a new dataset \""crashed cars"
803,1,"\n\nIn Fig. 1, I didn't find the meaning of the acronym NN with no specified width."
804,1," For example, the gating mechanism for producing the action logits is rather complex and seems to only help in a subset of settings (if at all)."
805,1,\n\n* The clarity of this paper is not high as the proposed method is not well explained.
806,1, The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).
807,1,  I would strongly suggest avoiding your notation a(x|\\Theta) and using \\pi(x) (subscripting theta or making conditional is somewhat less important). 
808,1, The authors should report statistics over\nmultiple runs.
809,1," This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5."
810,1,"However, it could benefit from additional details and a deeper analysis of the results."
811,1," To learn the mapping from image to high-level representations, an auxiliary encoder was introduced."
812,1," This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory."
813,1," \n\nFurther, the analysis of this paper is not convincing."
814,1,"  Lines should be thicker.[[CNT], [PNF-NEU], [SUG], [MIN]]  Even when zooming, distinguishing between colors is not easy."
815,1," In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better."
816,1," In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions."
817,1," \n- Also: the name P_g does not appear anywhere else in the theorem, I think."
818,1," In Section 4.1, the target x and y have time steps from t1 to t2."
819,1,"\n\nSuggestions:\n- It would be great if authors can add more details of the multi-layer perceptron, used for predicting weights, in the paper."
820,1,. The word pairs are further filtered and clustered to improve the representation
821,1, The only way to get an SMC estimator\u2019s variance to 0 is to drive the variance of the weights to 0.
822,1,For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials?
823,1," \n\nIn general, the proposed work is very interesting and the idea is neat."
824,1,". In the experiments section, the authors apply their pruning approach on a few representative problems and networks. "
825,1,\n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.
826,1, \nLearning a meaningful representation is needed in general.
827,1," I have only one question:\n\nin the simplified versions, content(x_t) = Wx_t , which works very well (outperforming full LSTM)."
828,1,"\n\nArticle is based on recent works such as Wasserstein GANs and AC-GANs\nby (Odena et al., 2016)."
829,1," I now understand more the claims of the paper, and their experiments towards them."
830,1, The paper claims superior results using the described method.
831,1,  than your approach.
832,1, Ideally further insights could be derived from these results.\
833,1," However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words."
834,1," \n\nOriginality - The paper heavily builds upon prior work on hierarchical latent tree analysis and adds 'skip path' formulation to the architecture, however the structure learning is not performed end-to-end and in conjunction with the parameters."
835,1,"\n\n2. The authors write on page 4, \""Moreover, zero training error can be enforced by converting average loss into maximal loss\"". It is not clear to me what the authors mean here."
836,1, The experiments are very clearly presented and solidly designed.
837,1,\n3) although the paper indicate that there are different other few-shot methods that could be applicable here
838,1," So, the general claim is supported."
839,1," There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? Why?"
840,1, I would be curious to know what the FID looks like as a 'gold standard'.
841,1," Unfortunately, although I am quite sympathetic to the argument that state of the art results or architectures aren't necessary for a paper of this kind, the results on MNIST and CIFAR10 are so poor that they give me some concern about how the training was performed and whether the results are meaningful."
842,1, \n\nThe idea is simple and easily explained in a few minutes
843,1, \n\nThe authors propose an algorithm which uses sequential Monte Carlo + autoencoders.
844,1,"In light of this, the statement in the introduction that \u201cencoder-decoder training objectives cannot avoid mode collapse\u201d might need to be qualified."
845,1," In particular, the theorem shows that the kernel approximation error is O(1/D), which is the same as the original RFF paper."
846,1,"\n\n(3) As a consequence of (1) and (2), the result is essentially rigged."
847,1,\n\nI found no typo or grammatical errors which is unusual - good careful\njob
848,1," The sentence \""this paper investigates only the one versus rest approach\"" is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class."
849,1, Why would we expect these quantities to be small or bounded?
850,1,\n\nExperimental Setup and Training Details\n- How was the model optimized?
851,1, The standard LQG setup does not have this restriction.
852,1," In Figure 3, it is mentioned that the network uses a U-Net with recurrent connections."
853,1,"""This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL."
854,1, The authors propose pixel deconvolutional layers for convolutional neural networks.
855,1,\n\nI am not convinced about the novelty and contribution of the work.
856,1," First of all, it is unclear if the method works much better for the tail than the standard softmax."
857,1,"  Also, the figure shows that the performance of Adam+BackProp is worst than Adam+ProxProp even though the training loss of Adam+BackProp is smaller that of Adam+ProxProp."
858,1," The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards? "
859,1, What is the relevance/applicability of the method given this context?
860,1,"\n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant."
861,1, \n- The testbed is nice; the tasks seem significantly different from each other.
862,1,\n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper.
863,1,"\nWe apply the CP decomposition to a pretrained network, then restore it back into the dense format, optimize it, and then apply the CP decomposition again"
864,1, though some of the connections between the different parts of the paper felt unclear to me:
865,1," Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking."
866,1,\n\n4 The experimental settings are unfair.
867,1, So I\u2019m not sure how to interpret them: should they have been run for longer?
868,1, I think it would benefit the readability to provide the reader with a bit more guidance.
869,1," It proposes a new dataset based on an artist's work, and compares existing methods in terms of the realism of the synthetic faces they can create."
870,1," It is a pity that not also more general image classification has been considered (CIFAR100, ImageNet, Places365, etc), that would provide insights to the more general behaviour of the proposed ideas."
871,1,What would the results be with 45k?
872,1,"  Given the literature in topics / sentiment modeling, it is a real weakness of this article."
873,1, \u2019 That happens to be (as you\u2019ve proven) a good measure by which to select examples for the synthesizer.
874,1," If it is the case, you should explain that the optimal policy of the adversary is approximated by \u201cfast gradient sign method\u201d. "
875,1,\n\n2. I think the discussions around Eq. (1) are not well grounded.
876,1," The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence."
877,1,"\n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,"
878,1,"""The paper proposes a new method for detecting out of distribution samples."
879,1,The goal is to learn sparse structures across layers of fully connected networks.
880,1,"\n- There is no exact (\""alternating optimization\"" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm."
881,1,\n\nPros\n\nRelevant attempt to develop new predictive coding architectures
882,1," Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary."
883,1,"\n\n6) I do not see any novel contribution in the analysis of the batch normalization (end of section 5): bn has been \npreviously used for  domain adaptation\nRevisiting Batch Normalization For Practical Domain Adaptation, arXiv:1603.04779\nAutoDIAL: Automatic DomaIn Alignment Layers, ICCV 2017"
884,1,\n\n\n- Incorrect references to the literature
885,1," For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do."
886,1," Although  I found the results useful and potentially promising,"
887,1,"\n\n4- As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR-10 benign samples."
888,1,"\n\nCons:\n1. The writing of the paper could be improved and more clear: the conclusions on inner product and F-norm can be integrated into \""Theorem 5\""."
889,1, n4. Experimental results are averaged over 5 repeated runs - a bit too small in my opinion.
890,1,"\n\nSince this is so important to the results, more analysis would be helpful."
891,1, The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.
892,1,\n\nThe Search algorithm only shows an accuracy of 0.6% with MAX_VISITED=100.
893,1, Is it clear that this step in needed? 
894,1," It would be interesting to see more analysis on this, especially analyzing what the mechanism is attending to, as it is less clear what its interpretation should be than for intra-temporal attention."
895,1,"  \nHowever, I have the following concerns on novelty."
896,1," At test time, the gates with the highest values are kept on, while the other ones are shut."
897,1," E.g. \""The top stochastic layer z_L in FAME is a fully-connected dense layer\""."
898,1,"   The Langevin dynamics algorithm used by the authors to find the peaks (where the gradient is available) gives only weak theoretical guarantees (as the authors actually admit) and this is a well known method, certainly not a novelty of that paper."
899,1,"\nFinally, the pivot \""loss\"" seems a bit hacky."
900,1,  They use a REINFORCE-like algorithm to differentiate through the Monte Carlo.
901,1," It seems some of the details are in Appendix-A.[[CNT], [PNF-NEU], [DIS], [MIN]] It would be better if authors move the important details of the technique and also some important experimental details to the main paper."
902,1,\nWhat does the l2 metric stands for ? 
903,1,\n\nGTI uses the Louvain hierarchical community detection method to identify the hierarchy in the graph and METIS to partition the communities.
904,1, I couldn\u2019t find it based on the results presented here
905,1, It is just an MC approximation.
906,1, Having a separate subsection (2.1) discussing this seems unnecessary.
907,1, How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)?
908,1,"\n\n[1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio"
909,1,"\n- You state: \""singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations."
910,1,\n+ the tensor factorization set-up ensures that the embedding dimensions are aligned \n+ clustering by weights (4.1) is useful and seems coherent\n+ covariate-specific analogies are a creative analysis
911,1," The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network."
912,1, applying it to architecture search is novel.
913,1," As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable,"
914,1,"""This paper propose an adaptive dropout strategy for class logits."
915,1," The second paper uses arithmetic circuits rather than HE, but actually implements training an entire neural network securely."
916,1,\n\n\nAbout the description of problem statement in Section 3:
917,1,"\n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting."
918,1,\n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train.
919,1,n\nIt would be better to provide deeper analysis in Subsection 6.1.
920,1, Does single task mean average performance over the tasks?
921,1," \n\nHowever, I am concerned with the experimental results."
922,1," Non of these areas, with the exception of semantic parsing, are addressed by the author."
923,1,"""## Review Summary\n\nOverall, the paper's paper core claim, that increasing batch sizes at a linear\nrate during training is as effective as decaying learning rates, is\ninteresting but doesn't seem to be too surprising given other recent work in\nthis space."
924,1,\n\n- The experimental setup is problematic
925,1," As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance."
926,1," Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure."""
927,1, Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1.
928,1," but does not cover the important scenario where we only have access to (state,action) pairs given by an expert."
929,1,\n\nQuality: this paper is of good quality
930,1,"\n\nSIGNIFICANCE: I think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published."
931,1," The wasserstein distance is also called the \""earth mover distance\"" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another."
932,1, \n- Proof of Proposition 5 (cf. page 13): this is a sketch of proof to me.
933,1,\n\n\nCons\n\nThere is little novelty in the proposed method/models -- the paper is primarily focused on comparing existing models on a new task.
934,1," They assess their solution quantitatively, demonstrating their model performs better than first, a simple heuristic model (I believe de-centralized Dijkstra\u2019s for each agent, but there is not enough description in the manuscript to know for sure), and then, two other baselines that I could not figure out from the manuscript (I believe it was Dijkstra\u2019s with two added rules for when to recharge)."
935,1," Accordingly, I did not change my scores."""
936,1, It's not clear what kind of loss function is really being optimised here.
937,1, Maybe adding a section about the notation and developing more the intuition will improve the reading of the manuscript.
938,1,\n\nSeveral references I suggest:\nhttps://arxiv.org/abs/1706.08500 (FID score)\nhttps://arxiv.org/abs/1511.04581 (MMD as evaluation)
939,1,\nThe paper is organized well.
940,1,"\n\nAlthough I really wanted to like the paper, I have several concerns, First and most importantly, the paper is not citing several important related work."
941,1, \nThis method does not model spatial information.
942,1,\n\nMy primary concern is that this work has limited practical benefit in a realistic setting.
943,1,"\n - Section method, paragraph under equation (2) L(z(\\alpha),x,y)<=L(w,x,y) is NOT necessary."
944,1," As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6."
945,1,"""The submission describes an empirical study regarding the training performance\nof GANs; more specifically, it aims to present empirical evidence that the\ntheory of divergence minimization is more a tool to understand the outcome of\ntraining (i.e. Nash equillibrium) than a necessary condition to be enforce\nduring training itself"
946,1, \n\n\nDetailed comments/questions:\n-            The use of Laplace approximation is (in the paper) motivated from a probabilistic/Bayes and uncertainty point-of-view.
947,1," This would solve another issue, which is the weakness of their baseline measure."
948,1, What is a powerful model?
949,1,   Making an anomaly seem normal by distorting it is easy.
950,1,  This seems reasonable.
951,1,\n\nDeep Generative Replay section and description of DGDMN are written poorly and is very incomprehensible.
952,1,"\n5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017."
953,1,"\n\nAmong these results (1), (2), (4) are sort of known in the literature."
954,1,"\"" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?"
955,1,\n\nThis paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks.
956,1, It would be desirable to sum up the overall procedure in an algorithm.
957,1,"""Update:\nOn further consideration (and reading the other reviews), I'm bumping my rating up to a 7."
958,1, The results from this paper is also promising that it showed convincing compression results.
959,1," \n\n-In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$."""
960,1, \u201d Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])?
961,1,"\n(\""The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\"
962,1," In this model, the neurons must pay to observe the activation of neurons upstream."
963,1," \n\n[Strengths]\n\n1. I think this paper proposed interesting tasks to combine the vision, language, and actions."
964,1, Does it have anything to do with existing issues in DRL?
965,1, Is this important to achieving better results as well or is the guidance reward enough? 
966,1," In light of this, I see very little novelty in this paper."
967,1, (not 100% sure about the originality of the work though).
968,1," And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup."
969,1,"""The paper proposes combining classification-specific neural networks with auto-encoders."
970,1," Results show that with a fixed budget, it\u2019s better to label many examples once rather than fewer examples multiple times."
971,1,\n\n\nQUALITY\n\nThe symbol d_{rew} is never defined \u2014 what does \u201cnew\u201d stand for?
972,1," In the videos, it seems like the robot could get a slightly better view if it took another couple of steps."
973,1," I also do not see how to define an intuitive set of \""states\"" in that case."
974,1,"  \n- This work collected a new dataset for 3D face expression representation, which is great (the state of 3D face databases which are available to researchers is very limited, so this is a step in the right direction)."
975,1,\n\n4. Maybe overclaiming.
976,1," 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,;"
977,1," I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method)."
978,1, The contributions of the work are very limited.
979,1," On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below)."
980,1, The most important aspect is the capability to build a feature map of previously unseen environments.
981,1,"""This paper applies the boosting trick to deep learning."
982,1,"\n\nBeyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach."
983,1,"\n\n4) Even when comparing with simple baselines, the proposed model does not convincingly outperform them. "
984,1,\n\n[Cons]\n- The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks.
985,1," One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those."
986,1,\n\nThe paper presents interesting ideas and findings in an important challenging area.
987,1," Hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer, which predicts the word that would make the reader fail if the word is obfuscated.."
988,1,what are the image sizes for the CelebA dataset\n\n- page 5: double the\n\n
989,1,\n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem?
990,1,"\n\nMinor comments:\nSection 1.1: \""a affine\"" -> \""an affine\""\nTypo in section 3.4: \""of a of a[[CNT], [CLA-NEG], [CRT], [MIN]]\""\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse."
991,1, I do not see that from the paper.
992,1," Here is a hook into that literature: Goldman, S., & Kerns. M. (1995)."
993,1," There is absolutely nothing of interest to ICLR except for the fact that now we know that a trivial network is capable of obtaining 90% accuracy on this dataset."""
994,1," A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers."
995,1, So it's hard to tell if it's real improvement.
996,1, \n\nPros:\n- The research direction in combining model-based and model-free RL is interesting.
997,1,\nIt should be stated more clearly how the results from Figure 4 were obtained.
998,1," How does this method perform for more realistic data, even e.g. MNIST ?"
999,1, The proposed model variations (which replaces the \u201ccontent update\u201d that has a recurrent network in terms of context-independent update) and their evaluations seem rather more arbitrary.
1000,1," \n\n[1] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom."
1001,1," I suggested that the author should reform the structure, ignore some unrelated content and make the clear claims about the contributions on the introduction part. "
1002,1,  This would assist in understanding tradeoffs in the design space.
1003,1, how update in (7) is guaranteed to be DP?
1004,1,"\n\nSPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network."
1005,1,  Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations.
1006,1,\n\nMost parts of the paper provide a detailed review of the literature.
1007,1, A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal.
1008,1, This would not\nresult in excluding the unit.
1009,1," If not, I\u2019d be curious as to why."
1010,1," \n\nI like the idea of the paper,;"
1011,1, I am not sure how this is achieved in this work.
1012,1,"""The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples."
1013,1," Although DCN+ is an improvement of DCN, I think the improvement is not incremental."
1014,1, \n\n2) The confusion over the motivation is confounded by the fact that the experiments are very unclear.
1015,1,"\n\nThe idea is slightly novel, and the framework otherwise state-of-the-art."
1016,1,"  Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI)."
1017,1,". However, there is still a mismatch even with 1-step Q-learning because the bootstrapped target is also computed from the TreeQN."
1018,1,Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere.
1019,1," On the other hand, the usefulness of the learned representation for planning is unclear."
1020,1," Considering that their paper is titled as a work to use \""dependencies\"" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation."
1021,1," Therefore, I rate the current manuscript as a reject."
1022,1," To get a better impression\nof what approaches perform well, their parameters should be tuned to the\nparticular benchmark."
1023,1," I have a few concerns: First, I find the discussion around the training methodology insufficient."
1024,1," Basics on mass transportation are briefly recalled in section 2, while section 3 formulate the GANs approach in the Wasserstein context."
1025,1, This is never discussed.
1026,1," The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch."
1027,1," They also consider the JCP-S approach, where the ALS (alternating least squares) objective is represented as the joint objective of the matrix and order-3 tensor ALS objectives."
1028,1," However, as mentioned earlier, the paper writing and organization makes it hard to understand what exactly is being shown."
1029,1,\n\n* Using character-level models a la Ling et al.
1030,1,"\n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces"
1031,1,"\n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5."
1032,1," However, the training protocol gets complicated with the introduction of gate weights."
1033,1,\n\nVery nice paper.
1034,1, What does the lookup table consist of? 
1035,1," Hence, it\u2019s unclear to me whether a model should infer hidden information based on just a single screen + minimap observation (or a history of them) or due to how the dataset is constructed, all units are observed without spatial limitations of the screen. \n"""
1036,1,"""The authors consider the task of program synthesis in the Karel DSL."
1037,1, How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)?
1038,1," There are two such baselines: random fixed weightings of the n-step returns, and persisting with the usual weighting but changing lambda on each time step (either randomly or according to some decay schedule)."
1039,1,"\n- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method)"
1040,1,"\n- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)."
1041,1, Legend backwards?
1042,1,"First, although this metric makes intuitive sense, it is unclear to me how much it reflects control performance, which is what we ultimately care about."
1043,1,"\n\nIf I may summarize the key takeaways from Sections 5.4 and 6, they are:\n- GAN training remains difficult and good results are not guaranteed (2nd bullet\n  point)"
1044,1," Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time."
1045,1," This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are"
1046,1,"\n\n*Clarity*\n\nThe paper is, in general, well-written. However, the presentation in 4 is hard to follow."
1047,1,  \n- How network behaves by introducing noise on vertices?
1048,1, Do the multiple input distributions actually help?
1049,1,\n\nThere is also one step in the theorem that I cannot verify.
1050,1,"\n3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that."
1051,1,"\n\nI find this question especially important for MARL, because the assumption on synchronous and noise-free communication, including gradients is too strong to be useful in many practical tasks."
1052,1, Instead of reward.. \u201d Two sentences are disconnected and need to be rewritten.
1053,1,"\n- While overall the writing is clear, in some places I feel it could be improved"
1054,1," This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion."
1055,1," Without this information, the experiments have limited value."
1056,1,\n\nThe authors provide a clean variational inference algorithm to learn their model.
1057,1, \n* Would the speedups be more dramatic for a larger dataset like Imagenet?
1058,1, However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued.
1059,1, Training the generative model with fixed ground-truth ordering was similarly performed in [1] (\u201cstrong supervision\u201d) and is thus not particularly novel.
1060,1, This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.
1061,1," Because of this, the experimental results between the two architectures are incomparable."
1062,1, The authors state significant improvements in classification using generated data.
1063,1," \n\nCons:\n- The title of the paper seems to general to me, since target propagation is the only algorithm compared against backpropagation."
1064,1, The CW-SC kernel can be regarded as a redundant version of interleaved group convolutions [1].
1065,1,\n\nii) The authors do not have any descriptions for Figure 3.  Equation 1 is also very confusing.
1066,1,What is the objective for the images in Fig. 4? 
1067,1," The proposed ideas for the extension seem natural (i.e., use of SR and CNN)."
1068,1," The authors discuss the advantages of SMC and say that is scales better than other methods,"
1069,1," More generally, a now quite common way to handle this problem is to use \""pointing\"" or \""copying\"", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too."
1070,1,"\""\nWhen approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored."
1071,1,\n\n\t- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data.
1072,1," but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage?"
1073,1,", which is not true when also considering the running time of node2vec."
1074,1," A method for learning this is presented, and fine tuned with an actor-critic method."
1075,1, This seems to be in conflict with the observation drawn in the paper.
1076,1," The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers."
1077,1,. One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned. 
1078,1,"\n\n5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits?"
1079,1,"\n\nSignificance:\nThe result is not completely surprising,"
1080,1,"\n\nMinor-comment: \n1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP. "
1081,1,\n\t-\t\u201cThough they do not point out this insight as we have\u201d - This seems to be a bit overreaching.
1082,1,"  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation."
1083,1,  May this helped Q_MC have better perceptual capabilities?
1084,1," The authors design an architecture that works on image captioning, image classification, machine translation and parsing.\"
1085,1," After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task."
1086,1," \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing;"
1087,1,"\n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011)."
1088,1,". They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work. "
1089,1,\n(vii) No real credit is given for the Laplace approximation presented up to Eq. 10.
1090,1,"\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network."
1091,1,"""- The paper proposes to learn a soft ordering over a set of layers for multitask learning (MTL) i.e.\n  at every step of the forward propagation, each task is free to choose its unique soft (`convex')\n  combination of the outputs from all available layers."
1092,1, Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference.
1093,1," In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H)."
1094,1," \n2) I'm not sure, and I haven't seen evidence in the paper (or other references) that SNN is the only (optimal?) method for this context."
1095,1,"That result could make some predictions for experiment, that would be testable with chronic methods (like Ca2+ imaging) that can record from the same neurons over multiple experimental sessions."
1096,1,"\nMost important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it."
1097,1,"\n4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning  """
1098,1, The approach is likely to be useful for other\nresearchers working on related problems.
1099,1," Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach"
1100,1," I will try to summarize the main novel (i.e., not present elsewhere in the literature) results of these:"
1101,1,\n\n3) How should we set the parameter lambda?
1102,1," Furthermore, the story of generating medical training data for public release is an interesting use case for a model like this, particularly since training on synthetic data appears to achieve not competitive but quite reasonable accuracy, even when the model is trained in a differentially private fashion."
1103,1," \n\n- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables."
1104,1,"""This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck."
1105,1," A rich lists of the possible components of the neural network-based clustering methods are given, that include the different neural network architectures, feature to use for clustering, loss functions used and more."
1106,1, I'm not sure how relevant this graph classification task is.
1107,1," SDP while tractable, would still require very expensive computation to solve exactly."
1108,1, Did each points are sampled from same travelling distance or according to the same time interval? 
1109,1,"\nin the paper, and the approach is new as far as I know."
1110,1," The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function."
1111,1, I think this will make the first attack useless.
1112,1," \n\nThe proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process."
1113,1,\n2.  SRM needs to gather training samples which are 100 accuracy curves for T-1 epochs.
1114,1," They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet."
1115,1,\n\nExperimental results are not convincing.
1116,1," I expect these updates to be reflected in the final version of the paper itself as well. """
1117,1," Experiments on MNIST with show improved generalization (but the baseline is chosen poorly, read below)."
1118,1,"\n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions."
1119,1,\nThe AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space.
1120,1,\n2. Section 4: Are you using the soft unitary constraint in your experiments?
1121,1," The authors discover some interesting characteristics of MC based Deep-RL which may influence future work in this area, and dig down a little to uncover the principles a little."
1122,1, How did you generate these stories with so many sentences?
1123,1,"""The authors propose a penalization term that enforces decorrelation between the dimensions of the representation "
1124,1,"""Summary\nThe paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated updates."
1125,1, At least for the word translation task many of these common representation learning frameworks could have been easily evaluated.
1126,1,"\n\n\n[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015."""
1127,1,\n\nCons\nHard to extract the most important changes from the text.
1128,1,.\n\nQuality and significance:\nThe paper proposes an interesting direction for optimizing the computational cost of training and inference using neural networks
1129,1," however, I have some comments and questions:\n\n- It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case."
1130,1," If it was not done, as in your examples, the state would not be Markov and thus it would not be an MDP at all."
1131,1,"""This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text."
1132,1,"  (The authors state they are favoring the baseline in this comparison, but it would be nice to have those numbers.)\n- I don't understand when the authors say the deep model has better memory than baselines (which includes a perfect memory baseline)"""
1133,1,"""This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models."
1134,1," \n- TSP experiments show that \""in distribution\"" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success."
1135,1,\n-Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits
1136,1,\n\nOn the positive side: \n\nThis is the first paper to my knowledge that has shown that grid cells arise as a product of a navigation task demand.
1137,1," The analysis of manifold structure of DNN is important direction, but I am afraid novelty and insight of this work is not enough for acceptance."
1138,1," However, the paper lacks novel content."
1139,1, \n- For the binary setting you mentioned that you had to reduce the entropy thus added a \u201cbeta density regulariser\u201d. Did you add R(p) or log R(p) to the objective function?
1140,1," It feels rather contrived to focus so much on the datasets with exact matches since,;"
1141,1,"  In addition, the authors should compare with more baselines such as [1], [2], [3] and try with deeper networks as many networks used in the experiments are not very deep, with only 8 layers or less."
1142,1,\n\nCons\n-It is not clear why GANs are the only generative model considered
1143,1,\n \nQuality/clarity:\n\nThe paper is well written and easy to follow.
1144,1," In fact, there are obvious places where the exposition is excessively verbose, and there are clear opportunities to reduce the length of the submission."
1145,1, Language: the paper needs some careful editing to correct numerous language/grammar issues.
1146,1," \n\nOverall, the work is an interesting read, and a nice follow-up to Neal\u2019s earlier observations about 1 hidden layer neural networks."
1147,1, The data are just targets\nobtained by the q-learning rule.
1148,1,"  As the authors show based on the Fokker-Planck equation of the underlying stochastic process, there exists a conservative current (a gradient of an underlying potential) and a non-conservative current (which might induce stationary persistent currents at long times)."
1149,1," If we are serious about evaluating image realism and working towards passing the visual Turing test, we should report results without an artificial time limit."
1150,1,"""This paper studies empirical risk in deep neural networks."
1151,1," For example, we can use +1, 0, -1 to approximate the weights."
1152,1," \n\nThe method does prove that the Caffe reference model maintains some information that can be used for classification, but this doesn't really suggest a generalizable method that we could confidently use for a variety of tasks."
1153,1, But I do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it.
1154,1," \n- missing discussion to the \""attention is all you need paper\"", which seems highly relevant[[CNT], [SUB-NEU], [DFT,SUG], [MIN]] \n\n() Typos:\nPage 1\n\""a support vectors machineS\"" -> \""a support vector machine[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""performs good\"" -> \""performs well[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""the n-grams was widely\"" -> \""n-grams were widely[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""to apply large region size\"" -> \""to apply to large region size\""\n\""are trained separately\"" -> \""do not share parameters\"""
1155,1," The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations."
1156,1,"""The paper presents a method that leverages demonstrations from experts provided in the shape of sequences of states (actually, state transitions are enough, they don't need to come in sequences) to faster learn reinforcement learning tasks."
1157,1, The proposed method introduces hyper-hyperparameters which may be hard to tune.
1158,1," However, the paper as it stands seems rather preliminary, and there are several issues with the paper I think need to be addressed."
1159,1, \n\n3.  What do different points in Fig 3 and 4 represent.
1160,1,"\n\n[1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016."""
1161,1, Are there metrics other than overall accuracy that could be used to measure performance? 
1162,1,"  I understand that the authors wanted to\nhave \""what\"", \""which\"" and \""how\"" sections but this is not clear at all."
1163,1, That this approach works well implies that amortization error cannot be ignored.
1164,1,\n- Suggested modifications from DTP to STDP increase its biological plausibility without making its performance worse.
1165,1,\n2) Could margin-based generalization bounds explain the superior generalization performance of the linear model trained on random vs. non-random data?
1166,1,"\n\n1) The paper emphasizes the \""fully-differentiable tree planning\"" aspect in contrast to VPN that back-propagates only through \""non-branching\"" trajectories during training."
1167,1, \n-- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form).
1168,1,\n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field.
1169,1,"\"" The reward suggests that the goal is to collect as many health kits as possible, for which surviving and maintaining health are secondary."
1170,1," \n\nLast, show the sensitive results of the proposed method by tuning alpha and beta."
1171,1," For example, the model does not guarantee to be able to convert resulting \""indices\"" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector."
1172,1, \n\npage 14:\n- Proposition 1: the proof could be shorten by simply stating in the proposition that f and g are distribution...
1173,1,\n\nOne thing I hope the author could provide more clarification is the use of NER.
1174,1,"""This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm."
1175,1,"""The authors propose an approach for zero-shot visual learning. "
1176,1,"  It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss."
1177,1,"\n\nRegarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help?"
1178,1,  n\nCons:\nThe paper shows no results.
1179,1,\n - Can the results be applied to some practical task? Why are the results interesting and/or useful?
1180,1," \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn\u2019t require assumptions on parameter dimensions and data matrices."
1181,1,"\u201d However, in table 4, evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC."
1182,1," In the proposed example, it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swaps."
1183,1, Maybe sec3 can be clarified in its structure by re-ordering points on the quadratic error function and early stopping (eg avoiding to split them between end of 3.1 and 3.2).
1184,1,\n4. How is this work related to the extensive work in NLP in applying parsing to various tasks?
1185,1," Carlini-Wagner attack is still strong, but the authors only use 40 iterations (should be at least 500) and setting the confidence=0, which is known to be producing non-transferable adversarial examples."
1186,1, it might be useful to look into these design choices
1187,1," however, in its current form I do not think that the novelty\nof the contributions are clearly presented and that they are not thoroughly\nevaluated in the experiments."
1188,1," Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST"
1189,1,\n* The experimental evaluation appears to be sound.
1190,1,\n\nCons:\n\n0. The whole paper just presented strategies and empirical results.
1191,1," In addition, it seems that all computations are done\ninto a \\ell^2 space instead of in the RKHS (equations 5 and 11)."
1192,1," The evaluation consists of hypernym detection on WordNet and graded lexical entailment, in the shape of HyperLex."
1193,1, The review of NAT is too brief and makes it too hard to understand the remaining of the paper.
1194,1,"  \n\nIn terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works."
1195,1, 3) by avoiding introducing false claims based on a misunderstanding of terminology 
1196,1,"\n\nThus I think that although this paper is written well,"
1197,1, \n\nComments:\n-\tThe paper mainly focuses on the soft sensor selection.
1198,1,"? If it is set as hyper-parameter, how does the performance change concerning them? "
1199,1," I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular."
1200,1,  My honest impression is simply that this is still work in progress and that the write up was done rather hastily.
1201,1, The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest.
1202,1," Of course, the elephant in the room is how to determine the groups across which the posteriors can be shared and their information costs amortized."""
1203,1,"  It points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the MMD distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the MMD has been observed in the past to perform poorly for face generation [Dziugaite et al. UAI 2015]). "
1204,1,"\n\n2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage."
1205,1, This seems to suggest that the only thing that the master can communicate is action information?
1206,1,\n- what is the relationship of the presented ASG criterion to MMI?
1207,1,  \n\n- The center topic does not fit ICLR
1208,1," \n\nOn the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently."
1209,1,\nFigure 6/7: are very hard to read. I am still not sure what exactly they are trying to say.
1210,1,"""This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page."
1211,1," I was also not sure why there is a need to copy items from the input question, since all SQL query nouns will be present in the SQL table in some form."
1212,1,"   From a scientific point of view, this seems orthogonal to the point of the paper, though is relevant if you were trying to build a system."
1213,1," For example, it is unclear whether the baseline in Table 3 also uses the two-pass decomposition or not."
1214,1, A good example of this is a supervised actor-critic by Barto (2004).
1215,1," It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015]."
1216,1,"\n\nAuthors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm."
1217,1,\nPIB pursues the very natural intuition outlined in the information bottleneck literature: hidden layers of deep nets compress the input X while maintaining sufficient information to predict the output Y.
1218,1," The dataset, as well as all investigated models/approaches, are existing work."
1219,1,"\n\n- Page 4. 3.3) \""Note that.. outdoor images\"" this is implicitly adding the designers' bias to the results."
1220,1," I think this section and the previous (\""The objective of unsupervised learning\"") could be combined, removing some repetition, adding some subtitles to improve clarity."
1221,1,\n13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images?
1222,1," The main results are two-fold: if the decision boundary are flat (such as linear classifiers), then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correlated."
1223,1,"  From the standpoint of the computer vision community, increasing performance on these datasets is no longer a meaningful objective."
1224,1,"""The paper presents a method for feature projection which uses a two level neural network like structure to generate new features from the input features."
1225,1," Every digit has at least one rotation that is not well classified, so this section could use more discussion and analysis."
1226,1,"  \n\n4. In section 4.3, the authors claimed that numerical diffentiation only hurts 1 percent error for second derivative."
1227,1, I am not able to accept this paper because of the latter point.
1228,1,"  Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change."
1229,1," However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added."
1230,1," Furthermore, the paper is easy to read with good organization."
1231,1," On Page 18, how is the squared removed for difference between U and Upi?"
1232,1," It is because the toy task author demonstrates is actually quite similar to copy tasks, that previous state should be remembered."
1233,1,\n\nThe toy experiment is not convincing.
1234,1,"\n\nClarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version."
1235,1," Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs."
1236,1," Firstly, word accuracy rate doesn't seem appropriate, as it does not measure morphological agreement."
1237,1,"\n\nSummary:\nThis is a nice paper which deals with an important problem, has some nice results and while not groundbreaking, certainly merits a publication."""
1238,1, The gap is so large that I am not convinced on the fairness of the comparison
1239,1," \""\nFormalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf )."
1240,1," This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks."
1241,1,Spatial resolution*: 1) The analysis seems to be done with respect to DNN not to a  CNN. 
1242,1,. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation.
1243,1,   Was the performance significantly worse without the Raiko estimator?
1244,1,"""The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space."
1245,1," Besides, the eigenoptions seem to help for exploration (as a uniform policy was used) as indicated by plot 3(d), but could they help for other tasks (e.g., learn to play Atari games faster or better)? "
1246,1," \n\nIn addition, the experiments section is not comprehensive enough as well the author only tested on two datasets."
1247,1," \n\nCooperative multi-agent problem solving is an important problem in machine learning, artificial intelligence, and cognitive science."
1248,1, It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best.
1249,1," For example, how does this compare to random perturbation (say, zero-mean) of the weights?"
1250,1," The paper seems to be very thin technically, unless I missed some important details."
1251,1, This model does not use any recurrent operation but it is not per se simpler than a recurrent model.
1252,1,"""This paper aims to provide a continuous variant of CNN."
1253,1,"\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level."
1254,1, \n\nMinor points: Section 5.3 Square map: how much more reward will the agent gain by taking the optimal path?
1255,1,"""This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. "
1256,1,"\n\nIn the non-tabular case, the authors have actually moved away from Q learning and defined an objective that is both on and off-policy."
1257,1," Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . "
1258,1," The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better! "
1259,1, Is this finetuned during training?
1260,1," Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel."
1261,1,\n(-) Generalization to other tasks not shown
1262,1, Why not simply use a dB criteria ?
1263,1, The basic problem is empirical risk minimization with a incremental penalty for each non zero weight
1264,1,\n3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task.
1265,1," Then in Table 3, we see that the baseline is less stable (i.e. its performance decreases across the different iterations of projected gradient descent)."
1266,1,   The presentation right now suggests that the scheme does in practise not work for deep networks...
1267,1,\n\n2. The relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear.
1268,1," Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc."
1269,1,\n- The speedup is only measured on CPU.
1270,1,\n\nI would be interested to know more about the intuition behind the proposed method.
1271,1,\n+ novel algorithm
1272,1," As is, while results are promising,"
1273,1, \n\nI'm also uncomfortable with the way most of the expert data are generated for experiments.
1274,1," The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter."
1275,1, At risk of being too reductionist:\nit looks as learning a set of filters on different coordinate systems given\nby the various powers of A.
1276,1,"\n\n- The paper is hard to read due to many abbreviations, e.g., the last paragraph in page 2."
1277,1,"  Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases."
1278,1,"\n- I'm not convinced that page 4 and the \""Bayesian\"" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \""this is similar to particle filter\"" and add the mathematical derivation after, rather than as if it was some complex formula derived."
1279,1,"\n- NS-GANs + GPs seems to be best sample-generating combination, and faster than\n  WGAN-GP"
1280,1,"\n  \u2022 [p4, Sparse rewards] I am not sure it is fair to say that the general difficulty is kept fixed."
1281,1,"\n\nOverall the paper is too vague on the mathematical part, and the experiments provided are not particularly convincing in assessing the benefit of the new penalty."
1282,1," For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts."
1283,1,"  The\nresulting simple prediction framework is then used for early stopping,\nin particular within the Hyperband hyperparameter search algorithm."
1284,1," Could you please provide more explanations and intuitions?"""
1285,1, They accept different inputs (raw pixels vs edges).
1286,1,\n\nThis paper instead proposes to learn a surrogate function for choosing which examples to select.
1287,1,\n- It may be helpful to indicate the standard deviations of the experimental results.
1288,1, The actual observations are a weighted linear combination of the emissions from each latent HMM.
1289,1," Unfortunately, the contribution seems rather small to be accepted for ICLR."
1290,1, \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe).
1291,1," However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized."""
1292,1," It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening. "
1293,1," The \""Reverse PPL\"" metric requires more\njustification, and it looks an awful lot like the long-since-discredited Parzen\nwindow density estimation technique used in the original GAN paper."
1294,1," Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points."
1295,1, Common batch sizes range from 64 to 1K (typically >= 128).
1296,1,\n\n- I think the analysis of section 5 is fairly trivial. 
1297,1,"  \n\nIn summary, this paper has limited novelty, incremental contributions, and lacks convincing experimental results due to weak attack implementation.  \n \n\n\n"""
1298,1,"  Besides, the intent of what possibly interesting PIR examples will be was unclear."
1299,1,"""This paper presents a tensor decomposition method called tensor ring (TR) decomposition."
1300,1, The approaches are evaluated in grid worlds with and without other agents.
1301,1," I would also change the caption to use \""meta-training losses\"" instead of \""training losses\"" (I believe those numbers are for the meta-loss, correct?)"
1302,1,"  The system uses logic formulas in truncated linear temporal logic (TLTL), which lacks an Always operator and where the LTL formula (A until B) also means that B must eventually hold true. "
1303,1,\n\nSome detailed comments / questions:
1304,1,  Could you elaborate on this?
1305,1, \nSo this paper is innovative in two parts:\n- it applies GANs to adversarial example generation\
1306,1,\n\nI think the methodology presented in this paper is neat and the experimental results are encouraging.
1307,1, A projected sub-gradient descent algorithm is used.
1308,1,  I think further work is required (perhaps expanding the search space) to resolve the current limitations of automated architecture search.
1309,1," That said, I think the authors would need to convincingly address issues of clarity in order for this to appear."
1310,1, Some of the GAN figures could also benefit from having captions.
1311,1,\n- the x_i and t_i of section 3.2.2 should not be denoted with the same letters as in 3.2.1.
1312,1,"""This paper mainly focuses on the square loss function of linear networks."
1313,1," \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced."
1314,1,\n\nPros:\n(1) The paper is clearly written.
1315,1, \n\nSec 7\nThe ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table.
1316,1, \n\nAnother point that is not clear or at least misleading is the so-called Hilbert Maps.
1317,1, \n\nThe presentation of the paper can be improved a bit.
1318,1," In particular, the paper introduces a benchmark domain to model fleet coordination problems as might be encountered in taxi companies."
1319,1,\n\nThe initial description (section 2)  leaves way too many unanswered questions:\n- What embeddings are used for words detected as NE?
1320,1," The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss."
1321,1, It does not provide a meaningful comparison.
1322,1,"\nThis makes it different from the work by [Kim et al., 2013], [Piot et al., 2014], and [Chemali et al., 2015], which do not require such a restriction on the demonstration data."
1323,1,\n\n- I think it's important to state in table 1 what is the amount of distortion noticeable by a human.
1324,1, How will the method compare to EM algorithms and neural network based approaches? 
1325,1," Figure 3 shows that the proposed method learns faster than DQN,"
1326,1,"\n\nThe intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL."
1327,1, Only a single example in table 5 is not enough.
1328,1,\n\nThe evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting.
1329,1," In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power."
1330,1,\n\nThe main issues with the paper is that its contributions are not new.
1331,1,\n\nClarity:\n- The paper is well written and clarity is good.
1332,1,"\nAlso, \""K\"" is already used to denote the mini-batch size, so it's a slight abuse to reuse \""k\"" to denote the \""kth marginal\""."
1333,1," However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem."
1334,1," Given this, the presentation in the paper makes the idea look more novel than it is."
1335,1,\nhe resulting multi-channel image-like structures are then feed into vanilla 2D CNN.
1336,1, This idea is novel and interesting.
1337,1,"\n\nAs a minor comment, I advise the authors to use a different letter for \""word embeddings\"" and the \""projected word embeddings\"" (equation at the bottom of page 3)."
1338,1,\n- Thought-provoking approach
1339,1,I am not sure why you do not evaluate on mini-imagenet as well as most work on few shot learning generally do.
1340,1,"\n\nI think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice."
1341,1,"""[Main comments]\n\n* I would advice the authors to explain in more details in the intro\nwhat's new compared to Li & Malik (2016) and Andrychowicz et al."
1342,1," The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now."
1343,1, The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved).
1344,1,\n\n+ The paper is rather vague on the reason to go from Eq. (6) to Eq. (7). (gradient approximation between samples to gradient on samples).
1345,1, I've seen many papers that need to go through much more complicated derivations and theory and remain within a 8-10 page limit by being precise and strictly to the point.
1346,1,"\n\nThe method involves a nice, albeit minor, trick, where the chi-squared distribution of the sum of the z_{i}^{2} has its dependence on the dimensionality removed"
1347,1," Then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent, which in turn would improve the performance of the reader as well."
1348,1,  How is the conditional entropy term estimated?
1349,1,"\n\nCons\n\nUnclear what is gained compared to existing work."""
1350,1," Again parameter tuning would potentially change all of\nthese figures significantly, as would e.g. a change in hardware."
1351,1," \n\nMy biggest concern is novelty, as the modifications are minor."
1352,1, I think overall it is a good idea.
1353,1," \n\nSecondly, both structured update and sketched update methods adopted by this paper are some standard techniques which have been widely used in existing works."
1354,1," The basic idea of the lambda return assumes TD targets are better than MC targets due to variance, which place more weight on shorter returns."
1355,1,"\n\nAbout writing:\n- the paper has so many problems with wording, e.g. articles, plurality."
1356,1,"""The paper proposes a modified approach to RL, where an additional \""episodic memory\"" is kept by the agent."
1357,1, Do you have an hyper-parameter that sets the amplitude of the constraint?
1358,1," This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text)."""
1359,1," Using this system, the authors are able to harness much more compute to learn very high quality policies in little time."
1360,1," \n\nObserve also that in table 2, NNSE gets the highest performance in both MEN and MTurk."
1361,1,\n\u201cpure-randomly\u201d => \u201crandomly\u201d\n \u201cwith adaptive algorithm\u201d => \u201cwith an adaptive algorithm\u201d\n\u201cthe connection\u201d => \u201cconnections\u201d
1362,1,"\n\nIn Section 3.2, the authors listed a couple of loss functions."
1363,1," I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality)."""
1364,1,\n\nCons:\n- The contributions of the work are very limited.
1365,1, A network is trained to learn the transformations that minimize the Wasserstein distance between distributions.
1366,1,\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks.
1367,1," Given a reward function, one can define the Bayes decision rule."
1368,1,\n\nAuthors are suggested to perform experiments on more datasets to make the results more convincing.
1369,1,n\nSummary: \n\nThe authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty.
1370,1,"\n\nCons:\n1. It assumes that each individual piece represents an independent factor of variation, which can not hold all the time."
1371,1,"\n\nOther comments:\n - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a \""topic sentence\"" at the beginning."
1372,1, \n\nSection 2 presents a taxonomy for the different neural network clustering methods.
1373,1, And there should be a single definition for L_pred.
1374,1,"\n\n3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation."
1375,1,Or is the layer shown (e.g. \u201cstage3layer2\u201d) the penultimate layer?
1376,1,"It seems to be only an experimental fact."""
1377,1,"""The authors study the effect of label noise on classification tasks."
1378,1," I could imagine a line of experiments that investigate the idea of selectively stopping episodes when the agent is no longer experiencing useful transitions, and then showing that the partial episode bootstrapping can save on overall sample complexity compared to an agent that must experience the entirety of every episode.  """
1379,1, but also not very clear at pointing out\nthe major contribution and the motivation behind it.
1380,1,"  In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \""context unit\"" of size d x K, where d is the embedding size and K the region size."
1381,1,"""This paper combines a simple PAC-Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm."
1382,1," To the best of my knowledge, the method proposed by the authors is novel, and differs from traditional sentence generation (as an example) models because it is intended to produce continuous domain outputs."
1383,1,"  While ICLR is not focused on neuroscientific studies, this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation."
1384,1, The main idea is to design complementary kernels that cover the same receptive field as the regular convolution.
1385,1,"\n\nRegarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test."
1386,1,\n\n- The technical approach is not particularly novel.
1387,1," \n\nThe trick to reset $\\mu$ after half an epoch at the end of Section 3 is too heuristic.[[CNT], [null], [CRT], [MIN]] There lacks of explanation.[[CNT], [null], [CRT], [MIN]] \n\nThe experimental results are not convincing."
1388,1,\n\nMinor things:\n-Missing propto in Eq 7
1389,1,"\n\nAbout the technique details, this paper is clearly written,"
1390,1, This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section
1391,1,\n\nI have some small qualms with the presentation of the method.
1392,1,"\n* Strictly speaking it is correct to refer to the individual nets in the ensembles as \""branches\"" and \""basic blocks."
1393,1,"\n\nThe former set of works, while focused on machine translation also learns a translation table in the process"
1394,1,"""The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update."
1395,1, I found the paper to be lacking in that respect.
1396,1," \n\n On one hand, I would suggest that this work would be better placed in an engineering venue focused on fluid dynamics."
1397,1,"\n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). "
1398,1,"  At this point, the novelty is weaken."
1399,1,"""This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions."
1400,1,"  However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data."
1401,1," It is known that there does not exist a definition of regular grammars in graph (see Courcelle and Engelfriet, graph structure and monadic second-order logic ...)."
1402,1,"\nSpecifically the analysis involves three datasets and two visual domains for each dataset: besides the original version\nof each image a new version is created by inverting its colors, i.e. simply rescaling the color channels in [0,1] and then\napplying (1-pixel_value)."
1403,1,"""The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015)."
1404,1," However, the method needs to be further clarified and the experiments need to be improved."
1405,1," Unfortunately, since the game theory was not particularly well-motivated, I did not find the overall story compelling."
1406,1,\n\n(2) The proposed search space is boring.
1407,1," How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW"
1408,1,"  E is defined (in Section 2.2) to be \""all the inter-subgraph (community) edges identified by the Louvain method for each hierarchy."
1409,1, Is e_o fixed for the non-structured knowledge?
1410,1, The argument is that we tend to use the same notion of similarity and dissimilarity to define classes (known or unknown) and one can thus expect the similarity function learned from known classes to carry over to the unknown classes.
1411,1, Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past.
1412,1,\n \n-\tIntroduction: The motivation for detecting adversarial examples is not stated clearly enough.
1413,1,"  More seriously, a comma is missed in the definition of the inner product."
1414,1, \n\n3) The authors do not describe how the data from xywy.com were annotated.
1415,1,"\nLe, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton."
1416,1,"\n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster."
1417,1," Also, in the bottom right panel in Figure 2, GrandNorm and equal weighting decrease test errors effectively even after 15000 steps but uncertainty weighting seems to reach a plateau. Discussions on this would be useful."
1418,1, It would be nice to see the behavior for different values of lambda.
1419,1,"\n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure."
1420,1,". Finally, I would add a colorbar to indicate numerical values for the different grayscale values."
1421,1,\n\n2. The analysis of all facets of the proposal is missing.
1422,1,"\n\nThe paper is well written, but can use some proof-reading."
1423,1,"\n\nAdditionally, considering the generation quality, the CelebA samples in the paper are not the state-of-the-art."
1424,1, Or at least not in an intuitive way.
1425,1,n\nIn summary the algorithms are novel variants of SGD
1426,1, \n- The worse performance compared to backprop and CNNs underlines the open question how to yield biologically plausible AND efficient algorithms and network architectures.
1427,1,"""The authors consider the metrics for evaluating disentangled representations."
1428,1," Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system\u2019s latent dependency decisions."
1429,1," \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules."
1430,1, The work is clearly presented and the evaluations seem convincing.
1431,1," \n\nIf we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero."
1432,1, I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader.
1433,1,"However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work."
1434,1,"\n* The growing body of work on deep kernel learning, which \u201ccombines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes\u201d. E.g.: (i) \u201cDeep Kernel Learning\u201d (AISTATS 2016);"
1435,1," \n\nOverall, I believe this paper is a nice contribution to the deep learning theory literature."
1436,1, The performance is not strong enough to warrant acceptance by itself.
1437,1,"\n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)?"
1438,1,"""The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR."
1439,1," The very high accuracies reported seem to hint the latter, which is a serious methodological error."
1440,1,\n\nThe contributions of the paper are novel and significant.
1441,1," \n\n\n# Suggestions on readability\n\n* I have the feeling the paper inverts $\\alpha, \\beta$ from their use in Finn 2017 (step size for meta- vs task-training)."
1442,1," incl.:\n\nRuediger Ehlers. Planet. https://github.com/progirep/planet,\nChih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis\nAlessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351"
1443,1,\n\nTheir contributions are:\n\n1. Formulate complex valued convolution
1444,1,"  Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps."
1445,1,"  If this takes longer than synthesis, it defeats the purpose of using this method in the first place."
1446,1," Animals presumably continue to learn throughout their lives.[[CNT], [null], [DIS], [MIN]] With on-going (continous) training, do the RNN neurons' spatial tuning remain stable, or do they continue to \""drift\"" (so that border cells turn into grid cells turn into irregular cells, or some such)? "
1447,1, \n\n3. Label the y-axis in Fig 2.
1448,1,"""This paper studies a toy problem: a random binary image is generated, and treated as a maze (1=wall, 0=freely moveable space)."
1449,1," This step is the real meat of the paper, yet I struggle to find a concrete definition in the text.[[CNT], [null], [DIS], [GEN]] Is this really just an average over a few recent weights during optimization?"
1450,1,"\n\nExperimentally, the results are rather weak compared to pure model-free agents."
1451,1," The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework."
1452,1," Therefore\ncompared with pure in-between-agent communications, MS-MARL is more efficient in reasoning\nand planning once trained. [...] "
1453,1, The experiments show robustness to these types of noise.
1454,1,"""In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games."
1455,1," Interestingly, the way they do so is through the successor representation (SR)."
1456,1,\n- virtually no evaluation of algorithm
1457,1,\n\nA few remaining questions for the authors:\n* There is a parallel submission (presumably by different authors called \u201cResidual Connections Encourage Iterative Inference\u201d) which contains some related insights.
1458,1,"\n6. For the MS-COCO, examples can you provide more detailed results as shown for synthetic datasets? Majority vote is a very weak baseline."
1459,1,high \n\nSummary:\nThis paper studies the problem of unsupervised learning of semantic mappings.
1460,1,"""The paper presents a series of empirical studies of the generalization ability of convolutional neural networks (CNNs) applied to image recognition tasks related to shape images. "
1461,1,\n\nForm:\n- The paper does not maker clear how the exact schedules work.
1462,1,"\n\n- in page 2, \""in this figure\"": which figure is this referring to?"
1463,1," \n- The paper is incomplete without the appendices.[[CNT], [SUB-NEG], [DFT], [MIN]] In fact the paper is referring to specific figures in the appendix in the main text."
1464,1, Some theoretical aspect or at least some intuition should be more in depth detailed to understand when one should be better than the other.
1465,1," On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines."
1466,1, \n\n==\n\nThe authors do not put into perspective their approach.
1467,1, \n* The proposed approach suffers from a technical weakness or flaw.
1468,1,  And how does the performance of the technique depend on the setting of m?
1469,1, It's not differentiable and requires sampling for training.
1470,1," The only novelty is these \""HoW\"" inputs to the extra attention mechanism that takes a richer word representation into account."
1471,1,"\n\nThere are several different observations made via experiments in this work but one of the more interesting ones is quantifying that a deep generative model, when trained with a fully factorized gaussian posterior, realizes a true posterior distribution that is (more) approximately Gaussian."
1472,1,\n\n----------\n\nOVERALL JUDGMENT\nThe paper presents a clever use of VAEs for generating entity pairs conditioning on relations.
1473,1," Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution."
1474,1,"\n* Section 2.2 starts talking about \""deterministic layers h\""."
1475,1,\n\nRevised Review:\nThe main idea of the paper is very interesting and the work presented is impressive.
1476,1,"  Since only depth, width, and skip connections are considered, the end network must end up looking like a ResNet or DenseNet, but with some connections pruned."
1477,1, A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task.
1478,1, The discriminator LSTM takes a sequence (and conditional information) as input and classifies each element of the sequence as real or synthetic -- the entire sequence is then classified by vote.
1479,1,\n(c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task.
1480,1, Does it suggest the cosine similarity is not effective in measuring the state similarity?
1481,1,"\n\nSmaller details: some spacing issues, some extra punctuation (pg 5 \u201c. . Hence\u201d), a typo (pg. 7 \u201ctraining of the VAE did not lead to values as satisfactory AS what we obtained with the GAN\u201d)\n"""
1482,1," I\u2019m listing a small selection of relevant papers below, but I\u2019d encourage the authors to read a bit more broadly, and relate their work to the myriad of related older methods."
1483,1, What does DeltaVar mean in eq (2)?
1484,1," \n\nFor the mixed decoding objective, how is the mixing weight chosen and what is its effect on performance?"
1485,1,"\n\n# Technical Soundness\n- The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult, because the best estimated actions during tree planning does not necessarily match with the chosen actions."
1486,1, Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result? 
1487,1," But, the chosen benchmarks and datasets seem to be not very standard for evaluating geometric CNNs."
1488,1,. I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing
1489,1,"  I think this paper is still pretty borderline, but I increased my rating to a 6."
1490,1, I had a number of clarification questions spefically on this section:\n- Am I correct that the results in this section do not use the trace-norm regularization at all?
1491,1," The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results."
1492,1," In my opinion, the quality of the paper would be much improved by a brief discussion of this, and some reflection on what aspects of these variation contribute to the confidence vectors and what isn't captured."
1493,1,\n7. What's the reward used in the experiments
1494,1, The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar. 
1495,1, This paper fails to consider properly the work in its FDA context. 
1496,1,"   \n\nFor the unsupervised reconstruction loss, a static background is populated with objects, one at a time, each passing its state and feature through deconvolution layers to generate RGB object content."
1497,1," For example, how far does using the expected context vector deviate from marginalizing the monotonic attention? "
1498,1, Some analysis on the conditions that under which the continuation equilibria e.g. cooperation in the social dilemma is expected to arise would also be beneficial.
1499,1," Like several existing geometric CNNs, convolutions are performed on each point using nearest neighbors."
1500,1," \n\nThe main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on \""memorizing\"" a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality)."
1501,1, The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.
1502,1," \n\n- Judging from Figure 2 and Table 1, all the methods tested are not effective in hiding the private information S in the learned representation."
1503,1," However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above."
1504,1," With BLEU score differences being so low, the authors should specify how statistical significance is measured;"
1505,1, The only valid conclusion is that real and complex valued neural network cannot be directly compared using the same number of parameters.
1506,1, But the results are impressive.
1507,1," Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all."
1508,1,"""The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues."
1509,1, It is also impressive how much faster their model performs on tasks without sacrificing much performance.
1510,1," However, a ReLU can also be approximated by a smooth function and a Taylor series."
1511,1,"""This paper introduces a convolutional autoencoder for irregular graphs, specifically surfaces in the form of discrete meshes in 3D."
1512,1,What is the difference between the left and right plots?
1513,1,"""Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models"
1514,1,\n\nOverall I believe the paper is not mature enough for publication.
1515,1,  I would have liked to see separate precision and recall rather than accuracy.
1516,1,   The authors need to address the following:  \n\nFirst (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc).
1517,1,\n\n5. Algorithm 1 and Algorithm 2 call functions that aren\u2019t described/defined.
1518,1," \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?"
1519,1," The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers."
1520,1,"\n\nOverall: There's the start of an interesting idea here,"
1521,1,"""This paper proposes an iterative inference scheme for latent variable models that use inference networks."
1522,1, \n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family).
1523,1," More background information could be helpful in section 2.2.[[CNT], [SUB-NEG], [SUG], [MIN]] All figures (but in particular Figure 3) need more informative captions"
1524,1,"\nThey also examine random masking, eg a sparsification of the updates, that retains a random subset of the entries of the gradient update (eg by zero-ing out a random subset of elements)."
1525,1,n2. Given enough time would the basic RL agent reach similar performance? (Guessing no...) Why not?\n3.
1526,1," Moreover, the proposed experimental paradigm appears flawed."
1527,1,"There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences."
1528,1," For now, I think the\nsubmission is good for a weak accept "
1529,1,I believe that using cross-entropy with softmax should also push semantically similar labels to be nearby in the weight space entering the softmax.
1530,1," \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments."
1531,1,"\n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value."
1532,1, The experimental evaluation is now far more solid.
1533,1, 2) when two neurons in a layer compute the same function.
1534,1," \n\nIn general, this is an interesting direction to explore, the idea is interesting,;"
1535,1,The temporal truncation also requires the use of a specialized MDP formulation with an explicit and fixed time horizon T.
1536,1,"""This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches."
1537,1, The number of target words can be then derived and they're all predicted in parallel.
1538,1," I did not check the proof of this result in detail, but it appears to be correct."
1539,1, For example GCN has a representation\nspace (latent) much smaller than DSCG.
1540,1,"\nI have a few comments and questions:\n1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise?[[CNT], [PNF-NEU], [QSN], [MIN]] If bit-wise, can you elaborate why?[[CNT], [PNF-NEU], [QSN], [MIN]] I might have missed something.[[CNT], [CNT], [CNT], [CNT]]\n2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c?"
1541,1, The paper mentions a metabolic cost that is not specified in the paper.
1542,1," But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.\n\n """
1543,1,"\n- Finally, a minor point: I will challenge the authors to justify their claim\n  that the learned generative model is \""useful\"" (their word)."
1544,1,"""Summary:\nThis paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017)."
1545,1," Could you explain it in more details?\n\n"""
1546,1,  It\u2019s not properly controlled.
1547,1, An alpha-divergence formulation is considered to combine both methods.
1548,1,"  I suggest TR be replaced by \""Stochastic TR\"" everywhere."
1549,1,"\nTo me, the execution seems sound."
1550,1,"\n- the filtering of high-reward trajectories is what estimation of distribution algorithms [2] do as well, and they have a known failure mode of premature convergence because diversity/variance shrinks too fast."
1551,1, How many were used for training and testing?
1552,1,"n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify."""
1553,1,\n- Many subscripts are used without explanation.
1554,1," e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations."
1555,1," It only considers a slight modification into the loss function by adding a trace norm regularization."""
1556,1,\nAnother thought on this: is it possible to integrate the trigram occurrence with summarization reward?
1557,1, They show results on CIFAR-100 and ImageNet (as well as mini ImageNet).
1558,1,  Can you explain better (e.g. in the supp mat)\nthe effect of this for different values of q_i?
1559,1, Such an example would dramatically improve the paper's readability.
1560,1," \n\nVisually, these perturbations seem to have strong, oriented local high-frequency content \u2014 perhaps they cause very large responses in specific filters in the lower layers of a network, and conventional architectures are not robust to this? "
1561,1,"""In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs."
1562,1,"Theoritically, it solves the soft attention problems."
1563,1, Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?
1564,1,"\n\n- I don't think the content on pages 12, 13, and 14 adds much to the\n  paper---consider moving these to an appendix."""
1565,1,"\n\u201cOn optimization methods for deep learning,\u201d in Proceedings of the 28th\nInternational Conference on Machine Learning, 2011, pp. 265\u2013272.\n\n) \n\nor even a variant of BFGS which makes a block-diagonal approximation to the Hessian with one block per layer."
1566,1,\n\nThe proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order.
1567,1,   An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.
1568,1, It is somewhat unfortunate that the paper doesn\u2019t give more detail about the precise hyper-parameters involved and that there is no comparison with the associative LSTM from Danihelka et al.
1569,1,"""Summary: the paper proposes a new insight to LSTM in which the core is an element-wise weighted sum."
1570,1, What are \u201cproduction percentages\u201d?
1571,1,"  \""A survey on multi-view learning.\"" arXiv preprint arXiv:1304.5634 (2013)."
1572,1, This context is conditioned upon to generate missing words.
1573,1," Do you mean that the sentence contains l words?[[CNT], [CLA-NEG], [QSN], [MIN]] It could be interpreted that the span has l words."
1574,1," But this is not the kind of noise found in many applications, and this is clearly shown in the performances on real data (not always improving w.r.t state of the art)."
1575,1," Cognitive Psychology, 71, 55-89.\n\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016). "
1576,1," Although of course proving things about the actual outcomes of optimizing a deep network is extremely difficult, it would be helpful to have some kind of understanding of how the class of networks in use affects the solutions."
1577,1, RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.
1578,1," and showed that under this framework for smooth loss functions when not too much robustness is requested,"
1579,1, \n\nMinor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix).
1580,1,"At last, they also establish the sample complexity for recovery."
1581,1,\n\n2. Label embedding learning has been investigated in many previous works.
1582,1, The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task.
1583,1," Under such conditions, Corollary 6 of MPCB also reads as s^n.."
1584,1,\nCons\n-Experimental evaluation limited
1585,1," What happens when the encoder is optimized separately for each data point during training as well as testing?"""
1586,1," However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful."
1587,1,"\n[3] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee. Decomposing Motion and Content for Natural Video Sequence Prediction. In ICLR, 2017\n"""
1588,1,\nThe techniques used depend on previous work.
1589,1,"""The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm."
1590,1, In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n 
1591,1,"\n3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?)"
1592,1,"  Also, why is the same learning rate used for both small and large batch training for Figures 3 and 4?"
1593,1,\nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference.
1594,1,"\n[2] TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning\nhttps://arxiv.org/abs/1705.07878\n[3] Parallel SGD: When does averaging help? \nhttps://arxiv.org/abs/1606.07365\n\n"""
1595,1,\n\n- The improvement over the original models are of the order of less than 1 percent.
1596,1,  the rest of the experiments hinges on the definition of \u201cPIR\u201d (positive interaction rate)using a model of human interaction.
1597,1,\n\nSignificance\n==========\nHaving an RL approach that can benefit from truly off-policy samples is highly relevant.
1598,1,\n6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful.
1599,1,"""This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence."
1600,1, We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label.
1601,1,"""Summary\nThe paper presents an interesting view on the recently proposed MAML formulation of meta-learning (Finn et al)."
1602,1,"""\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful."
1603,1,\n\nI really like the reverse perplexity measure.
1604,1, Results are also interesting
1605,1, The idea is introduced clearly and rather straightforward.
1606,1," At the moment, the description in section 3 is fuzzy in my opinion. [[CNT], [PNF-NEG], [CRT], [MIN]]Interesting information could be:\n- how is the performance of the NMT system?"
1607,1,"   The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there."
1608,1," Indeed, in the current formulation the objective is completely dominated by the order-3 tensor factor, because it contributes O(d^3) terms to the objective vs O(d^2) terms contributed by the matrix part."
1609,1, The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters.
1610,1," For example, the introduction has some parts that look more like related work (that is mostly a personal preference in writting). Also in Section 3, the paragraph for distance functions do not provide any insight about what is used, but it is included in the next paragraph (I would suggest either merging or not highlighting the paragraphs)."
1611,1," They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice."
1612,1," If the performance (hit rate or coverage) of this paper is near stoa methods, then such experimental results will make this paper much more solid"
1613,1," In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small."
1614,1,The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima.
1615,1," The visualizations are interesting and provide some general intuition, but they don't yield any clear novel insights that could be used in practice."
1616,1, Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues.
1617,1,"\n\nThe sensitivity of the evaluation metric defined in equation 2 to the choice of hyperparameters of the classifier and the metric itself (e.g., alpha) is not evaluated. "
1618,1, Do the authors mean\nexp(o_t(x;w)) = 0 ?
1619,1,"""This paper applies the word pairs, instead to bag of words, to current RBM models"
1620,1, It is hard to understand the experimental setup of each experiment and what the conclusions are.
1621,1,"""This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks."
1622,1,\n\nI have a few questions on the motivation and the results.
1623,1," The authors point it out, but a further investigation might be interesting."
1624,1, What is the output (o_t) of the network?
1625,1," \n\nCons:\n(1) Authors claim that ISRLU is faster than ELU, while still achieves ELU\u2019s performance."
1626,1,"\n\nIn particular, the authors take an already existing dataset, design a trivial convolutional neural network, and report results on it."
1627,1," For example, the experimental result on structured QA task (section 3.1), where it states that the performance different between models of With-NE-Table and W/O-NE-Table is positioned on the OOV NEs not present in the training subset."
1628,1,\n-- Proposed architecture is general enough to be useful for other sequence-to-sequence problems
1629,1,"""The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL)."
1630,1,"  I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics)."
1631,1,\n\nComments on the details of the paper:
1632,1, Experiments are clearly described and results are significantly better compared to state of the art.
1633,1, But we cannot find any experimental results that is related to the effectiveness of proposed method and considered assumptions.
1634,1,"""The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields."
1635,1, This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier.
1636,1,"\n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read."
1637,1, The approach is evaluated in a multitask grid world domain.
1638,1," At the cost of additional notation, this restriction could easily be lifted."
1639,1,\n-            Sec 4.2: \u201cA straightforward\u2026\u201d: I think it would improve readability to refer back to the to the previous equation (i.e. H) such that it is clear what is meant by \u201cstraightforward\u201d.\n-
1640,1, Does the argument hold for general noise distributions ?
1641,1,"""I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity."
1642,1,"\n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings)."
1643,1, Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder.
1644,1,"  The illustration about the problem is clear, as well as the explanation for the formulations."
1645,1,"  Indeed the paper perhaps anticipates this perspective and preemptively offers that \""variational inference is a qualitatively different optimization problem\"" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work."
1646,1,The omission is conspicuous.
1647,1,"  Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution."
1648,1," The approach is evaluated in a tabular domain (i.e., rooms) and Atari games."
1649,1,\n\n2) The experimental evaluation is not convincing:\n- the selection of competing methods is not sufficient.
1650,1,\n \n\u201cThe counterfactual return is that by which we extend reward \u2026\u201d need to be rewritten.
1651,1,"n3. It is not clear how the adjacency information is used.\n"""
1652,1, So this paper could have real-world impact.
1653,1,"\n\nMy comments are the following:\n\n1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks.[[CNT], [null], [QSN], [MIN]]\n\n2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method."
1654,1,\n\nSummary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs.
1655,1,"""The paper argues for structured task representations (in TLTL) and shows how these representations can be used to reuse learned subtasks to decrease learning time."
1656,1," Thus, I would like to see a comparison between SN with vanilla DNN. """
1657,1, But in that case much of the theoretical story goes out the window.
1658,1,\n- on page nine in the last paragraph there is the word 'flow' missing: '.. estimating the optical [!] between 2 [!] images.'
1659,1," Essentially, if I understand correctly, this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier (which is not that away from the softmax classifier that CNNs usually use)."
1660,1," Some related works are mentioned in the paper, but those are spread in different sections."
1661,1,"(2) the latent variable parameters are functions of a CNN, "
1662,1,"\n\nThe first 3 sections of the paper are very clearly written,[[CNT], [CLA-POS], [APC], [MAJ]] but the remainder has many typos and grammatical errors (often word omission).[[CNT], [CLA-NEG], [CRT], [MIN]] The draft could use a few more passes before publication.\n"""
1663,1, This is a crucial parameters that is however not discussed nor analysed in the paper.
1664,1,"\n\nIt appears that the output for kennen-o is a discrete probability vector for each attribute, where each entry corresponds to a possibility (for example, for \""batch-size\"" it is a length 3 vector where the first entry corresponds to 64, the second 128, and the third 256)"
1665,1,"\n\nEq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content."
1666,1,  This perspective leads to some interesting theoretical results and some new interpretation.
1667,1,"\n \n \nMajor\n1. What is the model/baseline in Tables 3, 4, 5?"
1668,1,What is the variance of the mini batch estimate?
1669,1,\n\n4. The transferrability of features in Section 6 is an interesting problem to explore.
1670,1,\n\nOther issues:\n\n1) Is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $\\tilde{x}_i$?
1671,1,\n\n# Novelty and Significance\nThe proposed idea is novel in general.
1672,1,"""The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language."
1673,1,\n\nSome roughly chronological comments follow:\n\nIn the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet.
1674,1, I think even alphaGo uses some form of supervision.
1675,1, The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table.
1676,1, Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.
1677,1," More details follow.\n\n\nComments:\n1. All architectures and objectives (both classic and PIB-based) are trained using a single, fixed learning rate (LR)."
1678,1," The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed."
1679,1," After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding."
1680,1," For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization)"
1681,1, An existing node embedding method is used to learn vector representations for the nodes
1682,1,  Please state precise results using mathematical formulation.
1683,1," This is the problem where given just one labeled image of a class, the aim is to correctly identify other images as belonging to that class as well."
1684,1," Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline."
1685,1,  Figures 3 and 5 are\nconvincing evidence that MCMCP compares favorably to direct sampling of\nthe GAN feature space using the classification images approach.
1686,1," Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning"
1687,1,  The results support the method's utility.
1688,1, Am I misunderstanding something?
1689,1,"\n\nComment: I kinda like the idea of using chart, and the attention over chart cells."
1690,1,\n\nThe paper appeared to have be rushed. The presentation is not always clear.
1691,1, The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary.
1692,1,"\n\nA final note: this paper was difficult to read due to many grammatical errors and unclear or misleading constructions, as well as missing citations (e.g. sec 2.1)."
1693,1," \n\nBased on the theory developed, the paper presents a practical algorithm."
1694,1, It should be a proper control.
1695,1," In Fig3 (b), it seems assortativity is over-fitted beyond 40k iterations."
1696,1,"\n\nIt\u2019s a nice, simple idea."
1697,1,"\n-  Section 2.1, paragraph nr 5, \""algorihtm\"" -> \""algorithm\""\n"""
1698,1, The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.
1699,1," However, such batch approaches for tensor factorization are not new and I am quite skeptical about their correctness (see above)."
1700,1,"  On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships."
1701,1, \n\nThey choose to analyze mazes because they have many nice statistical properties from percolation theory.
1702,1,"  If this is the case, how to prevent it from happening?"
1703,1, Are we talking about a multi-tenancy setting where multiple processes execute on the same device
1704,1, and the paper is clearly written and easy to follow.
1705,1,\n \nIt is hard to judge the significance of the results on the left side of figure 2.
1706,1," Description of the model architecture is largely done in the appendix, this puts the focus of the paper on the experimental section."
1707,1," There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma."
1708,1, The paper could be more focused around a single scientific question: does the PATH function as formulated help?
1709,1, Specifically they take text and add common sense knowledge from concept net.
1710,1,"\n\nThe paper reports a comparison of real-valued and complex-valued neural networks, controlling for storage capacity (with an interesting discussion of controlling for capacity in terms of computational inference)."
1711,1,I believe this type of issue has to be examinated \nfor this type of approach to be widely use in inverse physical problems.
1712,1,"  Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution.\n"""
1713,1," The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3)."
1714,1," During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa."
1715,1,"""Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights."
1716,1,\n\nFigure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls
1717,1, \n\nThe proposal is practical.
1718,1," \n\nBoth in terms of general visual classification (only MNIST is used, while it would be nice to see results on CIFAR and/or ImageNet as in Bendale&Boult 2016), as in exploration of different scenarios (different number of unseen classes, different samplings) and ablation of the method (independent training, using OCN for hierarchical clustering, influence of Auto Encoder)."
1719,1," The experimental results on CIFAR-10, CIFAR-100 and SVHN show that CrescendoNet outperforms most of the networks without residual connections."
1720,1,"""The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space."
1721,1," Also, after equation (6) there is a reference command that is not compiled properly.\n\n"""
1722,1, Following a short review section per section.
1723,1," \n\nSecond, please report the results of the proposed method with comparable architectures used in previous methods and state clearly the number of parameters in each model."
1724,1,  The method relies on sentiment pre-processing from GloVe and image pre-processing from Inception.
1725,1, The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).
1726,1,"\"" I don't think this is a fair comparison."
1727,1,"\n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n"""
1728,1,"\n\nThe point is, \n\na) The second term will introduce  low correlation in saturated vectors, then the will be informative."
1729,1, How would it compare to other methods on those problems?
1730,1,"\n5. While the algorithm empirically improve over k-means,"
1731,1, My worry is if you're compressing these networks with your method are the weights not treated as binary anymore?
1732,1,\n\n3. The fact that partial observability helped to alleviate the credit-assignment noise caused by the missing customer penalty might be an artefact of the setting.
1733,1,"\n\nThe paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs."
1734,1,. The proposed approach will be useful when the new domain does not have enough data available.
1735,1," Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair."
1736,1, its motivation is not clear for\nme.
1737,1,"\n\nQuality: The quality of the writing, notation, motivation, and results analysis is low"
1738,1,"   \nFirst, there is quite a bit of recent work on learning to teach and curriculum learning."
1739,1," In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps."
1740,1," \n\nI missed a \""related work section\"", where authors clearly mention previous works on similar datasets."
1741,1,I strongly suggest the authors add some ablation studies.
1742,1, The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces.
1743,1," Since L is NON Convex, it could not be automatically considered as bounded."
1744,1," If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to. "
1745,1," This second part was confusing - although the maze can be viewed as a graph, many other works apply ConvNets to maze environments [1, 2, 3], and their work has little relation to other work on graph CNNs."
1746,1, The literature is still healthy today.
1747,1," In particular, I continue thinking that the contribution is limited."
1748,1,"""This paper adds an interesting twist on top of recent unpaired image translation work."
1749,1," \n\nMore detailed comments\n1. Instead of considering multi-class classification as one-vs-all binary classification, can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays."
1750,1," Particularly with some clarity on the experiments, I would be willing to increase the score."
1751,1, It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works.
1752,1," I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation."
1753,1," Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services."""
1754,1, Though the grounding or attention is performed for each word at each location of the visual map.
1755,1, What was the goal here?
1756,1,"\n\nWhile intriguing, a lot more work would be required to publish this at ICLR."
1757,1, The theoretical results in the paper are supported by experiments.
1758,1," Also, the number of hidden units in the 2 layer MLP from para 5 in Sec 2."
1759,1,"  \n\nThe proposed algorithm seems novel,"
1760,1, The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased.
1761,1," i.e., are they randomly permuted between layers so that the blocks mix?"
1762,1," Finally, experimental results for the task of image classification are reported using the MNIST, Fashion-MNIST, and CIFAR datasets."
1763,1," Also, the captions need to describe more details about the figures."""
1764,1,\n-The writing needs more work.
1765,1,"\n\n(2) Moreover, as the authors rightly point out, covariance functions of the form used in (4) have already been proposed."
1766,1,"\nBased on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer."
1767,1,"""This paper introduces a generative approach for 3D point clouds."
1768,1," \nIn fact, the entire motivation, at least for me, never went beyond the simple fact\nthat this happens to be a good way to improve performance."
1769,1,", but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules either"
1770,1,\nI ignore the complexities of n-step learning and discount factors for clarity.
1771,1," (It's fine to have results, but currently I can't tell under what conditions the results apply and under what conditions they don't. E.g. are there any extensions of this that apply to non-linear networks?)"
1772,1," It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction. """
1773,1," \n\nPrevious applications of TS to MDPs (Strens, (A Bayesian framework for RL) 2000; Osband 2013) commit to a posterior sample for an episode."
1774,1,"  Or, perhaps train a network on two datasets, one which is not learnable (iid random labels), and one which is very easily learnable (e.g., linearly separable)."
1775,1," We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision."
1776,1,   It's not like people expected complex-valued networks to somehow be extremely effective for the tasks discussed here -
1777,1,\n\nThe general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.
1778,1,"  Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general."
1779,1," It is unclear why discriminative training, such as MMI, sMBR, and lattice-free MMI, is mentioned in section 2.3."
1780,1, This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}).
1781,1," From a human standpoint, in those cases, the smoother meshes would in fact be preferable."
1782,1, Are you perhaps using natural logarithms to estimate and plot I(Z;Y)?
1783,1, It is also important to be able to benefit from off-policy data.
1784,1, Dict is much more useful than spelling here.
1785,1,"  Also, the state if the art in IRL and learning from demonstration is lacking a lot of references."
1786,1,"\n\n2) Page 2 minor typos\nWe study training problem -->we study the training problem\nIn the regime training objective--> in the regime the training objective[[CNT], [CLA-NEG], [DFT], [MIN]]\n\n3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al"
1787,1, (ii) \u201cStochastic Variational Deep Kernel Learning\u201d (NIPS 2016);
1788,1," In particular, the use of LSTMs helps take into account interdependencies between pattern labels."
1789,1,"""The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive."
1790,1," At the same time, no new architectures are presented to address these limitations."
1791,1,"\n5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser?"
1792,1, An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.
1793,1, This is the big challenge of SRM because training different variations of a deep neural networks to T-1 epochs can be a very time consuming process.
1794,1, A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.
1795,1,The paper is not written well and the idea seems to be trivial compared with previous methods.
1796,1," While the measure shows interesting trends towards a linear behaviour for simpler methods,"
1797,1," This is an interesting phenomenon and deservers further study, as currently doing the \u201cwrong\u201d things is better than doing the \u201cright\u201d thing."
1798,1,\n\nThe interpretation of figure 2 is off.
1799,1,",  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density."""
1800,1," Could the authors perhaps comment on:\na) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization (given the psd approximation to the Hessian), or does it only make sense after convergence? "
1801,1, The \u2018semantic\u2019 property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter).
1802,1,\n\nIn terms of specific criticisms:\n\nI found the motivation section to be somewhat weak.
1803,1,"""The authors adapts stochastic natural gradient methods for variational inference with structured inference networks."
1804,1,"\n- In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML."
1805,1," In essence, Machado et al. (2017) showed that, in the tabular case, if you interpret the difference between PVFs as pseudo-rewards you end up with useful options."
1806,1,"\n\n3) Most importantly, it shows that NMT\u2019s handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different."
1807,1," The only baseline comparison provided is the k-means clustering, but the comparisons were somewhat unfair."
1808,1," Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM."
1809,1," These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error)."
1810,1, but the contribution seems quite limited.
1811,1," It is argued that the ML approach has some \""discrepancy\"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity."
1812,1, It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method.
1813,1," In MNIST experiment, for example, better numbers are reported for larger LSTMs."
1814,1,"  \n\nThe correspondence of spiking neurons to sigma-delta modulation is incorrectly attributed to Zambrano and Bohte (2016), but is rather presented in Yoon (2017/2016, check original date of publication!). \n\n"""
1815,1,"\n- If the main contribution is the dataset, perhaps the claim that it is \""uniquely diverse\"" could be justified with some quantitative arguments / statistics, comparing to other datasets."
1816,1,  Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time.
1817,1,\n \nThis work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning.
1818,1,"""# Summary\nThis paper presents a new external-memory-based neural network (Neural Map) for handling partial observability in reinforcement learning."
1819,1,"\n\nFirst, as explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes)."
1820,1, The paper is very easy to follow.
1821,1, How long does the encoding time take with 10 million sentences?
1822,1,"""This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE."
1823,1," \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training."
1824,1,"""After reading the rebuttal:\n\nThis paper does have encouraging results."
1825,1," As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps."
1826,1,"\n- This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results."
1827,1, \n\n-BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution.
1828,1, The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search.
1829,1,  The objective is ambitious an deserve attention.
1830,1,"  In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting."
1831,1," There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound."
1832,1," It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing."
1833,1,\n\n2. Develop a similar relaxation for deep neural networks.\nThe author already mentioned that they are pursuing this direction.
1834,1," \n\nIn 3.1, at point (2), the authors mention that DSC filters are learned from the\ndata whereas GC uses a constant matrix."
1835,1, Can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work section.
1836,1, However I think the benefits are not clear enough.
1837,1, You should mention this\n3) Last paragraph of page 2 should not be in the intro
1838,1,"\n\nPage 3: \""using virtual observations (originally proposed by Qui\u00f1onero-Candela & Rasmussen (2005) for sparse approximations of GPs)\""\n\nThe authors are citing as the origin of virtual observations a survey paper on the topic."
1839,1," The different setting than the fundamental GAN-like setup of those models is intriguing, but specifying a cost function between the source and the target domains feels exceedingly unnatural compared to specifying a cost function just within one domain as in these other models."
1840,1,"""Deep neural networks have found great success in various applications."
1841,1,Is there a more sophisticated method that can avoid redundancy without this heuristic?
1842,1, but I do not understand the intuition behind the success of analogizing graph with images.
1843,1, \n\nEquation 3 sparsifies (i.e. prunes the edges) of a graph -- namely $re_{G}$.
1844,1,  It would be nice to have an even informal explanation.
1845,1," \""Training very deep networks.\"" Advances in neural information processing systems."
1846,1, What\u2019s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function?
1847,1,"""The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X."
1848,1, The paper also shows major speedup compared to ELU and TANH (unit-wise speedup)
1849,1,"""This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. "
1850,1, I see a benefit in interpretation which can help.
1851,1," I know how hard is to make robotic tasks work...   \n4) I\u2019m not sure that the comparison of the suggested architecture to one without any underlying additional variable Z or context (i.e., non-Bayesian setup) is fair. "
1852,1, It would also be nice to compare with Soudry et. al.
1853,1," Besides, giving interesting observations is not good enough."
1854,1,\n\nWhat is the size of the vocabulary used in all the experiments?
1855,1, This paper extends the existing results in some subtle ways.
1856,1,"\nThe experimental section is extensive, and offers new insights into both the presented algorithm and baselines."
1857,1, It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication.
1858,1, What is a powerful algorithm?
1859,1,\n\n- The structure of Section 3 needs to be improved.
1860,1, These are pros of the approach.
1861,1," Perhaps this is due to a lack of available graph training data, but it doesn't seem to make a lot of sense."
1862,1, This paper's introduction provides precisely 1 statement to suggest that may not work for deep learning.
1863,1,".\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017)"
1864,1,"Where the Hazan paper concerns itself with the system id portion of the control problem, this paper seems to be the controls extension of that same approach."
1865,1,"\n\nThe model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements."
1866,1, I thus recommend a clear rejection.
1867,1,"\n\nHowever, the originality and significance of this work is a significant drawback."
1868,1, Thus it is not a novel contribution.
1869,1," Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n"""
1870,1,  These are short duration sates lasting only seconds.
1871,1," Most of the citation styles are off (i.e., without parentheses)."
1872,1, The paper is easy to follow and the idea is interesting.
1873,1, To me it looks like the probability of it being larger than zero is something like 2/3.
1874,1,"\n\nFurther remarks:\nIt would be interesting to see the size and position of the center of mass attacks in the appendix.[[CNT], [CNT], [DIS], [MIN]] The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations."
1875,1,"""The authors present a method for learning word embeddings from related groups of data."
1876,1," Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact."
1877,1, Were these\n   viewpoints chosen randomly?
1878,1," The results are not particularly impressive, but that is not an issue for me."
1879,1,\n1. Benchmarking Deep Reinforcement Learning for Continuous Control\n2. Deep Reinforcement Learning that Matters
1880,1,"  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL."
1881,1," However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6."""
1882,1, \nThe main idea of the paper is to project examples into an RK Hilbert space\nand performs convolution and filtering into that space.
1883,1," Afterward, the authors argue that deterministic network cannot adequately several modalities."
1884,1,"  I'm not an expert in this area but the contribution seems relevant to me, and enough for being published."""
1885,1,"  The authors define the \""good\"" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans."
1886,1," and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST."
1887,1," In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks."
1888,1,\u201d\n7. Page 7: \u201cThis shows that a good initialization is important for this task.
1889,1, The current version tends to mix everything together and makes the misleading claim.
1890,1, \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained?
1891,1,  How does the performance change with the number of frames between checkpoints?
1892,1, What are the motivations for this particular approach?
1893,1, I think this is not sufficient.
1894,1, Why is paying to observe activations the one chosen here?
1895,1,"\n\nReferences:\nHenaff, Mikael, Arthur Szlam, and Yann LeCun."
1896,1," So I suggest that results for RL should be reported with and without intra-attention on both datasets, at least on the validation set."
1897,1," Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements."
1898,1,\n\nThere have been a number of MTL methods based on task clustering.
1899,1,\n- In addition it is unclear whether this synthetic process would actually generate results that are clinically useful.
1900,1," Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos."
1901,1, unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario.
1902,1, A hatch of how to learn the overall process is presented.
1903,1,n- in the experiments they trained the network on 64 x 64 patches and achieved convincing results
1904,1," However, the presented results focus on the performance on held-out data instead of improvements in training speed."
1905,1, The 1d results are not difficult to derive from previous results.
1906,1,"  \n \nMy strongest criticism for this paper is against the claim that Tumblr post represent self-reported emotions and that this method sheds new insight on emotion representation and my secondary criticism is a lack of novelty in the method, which seems to be simply a combination of previously published sentiment analysis module and previously published image analysis module, fused in an output layer."
1907,1,  The fragility of DNN models to marginally perturbed inputs themselves is well known.
1908,1,\n\nCons:\n- Some choices leading to the optimization problem are not sufficiently explained.\n- The theoretical discussion could be improved.
1909,1,\n\n1. Why is this model successful?
1910,1," The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition."
1911,1," Without a reasonable baseline, it is hard to see how difficult is this dataset and this problem and can't say anything about the significance of this problem in this paper."
1912,1,"""# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples."
1913,1," Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form."
1914,1, What happens for deeper and narrower baselines that have a similar number of parameters?
1915,1, The topic is an important one and a very difficult one.
1916,1,"\n\nI increased rating for the paper,"
1917,1,"  I think just counting number of layers as a complexity is not reasonable when we deal with images, with respect to  what preserves contents and what matches texture or style"
1918,1," While this task is artificial, it does make sense in the context of what the authors want to show."
1919,1," The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus."
1920,1,"\n\nDespite these questions, though, this paper is a nice addition to deep learning applications on software data and I believe it should be accepted.\n\n"""
1921,1,"\n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models."
1922,1,  Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique.
1923,1,"  In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified."
1924,1, or even how reliable they are since the effectiveness has not been evaluated.
1925,1,"\n\nPaper may be useful to practitioners who are looking to implement something like this in production."""
1926,1,"""This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks."
1927,1, But does not seem to offer significant insight or dramatic improvement.
1928,1,\n\nIt would improve the paper to also discuss that the non-negativity constrained Tucker2 model may be subject to local minima solutions and have issues of non-uniqueness (i.e. rotational ambiguity).
1929,1, The objective to be maximized is a lower bound to 1/alpha * the likelihood.
1930,1, Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.
1931,1,"\n\nManually designing novel neural architectures is a laborious, time-consuming process."
1932,1,"\nThe authors evaluate their methods on two tasks, program generation and molecule generation."
1933,1,  It's much more revealing to compare it to the empirical likelihoods of the words.
1934,1,\n\nAnother point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2.
1935,1, A section devoted to showing what practical problems could be potentially solved by this method would be useful.
1936,1, \n\nSignificance:   The paper does not make a great case for caring about complex-valued networks.
1937,1,"\""\n\nIn Theorem 2, do you need to care about boundary conditions for your equation? "
1938,1," At which point would it break?"""
1939,1," Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting.[[CNT], [EMP-POS], [APC], [MAJ]]\n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch?"
1940,1," \n3, The experimental results look convincing."
1941,1," Also, the authors should make this dataset available for replicability."
1942,1, Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion.
1943,1," The previous SotA result on VGG16 was 5x acceleration with 1% accuracy drop, and here the reported result is 6.2x acceleration with 1.2% accuracy drop."
1944,1,\n-\tThe results presented show that the method is probably not suitable for a practical application yet (high false alarm rate for good detection rate)
1945,1,".\n\n6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models."
1946,1," The paper is well written, the method is easy to implement, and the algorithm seems to have clear positive impact on the presented experiments."
1947,1,"\n- In addition to the above point, I guess the expectation is needed as the original formulation of GAN."
1948,1," The argument for motion accuracy is clear and is clearly stated: it\u2019s the measure that is actually tied to the intended application, which is using action-conditional motion prediction for control."
1949,1, Only the state-reconstruction error is shown now.
1950,1,"\n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct,;"
1951,1, Could the proposed approach be adapted to such cases.
1952,1," \n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin."
1953,1,"\n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function?"
1954,1,\ \n\nWeaknesses:\n- A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary.
1955,1,  \n- The empirical results need improvement to support the paper\u2019s claims.
1956,1,"\nPage 7, \u201cdocument by obfuscated this word\u201d needs to be fixed.\[[CNT], [null], [CRT], [MIN]]nPage 7, \u201coverall aspect of the two first readers\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 8, last para, references needs to be fixed."
1957,1, The\nadded stochasticity and the model ensembling interpretation is probably novel.
1958,1, -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins?
1959,1,Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task.
1960,1," \n\n2) The word embeddings used seem to be sufficient to capture the \""knowledge\"" included in the corpus."
1961,1,\n\nI believe the design choices made by the authors to be valid in order to get things to work.
1962,1,"  Presumaly we need a large corpus of syntax-checked training examples to learn this model, which means that, in practice, we still need to have a syntax-checker available, do we not?"""
1963,1, Is you Theorem 2 somehow an extension? Is Theorem 3 completely new?
1964,1," Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think. \n"""
1965,1,"\n\n---\n\nTypos:\npage 1 first para: \""One the first hand\"" -> \""On the first hand\""\npage 1 first para: \""minimize to probability\"" -> \""minimize the probability\""\npage 3 first para: \""compensate\"" -> \""compensated\""\npage 3 last para: \""softmaxis\"" -> \""softmax is\""\npage 4 sec 2.4: \""similar to the reader\"" -> \""similarly to the reader\""\npage 4 sec 2.4: \""unknow\"" -> \""unknown\"
1966,1," The motivation behind all this is to learn the input features to the SVM as opposed to hand-crafting them, and use the generalization ability of the SVM to do well on tasks which have only a handful of training examples."
1967,1,"\"" In International Conference on Machine Learning, pp. 2034-2042. 2016."
1968,1,\n\nSmaller nitpicks:\n\n> \u201cNew state of the art for evolutionary strategies on this task
1969,1,"\n\nWhen m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary."
1970,1,"  They do not mention whether they train the previous methods on the new dataset, and some of their reported improvements may be because of this."
1971,1," Ideally, this type of approach should allow not only to generate images from an observational distribution of labels (e.g. P(Moustache=1)), but also from unseen interventional distributions (e.g. P(Male=0 | do(Moustache =1))."
1972,1," In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed ."
1973,1,\n\nHow was Figure 5 computed ?
1974,1,"""The authors describe a method for encoding text into a discrete representation / latent space"
1975,1," It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora."
1976,1, \n\nAside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters
1977,1," \n- In particular, why is the dataset not used for the causal controller?"
1978,1,"\n\nA better result, hinting on how \""optimal\"" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist)."
1979,1," As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set."
1980,1," Yet, this is not discussed in the paper."
1981,1,"\nThe authors claim that \u201cBy relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing, which makes it a possibility to perform training directly on real systems."
1982,1,"""This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. "
1983,1," Part of this could be the notion of an \""iteration\"", which was not clear to me how this corresponded to actual time steps."
1984,1,"\nHowever, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach."
1985,1,"""This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization."
1986,1," The experiments are seriously lacking, an ablation study should have been made and the results are not good enough. "
1987,1,"\n\n[*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks."
1988,1,"""In summary, the paper is based on a recent work Balestriero & Baraniuk 2017 to do semi-supervised learning."
1989,1,"\n\nThere is a statement - \""even if the exact local minima is reached, the subsampled Hessian may still have negative curvature\"" - again, there is no evidence."
1990,1," Therefore, I am not convinced that the paper is ready for publication at ICLR'18."
1991,1,"  The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well."
1992,1,"\n  \u2022 On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with. "
1993,1," For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation."
1994,1," For examples of such methods, one may see the paper \""Transform Invariant Auto-encoder\"" (by Matsuo et al.) and references therein."
1995,1," The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks."
1996,1,\n\nComments for the author:\n\nThe paper is well-written and easy to follow.
1997,1,\n2. There are some typos in the related works section and the inferece procedure isn't clearly explained.
1998,1,"\n\nQ: For the multiple outputs, the k neighbor is selected at random?\n"""
1999,1, Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?
2000,1,"  They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods."
2001,1,"\n\nThe empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning."
2002,1," \n- It seems artificial to enforce that \""the attribute-irrelevant part [should] encode some information of images\""."
2003,1,"\n\nNota: \n     - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0?"
2004,1,"\n\nBesides this deficit, the paper does not present a proper statistical setup (e.g. 'Is censoring assumed to be at random? ...) , and numerical results are only referring to some standard implementations, thereby again neglecting the state-of-the-art solution. "
2005,1,\n(b)\tExperimental Results\n2.\tThe performance of the proposed method is not significantly better than other models in MT task.
2006,1," In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem."
2007,1,"\n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?"""
2008,1, the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable. 
2009,1,\n10. I would have liked to see a plot of how the value of lambda changes throughout optimization.
2010,1," This regularization, while helping exploration, do changes the original objective."
2011,1," While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset."
2012,1, \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs.
2013,1, The recent paper by Chen et al (2017) would be relevant here.
2014,1," In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD."
2015,1,"  The adversarial loss is presented as the central conceptual contribution of the paper, but doesn\u2019t actually make a difference in terms of task-relevant metrics."
2016,1,"\n\nComments: I have my concerns about the effectiveness of the notion of privacy introduced in this paper.[[CNT], [null], [SMY], [GEN]] The definition of privacy loss in Equation 5 is an average notion, where the averaging is performed over all the sensitive training data samples."
2017,1," When a tensor is decomposed as a sum of rank-1 tensors (outer products), the number of operations in a DNN forward pass decreases leading to a faster testing runtime."
2018,1," The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned."
2019,1," There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images ""there is no stochastic convolutional layer."
2020,1, The probabilistic framework itself is quite straight-forward.
2021,1, This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques
2022,1," Recently, it has been shown that averaging can be suboptimal for nonconvex problems, eg a better averaging scheme can be used in place [3]."
2023,1, \n\nThis is a significant and timely topic.
2024,1," Why did you use your own implementation of VIN and not Tamar et al.'s, which is publicly shared as far as I know?"
2025,1," \n\nMy main concern is that the result here is purely empirical, with no concrete theoretical justification."
2026,1,"\n\nWeaknesses\n - Although the proposed model is helpful to model counting information in VQA,"
2027,1," Taking into account the Lipschitz constraint and (non-) differentiability of optimal critic functions f are discussed in section 4 and Section 5 proposes a way to penalize candidate functions f that do not satisfy the Lipschitz condition using a tuning parameter lambda, ruling a trade-off between marginal fitting and gradient control."
2028,1, It is indeed a good method for dealing with the variance.
2029,1,Could go either way since the network\n   has to allocate resources to learn other games too.
2030,1,\nCons:  There is not much technical contribution.
2031,1,"  The problem is framed as RL problem, where the state space corresponds to learning configurations, and teacher actions change the state.  Supervision is obtained by observing the learner's performance."
2032,1,"  Thus, this Bayesian perspective can also help explain the observation that models trained with smaller batch sizes (noisier gradient estimates) often generalize better than those with larger batch sizes (Kesker et al, 2016)."
2033,1, It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x.
2034,1,"\n\nMoreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice."
2035,1, Some insights might be useful there.
2036,1," Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification."
2037,1, More advanced attacks need to be considered.
2038,1," A simpler speech task such as Keyword Spotting could also be investigated.\n"""
2039,1,"\n\nMinor points:\n- Is there a benefit to having a model that jointly predicts unit presence and count, rather than having two separate models (e.g., one that feeds into the next)? "
2040,1, Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough.
2041,1,"\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. "
2042,1," In the used model, there are two types of latent features, \""core\"" features and \""style\"" features, and the goal is to achieved by avoiding using the changing style features."
2043,1,"  In the introduction, it would also improve the paper to outline clear points of methodological novelty."
2044,1," Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut."""
2045,1, Yes A3C and TRPO seem like they perform very poorly compared to ARM
2046,1,  \n\nCons and questions:\n1. The presentation of the model is not clear.
2047,1," The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves."
2048,1,. I will give a few examples to highlight the point.
2049,1,"\tIn page 2, the line before the last line, \u201c\u2026 resolbing problem\u201d --> \u201c\u2026 resolving problem\u201d\n"""
2050,1," As a result, in my view, the paper has limited novelty and originality."
2051,1,  This paper shows some results to the contrary when applying RL to complex perceptual observation space.
2052,1, The novelty in the paper is implementing such a regression in a layered network.
2053,1," \n\n[1]\n@article{tatarchenko2017octree,\n  title={Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs},\n  author={Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},\n  journal={arXiv preprint arXiv:1703.09438},\n  year={2017}\n}\n\nIn light of the authors' octree updates score is updated."
2054,1,\n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.
2055,1," The student model is a deep learning model (MLP, CNN, and RNN were used in the paper)."
2056,1," In other words, what is supposed to be the take-away, and why should we care?"
2057,1,"\n\nHowever, there are also some negatives:\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused."
2058,1," When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1."
2059,1,"""In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network."
2060,1," Modern solvers\nroutinely solve instances with tens of millions of non-zeros in the constraint\nmatrix, but require a strong relaxation."
2061,1, The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T.
2062,1,\n\n\nQUALITY\n\nAblation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way.
2063,1, They both could be mini-batched similarly.
2064,1, The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images.
2065,1,"\n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)"
2066,1, (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)
2067,1, It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given.
2068,1," It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \""[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search."
2069,1,\nThe structure of the paper could also be improved.
2070,1,\n- It seems weird that the smoothed logical AND/OR functions do not depend on the number of inputs; that is unless there are always 3 inputs (but it is not explained why; logical functions are usually formalised as functions of 2 inputs) as suggested by Fig 3.
2071,1,\n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components.
2072,1, \n\nPros:\n+ The paper is well-written
2073,1,  \n- I would introduce the do-notation much earlier.
2074,1,\n\nMore generally I'd like to better understand what effect we'd expect this regularizer to have.
2075,1,  The authors argue for the advantages of a generative VAE approach (but without being convincing).
2076,1,"\n\n2.) Equation 1 contains the norm operator twice, and the first norm has no subscript, while the second one has an l_2 subscript."
2077,1,\n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?
2078,1, \nThe model is evaluated on tasks that it was not trained on which indicate that this model learns generalizable latent representations.
2079,1, It is only 0.4 improvement overall on the RACE dataset;
2080,1, This method is algebraic method closely related to spanning sub spaces and SVD.
2081,1,"  The datasets are too small with observations in low dimensions, and I found it not very fair to consider LSTM in such settings."
2082,1," For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words."
2083,1,"\n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')"""
2084,1," For example, how is the performance of a model containing SW-SC or CW-SC without deepening or widening the networks?"
2085,1," Both of the two paper don't have output gate and non-linearity of \""Wx_t\"" and results on PTB also stronger than this paper."
2086,1,\n\nPros:\nThe paper compares different classifiers on three datasets.
2087,1, \n\nA3C is known to be quite high variance.
2088,1," Thus, several factors of variation are excluded a priori and this undermines the significance of the analysis."
2089,1," \n\nOverall, the descriptions of the proposed model (Section 3 - Section 5) are hard to follow."
2090,1,"  ideally, the discrete encoding be simply a discrete approximation of the continuous encoding."
2091,1,\n   - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ?
2092,1," The theory is significant to the GAN literature, probably less so to the online learning community."
2093,1," \n\n* Information dynamics of learning process (Figures 3, 6, 7, 8) -> I am curious as to why you did not run the PIB for the same number of epochs as the SFNN?"
2094,1," While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning)."
2095,1," By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n"
2096,1," Also in the appendix, please restate the lemma that is being proven."
2097,1," For (1), the authors show that the DNN has a tighter bound on the depth."
2098,1," The CrescendoNet is like a variant of the FractalNet, and the only difference is that the number of convolutional layers in Crescendo blocks grows linearly."
2099,1,"""This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images."
2100,1,"\n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress."
2101,1,". But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified."
2102,1," Clearly, string matching would not work because a model could say \u201cdon\u2019t know\u201d whereas some other model could say \u201cunanswerable\u201d."
2103,1," From the navigation task, it seems like the system mainly learns a discover behavior that is better than random motion."
2104,1,"""This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization."
2105,1," Also, comparison needed to fitting GloVe on the entire corpus (without covariates) and existing methods Rudolph et al. 2017 and Cotterell et al. 2017."
2106,1, \n\n-It is unclear to what extends the novelty of the paper (specific architecture choices) are required.
2107,1,"\nc.\tFor BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)?"
2108,1,"  Similarly for the continuous control with sparse rewards environments \u2013 if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning."
2109,1," However, this is not a very sound excuse."
2110,1,"""This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes."
2111,1," Let's assume a 2d latent space, where each quadrant represents one MNIST digit (e.g. 1,2,3,4). You may observe a similar structure in this latent space yet still learn a good generator under even a standard 2d Gaussian prior. I guess my point is, a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it."
2112,1," You could use an NER for W/O-NE-Table and update the NE embeddings, and it should be as good as With-NE-Table model (and fairer to compare with too)."
2113,1,\n\nCons\n- Requires access to a black-box oracle to construct the dataset.
2114,1, But typically easy to train does not involve a specific preprocessing of gradients.
2115,1, DDQN wasn\u2019t proposed as an exploration method so it would be good to justify why this is an appropriate baseline (versus other exploration methods).
2116,1," The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks."
2117,1,"\n6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?)"
2118,1,"Further, experimental results of problems with previously untacked sequence lengths are reported."
2119,1, \n\nWhere exactly is input injection used?
2120,1,"\n\n5. The caption in Figure 1,3, \u201cwith 48 input and hidden units\u201d should clarify clearly."
2121,1,\n\n\nCons: \n- The authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network.
2122,1," In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed)."
2123,1," \n\nI am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. \n"""
2124,1," I recommend to reject the paper."""
2125,1, Experiments results are good for given synthetic scenarios
2126,1,"  The methods presented here could be of interest to those training language models for use in specific systems, and the paper reads reasonably clearly"
2127,1, How do the authors explain this?
2128,1, Have you run some experiments where you vary those parameters?
2129,1,"""In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network."
2130,1," For the 100 layer MLP, it's very hard to train a simple MLP and the training/testing accuracy is very low for all the methods."
2131,1," \n\nThe experimental section is also ok, although not perfect."
2132,1," They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action."
2133,1,"  While for the explicit gradient, it is assessed at the current point, and it is an unbiased one."
2134,1," As the primary contribution of the paper is this method for combining correction with completion, this shortcoming in the paper is pretty serious."
2135,1, A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.
2136,1, The authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model.
2137,1," In particular, authors aim to show that units behave as binary classifiers during training and testing."
2138,1,   \n\nThe Circumplex Model of Emotions (Posner et al 2005) the authors refer to actually stands in opposition to the theories of Ekman.
2139,1," If that is true, then is \\calL(\\theta, \\phi, \\phi_x, \\phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing \\theta? Please clarify?"
2140,1, \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works.
2141,1, Is this protocol derived from some other source?
2142,1,". What does \""biological activities on 11538 targets\"" mean?"
2143,1,"  If the amount is significant compared to task-specific training, then UA/A3C-L curves should start later than standard A3C curves, by that amount of data."
2144,1,"\n\nWhat is a \""base-level learner\""? I think it would be useful to define it more\nprecisely early on."
2145,1, Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices.
2146,1,\n\n2) High spatial resolution images. 
2147,1,\n\nIt could be made clearer how significance is tested given the frequent usage of the term.
2148,1,"""This paper considers the problem of self-normalizing models. This kind\nof approaches, such as NCE (Noise Contrastive Estimation) is very\npromising and important to provide efficient and large vocabulary\nlanguage models."
2149,1," \n\nThe theoretical result of the ProxProp considers the full batch, and it can not be easily extended to the stochastic variant (mini-batch)."
2150,1," \""In these and related settings, gradient descent has started to be replaced by inference networks."
2151,1," It is not clear what are the advantage of deep stacked RNN in that context."""
2152,1," Overall, the proposed method is a relatively simple tweak to softmax."
2153,1,"\n\nThe paper is written well,"
2154,1," As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies."
2155,1, It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings.
2156,1,"  I think there are a lot of little details that don\u2019t seem that explicit to me.[[CNT], [null], [CRT], [MIN]]  How many seeds are run for each curve (are the results an average over multiple seeds)."
2157,1,"  The main contributions of this paper are: \""U-max\"" idea (for numerical stability reasons) and an \""\""proposing an \""implicit SGD\"" idea.\n\nUnlike the first review, I see what the term \""exact\"" in the title is supposed to mean. I believe this was explained in the paper."
2158,1, the paper falls short in terms of providing experimental validation that would demonstrate the latter point.
2159,1,"""The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach)."
2160,1," From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining."
2161,1," With enough data, both might produce similar results."
2162,1," But if it is the case, shouldn\u2019t we have a gradient of Q in (15) too?"
2163,1,"\n4. In section 4.2, the name cluster is a bit confusing with the one in section 3.1. What's the relationship? The symbols C(Y*) and C(X*) are not used afterward."
2164,1," ...\u201d - it is not clear to me, it does look better than the other ones, but not clear."
2165,1, A deep neural network with this integral feedforward is called a deep function machine. 
2166,1,"\n8. What does \""iteration\"" mean in experimental results such as table 2?"
2167,1," Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums."
2168,1, Frames required from the environment? 
2169,1," And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other)."
2170,1,"\n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks."
2171,1,  Please refer to this paper \u201cInteractive Attention Networks for Aspect-Level Sentiment Classification\u201d.
2172,1,"\n\nOverall, the paper is well-written and explores the technical details of the presented approach."
2173,1,"\nIf there is a gain to learn a shared embedding manifold, which is plausible, this gain should be evaluated between a baseline, that learns separately the games, and an algorithm, that learns incrementally the games."
2174,1, \nLearning good similarity functions is also not novel [3] and Equations\n(6) and (7) corresponds to learning these similarity functions.
2175,1,"\n1. GAN are sufficient to learn \u00ab\u00a0semantic mappings\u00a0\u00bb in an unsupervised way, if the considered networks are small enough\n2. Controlling the complexity of the network, i.e. the number of the layers, is crucial to come up with what is called \u00ab\u00a0semantic\u00a0\u00bb mappings when learning in an unsupervised way."
2176,1," \n\nDetails:\n1. The citation format used in the paper is not appropriate, which makes the paper, especially the related work section, very inconvenient to read."
2177,1, Detailed critiques are as following:1. The idea proposed by the authors seems too quite simple.
2178,1,"\n\nIt does not make sense to say that \""The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations\"", and then proceed to do exactly that (in canonical coordinates)."
2179,1," The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule."""
2180,1," \n\nAs a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. """
2181,1,"   Even if there is randomness, can the adversary take actions that account for that randomness?"
2182,1," I assume that it involves \\hat{M}, but it would be good to formally define this notation."""
2183,1," The only quantitative evaluation is in Table 1, and it seems the model is not able to generalize reliably to all rotations and all digits."
2184,1, The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared.
2185,1," However, the fact that reward prediction loss is used to train TreeQN significantly weakens this claim, and there is no such an evidence in the paper."
2186,1," The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings."
2187,1,"\n\nMy main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?)."
2188,1,    \n\n\nI thank the author for their detailed answers.
2189,1,"""This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting."
2190,1, The update rule seems to be clearly wrong.
2191,1, All these factors should play a very significant role in the experimental validation of their hypothesis.
2192,1," \n2. The authors should do a better job at explaining the details of the state definition, especially the student model features and the combination of data and current learner model."
2193,1," The authors fail to include the most standard baseline attack, namely FSGM."
2194,1,"\n\nOverall, the proposed method is novel \u2014 even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant."
2195,1," There are way better curves reported in the literature (eg in \""Unitary Evolution Recurrent Neural Netwkrs\"" or \""Recurrent Batch Normalization\"")."
2196,1,\n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.
2197,1, The results are promising.
2198,1,  The contribution of this paper seems to be the specific way the decomposition is used in training the DNN.
2199,1," The paper shows that even under a random policy, the eigen options can lead to purposeful options\n\n"
2200,1,\n\n2. Insufficient attack evaluations - the attacks used in this paper to evaluate the performance of PCL are either weak (no longer state-of-the-art) or incorrectly implemented.
2201,1," To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties. "
2202,1, This is especially the case for the RNN experiments.
2203,1, I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models.
2204,1,"  It is unclear from the text that this is an ongoing process, in parallel to the feedforward pass."
2205,1,"  Style transfer has two key components, the first is how well it is transferred to the target style; second is how well it preserves the original contents."
2206,1,"""The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks."
2207,1," In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017."
2208,1, I would recommend the authors to rerun these experiments but truncate the iterations early enough.
2209,1," \n\nHowever, the motivation above is not justified well."
2210,1,"\n\n\nWhile this paper addresses an important problem, in its current form the novelty and analysis are limited and the paper has some presentation issues."""
2211,1, the experiments are somewhat lacking
2212,1, I will take this point to be moot.
2213,1,"\n\nBecause of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline."
2214,1,\n* Coupled ensembling requires joint training of all nets in the ensemble and thus is limited by the size of the model that can be fit in memory. 
2215,1,"\n\n\nMinor comments and questions:\n7. Mutual information is typically typeset using a semicolon instead of a comma, eg. I(X;Z).\n8."
2216,1," \n- Learning Steerable Filters for Rotation Equivariant CNNs, Weiler et al."
2217,1, Do you observe a similar performance vs FNNs in existing methods?
2218,1, This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.
2219,1," \nRegarding the MiniImageNet experiments, I read the exchange on TCML and agree with the authors of the paper under review."
2220,1, There is some empirical testing\nwhich show the presented method in a good light.
2221,1, Why is this an interesting problem?
2222,1,\n\n\nCons:\n- The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones.
2223,1," In International Conference on Learning Representations (ICLR)."""
2224,1,  It may inspire more useful research in the future.
2225,1," If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?"
2226,1," In the equation, the notation x_t and i_t is not clearly defined."
2227,1, the claims are a bit strong for CNN and need further theoretical and experimental verification.
2228,1, Are there theorems to be had?
2229,1, What is the accuracy drop after fine-tuning in both scenarios?
2230,1,\n\nFigure 4 seems kind of strange.
2231,1,"\n\nTheir experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun)."
2232,1,"\n\nThe writing of the paper needs a lot of improvement.\n\n\n\n\n\n\n"""
2233,1,"\n\nThe paper claims that \""discriminative approaches\"" need to iterate over all possible entity pairs to make predictions. "
2234,1,\nWhat is exactly the communication protocol?
2235,1,". Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures."
2236,1,"   This seems a bit surprising, or at least would seem to warrant further explanation."
2237,1,"\n\n(*) Cons\n-minor: in the title, I find the expression \""unsupervised clustering\"" uselessly redundant since clustering is by definition unsupervised."
2238,1," Therefore, (a) computing the symmetric and not non-negative symmetric decomposition does not give any good theoretical guarantees (while achieving such guarantees seems to be one of the motivations of this paper) "
2239,1, \n  Positives:\n- the output kernel update is well justified
2240,1,\n\n\nEVALUATION:\n\nI think exploring and understanding entropy-regularized RL algorithm is important.
2241,1,"  However, for complex domains with sparse reward (e.g. Montezuma\u2019s Revenge) parameter space noise is just not going to get you very far."
2242,1,". Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea."
2243,1," Also, if there is an intrinsic 2D hidden structure in the data, then imposing a 2D representation can help (as a sort of a prior)."
2244,1,"\n\n3. \u201cThe average recent iterate\u201c described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose \u201c3\u201d, and the effectiveness of different choices should be discussed, as well as the \u201c24\u201d used in state features."
2245,1," \n- Male -> Bald does not make much sense causally (it should be Gender -> Baldness)... Aha, now I understand: [[CNT], [null], [DIS], [MIN]]The authors seem to switch between \""Gender\"" and \""Male\"" being random variables.Make this consistent, please. [[CNT], [null], [DIS], [MIN]] \n- There are many typos and comma mistakes."
2246,1,"  I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors\u2019 comments in case I\u2019m missing some simplification."
2247,1, Showing versus Doing. Teaching by Demonstration.
2248,1, I did not find strong algorithmic ideas in the paper.
2249,1," Hence, CrescendoNet does not have the best performance among skip connection free networks."
2250,1, I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.
2251,1,\n\nOne last related literature is pedagogical teaching from computational cognitive science.
2252,1,\n\nPros:\n* Theoretically well motivated\n* Promising results on synthetic task\n* Potential for impact\n
2253,1, And then train another net to predict future frames given the input and residual error from the first network.
2254,1, The representation is clear with detailed empirical studies.
2255,1,"\n\nOverall:\nPros:\n-\t A nice idea with some novelty,  based on a non-trivial observation"
2256,1," Most importantly, in the FL setup, communication is the bottleneck."
2257,1, \n\nThe paper is written in a clear way.
2258,1,"\n+For ImageNet and MS-COCO experiments with a fixed budget, you reduced the training set when increasing the redundancy, which is unfair."
2259,1, Please extend and describe in more detail.
2260,1,. I do think that it is a solid contribution with thorough experiments.
2261,1,"\n- the A2C results are much worse, presumably because batchsizes are different?"
2262,1," The paper is written in a procedural fashion - I first did that, then I did that and after that I did third."
2263,1," \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach."
2264,1,\n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches
2265,1, I think it should be c_{ijk} * \\delta_{ijk} under the summations instead.
2266,1," Since most theory around divergence minimization is based\non the unmodified loss function for generator G, the experiments carried out in\nthe submission might yield somewhat surprising results compared the theory."
2267,1, \n\nI would also like to see some analysis of what's actually being learned by the teacher.
2268,1, It also proposed to use budgeting techniques to overcome computational costs.
2269,1,"\n\nAlso a bit confusing is the notation \""conv2\"", \""conv3\"", etc."
2270,1,  Experimental results are decent \u2014 there are clear speedups to be had based on the authors' experiments.
2271,1," Why did the authors not cite and consider (Parisotto et al, 2017, \u201cNeural Map: Structured Memory for Deep Reinforcement Learning\u201d), which explicitly claims that their method is \u201ccapable of generalizing to environments that were not seen during training\u201d?"
2272,1, but I see the data is actually already available as the photographic work of an artist.
2273,1," If not, your fully-connected baseline may be unnecessarily overfitting the training data."
2274,1, \n\nThe presentation of the paper is clear.
2275,1,  No analysis is reported on how these affect the performance of GTI.
2276,1, All one can conclude is that it performs slightly better than grid search with a small number of runs.
2277,1," \n\nFirstly, the problem context and setting is kinda synthesized."
2278,1,"\n\nIn its current form, it's not clear how the proposed approach tackles the shortcomings mentioned in the introduction."
2279,1,"\n  \u2022 [p4, Delayed rewards] It might be interesting to have a delay sampled from a distribution with some known mean."
2280,1,"\n- In Figure 7, the baseline of \\hat{h_t}=h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if \\hat{h_t}=h_t) to see how much performance suffers as a result of model error."
2281,1," \n\n2. Could the authors comment on the relative training time of the models with the trace-norm regularizer, L2-regularizer and the unconstrained model in terms of convergence time."
2282,1," \n\nFrom a game-theoretic point of view, the paper begins with a game-theoretic analysis of a cooperative strategy for these markov games with imperfect information."
2283,1," The paper does not clearly provide information about how the functional nature and the infinite dimensional can be handled in practice. In FDA, generally this is achieved via basis function approximations."
2284,1,These are very impressive numbers for neural architecture search.
2285,1,"\""\nAs our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that \""doing xxx should improve things\"" without actually trying it."
2286,1,\n\nThe model is learned from a single large input graph (for three real-world networks) and evaluated against one baseline generative graph model: degree-corrected stochastic block models.
2287,1,"""The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or \""batches\""."
2288,1,"\n- The main originality of paper is the block style. However, the paper doesn\u2019t analyze how and why the block brings improvement."
2289,1,"""The main contribution of this work is just a combination of LSH schemes and SGD updates."
2290,1,"\n\n- The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper."
2291,1, Only the basic LeNet and another network are considered on Cifar-100.
2292,1,"\n\nThe main baseline technique CEGIS (counterexample-guided inductive synthesis) addresses this problem\nby starting with a small set of examples, solving a constraint problem to get a hypothesis program,\nthen looking for \""counterexamples\"" where the hypothesis program is incorrect."
2293,1,  \n\nI feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does.
2294,1," Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason?"
2295,1,"[3] Patterson, S. and Teh, Y.W., 2013."
2296,1, One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter.
2297,1,"""(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems."
2298,1, It is hard to generalize to any other RC/QA tasks.
2299,1,"\n\n4. Equation 2, bottom: C_in, W_f, H_f, and C_out are undefined at this point."""
2300,1," However I was unable to identify any real novelty in the theory: the Fokker-Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years, and this paper mostly rephrases those results."
2301,1," Again, the utility of the evaluation metrics proposed in this work is unclear."
2302,1, I feel that the first two ideas are particularly interesting. 
2303,1,n\nMy one concern is that the supervised approach that the paper compares to is limited:
2304,1,"\n- The datasets used are not very interesting: Cifar is too small, and ImageNet is essentially solved."
2305,1," The whole network is trained on the \""fill-in-the-blank\"" task using the sequence-to-sequence architecture for both the generator and the discriminator."
2306,1," The focus is on the key aspect, which is generalisation across heteregeneous data."
2307,1,"  The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?"
2308,1," Nonetheless, in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments."
2309,1,\n* sec4.1 innaccurate\n* well approximated\n* sec4.2 an curvature\n* (Amari 1989)\n* For the the Laplace\n* O(n^3) : what is n ?
2310,1,"  Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems."
2311,1, \n\nThe results presented in the paper are convincing
2312,1, This is also the case for hierarchical community structures.
2313,1,"\nThe contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models)."
2314,1," However, I think the proposed architecture in this paper is less motivated."
2315,1,"\nNot much technical novelty to be found,"
2316,1, The fact that the variable d is equal to 768 is not explicitly stated.
2317,1, The paper is technically correct and nontrivial.
2318,1, How larger values of T to better model inter-community links?
2319,1," The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits,"
2320,1, This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference.
2321,1," The main contribution is a) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors,"
2322,1," To me the reported performance plateau with number of map size is normal expected behavior, only symptomatic that this deep RL agent has finite capacity."
2323,1,  Going through the paper I\u2019m not sure I know how this latent space is constructed.
2324,1,"    For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data. "
2325,1,"? It looks like the loss at initialization was reported instead?"""
2326,1," 3) Section 3 and 4 are very hard/impossible to understand, it is not clear how the formulas help the reader to better understand the concept in any way."
2327,1, The work also investigates various schemes for selecting negative samples.
2328,1,\n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.
2329,1, So I gather our policy takes the form of a composed function and the chain rule gives close to their expression in 2.2
2330,1,"\n\n4.) \u201cDense pixel-level correspondences\u201d are discussed but not evaluated.\n"""
2331,1,\n  - good overview of the related literature;
2332,1,\nWould it be possible to include a non-deterministic baseline in the experimental comparison?
2333,1," For instance, why is that particular sigmoid formulation used?"
2334,1, I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison.
2335,1, The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy.
2336,1,"  Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive."
2337,1," I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods."
2338,1," (In fact, the intro claims this is an advantage of not using hand-engineered features for malicious domain detection, seemingly ignoring the literature on adversarial examples for deep nets.) For example, in this case an attacker could start with a legitimate domain name and use black box adversarial attacks (or white box attacks, given access to the model weights) to derive a similar domain name that the models proposed here would classify as benign."
2339,1, Details are also given on how the authors are able to achieve realtime completion
2340,1,\n\nReview Summary:\nThe proposed technique is interesting and the experiments indicate its superior performance over existing techniques. 
2341,1, and 2) is reasonable to not be good.
2342,1,"\"" So, please show that."
2343,1,"""Summary of paper:\n\nThe authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations."
2344,1, Why is this result not compared to in Table 1?
2345,1, However the description of the reinforcement learning step could have been made a bit more clear.
2346,1,"\n\n2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct."
2347,1," The authors showed that across the test images, they were able to perturb the ordering of the training image influences."
2348,1,\n- Figure 4 is very small and not easy to read the text.
2349,1,  I commend the authors for making their code available already via DropBox.
2350,1,.\n\n- Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal.
2351,1, The \u201clooks linear\u201d initialization proposed in \u201cThe shattered gradients problem\u201d (Balduzzi et al) implies that alpha=0 may work better.
2352,1, The proposed method does not assume that the source sentences have only one style and allows them to have unknown styles.
2353,1," Named entities, and rare words in general, are indeed troublesome since adding them to the dictionary is expensive, replacing them with coarse labels (ne_loc, unk) looses information, and so on."
2354,1,"\n3. Auto encoder network\n\nThese network are jointly trained, and the joint-loss is simply the addition of a cross-entropy loss (from OCN), the binary cross-entropy loss (from PCN) and a pixel wise loss (from AE). "
2355,1, \n\nI thought this was an impressive paper that looked at theoretical properties of CNNs.
2356,1," The theoretical analysis is carried-out very nicely, and the theory is supported by experiments on two-dimensional toy examples, and Fourier-spectra of the iterates of SGD."
2357,1, \n2. Using a new loss function for pointer / copy mechanism.
2358,1, Hence I don't think the submission is ready for publication at this moment.
2359,1," - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem)."
2360,1,"  Further on you cite that L_PIB is intractable due to the high dimensionality of the bottleneck variables, I imagine that this still yields a high var MC estimator in your approximation (in practice)?"
2361,1,"\n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN"
2362,1,"Yet, the results do not support the claim."
2363,1," Thus, the author's response is still not convincing to me."
2364,1, Mimicking the mutation by a gradient step is very unreasonable.
2365,1,"""Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences."
2366,1,"  It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is."
2367,1," Would learning a mixture of HMMs (one per community) have similar performance?\n"""
2368,1," I have a few questions about the\nevaluation, but most of my comments are about presentation."
2369,1,"\n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition. "
2370,1, The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions.
2371,1, This would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the Tucker2 representation to be removed by the preprocessing.\n\n\n\n
2372,1,"""An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated."
2373,1,"\n- The demarcation of \""RL\"" and \""evolutionary strategies\"" suggests a pretty poor understanding of the literature and associated concepts."
2374,1,\n(5) Somewhat surprising: MC methods seems to be on-par with TD methods when the reward is sparse and even longer than the rollout horizon.
2375,1,\n\nHow important is using the pretrained weights from the deterministic RNN?
2376,1,  They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs.
2377,1,"""This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems."
2378,1, They found it improved the accuracy.
2379,1,"\n\nOverall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays."
2380,1,"\n\n* \""On average, 2H(X|Z) elements of X are mapped to the same code in Z."
2381,1,"\n- Nice engineering achievement, reaching the top of the leaderboard (in early October)."
2382,1, I first thought that performance would be better when the generator's encoder uses the unmasked sequence.
2383,1," To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers."
2384,1,"\n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions?"
2385,1,\n- Fig. 3 was not very clear to me.
2386,1,"\n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better."
2387,1,"\n\n7) In Figure 2, what exactly do you mean by \""Results are averaged over 30 translation scenarios\"". Can you please elaborate ?"""
2388,1,"""This paper analyzes the expressiveness and loss surface of deep CNN."
2389,1,\n\n2. Why is the computational complexity not a function of the number of spans?
2390,1, \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision.
2391,1, So there is no baseline to compare.
2392,1, The local dimensionality of this paper is the SVD dimensionality on the augmented images of the same class images which classified by the neural network as high probability.
2393,1," Nevertheless, I do think there are some interesting ideas theoretically and algorithmically."
2394,1,"\n\nComments\n - It is not clear if the value of count \""c\"" is same with the final answer in counting questions. \n\n"""
2395,1,\nThe work seems to have been written in a rush leading to a big number of typos and quickly filled experiment tables (7 and 8 are full of zeros ?).
2396,1," Unfortunately, the proposed \""branch and bound method\"" does not explain\nhow to implement the \""bound\"" part (\""compute lower bound\"") -- and has been used \nseveral times in the same application,;"
2397,1, \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL
2398,1,"\n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain."
2399,1,"\n\nRegarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods."
2400,1," There has been some work done on constructing interpretable neural networks, such as stimulated training in speech recognition, unfortunately these are not discussed in the paper despite interpretability being considered important in this paper. """
2401,1," Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence \""up-left-left\"", while back-propagating through \""up-right-right\"" action sequence in the TreeQN's plan."
2402,1,\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum.
2403,1,"\n\nSignificance:\nThe proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way."
2404,1, The usage is inconsistent.
2405,1," Also, what do you mean by *all directions* in the sentence below eq.2?"
2406,1,"\n\nWhile this model is proposed as an extension of Kohonen's self-organising map (SOM), the paper fails to mention, or compare with, several historically important extension of SOM, which should perhaps at least include the generative topographic mapping (GTM, Bishop et al. 1998), an important probabilistic generalisation of SOM."
2407,1,\n2. Formulate two complex-valued alternatives to ReLU and compare them
2408,1, \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?
2409,1, It would be useful to compare with a simple neural network baseline trained for K-way classification with standard backpropagation (though the UCI datasets may potentially be too small to achieve good performance).
2410,1, [But note here that mini-batch SGD is not a closed chapter.
2411,1,"\n\n- The experimental section is very strong; regarding the partial observability experiments, assuming actions are here factored as well, I could see four baselines \n(two choices for whether the baseline has access to the goal location or not, and two choices for whether the baseline has access to the vector $a_{-i}$)."
2412,1,\n\nThe only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model.
2413,1, It can be further improved as mentioned in the comments.
2414,1,"\n\nAccording to the authors, the contributions are the following:\n1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to \""make sure\"" that the representation learned is used in the most efficient way."
2415,1,"\n\nThe proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results."
2416,1," For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward."
2417,1, but its application for learning discrete networks is to my best knowledge novel and interesting.
2418,1,"n\nFirst, the computational cost of the proposed method seems very high. "
2419,1,References should be added to the relevant methods.
2420,1,"\n- many terms are incorrect, e.g. \u201cdependent parsing tree\u201d (should be \u201cdependency tree\u201d), \u201cconsistency parsing\u201d (should be \u201cconstituency parsing\u201d)"
2421,1,"\n\nThe \""window size\"" of the local reordering layer looks like the \""distortion limit\"" used in traditional phrase-based statistical machine translation methods, and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model; small window sizes may drop information about long dependency."
2422,1,"\nMore arguments are desirable for the advantage of this paper, i.e. quantitative\nevaluation of diversity of generated text as opposed to LSTM-based methods."
2423,1," Since it is not clear from reading the title whether (A) or (B) is true, please reword it."
2424,1,"""This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors."
2425,1," Indeed, a single error in some given state will often generate totally different trajectories and not affect a single transition."
2426,1, Their paper provides a proof that in the non-parametric case the optimum on NCE objective function is at the data distribution with normalisation constant either learned or held fixed (0 or any value you like).
2427,1,"\n[c] \u201cwhen tested on unseen maps the algorithm utilizes a wall-following strategy to find the goal without doing any mapping or path planning\u201d,"
2428,1," Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet."
2429,1,These values are finally used as features in classifying adversarial examples from normal and noisy ones.
2430,1," \n\n* The paper claims that the method permits training without any collisions, even for real training runs (strong claim), however it isn't clear how this is guaranteed beyond the assumption that you have a low-level controller that can ensure collisions are avoided."
2431,1,"""The paper addresses the task of dealing with named entities in goal oriented dialog systems."
2432,1,\n\nMy main issue with the paper is that these two topics are actually not new and are well covered by the existing RL formalisms.
2433,1," The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs."
2434,1,"\n\n2) \"" Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work."
2435,1,"\n2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc."
2436,1,"  In the first equation on page 5, is tilde y involved?"
2437,1," Is it possible to change more than one attribute each time in this method?"""
2438,1, \n\n3. The experimental results are not so convincing.
2439,1, The generative model defines the process that produces the dataset.
2440,1," One can easily understand that if there is one more class than the number of dimensions, the assumption should be feasible, but beyond it starts to get problematic."
2441,1," However, y^* is also estimated by the encoder as shown in Eq. (2) and it varies depending on the input to the encoder."
2442,1," \n\nThis paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results. "
2443,1,"It mixes the method, the heat sink example and the airfoil example throughout the entire paper."
2444,1," The other mentions provide signal, but does not provide conclusive evidence."
2445,1,"""- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations."
2446,1,\n==========================\n\nThe authors present an interesting new method for generating adversarial examples. 
2447,1," It's only baseline is a GAN model that isn't even very convincing (GANs are finicky to train, so is this a badly tuned GAN model?"
2448,1," This is a plausible solution, yet in total I miss an exploration of the solution."
2449,1," The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction."
2450,1, It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper.
2451,1, \n\n\nMinor Weaknesses:\n- There are no training time comparisons between the proposed technique and the standard fixed loss learning.
2452,1," In my opinion, this is one of the crucial contributions of this paper."
2453,1," I'd re-evaluate my rating after looking at the data in more detail.\n"""
2454,1,"""This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases."
2455,1,\n\n- Page 3- 2.2.It is highly motivating to use users feedback in the loop but it is poorly explained how actually the user's' feedback is involved if it is involved at all.
2456,1,\n \n\u201cA buyer which cannot receive the activation approximates x_i with \u2026\u201d It is unclear why a buyer need to do so given that it cannot receive the activation anyway.
2457,1,\n\nIt would be great if the above could be addressed.
2458,1," While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties."
2459,1,"\n\nThe idea is interesting. It could be viewed as a type of adversarial training for large-margin structured predictors, where counterexamples, i.e., structures with high loss and low energy, cannot be found by direct optimization. "
2460,1, they propose to use a latent variable gan with one continuous encoding and one discrete encoding.
2461,1," The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized."
2462,1," Can this be related to the \""flat vs sharp\"" dilemma ?"
2463,1, Using data generated by a pre-trained network is usually not representative of what will happen in real life.
2464,1," Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ? "
2465,1,\n\nClarity:\n\nThe paper is clearly written and easy to follow and understand.
2466,1,"""Pros: \nThe paper is clearly written and studies an interesting problem."
2467,1,"""The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms"
2468,1,\n\nThis needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty.
2469,1,  This analysis is hard to follow for non experts graph theory.
2470,1,\n- The experimental setting is very non-trivial and novel.
2471,1,"\n\n4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W."
2472,1," When the game is symmetric, this might be \""the natural\"" solution but in general it is far from clear why all players would want to maximize the total payoff."
2473,1, This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes.
2474,1,I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly.
2475,1,"  To\"": Where is Table 4.1??"
2476,1,"\n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc."
2477,1,  but optimization with two latent distributions and one discriminator can be hard.
2478,1, Is there a principled reason AT+FGSM defends against universal perturbations?
2479,1," Having a generative model for causal relationships between symptoms and diseases is \""intriguing\"" yet I am really struggling with the motivation of getting such a model from word co-occurences in a medical corpus."
2480,1," In the variable trace embedding, the input to the model is given by a sequence of variable values."
2481,1,  \n\n- The evaluation is small.
2482,1," Suppose that TreeQN estimated \""up-right-right\"" as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing \""up-left-left\"" (this is possible because the agent re-plans at every step and follows epsilon-greedy policy)."
2483,1," However, it would have been great to have an experiment that actually makes use of the learned features to make predictions."
2484,1," \n\nOverall, the paper is well written and easy to follow."
2485,1, Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does?
2486,1, Can it solve the halting problem?
2487,1,\ Can this be generalized to arbitrary PGM structures? 
2488,1,"\""\n - should state in the abstract what your \""notion of generalization\"" for gans is, instead of being vague about it."
2489,1, It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case
2490,1," \n\n4, The experiment is weak."
2491,1, What about a graph with poorly connect communities?
2492,1,". While the authors do not empirically validate whether this is a good model of the unknown function, it appears to be a reasonable assumption (the authors *do* empirically validate their overall approach)."
2493,1, A better citation would be Jordan et\tal 1999.
2494,1, It will be interesting to compare with some existing second-order optimization algorithms for deep learning.
2495,1,  This concept is not new.
2496,1," While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry."
2497,1, The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads.
2498,1,. \n\nClarity: The paper is well-written and clear
2499,1,"  Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences)."
2500,1,  I am also concerned to see test performance significantly better than development performance in table 3.
2501,1,\n\nComment:\n\n- I found the paper very hard to follow.
2502,1,"\n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings."
2503,1,\n\nWas the edge-importance reported in Section 2.3 checked against various measures of edge importance such as edge betweenness?
2504,1,  I do not have any suggestion for improvement.  This is good work that should be published.
2505,1," \n\nWeaknesses:\n- The paper does not introduce strong technical novelties -- mostly, it seems to apply previous techniques to the medical domain."
2506,1,\n\nThere are several mistakes in the notation and background section.
2507,1, I upgraded the rating.
2508,1, This is very problematic if the authors want to make claims about their attack being effective under defenses that know OptMargin is a possible attack.
2509,1," \n\nA criticism of the paper is that it does not require Stein\u2019s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate."
2510,1," The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings."
2511,1," The authors propose the CP-S (stands for symmetric CP decomposition) approach which tackles such factorization in a \""batch\"" manner by considering small random subsets of the original tensor."
2512,1," \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c."
2513,1,but: 1/ you must provide baseline results with a well tuned phrase based mt system;
2514,1,\n\nPapers 's pros :\n- clarity\n- technical results
2515,1,"  Further, the work is clearly presented."
2516,1,"\n\nFor document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this."
2517,1,"\n\nAs with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1]."
2518,1, The authors simply show set of generated images.
2519,1, but not to better results and it looks like the difference is between few and very-few training steps anyway.
2520,1,\n\nIn the abstract the authors mention that the Depthwise Separable Graph Convolution\nthat they propose is the key to understand the connections between geometric\nconvolution methods and traditional 2D ones.
2521,1,  It has a few shortcomings. 
2522,1,\n\n(6) Below Definition 2.3: What is capital X?
2523,1,"""This paper introduces a neural network architecture for continual learning."
2524,1,\n- Could we look at the two distributions of inputs that each neuron tries to separate?
2525,1, Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10.
2526,1," \n\nIn addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM)."
2527,1, This may allow you to avoid truncation altogether.
2528,1,"\nThere are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper."
2529,1," Rather than starting from introducing those new concepts directly, it might be better to mention that the proposed method is related to many important concepts in game theory and GANs."
2530,1, I appreciated the set-up of the introduction with the two questions.
2531,1,\n5) It was unclear to me why momentum was used in the MNIST experiments.
2532,1,\n\n- a number of approaches developed for the basic idea
2533,1, I could not imagine how VAE and T are trained simultaneously.
2534,1,\n\n- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? 
2535,1," I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n"""
2536,1, What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4.
2537,1," \n\npage 5:\n- the examples are hard to understand. It would be helpful to add the value of \\pi^* and f^* for both models, and explaining in details how they fit the authors model.\n- in Figure 2 the left example is useless to me."
2538,1,". Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results."
2539,1," The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition."
2540,1,"  \n \nLu et al. , Flexible Spatio-Temporal Networks for Video Prediction, CVPR 2017\n\nThis recent work builds on another highly relevant work, that is also not mentioned in the paper:\n\nPatraucean et al."
2541,1," It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework.\n"""
2542,1,"  BTW, I would suggest to refer to published papers if they exist instead of their Arxiv version (e.g. Hester et al, DQfD). """
2543,1, The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels.
2544,1,"\n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  \u2018if the fear model reaches arbitrarily high accuracy\u2019 or \u2018if the probability is negligible\u2019."
2545,1, The three main decision functions in the sequential process are computed with neural nets.
2546,1,\n\nConsidered paper is one of the first approaches to learn GAN-type generative models.
2547,1,"""Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100."
2548,1,"\n\n\nComments on prior work:\n\np 1: authors write: \""vanilla backpropagation (VBP)\"" \""was proposed around 1987 Rumelhart et al. (1985)."
2549,1," There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works."
2550,1,\n\nRevision: I thank the authors for the updates and addressing some of my concerns.
2551,1, \n\n4) The experimental section can be significantly improved.
2552,1,"\n\n- Significance:\nWhile looking at RNN from an iterative estimation is interesting,"
2553,1,"\n\nThe explanation of the cause of \""super-convergence\"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments."
2554,1," The core idea is two fold: when passing a new image through the (already trained) classifier, first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second, add a temperature to the softmax."
2555,1," \n\nThe proposed approach is interesting,"
2556,1,?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.
2557,1,"  Also, it is quite important for the paper, I think it should be in the main part."
2558,1,"\nIn fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see \""Decoding Distributed Tree Structures\"" and \""Distributed tree kernels\""."
2559,1," But that is not very interesting (if you can propose new loss functions, that would be way cooler)."
2560,1,"  \n\n\n3. High level technical\n\nI have a few doubts about this method as a privacy-preserving technique:\n- Nearly every privacy-preserving technique gives a guarantee, e.g., differential privacy guarantees a statistical notion of privacy and cryptographic methods guarantee a computational notion of privacy."
2561,1,  So given a fixed policy and these constraints it is not surprising that it underperforms the Q-masked Q-learning algorithm.
2562,1," This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well."
2563,1,\n\nAn important argument in favour of using complex-valued networks is said to be the propagation of phase information.
2564,1," Indeed, w_j^t is only ever implicitly defined in Eq 8, whereas things like the input and forget gates are defined multiple times in the text.."
2565,1," \nThe architecture of the action value estimator does not seem novel, it's basically just an extension of DQN with an extra parameter (subgoal g)."
2566,1," To make things worse, some variables are simply not defined."
2567,1,  How is the terminal time known a priori?
2568,1,"There are also a number of items that could be added that I believe would strengthen the contribution and novelty, in particular:\n\nSome highly relevant references on (prosocial) reward shaping in social dilemmas are missing, such as Babes, Munoz de cote and Littman, 2008 and for the (iterated) prisoner's dilemma; Vassiliades and Christodoulou, 2010 which all provide important background material on the subject."
2569,1, They first consider the dynamics generated by the following procedure.
2570,1,"\n\nThe approach is evaluated on three tasks, two synthetic and one real world."
2571,1, \n\n() Pros / Cons:\n+ simple yet powerful method for text classification
2572,1," However, I think it would also be important to compare to other tensor factorization approaches."
2573,1, How many classes and examples does the testing problems have?
2574,1,\n\nI have some question and some consideration that can be useful for improving the appealing of the paper.
2575,1,"\n\nFirst, though a minor point, but where does the name *YellowFin* come from?"
2576,1,"\nThe presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.\nThe effect of using the faster approximation on performance also remains to be investigated."
2577,1," Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand."
2578,1," The adversarial objective is not novel as mentioned by the authors and has been used in [2,3]."
2579,1,"""This paper considers the problem of Reinforcement Learning in time-limited domains."
2580,1, A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure.
2581,1,"  In particular, many parts are quite technical and may not be accessible to a broader machine learning audience."
2582,1,\n\nThe evaluation criteria reported on Table 1 is not clear.
2583,1,\n\nCons:\n  1. The overall justification is somewhat unclear
2584,1," It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).\n"""
2585,1,"  Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\"
2586,1," But I feel a lot can still be done to justify them, even just one of them."
2587,1,"""This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks."
2588,1,\n\nThere is a lot going on in this paper.
2589,1,\n\n\nPaper Strengths:\n- The proposed technique seems simple yet effective for multi-task learning.
2590,1," Per se, the model is incrementally new,"
2591,1,"\n\n- The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t."
2592,1,\n\nc) the paper should run some experiments on language applications where RNN is widely used
2593,1,"""Quality: Although the research problem is an interesting direction"
2594,1,"""The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks."
2595,1," However the corpus the authors choose are quite small,"
2596,1,Con:\n-            The paper is generally well-written 
2597,1,"\n\nThis makes me wonder whether the \""complex numbers\"" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in \""Compressing neural networks with the hashing trick\"" by Chen et al.)."
2598,1," In the beginning of the section, it is not clarified why, if a 75 character string is encoded as a 128 byte ASCII sequence, the content has to be stored in a 75 x 128 matrix instead of a vector of size 128."
2599,1," \n\nThe affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment."
2600,1,\n\nMinor: fix margins in formula 2.7.
2601,1,\n\nI found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated.
2602,1,"\n\nSome other comments:\n\nYour presentation of completion cost versus edit cost separation in section 3.3 is not particularly clear, partly since the methods are discussed prior to this point as extension of (possibly corrected) prefixes."
2603,1,"\n\nNegative points:\n- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches."
2604,1,\n\n- Have you thought of an end to end way to learn the prior generator GAN?
2605,1," Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point."
2606,1,"As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest."
2607,1,\n\n2. The central contribution is extending the single step LP to a multi-step formulation.
2608,1,"\n\nAuthors argued some differences between conventional attention mechanism and the local reordering mechanism, but it is somewhat unclear that which ones are the definite difference between those approaches."
2609,1,"  That such shortcomings are not noted in the paper is troublesome, particularly for a conference like ICLR that is focused on learned models, which this is not. "
2610,1,"\nSection 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used."
2611,1,"\n\n- There are some errors and unclear things in the mathematical derivations:\n* In the equation above Eq. 2, \\alpha should in fact be \\alpha_i, and it is not a vector (no need to transpose it)"
2612,1,"\n7. The ALS is so slow (if looking at the results in section 5.1), which becomes not practical."
2613,1,"""This paper proposes a multi-view semi-supervised method."
2614,1, The addition of one or two tables with either a standard task against reported results or created tasks against downloadable contextual / tensor embeddings would be enough for me to change my vote. 
2615,1," On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors."
2616,1,"""This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions."
2617,1,". It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function."
2618,1,\n- The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios.
2619,1," \n\n(5) Given the above considerations, there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation."
2620,1," This does not require any \""memory\"" because all necessary information is available to the network."
2621,1,"  But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient."
2622,1, \nAuthors refer to prior work for methods to learn this backbone model. Liu et.al (http://www.cse.ust.hk/~lzhang/ltm/index.htm) and Chen et.al. (https://arxiv.org/abs/1508.00973) and (https://arxiv.org/pdf/1605.06650.pdf).
2623,1,"\n\nHowever, I think this paper has limited contribution and novelty,"
2624,1, \nIt seems that one of the claimed benefit is that the proposed method is effective at identifying the k.
2625,1, This is one of the strongest points of the paper.
2626,1,"""This paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources."
2627,1,"If I understand correctly, Stachenfeld et al. discussed this result, but didn't prove it."
2628,1," From the security perspective, the scenarios are too simplistic."
2629,1, Are there any obvious modes?
2630,1," Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters."
2631,1, This seems inconsistent with the previous definition of \\psi\n\n- p.
2632,1," On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me)"
2633,1, Why is this term included. Is this term not equal to 0?\
2634,1, The work also propose a neat model motivated by the environment and outperform various baselines.
2635,1, The connection between Hilbert maps and RKHS in that sense is not clear in the paper.
2636,1,"\n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs."
2637,1, The paper advocates two primary metrics: accuracy of the predicted motion and perceptual realism of the synthesized images.
2638,1," In 7.2.2 it says that a BiLSTM is used for the inference network in Twitter POS tagging, but Tables 4 and 6 indicate both CRFs and BiLSTMS?"
2639,1,"\n\nCONS:\nThe criteria of disentanglement, informativeness & completeness are not fully clear as they are presented.."
2640,1, The contributions are interesting and experimental results seem promising.
2641,1,"n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem?"
2642,1,  The 7% accuracy from the proposed model is not in the range of modern deep learning models (The state-of-art accuracy is <0.3%).
2643,1, but it\u2019s too bad for everyone cheering for linguistically-informed syntax that the results weren\u2019t better.
2644,1,\n\nHere are the pros of this paper:\n1) Useful contribution in terms of using broader context for embedding a sentence.
2645,1,"\n\nThe method proposed in the paper already exists for classical function, they only transpose it to neural networks."
2646,1,"\n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN."
2647,1," The work builds on previous approaches, such as Vilnis and McCallum's Word2Gauss and Vendrov's Order Embeddings, to establish a partial order over probability densities via encapsulation, which allows it to model hierarchical information."
2648,1,"  \n-- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant."
2649,1," They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning."
2650,1,"\n\nThis paper is well written, easy to understand and presents a simple and apparently effective method of detecting out of distribution samples."
2651,1, The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it.
2652,1," In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well."
2653,1, Any observations about how it breaks down?
2654,1, The method is also appealing for its use of some kind of emergence between two levels of hierarchy.
2655,1," In high-dimensional state space, the authors propose to approximate that matrix with a convolutional neural network (CNN)."
2656,1,\n\nMinor things:\n\n- y' (prime) gets overloaded in Section 3.1 as a derivative and then\n  in Section 4 as a partial learning curve.
2657,1,\nThings not addressed:\n - The risk analysis is still not relevant.
2658,1,The results from the proposed method do not seem much better than the baselines.
2659,1, Citation?
2660,1," To clarify: I think the proposed method is genuinely novel, but a bit of context would help the reader understand which aspects are and which aspects aren\u2019t."
2661,1, it would be clear that BDQN is outperforming DDQN..
2662,1,"\n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity?"
2663,1, but as a standard application of attention lacks novelty.
2664,1," To make a stronger case for this research being relevant to the real autonomous driving problem, the authors would need to compare their algorithm to a real algorithm and prove that it is more \u201cdata efficient."
2665,1,"3) And finally, perhaps complementary to point 2), you really need to provide examples on more than one environment."
2666,1,"\n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI."
2667,1," The proposed solution of bootstrapping from the value of the terminal state v(S_T) clearly works, and I suspect that any RL-practitioner faced with training time-limited policies that are evaluated in time-unlimited settings might come up with the same solution."
2668,1, The gains from introducing gate weights in the input of the residual modules vanish when increasing the network size.
2669,1," Given the advancement in gradient based inference for HB the last couple of years (e.g. variational, nested laplace , expectation propagation etc) for explicit models, could the authors perhaps indicate why they believe their approach of looking directly to the MAML objective is more scalable/useful than trying to formulate the same or similar objective in an explicit HB model and using established inference methods from that area ?"
2670,1,"""The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced."
2671,1,"\n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies"
2672,1,"\n\nOn the other hand, the theoretical part of the paper is not really improved in my opinion,"
2673,1," c) based on these insights, the paper proposes a variant of MALM using a Laplace approximation (with additional approximations for the covariance matrix."
2674,1, This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting.
2675,1,\n* It appears that a minus sign is missing in Eq. 7.
2676,1,There is no contribution in the GAN / neural network aspect.
2677,1,".\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max"
2678,1, Please add the Swiss roll example.
2679,1," With this approach, one can generate text at test time by setting all inputs to blanks."
2680,1,"\n5) The mathematical notation needs a bit of improvement. E.g. argmin, sgn, min, etc. should be set in roman and parentheses should be enlarged to match the size of the enclosed expressions."
2681,1,\n\n* A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8.
2682,1," While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to."
2683,1, I would expect some further elaboration of this question in the paper.
2684,1,"\n\nCons:\n- Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers."
2685,1,"""The paper proposes to use the start-end rank to measure the long-term dependency in RNNs."
2686,1,\n\n3. I find the paper mingles notions from GAN and VAE sometimes and misrepresents some of the key differences between the two.
2687,1,\n\nThe use of a non-parametric definition for the activation function should be contrasted with the use of a parametric one.
2688,1," Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets."
2689,1,  It would be very helpful to include algorithms.
2690,1, Of which dimension?
2691,1,  The authors analyzed the the generalization for the following scenarios\n\n- the generalization ability of RNNs on random subset of SCAN commands
2692,1,  \n\nABOUT EXPERIMENTS\n1.\tIt is unclear how to tune the hyperparameters.
2693,1,"\n\nOverall, this paper appears very interesting."
2694,1,\n\nThe paper presents comparisons with baseline methods.
2695,1, The state trace embedding combines embeddings for variable traces using a second recurrent encoder.
2696,1, \n\n3. The proposed model is not well explained.
2697,1," \n\nUnderstanding how to make complex models interpretable is an extremely important problem in ML for a number of reasons (e.g., AI ethics, explainable AI). "
2698,1,"\n\nand\n\nJ. Ngiam, A. Coates, A. Lahiri, B. Prochnow, Q. V. Le, and A. Y. Ng,"
2699,1," With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm."
2700,1," The experiments are serious, and done using standard state-of-the-art tools and architectures."
2701,1,  \n\nWEAKNESSES: While the strengths mentioned above are obvious I had the impression through the whole paper that a whole part is missing.
2702,1, I find this component of the proposed algorithm  very heuristic.
2703,1," Although a thorough and careful empirical investigation of this phenomenon would be a welcome addition to the research literature, this paper does not yet reach this standard."
2704,1,"\n5. For the experiments on synthetic datasets, workers are randomly sampled with replacements. Were the scores reported based on average of multiple runs."
2705,1,"  With the low level controller off, collisions became possible."
2706,1," In experiments, no comparisons against existing works is performed (at least on toy/controlled environments)."
2707,1,"\n2) Novel and simple \""trick\"" for generating OOV words by mapping them to \""local\"" variables and generating those variables."
2708,1,\n\nI would say the current results indicate the conventional approach to TD is working well if not better than the new one.
2709,1, The only technical contributions are (4) and the way to construct the co-occurrence information A.
2710,1," Also, several parts of the paper spend too much time on describing other work or on implementation details which could be moved to the appendix."
2711,1," The general difference between plain and non-plain networks is very likely due to optimization difficulties alone, and due to the above issues further comparisons can not be made from the results."
2712,1," Among\"".\n- Please add commas/periods at the end of equations."
2713,1, The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines.
2714,1,"""Language models are important components to many NLP tasks."
2715,1,"\n-Authors are fairly upfront about how, overall, it seems their technique isn't doing *too* much--null results are still results, and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much."
2716,1,Minor: \nThe approach corresponds to a Tucker2 decomposition with non-negativity constrained factor matrices and unconstrained core - please clarify this as you also compare to Tucker2 in the paper with orthogonal factor matrices.\n\nDing et al. in their semi-NMF work provide elaborate derivation with convergence guarantees.
2717,1, 2. human subjects driving cars in the simulator.
2718,1,"  The result, spikes having an exponentially decaying effect on the postsynaptic neuron, is similar to that observed in biological spiking neurons."
2719,1,.\n\u2022\tIt\u2019s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm.
2720,1," \n\nThe recurrent GAN architecture does not appear particularly novel --- the authors note that similar architectures have been used for discrete tasks such language modeling (and fail to note work that uses convolutional or recurrent generators for video prediction, a more relevant continuous task, see e.g.  http://carlvondrick.com/tinyvideo/, or autoregressive approaches to deep models of time series, e.g. WaveNet https://arxiv.org/abs/1609.03499) and there is no obvious new architectural innovation. "
2721,1, The goal of this process is to extract a function that maps any given state to a subgoal.
2722,1,"\n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner,"
2723,1,"\n-\tIts not clear enough what exactly is the \u2018PSNR\u2019 value which is used for the adversarial example detection, and what exactly is \u2018profile the PSNR of legitimate samples within each class\u2019."
2724,1,". This seems tt the authors propose to add a second activation function (the softsign), why not use the one is in teh layer?"
2725,1," The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads."
2726,1," To fix this the paper proposes to learn a second GAN to learn the prior distributions of \""real latent code\"" of the first GAN."
2727,1,"\n- some of the other papers might use data augmentation, which would increase noise robustness (did not check, but this might explain some of the results in table 4)"
2728,1, Unclear if this would work at all in higher-dimensional time series.
2729,1,\n* Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs.
2730,1," However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via \\phi_1 and another via \\phi_2."
2731,1, Goyal et al (2017) show how\nto train deep models on ImageNet effectively with large (but fixed) batch\nsizes by using a linear scaling rule.
2732,1,\n\nWhat cost is used for generative modeling on MNIST?
2733,1, i.e. Could you please elaborate on what's different (in terms of learning) between 3.2 and a normal latent Z that is definitely allowed to affect different classes of the data without knowing the classes?
2734,1,"  Basic attention models, which have been shown to help model structures, are not included (or compared)."
2735,1," but it makes the model considerably simpler than any practical setting.\n\n"""
2736,1,"\n\n(3) Regarding the ablation in Table 2, my first concern is that the improvement seems small (~0.5%). "
2737,1, I was wondering how the encoder differs from tree-lstm.
2738,1, but less convincing for real data. 
2739,1," This is also problematic, because while the proposed method works for a car that is green or a car that is red, it will fail for a car that is black (or white) - because in both cases the \""colorfulness\"" is not relevant."
2740,1," They evaluate that their method can learn to provide learning items in an efficient manner in two situations: (1) the same student model-type on a different part of the same data set, and (2) adapt the teaching model to teach a new model-type for a different data set."
2741,1,"\nHow would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing."""
2742,1,\n\nThe motivation is clear and proposed methods are very sound.
2743,1," Moreover, this leads to good performances, so there is no needs to have something more complex."
2744,1,"\n\n\nFor the experiments, the following should be addressed."
2745,1, The outputs are evaluated by ROUGE-L and test perplexity.
2746,1, This can be used to determine which model should be trained (further).
2747,1, 2) it works as described above but propagates the rechable region on a checkerboard only.
2748,1, The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield.
2749,1," Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting."
2750,1, However I am fine accepting it.
2751,1,"\n\n\nTechnical quality\n==============\n\nThe main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on\na data point to fool the network."
2752,1, \n\nThe method is also very close to the simplest IRL method possible which consists in placing positive rewards on every state the expert visited.
2753,1,"\n\nDeatailed comments:\n1) at the end of page 2 \""most conventional data transformation only introduce slight variations...limited in evaluating \nthe generalization capabilities\""."
2754,1,\nIt is not clear what the new proposal in the paper.
2755,1,The paper obtains a lower bound on the rank of the resultant grid tensors
2756,1, It is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor.
2757,1,"\n\nIn the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture)"
2758,1,"  For example, when skimming Fig. 3's caption there is no such\ninformation."
2759,1,"and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks."
2760,1, Multi-mention reasoning or more document context?
2761,1, The authors should at least compare with one-layer conditional GAN. 
2762,1,In all experiments CCC-based agents fare better than agents operating based on a specific strategy.
2763,1, In\nsection 5 (choice of hyper-parameters) the authors describe a quite exhaustive\nhyper-parameter tuning procedure for BDQL.
2764,1, Earlier in the work (e.g. before experiments and discussion) would allow the comparison with MMD to inform the context of the introduction.
2765,1, What kind of clusters are discovered?
2766,1,"""Typical recurrent neural networks suffer from over-paramterization."
2767,1,"""The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly."
2768,1,\n\nThe experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy.
2769,1,"""The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation."
2770,1," I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting."
2771,1,\n\nWRITING QUALITY\nThe paper is not well written in a good shape.
2772,1,"\n\nThere are also a number of grammatical errors in the paper, including the following non-exhaustive list:\n2: as well as how to do -> as well as how to do it"
2773,1,"\n- How were the network architecture and network size chosen, especially for the multitasker?"
2774,1,"\n\nMain claim of the paper\nDevlin et al., 2015 show a simple nearest neighbor baseline which in my opinion shows this more convincingly."
2775,1,"  \n\nSmaller comments\n---\n- As a stylistic thing, I would suggest not pluralizing \""attention\"" (i.e., remove \""Attentions\"")."
2776,1," After obtaining this operator, this paper substitutes this operator to the optimal control problem and solve the optimal control problem to estimate the optimal control input, and show that the gap between the true optimal cost and the cost from applying estimated optimal control input is small with high probability."
2777,1,\n\n# After the update\n\nEvaluation section has been updated threefold:\n- TSP experiments are now in the appendix rather than main part of the paper
2778,1,\n\n\nA few comments: \n\nI find that there are several ways the paper could make a stronger contribution:
2779,1,\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions.
2780,1,"\n- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity"""
2781,1," Since the influence of the supervision is increased by increasing alpha, it can be expected that results should be better for increasing alpha."
2782,1,    This makes it hard to reproduce results.
2783,1,"\n[2] Li C, Xu K, Zhu J, et al. Triple Generative Adversarial Nets[J]. arXiv preprint arXiv:1703.02291, NIPS 2017."
2784,1," \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically. "
2785,1, Even the control task is very similar to the current proposed task in this paper.
2786,1,"""This work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster, which represents a set of similar data point, e.g., sentence, according to a metric."
2787,1,\n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.
2788,1, Then the result is c^{B}x + c^{B-1}r_1 + c^{B-2}r_2 + .. + r_B.
2789,1, This is not clear at all after reading the paper.
2790,1,"  First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered."
2791,1, This would have an impact on the relationship to the MMD
2792,1, \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm.
2793,1, Please refer to the paper \u201cDiscovering structure in multiple learning tasks: The TC algorithm\u201d published in ICML 1996.
2794,1," While these results are enlightening,"
2795,1,"  The engineering endeavor is impressive,"
2796,1," \n\nFew-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]\n\nand the reference therein give a few more recent baselines than your table."
2797,1,"- Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017."
2798,1,"\n\n- What are \""heterogeneous graph domains\""?\n"""
2799,1,"  Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications."
2800,1,"\n\nIn the experimental section, there is no discussion about the ration between the dataset size and the number of parameters to estimate."
2801,1," I did not get the time to go through the proofs, but they handle the fully connected case as far as I understand."
2802,1,"  This means, it's \""just\"" using NNs as a model class."
2803,1," However, \\alpha* is actually 0, and this choice would catapult w far away from w*."
2804,1," The main result here, is to show that using orthogonal random features approximates well the original kernel comparing to random fourrier features as considered in PSRNN."
2805,1,  There are two levels of sampling: of the latent X and of the observed value given the latent.
2806,1," The authors evaluate on several synthetic tasks, as well as an ICU timeseries data task."
2807,1,\n\nThe experiments are convincing and solid.
2808,1,"  If so, the training time becomes extremely important (and should be included in the \u201cNN Phase\u201d time measurements in Figure 4)."
2809,1," The approach is evaluated on an in-house dataset and a public NIH dataset, demonstrating good performance, and illustrative visual rationales are also given for MNIST."
2810,1, Is there any specific attempt to visualize or understand the embeddings inside the tree?
2811,1," The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting, "
2812,1," Readibility could be slightly increased by putting the figures on the respective pages.\n"""
2813,1,"""As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed."
2814,1,\n\nSome discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate.
2815,1,"\n\n- I'd like a more explicit accounting of whether 0.00006 seconds vs\n  0.024 seconds is something we should care about in this kind of\n  work, when the steps can take minutes or hours on a GPU."
2816,1,"""The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision."
2817,1, This is a bad way of introducing the main method.
2818,1,"  The additional information about the systems included in section 4.4 is pretty nominal, though, and I worry about the ability of others to replicate these results."
2819,1,"\nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me."
2820,1, Experimental results across a variety of tasks are convincing; in all tasks the Skip-RNNs achieve their goal of performing as well or better than equivalent non-skipping variants.
2821,1," The research itself seems fine,"
2822,1,"  But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper."
2823,1, And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output?
2824,1,"\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent"
2825,1,  Do you have insights on the role played by the architecture of the inference network and generative model?
2826,1," However, in the case of  active learning and semi-supervised learning the method is not compared to any baselines (other than the random one), which makes conclusions hard to reach."
2827,1, Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y).
2828,1, The contribution of this paper of this paper is two-fold.
2829,1,"\n\nOverall I think the trick needs to be motivated better, and the experiments improved to really show the import of the d-independence of the KL."
2830,1,"\n\nOn the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text. "
2831,1,"  Perhaps the TAXI problem, but you have two different taxis that require different actions in order to execute the same path in state space."
2832,1,"\n\n=== update ===\nI lowered my score and confidence, see my new post below."""
2833,1, which also attempts at unifying VAEs and GANs.
2834,1," So I suggest the authors do not claim that this method is a \""general-purpose\"" sentence embedding model."
2835,1,"\n\nMy main concerns are:\n\nControls\nTo generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced."
2836,1," There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous."
2837,1,"  Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings."
2838,1, Is mini-RTS a deterministic environment? 
2839,1," In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier)."
2840,1,\n\nI am interested in the following experiment to see how useful it is to increase the batch size compared to fixed batch size settings.
2841,1," For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions."
2842,1,"\n\nMinor:\n\nSecond paragraph, GloVec should be GloVe\n\n\""given many domains with uncertain noise for the new domain\"" -- not clear what \""uncertain noise\"" means, perhaps \""uncertain relevance\"" would be more clear."
2843,1,"...\n\nOverall I would vote against acceptance for this version.\n"""
2844,1,"""The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem."
2845,1," The experiments focus very specifically on the omniglot dataset, and it is not entirely clear to me what  should be concluded from the results presented. "
2846,1," So I think the idea is OK, but not a breakthrough."
2847,1,"\n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012]."
2848,1,\n6) It was unclear to me whether the analysis of SGD as a stochastic differential equation with noise scale aN/((1-m)B) was a contribution of this paper.
2849,1,  This dataset has not attracted much attention and most work in reading comprehension has now moved to the SQUAD dataset for which there is an active leader board.
2850,1," Is accuracy stable, can it drop back down below the threshold in the next epoch?"
2851,1, Section 3 makes connections to previous work on\nfinding optimal batch sizes to close the generaization gap. 
2852,1, However at less than optimal performance the reward fails to quality  the agent's ability to do SLAM.
2853,1,"\n\n\n3) Cons:\nImage captioning experiment:\nIn the experimental section, there is an image captioning result in which the proposed method is used on top of two baselines."
2854,1,"If only\n  \""the portion of\"" general English must be communicated, why is it validated?"
2855,1,y\n- Limited experiments
2856,1," I unfortunately have several issues with the paper in its current form, most importantly around the experimental comparisons."
2857,1,\n\nThe key issue is that the justification for the constrained gradients is lacking.
2858,1," However, my primary concern is that the model seems somewhat lacking in novelty."
2859,1, These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline.
2860,1," This is the case since for the generated realizations, the noise values are known."
2861,1, A formal definition of PSNR and\u2019profiling\u2019 is missing (does profiling simply mean finding a threshold for filtering?)
2862,1," Actually, authors only consider synthetic (noise-free) data belonging to one class only, thus not including the factors of variations related to noise and/or different classes."
2863,1," In practice the class means are \""learned\"", yet regularised towards the batch class means."
2864,1,"""The paper considers distribution to distribution regression with MLPs."
2865,1," \n\nThe paper works out a probabilistic analysis arguing that when either of these conditions obtains, there exists a fooling perturbation which affects most of the data. "
2866,1," If that is not the case,\nthe authors may have some contribution to make, but have not made it in this\npaper, as it does not explain the lower bound computation other than the one\nbased on LPs."
2867,1," \n\nOn the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study."
2868,1,"  It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n"""
2869,1,"  however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands)"
2870,1,\n\n+++ Section 3.1 problems +++\n\n- I think the toy example presented in section 3.1 is more confusing than it is helpful because the skip connection you introduce in the toy example is different from the skip connection you introduce in VANs.
2871,1,   Also the attack model is described without considering if the adversary knows the learner's algorithm.
2872,1,"\n\nThe approach is tested on two artificially generated datasets and on two real-world datasets, and compared with standard approaches such as the autoregressive model, the Kalman filter, and a regression LSTM.\"
2873,1,"\nFor example, in linear networks this is the same as RLSVI, bootstrapped DQN is one way to extend this idea to deep nets, but this is another one and it is much better because XYZ. (this discussion could perhaps replace the rather mundane discussion of BLR, for example)."
2874,1,  It would be helpful to understand how this approach avoids this issues;
2875,1," However, confirmation of this intuition is needed since this is a central claim of the paper."
2876,1,"""The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space."
2877,1,\n4. The authors can indicate the application scenario of this work.
2878,1,\n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves.
2879,1," Especially since uncertainty estimates are required, a Gaussian process would be the obvious choice."
2880,1,"\n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case."
2881,1,"  I fear the paper is not ready yet, but I am not opposed to publication as long as there are warnings in the paper about the shortcomings."
2882,1," I advise avoiding the use of the symbol f, which appears in only two places in Algo 2 and the end of sec 3.1."
2883,1, This also makes me question the generality of the approach since the pre-trained policy is rather simple while still providing an apparently strong score.
2884,1, This is just using the idea of adversarial learning (like GAN) and it is not related to self-play.
2885,1,\n\nBoth are useful contributions as long as deep wide Bayesian NNs are concerned.
2886,1," \n\nA non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.\n"""
2887,1,. It is hard to conclude which method is better based on a single real-world experiment
2888,1," \n\n---\nThe additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted. \n"""
2889,1," The paper makes a number of observations about the learning or transfer learning of shape bias vs. color by controlling and manipulating the input of training data based on shapes, negative shapes, and random images."
2890,1, \n- the the -> the\n\npage 6:\n- deterministic coupling could be discussed/motivated when introduced.
2891,1,\n- The computation of the encoder and decoder is not novel.
2892,1, \n\n\nPROS AND CONS\n\nImportant problem generally.
2893,1," Exploiting compositionality in model design is not novel per se (e.g. [1,2]),"
2894,1,"\n\nMODEL PRESENTATION\n\nI found section 2 difficult to read: in particular, the overloading of \\Phi\nwith different subscripts for different output types, the general fact that\ne.g. x and \\Phi_x are used interchangeably, and the large number of different\nvariables."
2895,1," but I have a few concerns which I\u2019ve listed below:\n\n1. Section 4, which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit \u201cdisjoint\u201d from the rest of the paper."
2896,1," On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet)"
2897,1, What is the benefit of joint-training?
2898,1,"  \n- Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant."
2899,1,\n\n- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent.
2900,1,"\n3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization."
2901,1, Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)?
2902,1,"""This paper proposes a WGAN formulation for generating graphs based on random walks."
2903,1,\n5-\tThe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.
2904,1,"  \n\nConcerning the related work, the authors didn't mention the Universal Value Function Approximation (Schaul et al, @ICML 2015) which precisely extends V and Q functions to generalize over goals."
2905,1," \n\nHowever, the submission also suffers from multiple CONS:\n\n- The novelty of this paper is limited."
2906,1, Try to remove any sentence or word that doesn't serve a purpose (help sell the algorithm).
2907,1, There is an entire research field about closing the reality gap and transfer learning.
2908,1,"\n\n\""four automaton states Q\u03c6 = {q0, qf , trap}\"": Is it three or four?"
2909,1,\n\nQuality:\nI found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simple.
2910,1," However, the proposed approach and the experiments are not convincible enough."
2911,1,\n\nThe idea of using dilated convolutional networks as drop-in replacements for recurrent networks should have more value than just reading comprehension tasks.
2912,1," Further, the first two \""observations\"" in Section 2.2 would be more accurately described as \""intuitions\"" of the authors."
2913,1, The distribution \ncould be replaced with a normalizing flow. 
2914,1," Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop."
2915,1, \n\nComments:\n\n1. How do the PAG scores differ when using a full covariance structure?
2916,1,\n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?
2917,1,"\n\n\nFrom the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction.\nAlso, there is the work by Hendricks et al on Generating Visual Explanations."
2918,1,"\n- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)?"
2919,1, Does it mean that you have not run enough iterations for the baseline methods?
2920,1, What is the range of t1 and t2?
2921,1,"""In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules."
2922,1,"\n\nTo summarize, I like the work and I can see clearly the motivation."
2923,1,\n\nMinor:\nPenultimate paragraph of the introduction section: \u201cget get\u201d -> get
2924,1,"""Quality\n\nThis paper demonstrates that human category representations can be inferred by sampling deep feature spaces."
2925,1,\n2. The proposed experiments are not very conclusive.
2926,1,"  \n\nIn complex environments learning the PATH network is far from easy.[[CNT], [null], [DIS], [MIN]]  I.e. random walks will not expose the model to most states of the environment (and dynamics).[[CNT], [null], [DIS], [MIN]]  Curiosity-driven RL can be quite inefficient at exploring the space. "
2927,1, What happens if 0 has semantics ? 
2928,1,"\n\nSection 4.4: It\u2019s very very good that you compared to \u201cflat attention\u201d,"
2929,1,". \nOnly whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task"
2930,1,"The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2."
2931,1,"""The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments."
2932,1,"\n\nSecondly, the experimental analysis is not very extensive. "
2933,1,"""SUMMARY.\n\nThe paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting."
2934,1," I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of \""deep exploration\"" and you should be clear that your parameter noise does *not* address this issue."
2935,1," Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics."
2936,1," \n\nORIGINALITY: The idea of using a pointwise mutual information tensor for word embeddings is not new, but the authors fairly cite all the relevant literature."
2937,1, This choice makes their algorithm not currently relevant to most autonomous vehicles that use ego-centric sensing. 
2938,1," \nIn Neal\u2019s case, the method was explored for single output regression, where the fact that we lose this sharing of basis functions may not be so restrictive."
2939,1," Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction."
2940,1, I was wondering if the problem is from the tanh activation function (eq 2).
2941,1, The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed.
2942,1,"\n7) As always, it would be illustrative if you could include some examples of failure cases, which would be helpful both in suggesting ways of improving the proposed technique, and in providing insight into where it may fail in practical situations."""
2943,1," For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector)."
2944,1,\nGood results.
2945,1,"\n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \""noisy\"" training set and adding that to clean data?"
2946,1,"""The main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of Jia and Liang, EMNLP 2017."
2947,1,"\n\nMaybe I misunderstood something, but one big problem I have with the paper is that for a \u201ccausalGAN\u201d approach it doesn\u2019t seem to do much causality."
2948,1,".In particular, the concave player plays the FTRL algorithm with standard L2 regularization term."
2949,1, How do the features learned at different layers compare to the ones of the original network trained for image classification?
2950,1," Furthermore, the paper lacks in novelty aspect, as it is uses mostly well-known techniques."
2951,1, They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried.
2952,1,"\n\n[Ref1] Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, Erkki Oja. Clustering by Nonnegative Matrix Factorization Using Graph Random Walk.In NIPS 2012."
2953,1,"""The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin)."
2954,1, A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards.
2955,1,n- it would be interesting to apply the approach to graphs with discrete and continuous labels
2956,1,"\n\n[1] Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. \""Wavenet: A generative model for raw audio.\"" arXiv preprint arXiv:1609.03499 (2016)."""
2957,1,"n\nIn general, I think the idea of using saliency to detect adversarial attacks is very good and represents an interesting avenue of research."
2958,1,  This however should be quite important.
2959,1, Proposed method to visualize the loss function sounds too incremental from existing works.
2960,1," While the revised version contains more experimental details,"
2961,1,\n\nWhat I like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context.
2962,1," \n\n\n-- Comments and questions to the authors:\n\n1. In the introduction, please, could you add references to what is called \""traditional solutions\""?\"
2963,1,"\n5) Learning rates and their drops should be rescaled taking into account the schedule of the batch size and the rules to adapt learning rates in large-scale settings as by Goyal. """
2964,1, But the Atari results is still significantly worse than the current SOTA.
2965,1," It might help to emphasize the speed-up in compute more in the contributions.  """
2966,1, I feel that this is\nsomething the authors should at least compare to in the empirical evaluation.
2967,1, They also observe that the ID drops with each successive layer.
2968,1, The authors show that this effect is caused by the noise in the obective function.
2969,1, \n+ A theoretical guarantee of the efficiency of an aspect of the proposed method is given.
2970,1, The paper is interesting and well-written.
2971,1,"  It is first introduced in 2.3\nand described as the relative position of pixel i and pixel j on the image, but\nthen used in the context of a graph in (4)."
2972,1,"\n\n- The notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology, e.g., clustering."
2973,1," As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset."
2974,1," The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping."
2975,1, The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles.
2976,1,\nIt is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.
2977,1," This is not natural language, firstly because the language in the dataset is synthetically generated and not \u201cnatural\u201d."
2978,1," Specifically,\na.\tThe author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d."
2979,1," The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples."
2980,1, A bit more discussion on these choices would be helpful.
2981,1,"\n\nIn the negatives, the paper should mention in the discussion and intro that all the TP variants ignore the issue of dynamics."
2982,1,\n* More analysis can be conducted on the training process of the model.
2983,1," As the paper says, a difference of one word can lead to completely different goals and so, the noise robustness experiments seem to test for the biases learned by the agent in some sense (which is not desirable)."
2984,1,"""General-purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcome\u2014this paper included."
2985,1,"""This paper is an experimental paper."
2986,1,"""(Last minute reviewer brought in as a replacement).\n\nThis paper proposed \""Bayesian Deep Q-Network\"" as an approach for exploration via Thompson sampling in deep RL."
2987,1,"The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks."
2988,1,  This is an informative negative result.
2989,1,"   It seems that without bounding D, we can make the model arbitrarily bad, no? "
2990,1," The results might be of theoretical interest,"
2991,1, Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP.
2992,1," The method seems solid, and the writing is pretty good."
2993,1,"  In Advances in Neural Information Processing Systems 24, pages 2402\u20132410. 2011. """
2994,1, Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.
2995,1, The method is evaluated on multiple experiments.
2996,1," While performing worse than the amTFT approach and only working well for larger number of iterations, the outcome-based evaluation shows benefits."
2997,1," However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al."
2998,1,   What if phi_1 is o ((A v B) Until C) and phi_2 is o ((not A v B) Until C).
2999,1, \nThis algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty.
3000,1," Some examples of related work missing in this area is \""Variational Sequential Monte Carlo\"" by Naesseth et al. (2017) / \""Filtering Variational Objectives\"" by Maddison et al. (2017) / \""Auto-encoding sequential Monte Carlo\"" Le et al. (2017)."
3001,1,which hyper parameters were tuned and with which values?
3002,1, I requested for the authors to use their method on the best performing baselines (i.e. Yao et al. 2016 or Liu et al. 2017) or explain why this cannot be done (maybe my request was not clearly stated).
3003,1, It would be great that the paper could be more explicit about its limitations.
3004,1,   \n\n    2. Are the x_1 ... x_n here word embeddings or one-hot encodings?
3005,1,"\n\nA few questions/comments:\n1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to?"
3006,1," I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting."
3007,1, Removing baseline is a reasonable step and the paper includes analysis of several spike-train datasets.
3008,1,"""In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network."
3009,1, Not much novelty.
3010,1,\nWhat are the context vectors in Figure 1? 
3011,1,"\nIn my opinion, this result is not too surprising given the existing power of deep learning to fit large datasets and generalize well to test sets."
3012,1,"  Moreover, as a result, I believe this extra length has earned this paper an unfair advantage relative to other submissions, which themselves may have removed important content in order to abide by the length suggestions."
3013,1,"\n\nFor all these reasons, I'm afraid I must recommend this paper be rejected."""
3014,1,\n- I like the new attention over chart cells.
3015,1, \n\nThe evaluation remains superficial with minimal quantitative comparisons.
3016,1, What is the purpose of including two rows in table 2 with the same\nmethod?
3017,1,"""This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation."
3018,1," It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests."
3019,1," These works go way back to the years 2005 - 2007, when deep learning was not called deep learning. "
3020,1,"\n- give a reference for the expressions given for A(k,n) in 24 and 25\n- (27) and (28) should be explained in more detail."
3021,1,"\n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017."
3022,1,The problem and its potential applications are well motivated.
3023,1," Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention?"
3024,1, LSTMs as well could have been a point of comparison.
3025,1," The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets."
3026,1,"  In the real world, however, it is unlikely that any low level controller would be able to do this perfectly."
3027,1,"""[Apologies for short review, I got called in late."
3028,1," Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997)."
3029,1,"  As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments."
3030,1, The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks.
3031,1,"\n\n- The paper simultaneously proposes methods for generating point clouds, and for evaluating them."
3032,1,\n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\
3033,1,\n\n* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s?
3034,1,\n2. New latent variable model bound that might work better than classic approaches.
3035,1,\n2. Strong experimental setup that analyses in details the proposed extensions.
3036,1, How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only?
3037,1," \n\nWeaknesses:\n- It is unclear that this idea would generalize beyond a logistic regression classifier, which might limit its applicability in practice"
3038,1,  The paper proposes a GAN model to generate graphs with non-trivial properties.
3039,1," \n\nSecondly, there seems to be a big disconnect between attaining a high score in navigation tasks and perfectly solving them by doing general SLAM & optimal path planning. "
3040,1,"\n+When n = O(m log m), the result that \\epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this?"
3041,1,"\n\n(C) Do the authors expect that it will be straightforward to remove the assumption that A is symmetric, or is this an inherent limitation of the approach?"
3042,1," The problem formulation is mostly reasonable, and the evaluation seems quite convincing."
3043,1,"r.\n\nCons:\n- It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b)"
3044,1,"""This paper presents a method for choosing a subset of examples on which to run a constraint solver\nin order to solve program synthesis problems."
3045,1, And the parameter sharing in the parametric one might actually be beneficial.
3046,1,\n\nI would like to see more explanations of Figure 4.
3047,1,"\n\nThe authors seem unaware of a large literature on \""knowledge base completion.\""  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015],"
3048,1," Note that exposition in Le and Zuidema (2015) discusses the pruned case as well, i.e., a compete parse forest."
3049,1," Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer?"
3050,1,".\n\n(3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing"
3051,1," The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small."
3052,1,"\n\nThe idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales."
3053,1,  \n\nTechnical: It is hard to tell how some of the components of this approach are technically justified.
3054,1,"\n\n-\tThe paper shows how state of the art relational networks, performing well on multiple relational tasks, fail to generalize to same-ness relationships,"
3055,1," SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y' = argmin_y E(f(x), y)."
3056,1, They authors then extend the approach to \u201cpermuted order\u201d and finally present their proposed \u201csoft ordering\u201d approach.
3057,1,"""This paper proposes an interesting  approach to prune a deep model from a computational point of view. "
3058,1, \n\n*Significance*\nThe significance of the proposed evaluation framework is not fully clear.
3059,1, This request was not addressed and instead the authors just improved the average performing baselines in Table 3 to meet the best baselines.
3060,1,"\n- The unconditional generative model *only* relies on the \""injected randomness\"" for generating drawings, as the initial state is initialised to 0."
3061,1,"""The paper makes some bold claims."
3062,1,"\n\nThough this is an important direction to investigate, there are several issues:\n\n1. Comparison with previous results is misleading:\na.\t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%.\nb.\tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper. "
3063,1, The experiments in this direction again feel somewhat contrived by imposing time limits and then removing them.
3064,1,\n\nExperiments in a simple car simulator on a task which requires the car to take a certain exit while navigating through traffic are presented with two baselines: 1. a greedy policy which navigates to the right-most lane asap and then follows traffic till the exit is reached.
3065,1," \n\nWhile the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments."
3066,1," As such, I recommend that the paper be rejected."
3067,1," Your a(.) function seems to be the policy here, which is invariable denoted \\pi in the RL literature."
3068,1," They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models."
3069,1,"\n* As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm."
3070,1," The DTP algorithm is slightly adapted to make it more biologically plausible, by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation (leading to the variant SDTP), and by making the optimisation of both involved losses parallel."
3071,1,"""This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data."
3072,1,\n\nCons:\n- Missing comparison to prior work on sequential MNIST
3073,1," I am confident that it is easy to identify many precursors in the optimization literature, but I am not an expert on this."
3074,1,\n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee.
3075,1,\n2. Good empirical evaluation with ablations.
3076,1,"\n\nI like the idea of learning an \""output stochastic\"" model -- it is much simpler to train than an \""input stochastic\"" model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable. "
3077,1, I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough.
3078,1, \n2.\tThe experiments are insufficient to show the effectiveness.
3079,1,"  Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer."
3080,1," \n\nLast, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks."
3081,1,  But it also requires extra work to ensure they are generating meaningful graphs.
3082,1," At this moment, these identified class-specific core units are useful for neither reducing the size of the network, nor accelerating computation. """
3083,1,"""I am not sure how to interpret this paper."
3084,1," There are a few problems with the paper.\n\nFirstly, the title and abstract discuss \""modifying memories\"", but the content is only about a rotation operation."
3085,1,"\n\n* On the convexity vs non-convexity (Sec. 6), it is interesting to see how pushing the Id through the net changes the look of the loss for deep nets."
3086,1,"\n\nUnfortunately, the practical usefulness of the algorithm has not been demonstrated."
3087,1," Although there have been much previous work, this paper is along this line."
3088,1," I lean on accept side.  \n\n\n"""
3089,1,There is only one mostly minor issues with the algorithm development and the experiments need to be more polished. 
3090,1,"""+ Quality:\nThe paper discusses an interesting direction of incorporating humans in the training of a generative adversarial networks in the hope of improving generated samples."
3091,1,\n\nThe UIs that are represented in the dataset seem quite simple; it\u2019s not clear that this will transfer to arbitrarily complex and multi-page UIs.
3092,1," It seems highly problematic to simply remove a whole part of the gradient, to reduce effect on the next state."
3093,1,"""This paper proposes a new way of sampling data for updates in deep-Q networks."
3094,1,\n\n4. Are you willing to release your code to reproduce the results?\
3095,1,"""The authors show how techniques typically applied to real-valued networks (e.g. with real-valued inputs and parameters) can be straighforwardly generalized to complex-valued networks (e.g. with complex-valued inputs and parameters)."
3096,1," \n2-\tIt is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume)."
3097,1," As is, the paper does not bear enough novelty on top of [Defferrard et al. 2016], or is not demonstrating it even if there exists any."
3098,1," There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago."
3099,1,The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function.
3100,1, \n\nOriginality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.
3101,1, I do not see a clear novelty in the proposal.
3102,1,"\n2.) Since the paper used \u201cthe pre-trained PixelNet to extract surface normal and edge maps\u201d for ground-truth generation, it is not clear whether the approach will work as well when the input is a ground-truth semantic segmentation map."
3103,1, Experiments are only conducted in CIFAR100.
3104,1,"  Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. """
3105,1,\n\nThe overall significance of the results is not very strong since the paper focuses solely on experiments conducted on very limited data and artificially manipulated data.
3106,1, \n\nCons:\n- The visual depiction of the auto-encoded meshes looks a bit strange.
3107,1," However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem."
3108,1,\]n-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings
3109,1,"""The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel."
3110,1, It tries to provide an explanation for the phenomenon and a procedure to test when it happens.
3111,1,"""The paper takes a recent paper of Zhang et al 2016 as the starting point to investigate the generalization capabilities of models trained by stochastic gradient descent."
3112,1,"""In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. "
3113,1,\n\nI like the idea of this paper.
3114,1," \n\n- The format is inconsistent. Section 1 is numbered, but not the other sections."
3115,1, Any practical way of choosing the mask?
3116,1,"""Quality:\nThe paper appears to be correct\n\nClarity:\nthe paper is clear, although more formalization would help sometimes\n\nOriginality\nThe paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know."
3117,1," However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non-shallow architectures are not considered for comparison here."
3118,1,\n\nThe lack of completeness (definitions of tasks and robustness) also makes the paper less impactful than it could be.
3119,1,"\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory."
3120,1," I think that such a paper could have merits if it would really push the boundary of the feasible, but I do not think that is really the case with this paper: the task still seems quite simplistic, and the empirical evaluation is not convincing (limited analysis, weak baselines)."
3121,1," Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like."
3122,1," \n\n- Page 4- sec 3 \"".. it should be seen as a success\""; the claim is not supported well."
3123,1, Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning?
3124,1,"\n\nThrough a set of experiments, the paper shows the effectiveness of the method."
3125,1,\n\nThe major critique would be the qualitative nature of results in the sections on \u201cDecipering the latent code\u201d and (to a lesser extent) \u201cMixed sample-beam decoding.
3126,1," One could additionally consider that removing the skip connections at the end of training reduces the computational expense (slightly), but beyond that the expected utility of this investigation is very hazy from the description in the paper."
3127,1, but it is unclear why the problem should be formulated in such a way..
3128,1, This means that they bring\nall necessary information for rebuilding their continuous counterpart.
3129,1,"   However I am surprised at the low absolute performance of all reported results, and would appreciate if the authors could help to clarify whether this is due to differences in experimental setup or something else."
3130,1," This is not a criticism, however, it is difficult to see the reason for including the structured low-rank experiments in the paper (itAs a reader, I found it difficult to understand the actual procedures used. "
3131,1,"""The paper proposes a simple dilated convolutional network as drop-in replacements for recurrent networks in reading comprehension tasks."
3132,1,"\n\nAlso, I think it\u2019s a bit of an exaggeration to call a gap of 2.71 nats \u201cmuch tighter\u201d than a gap of 3.01 nats."
3133,1,"The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating."
3134,1, We do not get supervised targets of the confidence of our value estimates.
3135,1," I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct?"
3136,1,  The authors perturb input images and create explanations using different methods.
3137,1,"\n* In practice, how much more GPU memory is required to train the model with parallel branches (with same parameter budgets) because memory consumption is one of the main problems of networks with multiple branches."
3138,1,"\n\nThe key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization. "
3139,1," To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters."
3140,1,"  They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom."
3141,1,\n2) faster gradient update than Vorontsov et al. (2017)
3142,1,   The paper uses such object representation to inform state representation for reinforcement learning.
3143,1,  It's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results.
3144,1,\n\n- A similar classification of collaborative filtering models with covariates is proposed in this thesis (p.41):\nhttps://tspace.library.utoronto.ca/bitstream/1807/68831/1/Charlin_Laurent_201406_PhD_thesis.pdf
3145,1, b) perturbed decoded image;
3146,1, Experiments were conducted on various data sets and experimental results were reported.\n\nPros:\n* Studying semi-supervised learning techniques for deep models is of practical significance.
3147,1, The author should also have a discussion on them.
3148,1," Their \""related work section\"" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations."
3149,1,  Initial node embedding comes from both type and tokenized name information.
3150,1,\n\n\nPros:\n\nThe result highlights an interesting relationship between deep nets and Gaussian processes.
3151,1, \n\nThe basic privacy paradigm proposed seems to be:\n1. train a GAN using private data
3152,1,\n(+) The authors ablate and experiment on large scale datasets
3153,1, The authors must nonetheless have some intuition about this.
3154,1," For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?\n\nSection 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results."
3155,1,"\n\n3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning."
3156,1, It doesn\u2019t live up to the promise of rotation and scale equivariance in 3D.
3157,1," Such a model would indeed be found if i and j are put in the same cluster, but the method would fail to do so, leading to high fragmentation."
3158,1,\n- The proposed method is not much novel.
3159,1," In fact, Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy."
3160,1,"\n\nIn Section 5.1, experiments training a 16-4 ResNet on CIFAR-10 compare three\npossible SGD schedules: * increasing batch size * decaying learning rate *\nhybrid (increasing batch size and decaying learning rate) Fig. 2, 3 and 4 show\nthat across a range of SGD variants (+/- momentum, etc) these three schedules\nhave similar error vs. epoch curves."
3161,1,\n\nThe paper is well written and easy to understand.
3162,1," But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR."""
3163,1,"""This paper introduce a times-series prediction model that works in two phases."
3164,1,"""UPDATED COMMENT\nI've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments."
3165,1," Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \\lambda and the choice of points for gradient computation."
3166,1,"\nAs a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point."
3167,1, \n\nThe paper follows with some fun experiments implementing these new game theory notions.
3168,1,\nThe experiments show clearly that a) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks
3169,1, A lot of tricks are stacked to reduce the performance degradation.
3170,1, This paper proposes to generate text using GANs.
3171,1,\n\nPros: \n-Authors provide some empirical evidence for the benefits of using their technique.
3172,1," The paper then discusses how SGD can be seen as an algorithmic way of finding minima with large \""evidence\"" --- the \""noise\"" in the gradient estimation helps the model avoid \""sharp\"" minima, while the gradient helps the model find \""deep\"" minima. "
3173,1," However: there is a strong risk of overfitting in the current situation and we will always wonder if the given figures correspond to \""lucky trial\""."
3174,1," It might find the optimal policy quickly though, even though it doesn\u2019t resolve the entire value function completely."
3175,1,\n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation
3176,1," Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary."
3177,1,"\n\nAs long as the analysis is experimental, the state of the art should be considered."
3178,1,  By choosing the hierarchy level and type of filter the results of the GAN differ.
3179,1,"   In the conclusions the authors admit that: \""Many theoretical questions remain, such as accelerating the search for Fourier peaks\""."
3180,1," The assumption here is that labels (aka outputs) are easily available for all possible\ninputs, but we don't want to give a constraint solver all the input-output examples, because it will\nslow down the solver's execution."
3181,1," However, the work is not mature enough for publication."
3182,1,"\n\nCons:\n- Clearly, the proposed function is not faster than ReLU."
3183,1," This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L."
3184,1,"\n- There are problems with capitalization in the references. """
3185,1,\n- The objective function also seems somewhat non-intuitive.
3186,1,  The removal strategy is not at all well explained \u2013 the objective function details and solving it are not discussed.
3187,1," \n- Unless I am mistaken, Gated Orthogonal Recurrent Units: On Learning to Forget from Jing et al. also reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively?"
3188,1, More about conditions on eta_v would be illuminating. 
3189,1, The experiments are conducted on the Shapeworld dataset.
3190,1," They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem."
3191,1,"  Thus, the comparisons in Tables 2 and 3 are not fair.\n\nThe most realistic graph generator is the BTER model.  See http://www.sandia.gov/~tgkolda/bter_supplement/ and http://www.sandia.gov/~tgkolda/feastpack/doc_bter_match.html.\n\nA minor point: The acronym GTI is never defined."""
3192,1, It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores.
3193,1,"""This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression."
3194,1,"""This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history."
3195,1,"  Neural Networks, 2001. 14(6-7): p. 907-919."
3196,1,\n\n- I\u2019m a bit puzzled by Theorems 4.1 and 4.2 and why they are useful.
3197,1, \n(iii) The results on CIFAR10 are very poor.
3198,1, Results further outline the resemblance between the compared methods.
3199,1,"\n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older)."
3200,1,\n\nI don't see what Socher et al. (2013) has to do with the loss in equation (7).
3201,1,"\n\nI believe the paper should be accepted.\n"""
3202,1, They demonstrate that proposed model could work better and rational of write network could be observed.
3203,1,.\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power.
3204,1," \n\nIt is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w)."
3205,1," This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets."
3206,1, The recurrent model takes the form of a graph neural network.
3207,1,"?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition"
3208,1,".\n\nMoreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the \""related work\"" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis)."
3209,1," \n\nTo me, there is a major flaw in the approach."
3210,1,"\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and J\u00fcrgen Schmidhuber."
3211,1,It needs to but \n                  that was not explicit in the paper.
3212,1,"It may be better to relate the\nstructure of the network to other measures of the hardness of a problem, e.g.\nthe phase transition."
3213,1," The basic idea is that for fitting nonlinear models with a convex loss, if the mapping from the weights to the outputs is open, then every local optimum in weight space corresponds to a local optimum in output space; by convexity, in output space every local optimum is global."
3214,1,"But, the paper does not provide in-depth discussion on this."
3215,1,\n\nExperiments on two domains are presented.
3216,1,\n\nQuestion:\n- Which layer mean and variance are reported in Figure 2? 
3217,1," As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15)."
3218,1, I am also wondering why authors have not tried their method on at least one more task?
3219,1, The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback);
3220,1,"  All methods work on \u201cfc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge\u201d, and the best-performing method (Skip GRU) achieves 9.02 mAP."
3221,1, A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN.
3222,1," Base your definition on F=(V,E) or F=(V,A)."
3223,1," It is shown that certain color invariant \""input\"" layers can improve accuracy for test-images from a different color distribution than the training images."
3224,1,"""The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size;"
3225,1," \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems. """
3226,1,"""The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates."
3227,1,"In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator."
3228,1," The above heuristic is obviously specific to the domain, but similar\nheuristics could be easily constructed for other domains."
3229,1, The paper is badly written.
3230,1, I guess it was softmax probability.
3231,1,\n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches.
3232,1," \n\nFor the novel approach and the theoretical backing, I consider the paper to be a good one."
3233,1," There are a few things I think could be cleared up, but this seems like good work (although I'm not totally up to date on the very recent literature in this area)."
3234,1," Unlike prior work, this is a real-valued instead of a binary quantity."
3235,1,"  How is the structured update \""learned\""?"
3236,1,"  but if 18% of my program variables were erroneously flagged as errors, the tool would be useless."
3237,1,This paper proposes to use automatic differentiation to compute the inverse function efficiently.
3238,1,"\n\nCriticisms\n\nBased on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method."
3239,1," It is known that even basic multi-layer perceptrons (MLPs) result in much lower classification errors, e.g., for MNIST. LeCun et al., 1998, is a classical example with less then 3% error on MNIST with many later examples that improve on these."
3240,1," Section 4.1 states\""We use a method similar to the DAGGER algorithm\"", but what is your method."
3241,1," Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies?"
3242,1, Such a case cannot be resolved by a short horizon MC method.
3243,1,  \n\nI would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets).
3244,1, Can the system improve further if speaker-adaptive features are used instead of log mels? 
3245,1,\n\nMy first concern is the motivation.
3246,1,\np7-8. I had trouble following the left/right & front/back notation.
3247,1, It\u2019s a simple method that can be applied post-training and seems to be effective.\
3248,1," First, the description of what\nis the prior used by batch normalization in section 3.3 is unsatisfactory."
3249,1, \n\nCons:\n1. The application range of the method is very limited.
3250,1,"\n\nMisc:\n\nTables 3 and 4 would be easier to parse if resources were simply reported in terms of total GPU hours."""
3251,1,. No insights can be derived from the presented results.
3252,1, I struggle a little to see the relevance of the proposed method without a good motivating example.
3253,1, \n\nSmall comments that did not impact paper scoring:\n1) eq 1 we usually don't use the superscript \\gamma
3254,1," So, the comparison should be to any other work that can deal with \""similar environment but different details\""."
3255,1," Although the authors cite QSGD, they do not directly compare against it in experiments."
3256,1, \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples).
3257,1, I also like the unsupervised learning through gating function and label difference cost
3258,1,"  But to the best of my knowledge, this is the first paper that applies this concept to the open world classification task."
3259,1," We see that CCC is a successful, robust, and simple strategy in this\ngame."""
3260,1," It is not clear to me as why is the case, if anything it should be the opposite."
3261,1,"\n\nFinally, there are also many other weaknesses. The paper is quite poorly written in places, has poor formatting (citations are incorrect and half a bibtex entry is inlined), and is highly inadequate in its treatment of related work."
3262,1," Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017."
3263,1,"\n\n\""Both deep learning and FHE are relatively recent paradigms\"". Deep learning is certainly not recent, while Gentry's paper is now 7 years old."
3264,1," Including explicit formulas would be very helpful, because, for example, it looks like when reported in table 1 the metrics are spatially averaged, yet I could not find an explicit notion of that."
3265,1,"\n\nThis paper merits acceptance on theoretical merits alone, because the FTRL analysis for convex-concave games is a very robust tool from theory (see also the more recent sequel [Syrgkanis et al. 2016 \""Fast convergence of regularized learning in games\""]) that is natural to employ to gain insight on the much more brittle GAN case."
3266,1," \n\nOverall speaking, this work is quite interesting."
3267,1,\n\n- The authors should clearly highlight what is their main technical contribution.
3268,1, The encoding-decoding mechanism being attacked is too simple without any security enhancement.
3269,1," However, the bounds also depend polynomially on L, and as far as I can tell, L can be polynomial in T for certain systems if we want to achieve a good overall cost."
3270,1,"Even under the assumption that a normalized edit probability is not particularly critical (an issue that was not raised at all in the paper, let alone assessed), the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural language."
3271,1, In these cases focusing on the hardest negative reduces performance.
3272,1,"""The paper demonstrates the need and usage for flexible priors in the latent space alongside current priors used for the generator network."
3273,1," Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections."
3274,1,"   I would also argue that you did not run either method as long as you should have (both approaches lack the longer term 'compression' stage whereby layers near the input reduce I(X,Z_i) as compared to their starting condition)?"
3275,1,"""The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid."
3276,1,". In my view, it is not a correct statement to call the feature extraction from a CNN unsupervised feature learning as the referred CNNs in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as well. "
3277,1,"\n\n**EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept)."""
3278,1,", but agree with the authors that the theoretical results are non-trivial and interesting on their own merit."
3279,1,. I would suggest keeping the main results in the main body and move extended results to an appendix.
3280,1,"In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet."
3281,1," At some level, I question whether the proposed framework is doing any more than just value function propagation at a task level, and these experiments would help resolve this."
3282,1, It feels a bit that the authors want to hide the fact that they only\ncompare with a singe epsilon-greedy baseline (DDQN).
3283,1," Moreover, it is important to report the training performance of deeper VANs (which have a worse generalization error) to clarify if the VANs suffered difficulties in optimization or generalization."
3284,1, We ignore this detail as implementation requires application of the sum instead of integral.
3285,1,"  Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input?"
3286,1,"\n- AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change."
3287,1,\n\nThe authors do a great job walking us through the formulation and intutition of their proposed approach.
3288,1, Table 1 does not necessarily contradict this view. 
3289,1,"""This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means)."
3290,1,\n\npros:\n(a) An interesting problem to evaluate the robustness of black-box classifier systems
3291,1, This incorporates information from more distant vertices in one propagation step.
3292,1, It yields smaller receptive field than the proposed method when the model depth is very small.
3293,1,  This method is based on convnets that map raw pixels to a mask and feature map.
3294,1,. These questions should really be answered in [1]
3295,1," \n\u201cSubsequently, we present that learning counterfactual return leads the model to learning optimal topology\u201d => Do you mean \u2028\u201cmaximizing\u201d instead of learning."
3296,1, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste.
3297,1,"\n- It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc."
3298,1," A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm."
3299,1,"  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage."
3300,1," By contrast, the DCD method in [Ref3] can achieve 0.54."
3301,1,\n\nSGD comparison: \u201cfixed learning rate.
3302,1," For example, in binary classification tasks, a very small S_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each other."
3303,1,"\""\n\n* You should add citations for the statement \""In these and related settings, gradient descent has started to be replaced by inference networks."
3304,1,". \n\nComments: There\u2019s lots of typos, please proof read to improve the paper."
3305,1,"\n\u2013 The primary baselines compared to are unsupervised methods (PCA and LDA), and so demonstrating improvements over those with a supervised representation does not seem significant or surprising."
3306,1,". The paper will be much stronger if it has some experiments along this line. """
3307,1,\nThe experimental results make more sense now.
3308,1,"\n\nThis paper is a critique of deep reinforcement learning methods for learning to navigate in 3D environments, and seems to focus intensively on one specific paper (Mirowski et al, 2016, \u201cLearning to Navigate in Complex Environments\u201d) and one of the architectures (NavA3C+D1D2L) from that paper."
3309,1,"\n2. With the same number or less of parameters, this method is able to outperform previous methods, with much less time."
3310,1,\n\nYou never explicitly mention what your training loss is in section 5.1.
3311,1,"\n\nMinor Weaknesses:\n- Since this paper is closely related to Monti et al., it would be good if authors used one or two same benchmarks as in Monti et al. for the comparisons."
3312,1,"However, the paper does not properly relate their results, assumptions in the context of the existing literature."
3313,1,"\nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\""Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\""\n \nwhich could have shed more light on the matter."
3314,1,  Synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases.
3315,1,"\nHow about min, max ?"
3316,1,"  While the proposed approach is more scalable, it is hard to judge the extend of this."
3317,1," Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner."
3318,1,However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate.
3319,1,"""The authors propose a new exploration algorithm for Deep RL"
3320,1,\n\n* The training procedure of the model is not explained in the paper.
3321,1, You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. 
3322,1," In this way, your guarantees don't say much more than those of Arjovsky et al., who must assume that their \""critic function\"" reaches the global optimum: essentially you add a regularization term, and show that as the regularization decreases it still works, but under seemingly the same kind of assumptions as Arjovsky et al.'s approach which does not add an explicit regularization term at all."
3323,1, I thank them for that.
3324,1,  Section 3 continues with a sensitivity analysis now considering performance and storage of the method.
3325,1, The authors discuss how this architecture can solve the problem exactly.
3326,1,"\n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious"""
3327,1,"\n\n2. Indeed, AlexNet is a good seedbed to test binary methods."
3328,1, \n-- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions?
3329,1,"\n\nThere are some concerns the authors should consider:\n- Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper."
3330,1,"""The work claims a measure of robustness of networks that is attack-agnostic."
3331,1," The idea of functional PCA is to view \\x as a function is some appropriate Hilbert space, and expands it in some appropriate basis."
3332,1,"\n\n- Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct?\"
3333,1," Moreover, when the performance becomes more stable for the classes in the tail, I'd have expected that the standard deviation of the mean class accuracy would decrease, from the results there is no difference between Softmax and the proposed method: 44 +/-1 for Softmax (miniImageNet) to 41 +/-1 for the proposed NCM approach."
3334,1, This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established.
3335,1,\n\nThe main problem boils down to the fact that the paper does not start from a loss function and compute all the gradients in a systematic way.
3336,1," Even if the number of parameters is small, learning them might require complex computations on the whole data set."
3337,1, The only novelty I can pinpoint is the proposed visual Turing test.
3338,1," I suspect that the results will look different when plotted in different ways, and would enjoy some extra plots in the appendix."
3339,1, The final sentence probably relates back to the CI approach.
3340,1, I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view?
3341,1," This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle)."
3342,1," The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts."
3343,1, Have I missed the reference to the proof of Thm 2?
3344,1,"\n- not clear what's new"""
3345,1, \n\nI found it very difficult to follow the claims in the paper.
3346,1," \n\nTo summarize, the first part is interesting and nice,"
3347,1," The overview of the literature is also very well done. """
3348,1,"\n\nThe paper is well presented, the result is explained and compared to other results, and the proofs seem correct."
3349,1, The paper describes the use additional mechanisms for synchronization and memory loading.
3350,1,"It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on."
3351,1, This underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour.
3352,1," \n\nCons:\n- No ablation studies. \n"""
3353,1,"   However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies."
3354,1,"  The reason behind is that the latter is the cost of the first one, just the old saying that \""there is no such thing as a free lunch\""."
3355,1, Am I missing sth?
3356,1," They offer a rigorous analysis into the behavior of optimization in each of these cases, concluding that there is an essential singularity in the cost function around the exact solution, yet learning succumbs to poor optima due to poor initial predictions in training."
3357,1,"\n\n-\tThe paper describes two sets of relationships, but it soon suggests that current approaches actually struggle in Same-Different relationships."
3358,1,   But we want that the corrupted passage has the same correct answer as the uncorrupted passage and this difficult to guarantee.
3359,1,"\n\n5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this."
3360,1,"  Moreover, I don\u2019t see why the learning rule equations (14-15) are described in the appendix, while they are referred constantly in the main text.[[CNT], [PNF-NEG], [CRT], [MIN]]  The final impression is that the problem of the dynamical range of the hidden layer activations is not fully resolved by the empirical solution described in Appendix D: perhaps this problem affects CCNs more than FFN."
3361,1," Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero."
3362,1,"\n\nIn summary, I like the idea, the application and the result of this paper."
3363,1,"\nMoreover, the fact that there are huge oscillations makes me think that the\nauthors are measuring the function value during the line search rather than\nthat at the end of each iteration."
3364,1," If so, it would be necessary to compared the proposed method to some classic methods for identifying k with kmeans, such as the elbow method, BIC, G-means etc, especially since kmeans seem to give much better NMI values.\n\n\n"""
3365,1,"\nThe result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable."
3366,1, There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights.
3367,1," \nNOVELTY: Honestly, I had difficulties to see which parts of this work could be sufficiently novel. "
3368,1,"\n-- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated?"
3369,1,\nWhat it means is that the actual impact of the work is probably limited.
3370,1, Second\nis what are the contributions to the state-of-the-art of the 2\nmethods introduced?
3371,1, Then a linear classifier is applied on top of the sum of the region embeddings. 
3372,1,"\nTable 1, \u201cGMenN2N\u201d should be \u201cGMemN2N\u201d."
3373,1," This is clearly not true because FractalNet has ensembling directly built in, i.e. different paths in the network are explicitly averaged."
3374,1,"\n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE."
3375,1,"  In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases."
3376,1,"\n\n* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance."
3377,1, It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial.
3378,1,\n\nTable 2: It would be good to see standard errors on these numbers; they may be quite high given that they\u2019re only evaluated on 100 examples.
3379,1,   The argument in Section 3.3 seems rather weak to me.
3380,1," What I thought was especially interesting is how their analysis can be extended to other graph problems; while their analysis was specific to the problem of maze solving, they offer an approach -- e.g. that of finding \""bugs\"" when dealing with graph objects -- that can extend to other problems."
3381,1,\n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.
3382,1,"""This paper proposes to optimize neural networks considering the three different terms: original loss function, quantization error and the sum of bits."
3383,1, \n\n\n2. High level paper - subjective\n\nI think the presentation of the paper is somewhat scattered: In section 2 the authors introduce their network and their metric for utility and privacy and then immediately do a sensitivity analysis.
3384,1," Indeed the functional model considered in the linear case is very similar to Eq. 2.5 or Eq. 3.2. Moreover, extension to nonparametric/nonlinear situations were also studied."
3385,1,"""Summary of the paper: \n\nThis paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), "
3386,1,"\n- Many papers get rid of the HMM pipeline, I would add https://arxiv.org/abs/1408.2873, which predates Deep Speech"
3387,1,", it is especially important to compare your method more thoroughly to simpler methods."
3388,1," Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated."
3389,1," That the same observation is made in such disparate brain areas (V1, EC) suggests that sparsity / efficiency might be quite universal constraints on the neural code."
3390,1,"""The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems."
3391,1," The authors should provide more competing algorithms in batch mode active learning."""
3392,1, Average case notions of privacy are usually not appreciated in the privacy community because of their vulnerability to a suite of attacks.
3393,1,"\n\nIn summary, I feel that the paper cannot be accepted in its current form."""
3394,1,"and b) how exactly the nuisance variation is reduced."""
3395,1, The MSCOCO captioning and Flickr30K datasets are used for evaluation.
3396,1," I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. """
3397,1,  \n\nThe experimental side of the paper is less strong.
3398,1," According to the paper, this is the first time such kind of performance are demonstrated for limited precision training."
3399,1, Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture?
3400,1,"\n\nPros: Rigorous analysis, well motivated problem, generalizable results to deep learning theory"
3401,1," However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations?"
3402,1," \n\nThe horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions. \n"""
3403,1, Numerical experimental results are also presented to verify the theory.
3404,1, I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).
3405,1,"\nIt would also be nice to see the full learning curves for all experiments, where different stages (decompose->optimize->decompose->finetune->...) are explicitly marked."
3406,1,"\n - On p.8, I'm a bit suspicious of the \""Is additional knowledge used?"
3407,1,"\n- In table 4, why do Hill et al lstm and bow perform much better than the others?\n"""
3408,1,  You claim that downsampling the data to 15 minute time steps still captures the relevant dynamics of the data -- is it obvious from the data that variations in the measured variables are not significant over a 5 minute interval?
3409,1, The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough.
3410,1, I have 2 main concerns:\nThe data required for learning a good Path function may include similar states to those visited by some optimal policy.
3411,1,"\n\n8. In Section 4, the authors mention that quantization could be used to compress models further although this is usually degrades WER by 2--4% relative. I think the authors should consider citing previous works which have examined quantization for embedded speech recognition [1], [2]. In particular, note that [2] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after training."
3412,1," It would be better if the paper could point out the importance of NER for user utterances, and the fact that using the knowledge of which words are NEs in dialogue models could help in tasks where DB queries are necessary."
3413,1,n\nI think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them.
3414,1," \n- In Section 7, it's less useful to spend time describing what happens when the visualization is done wrong (i.e. projecting along random directions rather than PCA vectors) -  this can be put in the Appendix."
3415,1, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.
3416,1,"""This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program. "
3417,1,"\nFinally, a generator is used to generate entity pairs give a relation."
3418,1," Are they a random sample across the whole dataset?\n\nFinally, the paper spends significant time on describing MaxMin and MinMax and the graphical visualizations but the paper fails to show these graphical profiles for real models."""
3419,1," The whole analysis of the connection is built solely on this one sentence \""At the same time, the output does not change if we adjust the weight vectors in Layer 1\"", which is nowhere verified."
3420,1," If that is the case, it is not demonstrated in the paper that it actually works."
3421,1,"  I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper)."
3422,1, It lacks both in sound theoretical justification and intuitive motivation of the approach.
3423,1," For example, the COIL20 accuracy is only 0.762, much worse than the state of the art."
3424,1," The novelty of the attack is a bit dim, since it seems it's just the straightforward attack against the region cls defense."
3425,1,"""This paper investigates identity space learning with well-controlled variations using an artistic portraits dataset."
3426,1,\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning.
3427,1,"""This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles)."
3428,1,"\n\nMore details\n\nPrevious work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently."
3429,1, \n\nPage 5: \u201cFigure blah shows our neural network architecture\u201d - missing reference to Figure 3.
3430,1,"\n\nSpecific comments/questions:\n- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting."
3431,1,"  However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification)."
3432,1,\n\n4) the experiments in figure 3 show that the tailored data augmentation can be done even for a subset of the\nclasses and still work well for all of them.
3433,1, What was the rational for choosing a vanilla RNN for the slave modules?
3434,1," By including unlabeled examples in an episode, it is already known that they belong to one of the K classes."
3435,1," Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning."
3436,1," Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful."
3437,1,"   But, this is clearly missing in the paper."
3438,1," For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a \""reward\"" function, but I don't know what it means and the authors should perhaps explain further."
3439,1," They seem to optimize over this parameter, but this requires access to the out of distribution set prior to the final evaluation."
3440,1," \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit)"
3441,1,"""I thank the authors for the thoughtful response and rebuttal. "
3442,1," On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness."
3443,1, \n3) the robot task is impressive.
3444,1," Moreover, the proposed formulation does not seem useful for the fully connected layer at any time."
3445,1,"\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases."
3446,1,  A new training objective is defined as a sum of mutual informations (MI) between the successive stochastic hidden layers plus a sum of mutual informations between each layer and the relevance variable.
3447,1," The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x."
3448,1,nThe authors provided detailed and convincing answers to my questions.
3449,1,"\n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works."
3450,1, It would have been insightful to see effect of \\sigma_0 on the performance rather than report the best result. 
3451,1, The adjacency matrix (or a subgraph of it) is first re-ordered to produce some canonical ordering which can then be fed into an image representation method.
3452,1, \n\nThe method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human performance.
3453,1,"\n-\tGiven the high flares alarm rate, it is surprising that experiments with multiple checkpoints are not presented (specifically as this case of multiple checkpoints is discussed explicitly in previous sections of the paper)."
3454,1,\n\n\nI enjoyed reading this paper and would like it to be accepted.
3455,1, It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv!
3456,1,\n\n2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector?
3457,1," Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas."
3458,1,"  \n\nFinally, I also think that using expert data generated by a pre-trained network makes the experimental section very weak."
3459,1,"  In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients."
3460,1,\n- The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting.
3461,1, The question this leaves open is whether f-Hyperband would reach the same performance when continued or not. 
3462,1," The authors completely ignore all this previous work and their \""related work\"" section  starts citing papers from 2016 and onwards!  Is it any benefit of learning objects with the current (very expensive) method compared to simple methods such as  \""background subtraction\""?"
3463,1,\n\niv) What is the average length of the answers in both ParaphraseRC and SelfRC?
3464,1,"n\nIn general, survey papers are not very suitable for publication at conferences. \n"""
3465,1, The authors also highlight that their sample complexity depends only logarithmically on the time horizon T.
3466,1, Can you provide a citation for this?
3467,1," you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific."
3468,1, So it does not make sense to compare your model + data augmentation against other models without data augmentation.
3469,1," For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers."
3470,1,"  Is there any reason this sort of approach wasn't used previously, even though this vein of thinking was being explored for example in the semi-dual algorithm?"
3471,1,  Convolutional neural networks on graphs with fast localized spectral filtering.
3472,1,"\n\nOverall, I am leaning towards a rejection mostly due to limited novelty."
3473,1, The experimental results show that the span based model performs better than the model which generates the answers.
3474,1,\n- Moving the state of the art in low precision forward
3475,1," However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network\u2019s ability to separate that particular class from the other ones."
3476,1, They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet.
3477,1," If one can initialize lambda arbitrarily and have this method find the optimal lambda, that is more impressive than a method that works simply because of a fortunate initialization."
3478,1, It is introduced with 'We ask' followed by two well formulated lines that make up the hypothesis.
3479,1,"  Additionally MNIST is a uniquely bad dataset for evaluating pruning methods, since they tend to work uncharacteristically well on MNIST (This can be seen in some of the references the paper cites)."
3480,1," [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi-step process is left for future work."
3481,1," \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections."
3482,1," The decoder predicts the number of children first, but the authors don\u2019t explain why they do that, nor compare this to existing tree generators."
3483,1, \n\nSeveral figure captions should be updated to clarify which model and dataset\nare studied.
3484,1," They learn a distribution q(z | x, y) that randomly throw class logits."
3485,1," There are some typos and grammatical problems (indicated below),"
3486,1," If the authors refer to the objective being new in the sense of using it with an action conditioned video prediction network, then this is again an extremely minimal contribution."
3487,1,"\n\nTo me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple."
3488,1," The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2)."
3489,1, \n\nThen the authors present the results for machine translation task and also analysis of their proposed method.
3490,1,\n\nExperiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).
3491,1, Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere).
3492,1," What are the inputs, what are the outputs, what kind of problems should be solved?"
3493,1," the paper is not missing any information.\n"""
3494,1,"\n\nMoreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold."
3495,1,"""The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding."
3496,1,"\n\nThe theoretical analysis in the paper is straightforward, in some sense following from the definition."
3497,1," For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique."
3498,1," To me, this is not a fundamental way to solve the long-tail problem, in the sense that by fixing isotropic likelihoods and flat priors, authors are also ignoring information that can be relevant in some classification problems, where a good prior can be useful to disambiguate confusing situations."
3499,1,\n+ The idea is clearly explained and well motivated.
3500,1, I am afraid I have to disagree as\nthe proposed approach is not giving any better understanding of what needs to be\ndone and why.
3501,1,"\n8. Although in terms of \""effective training data points\"" the proposed method outperforms the other methods, in terms of time (Fig.5) the difference between it and say, NoTeach, is not that significant (especially at very high desired accuracy)."
3502,1,"\n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \""ensemble voting\"" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?)"
3503,1,  What is the takeaway from these two tables supposed to be?\n\n5) Part of the motivation for the work is said to be the increasing interest in inference networks: 
3504,1, This is because you train individual model on i and apply it to j.
3505,1, However comparisons are lacking and the paper is not presented very scientifically.
3506,1,\n\nPros:\n  1. Experimental study on retrofitting existing word vectors for ESL and TOEFL lexical similarity datasets
3507,1,"  \n\nOverall, I think the paper proposes an interesting environment and task that is of interest to the community in general."
3508,1, I don\u2019t understand the significance of each test and why the relative performance of the techniques vary from one to the other.
3509,1,"\n\nOverall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered."
3510,1, I also don\u2019t see why using Adam is more convenient than using SGD.
3511,1,  \n\nThe paper does not clearly describe how D_s is obtained.
3512,1, (This is totally consistent with Krishnan et al.\u2019s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.)
3513,1, Sufficient implementation detail and analysis on results.
3514,1," Then, these image features are organized in a fully connected graph."
3515,1," \n\nSecond, the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach."
3516,1," Although this criticism also applies to most previous work using \u2018cluttered\u2019 variants of MNIST, I still think it needs to be considered."
3517,1, Please cite some work in that case)
3518,1,"\n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC)."
3519,1," As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy."
3520,1," In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12).[[CNT], [CNT], [DIS], [MIN]] It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples."""
3521,1, But its current form has several serious issues.
3522,1,\n\nClarity: This paper clearly written;
3523,1, The motivation for this choice would be helpful.
3524,1,\n\n3. Experimental results for classification are not convincing enough.
3525,1," However, some of the\nexperiments are not obvious that the model is really doing a good job."
3526,1,".\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works."
3527,1, More detail here would be helpful.
3528,1,"\n-For a, more or less, entirely empirical paper, the choices of experiments are...somewhat befuddling."
3529,1,"\n  \nThe papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures"
3530,1,". I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model."
3531,1,  what we get in practice is pretty unclear.
3532,1, It extends a series of recent papers correctly cited.
3533,1, The use of unlabeled data is in fact a self-training process.
3534,1,"""The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \""fully-aware\"" of all levels of abstraction, e.g. word-level, phrase-level, etc."
3535,1,.\n\nCons:\n1) Coreference eval: No details are provided for how the data was annotated for the coreference task.
3536,1, And such representation is generally regarded as essential for the success of deep learning.
3537,1," However, Corollary 3 is only a concentration bound on the gradient."
3538,1, Otherwise one will have to scroll up and down all the time to understand the proof.
3539,1," For example, simply applying kmeans to PCA features of the images on the MNIST data can get you pretty good performance."
3540,1,  Would it make more sense to show that on large RNNs with thousands of hidden units?
3541,1,n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\
3542,1," In particular, please mathematically formulate each proposed technique in Section 4."
3543,1, It would be very helpful to describe a potential scenario where the proposed approach could be useful.
3544,1,\n\nThe paper is well written and the method reasonably well explained
3545,1,"\n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n"""
3546,1,"Further discussion would be helpful.\n"""
3547,1," Based on my understanding, the claims are 1) the new formalization of the goal of GAN training and 2) using this test to evaluate the success of GAN algorithms empirically? "
3548,1, Essentially the expert defines a new Q-value problem at every state \nfor the learner?
3549,1, So I am not convinced by these two arguments made by this paper.
3550,1," This work is an extended version of [1], aiming to address the high-dimensional problem."
3551,1,"\n- The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up."
3552,1,"""SUMMARY\nThe major contribution of the paper is a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. "
3553,1, Why not adding (non Bayesian) context (not label) to the task will not work as well?
3554,1," Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?)"
3555,1,"3. In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs."
3556,1, It presents experiments during training and with varying network sizes.
3557,1, Does that impact the MI of the final layer and Y?
3558,1,\n\nPros:\n- Baseline performance is exceeded by a large margin\n- Novel use of adversarial perturbation and temperature\n- Interesting analysis
3559,1, \n\n- Only single runs for the results are shown in plots.
3560,1, My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation.
3561,1,  It should help make it clearer in the paper as well.
3562,1,"""In this paper, the authors propose  a new network architecture, CrescendoNet, which is a simple stack of building blocks without residual connections."
3563,1, The core technique is to construct a coreset of points whose labels inform the labels of other points.
3564,1,\nOriginality: this paper introduces block diagonal matrices to structure the weights of a neural network.
3565,1, The method is inspired by the recent work on max entropy reinforcement learning and links between Q-learning and policy gradient methods.
3566,1, The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks.\
3567,1,"\n\nThe morphological agreement task would be an interesting contribution of the paper, with wider potential."
3568,1, Does it reduce the excess representational power compared to the LSTM cell that could result in better models?
3569,1, The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results.
3570,1,"  Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise."
3571,1," Similar remarks for the many more examples in the supplementary materials.[[CNT], [null], [DIS], [MIN]]\n\nThe most intriguing question is the one raised in the first paragraph of the conclusion: While prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical, the kind of concerns raised here are also applicable to a whole bunch of word types, and it would be natural to want to extend the method to them."
3572,1, Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode.
3573,1," In some cases, MNIST is too easy to consider the complex structure of deep architectures."
3574,1,\n\n# Paper discussion:\nIn general I like the idea of looking further into the effect of adding network structure on the original\ninformation bottleneck results (empirical and theoretical).
3575,1, Is it due to the well designed mutation operations?
3576,1,". According to Fig. 2, the conditional probability in the product operator should be revised to p(a_t | x_{1:t}, a_{1:t-1}), and the independence approximation to remove a_{1:t-1} from the conditions should also be noted in the paper."
3577,1," However, the theoretical results are not very satisfactory."
3578,1," Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework. "
3579,1,. \n\n- Many citation errors exist
3580,1,Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.
3581,1,"\n- \""large training error on wrongly labeled examples\"" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels?"
3582,1,"""The paper presents a method for improving the diversity of Generative Adversarial Network (GAN) by promoting the Gnet's weights to be as informative as possible."
3583,1," For example, the authors mentioned that \""there has been no complete implementation of established deep learning approaches\"" in the abstract, however, the authors did not define what is \""complete\""."
3584,1,\n The authors show they can retrain a new policy that does not require the expert traces.
3585,1,I am not sure if it is a good idea to examine the convergence purely from an optimization perspective.
3586,1,"""The paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter (Spectral Density Ratio from [Shajarisales et al 2015]) to cascades of two filters in successive layers of deep convolutional networks."
3587,1, They investigate pros and cons in detail adding more valuable analysis in the appendix.
3588,1,", the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning"
3589,1,"  The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks."
3590,1,\n \nwhat is the std for CartPole in table 1
3591,1, Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?
3592,1,\n\nThe use of the heatmap centroid as the prediction for the focal point is potentially problematic as well.
3593,1,"\n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others?"
3594,1, But adversarial images should be visually undistinguishable from original images.
3595,1," Presently, it is not clear if only one of the\ncomponents is providing most of the benefits, or if both things are useful."
3596,1, It would be of particular interest to highlight connections to algorithm regularly applied to neural network training.
3597,1, What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)?
3598,1, \n\nSummary\nThe authors study state space models in the unsupervised learning case.
3599,1,\n\n+ Originality: \n- The main direction of incorporating human feedback in the loop is original.
3600,1,\u201d: Why not just compare the optimal with the AIS evaluation?
3601,1, Computation accuracy of the geodesic distances in high-dimensional spaces can be poor.
3602,1," I strongly recommend to the authors, to provide technical details of topologies used, hyper parameters and any other important detail that would help a third party research to reproduce these results."
3603,1, --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort. 
3604,1,\n\n- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images.
3605,1, the computation cost of activation function is still only a very small part (less than 1%) in the overall running time of training/inference
3606,1," In general, the paper is poorly written, with many notation mistakes and inconsistencies."
3607,1,\n\nThe proposed methodology relies on multiple choices that could sometimes be better studied and/or explained.
3608,1, The idea of learning the basic relations between actions and state through self exploration is definitely interesting.
3609,1,"\n\nCons:\n - notation (i.e. precise definition of r_{i,c})"
3610,1,\n\nFractalNets amd DiracNets (https://arxiv.org/pdf/1706.00388.pdf) have demonstrated that it is possible to train deep networks without skip connections and achieve high performance.
3611,1,"\n\nE.g. in the beginning of the 2nd paragraph in Introduction, the authors write \""Generative models like GANs, VAEs and others typically define a generative model via a deterministic generative mechanism or generator ...\"". While I think the use of a **deterministic** generator is probably one of the unique features of GAN, and that is certainly not the case with VAE, where typically people still need to specify an explicit probabilistic generative model."
3612,1,"\n2) How does the approach generalize to other languages? (Presumably well, but something to consider for future work.)"
3613,1,\nLe et al. (2015) for instance perform a coarse grid search for each model.
3614,1,"There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable."
3615,1, In equation (10) there appear to be two problems.
3616,1,\n\n+ This is more of a suggestion.
3617,1,". However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments. "
3618,1,"\n6. Just claiming the generalization capability of deep networks is not enough, need to show how much the model can interpolate or extrapolate?"
3619,1, The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting.
3620,1, But the real data CIFAR experiments are much less detailed with only a final inception score (very similar to the competing method) and no images even in appendix.
3621,1,"""The authors propose a new sampling based approach for inference in latent variable models."
3622,1,\n- Introduces novel semi-supervised and active learning variants of few-shot classification.
3623,1," Additionally, in the 5-shot non-distractor setting on tiered ImageNet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset?"
3624,1, The concerns I have about this work revolve around their choice of two small datasets and how much their results are affected by variance in the estimators.
3625,1, -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer.
3626,1,"\n\nQuestions:\n\n1. What is \\rho on page 4?.[[CNT], [null], [QSN], [MIN]] I assume it is some nonlinearity, but this was not specified.[[CNT], [null], [QSN], [MIN]]\n2. On page 5, it says the merge block takes as input two sequences.[[CNT], [null], [DIS], [MIN]] I thought the merge block was defined on sets?"
3627,1," It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547)."
3628,1,\n\nWhy not report actual wallclock times?
3629,1,\nYour data set consisted of C# code.
3630,1,"\n\nThe contribution to the practice of PSRNNs seems significant (to my non-expert eyes): when back-propagation through time is used, using ORFs to do the two-stage KRR training needed visibly outperforms using standard RFMs to do the KRR."
3631,1," Pixel saliency, in this case, is defined as the partial derivative of the output of the classification network with respect to each input pixel. "
3632,1, This suggests that the number of updates in this segment was chosen unnecessarily large to begin with.
3633,1," In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations. "
3634,1,"\n[2] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016."
3635,1, I think this information would be very useful to the community in terms of what to take away from this paper.
3636,1,\n\n- The DSL is richer than that of previous related work like Balog et al. (2016).
3637,1,\n2. Evaluates how informative the latent states are via state reconstruction.
3638,1,\n\nThe references need work. There are repeated entries for the same reference (one form arxiv and one from conference).
3639,1, Should the risk bound only depends on the dimensions of the matrix W?
3640,1, and the contribution seems to be novel and significant.
3641,1," The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution."
3642,1," Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector."
3643,1,\n\nPros: \n(1) The paper is easy to follow.
3644,1,"\n\n3. In the decoding phase, how does the system decide whether to query the DB?"
3645,1," (p.2)\n - the related work section is well-written and interesting, but it's a bit odd to have it at the end."
3646,1," however, I would like to see more experiments."
3647,1, Why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselines?
3648,1, \n\nThe quality and clarity of the paper can be improved in some sections
3649,1,"  I think what the authors intend to note here is that the W parameters are independent of this, or as expounded upon in the concrete example that follows, that context words and weights W both are."
3650,1,\n\nAnother comment is related to the overall content of this paper.
3651,1,\n6. Page 7: The following sentence is confusing and should be clarified:
3652,1,"On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs."
3653,1, Different models?
3654,1," First, the paper is way too long and unfocused. "
3655,1, but it could be more fluent.
3656,1, The motivation of the work is clearly explained and supported with relevant related work.
3657,1," For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?"
3658,1,3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset.
3659,1," Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \""soft labels\"" for whether or not pairs of target data belong to the same class."
3660,1, Why do we require the correspondence between the classification confidence of tranformed and original data?
3661,1," \n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81."
3662,1," \n\nIn summary, I think this paper needs more work on the presentation to make clear what they are proving and under what conditions, and with experiments that are closer to those used in practice to support their claims.\n"""
3663,1, I would perhaps even call this a distillation network rather than a crossover network.
3664,1, I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner.
3665,1," Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words?[[CNT], [PNF-NEU], [SUG], [MIN]] And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away?"
3666,1,\n\n\n- Unnecessary complexity
3667,1,  Would this affect the distributions?
3668,1,"Clearly if the agent receives the maximal possible reward for a well designed navigation task it must, by definition, be doing perfect SLAM & path planning."
3669,1,"  Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling."
3670,1, Would the special cases lead to worse performance and if so why is there a difference?\
3671,1, They do this by adapting a technique from computational cognitive science called rational pedagogy. 
3672,1," I guess it\u2019s not possible to build full 3D rotation/scale equivariance with the authors\u2019 approach (spherical coordinates probably don\u2019t do the job), but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in 3D."
3673,1," With the current focus of the paper being the proposed system, it is interesting to the computer vision community."
3674,1, The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).
3675,1, It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM.
3676,1,"\nHence, several policies which are better than random are likely to be required for sampling this data, in general. "
3677,1," This will make it less usable, I think it's necessary to provide the training time comparison."
3678,1,"\n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores)."
3679,1, \n\nI think the author's statement on that this study leads to a more plausible psychological model of emotion is not well founded (they also mention to learn to recognize the latent emotional state).
3680,1,"Not only some other works such as [R1, R2, R3]; but also the other na\u00efve baselines should also be compared, such as directly nearest neighbor classifier, logistic regression, and neural network in traditional supervised learning."
3681,1, I have updated my score to 6 accordingly. 
3682,1, The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method.
3683,1,  It's not fair to say GAN-based methods require more training time because these methods can do generation and style-class disentanglement while the proposed method cannot.
3684,1, Are these multiple rollouts of the policies?
3685,1," With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof."
3686,1,"   The mistake is repeated for the following two terms. \n\n- Equations 3 and 4 suggest that despite their ordinal structure, sentiment labels are treated as unstructured at predict time."
3687,1," The main contribution are scaling rules that relate the batch size k used in SGD with the learning rate \\epsilon, most notably \\epsilon/k = const for optimal scaling."
3688,1," \n\nMy final decision is dependent on the author's response to my questions. """
3689,1, The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective.
3690,1," \n- Since the adaptions to DTP are rather small, the work does not contain much novelty."
3691,1, Can we afford to keep all of those frames around?
3692,1, I find the argumentation the authors give in Figure 6 much sharper.
3693,1,\n\n9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W.
3694,1,  Would the result be more inconsistent if the hypercolumns had smaller receptive field?
3695,1, The \u201czero-shot\u201d refers to the fact that all learning is performed before the human defines the task.
3696,1,"""**Summary**\nThe paper proposes an extension of the attend, infer, repeat generative model of Eslami, 2016 and extends it to handle ``visual attribute descriptions."
3697,1," In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks."
3698,1,"  In the runs that reach the goal, the proposed method is about 20% faster than the simple baseline, though it does not reach the goal every time."
3699,1," \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes."
3700,1, Would it be enough to match the DISTRIBUTION of the confidence? 
3701,1," Although point 2 seems novel, I am not convinced that it is significant enough for ICLR."
3702,1,\n- At many points in the paper the claims are quite overstated.
3703,1," Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable."
3704,1,but then the description and motivation of the model remains very vague.
3705,1," \n\nOne other paper of note given that the authors train a MLP is an optimal teaching analysis of a perceptron: (Zhang, Ohannessian, Sen, Alfeld, & Zhu, 2016; NIPS).\n\n\n"""
3706,1,"In Section 4, the authors show their method on permuted MNIST."
3707,1,\n\nThe main signal I lack from reading the paper is whether the proposed model actually does better than a reasonable baseline.
3708,1, Theorem 5 should be phrased as \n\nall critical points of the population risk that is non-singular are global minima.
3709,1,c. There are several (existing) ideas of how to combine node representations into a representation for the entire graph
3710,1, Also specific details should be reserved until a general explanation of the problem has been made.
3711,1," And those \""theorems\"" in section 4 are just some properties from previous definitions; they are not theorems."
3712,1," Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO."
3713,1," In addition, the point made by the TCML authors is fair (\""nothing prevented you from...\"") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote."
3714,1,   The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions.
3715,1," Specifically, the paper only parametrizes the Q function, and computes the policy gradient using the relation between the policy and Q function (Appendix A.1)."
3716,1," While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel,"
3717,1," Similarly for CIFAR, where only up to depth 3 is used."
3718,1," It does not require training from scratch for each architecture, thus dramatically saves the training time."
3719,1," The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input."
3720,1,"\n\nWhile the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression."
3721,1,. The main novelty is about using word pair embedding to improve the Topic model
3722,1,"\n\nIf my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and model-based scoring."
3723,1, Isn't it possible that the agent learns only one way of achieving such goal?
3724,1," For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy)."
3725,1,"\n\nCons:\n\n1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models"
3726,1,"\n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims."
3727,1, So these methods could be applied in the real world.
3728,1, \n\nCons:\n\n- The proposed approach follows largely the existing work and thus its technical novelty is weak. 
3729,1, Also Maei is the first author on this paper.
3730,1, The topic is important. Experiments on real data show improvements compared to several traditional approaches.
3731,1,"2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate]."
3732,1," The authors propose to use SVD tools to estimate the dimension of the deep manifolds, and conduct experiments on three categories of ImageNet."
3733,1, \n\nMy concerns with the paper include: \n\na) the paper says that the same method works for convolutional neural networks but I couldn't find anything about convolution.
3734,1, but the initial results are quite promising.\n\n
3735,1,\n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial.
3736,1, It is not clear why mSDA cannot handle time-series data but DAuto can.
3737,1, The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules).
3738,1," Therefore, SRM has huge preparing training dataset time complexity that is not mentioned in the paper."
3739,1,"\n4) Appendix: Section 8 states \u201cBelow are the results\u201d, but the figure landed on the next page."
3740,1, Also the example in Figure 3 is not very clear and did not help me in understanding this concept.
3741,1,"\nBidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs."
3742,1," Also, key experiments are missing: 1) NMS baseline"
3743,1," The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection."
3744,1," The work can be made stronger by addressing some of the questions above such as what role is played by the neural architecture and whether the results hold up under evaluation on a larger dataset."""
3745,1," One of the main distinctions is using filter-wise normalization, but it is somehow trivial."
3746,1,"""The paper describes a technique to incorporate dialog acts into neural conversational agents."
3747,1," I wish\nthe paper had explored a wider variety of dataset tasks and models to better\nshow how well this claim generalizes, better situated the practical benefits\nof the approach (how much wallclock time is actually saved?"
3748,1," Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods."
3749,1, The paragraph below this corollary is only a high level intuition. 
3750,1, There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations).
3751,1,\n\n+ Several possible combinations between datasets and domains are considered to evaluate the network behaviour.
3752,1, The work is therefore original and significant.
3753,1, The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch.
3754,1,"  Given the claim that not reaching the goal is considered a failure, it isn't clear which performance is preferred."
3755,1,"\n\nOverall, I think this paper is interesting and presents a quantitative analysis of where the errors accrue due to learning with inference networks."
3756,1,"I believe you mean standard attention, if so, please explain why standard attention is semantic."
3757,1,"   In short the learning objectives of the proposed composition algorithm are unclear after translation from TLTL formulas to rewards.\n"""
3758,1," Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition."
3759,1,  There can be many other approaches to mitigate the impact of angle bias.
3760,1,. The idea is to introduce the notion of pseudo labelling.
3761,1, Conventional data transformations are introduced for data augmentation without\nsupposing the existance of a significand domain shift between training and test data.
3762,1," The authors present some observations of the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors (GloVe in this work), and test the refined vectors on 80 TOEFL questions and 50 ESL questions."
3763,1,"  \n\nWhile this is an interesting idea on its surface,"
3764,1,"""This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL."
3765,1,"\n\nThese algorithms seem to be an improvement over the current state of the art for this problem setting,"
3766,1,"? The x-axis on plots, does it include the data required for crossover/Dagger"
3767,1, \n-\tHow did you select the maximum number of epochs in Figure 5?
3768,1,\n\nThen the paper talks about robustness properties of the momentum operator.
3769,1,  Does the generalization gap not appear when no momentum is used?
3770,1,"""This is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural networks."
3771,1, I wonder if the presented ideas won't suffer from the same curse of dimensionality.
3772,1,"\n\nMinor comments:\n- Sec 1: \""... optimization techniques like Adam, Attention, ...\"" -> Attention is not an optimization technique, but part of a model;"
3773,1,"""This paper proposes a new approach for multi-task learning."
3774,1," It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset? "
3775,1," Moreover, the experiments are not sufficient to support the arguments."
3776,1," These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network."
3777,1," Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point."
3778,1,"""This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions. "
3779,1,\n\nStrengths:\n- The paper is well-written and clear.
3780,1,"\n5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d)."
3781,1,"  The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.\n"""
3782,1,\n\n3 The novelty of this paper is insufficient.
3783,1,   However I feel that the authors should include some discussion of other published results.
3784,1,"\n\nI believe re-thinking new learning rate schedules is interesting,"
3785,1, In the image captioning experiment the authors choose two networks (Show & Tell and Soft attention) that they improve using the proposed method that end up performing similar to the second best baseline (Yao et al. 2016) based on Table 3 and their response.
3786,1, TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values.
3787,1,\n\nDo you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics?
3788,1, were hyper parameters tuned on a separate validation set?
3789,1," \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?"
3790,1,"\n\nLack of new Insights\nThe visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than \""something non-trivial is going on in these networks\"". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images). "
3791,1, The covered framework is limited to regularization parameters.
3792,1, The current approach could be prone to overfitting
3793,1," Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?   \n\n\n"" "
3794,1,"""In this paper, the authors show how a Deep Learning model for sea surface temperature prediction can be designed to incorporate the classical advection diffusion model."
3795,1," I would have preferred to see an extensive grid search done to find the best possible \\lambda, then seen how well the proposed method does compared to this."
3796,1,"""The authors introduce the task of \""defogging\"", by which they mean attempting to infer the contents of areas in the game StarCraft hidden by \""the fog of war\""."
3797,1,\n\nThe paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called \u201ceigenoptions\u201d.
3798,1," First, the visual data is composed of primitive shapes and colors in a black background."
3799,1," however, as far as I can see, they did not address my concern about the comparisons on the image captioning experiment."
3800,1,\n\n# Clarity\n- The problem statement and model description are not described well.
3801,1," Clearly, we can't expect perfect performance, but there are some troubling results: 5.2 accuracy on non-rotated 0s, 0.0 accuracy on non-rotated 6s."
3802,1,"\n- There are some loose and misleading descriptions of the algorithm computing \""the posterior\"" when actually this is very much an approximation method... that's OK to have approximations but it shouldn't be hidden away."
3803,1, Numerical experiments that demonstrate the diversity increment on the generated samples are shown.
3804,1,"  \n\nUnfortunately, the empirical evaluation is very preliminary, and provides no reassurance that this approach will have any practical relevance:"
3805,1," I believe the conclusions made by the authors are mostly valid.[[CNT], [null], [APC], [MAJ]] \n\nI would firstly like to point out that measuring generalization is not standard practice in RL."
3806,1,\n\n- Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated.
3807,1," Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs."
3808,1, \n\nI should mention that I'm not sufficiently familiar with the optimal transport literature to verify the detailed claims about where the proposed dual-based algorithm stands in relation to existing algorithms.
3809,1,"\n\n2.Similarly, it\u2019s not obvious that initializing the multipliers at 0.5 is the best choice."
3810,1," For example, it can be used for deduplication using A (eq 1) as the similarity matrix."
3811,1,"\n- This gives a more unified way of understanding, and implementing the methods."
3812,1," With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference."
3813,1,\n\nModel presented extends IW-GAN by using 3D convolution and also\nby supervising the generator using sample labels.
3814,1,"\""\n- In general, tables that report averages would do well to report error bars as\n  well."
3815,1, The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.
3816,1," But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.\n\n"""
3817,1,\n- Is log-likelihood a good loss here?
3818,1,\n\nClarity\n\nThe paper is well-written.
3819,1," For example, the RNNCell discovered can be fixed and used in other tasks, and the RNN controller for CNN architecture search could potentially be applied to other tasks too (though not reported).\n"""
3820,1,"\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers."
3821,1,"""This paper deals with the problem of learning nonlinear operators using deep learning."
3822,1,  In some sense are we are defining a model for the action taken by the expert?
3823,1," \n\nThe title is also misleading in using the word \""exact\""[[CNT], [null], [CRT], [MAJ]]. I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\"
3824,1,\n\nThe authors also prove a byproduct result stating that identifiability holds for Leaky ReLU networks with one hidden layer.
3825,1, whereas the details seem to be washed out for the point cloud results presented in this paper.
3826,1," \nIn my understanding, in experiment 2 the network, while learning to recognize the numbers, it also learns to\nbe invariant to color thanks to a tailored data augmentation and the good final results are expected."
3827,1,\nThis makes me wonder what would be the performance of GCN when the k-th power\nof the adjacency is used.
3828,1," However, even so one can clearly see that the architecture, the depth, the regularization techniques, and the evaluation are clearly behind the state of the art."
3829,1,"   From what I can tell, Hazan's paper introduces the idea of wave filtering (convolution of the input with eigenvectors of the Hankel matrix); the filtered output is then passed through another matrix that is being learned online (M)."
3830,1,"""This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip."
3831,1,\n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.
3832,1, the other aspects of the paper are limited.
3833,1," Furthermore, the gains from the sparsity appear rather limited over real applications."
3834,1,? I don\u2019t expect the authors to answer any of these questions in particular;
3835,1, but the usefulness of \u201celimination\u201d is not well justified for two reasons.
3836,1,\n2. The experimental section of this paper needs improvement.
3837,1,  The terms about sharp and wide minima are ambiguous.
3838,1, The authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset.
3839,1,"\n\nSmall notes:\nThe paper uses the term \""data flow structure\"" without defining it."
3840,1," The provided analysis is insightful,"
3841,1,"\n\nThird, it seems that the proposed multi-attribute generation pipeline works for binary attribute only."
3842,1, I gather the tensor Wa = R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding \\phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time? 
3843,1,"""Twin Networks: Using the Future as a Regularizer"
3844,1,  Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images?
3845,1,"\n\nOverall, the paper is well-written."
3846,1,"""The authors proposed a generative model of random walks on graphs."
3847,1,Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017. 
3848,1,"  However, I feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate acceptance.   """
3849,1," \n\nAdditionally, when I first read the paper I thought that the ZS1 experiments\nfeatured no QA training at all."
3850,1,"\n\nOn the top of page 5, \""Line 10 of Algorithm 1\"": I think you mean Line 11 of Algorithm 2."""
3851,1," If yes, can you please report the error bars."
3852,1, Section 3 is a little hard to understand.
3853,1,"\n\nFinally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k)."
3854,1,". The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples."
3855,1, how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%).
3856,1,"\n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method."
3857,1," \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data."
3858,1,"\nThese experiments show that modern sequence to sequence models do not solve the systematicity problem, while making clear by application to machine translation, why such a solution would be desirable. "
3859,1," However, I have some concerns about the specific implementation and model discussed here."
3860,1,"\n- Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task."""
3861,1,"\n\nComments:\n\t\n1-\tThe results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach."
3862,1,"""In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations."
3863,1," but could be improved with some more details on the mathematical derivations (e.g., explaining where the constraints on \\alpha come from), and on the experiments (it is not entirely clear how the distributions of accuracies were obtained).\n"""
3864,1," Thanks to the reviewer for clarifying this.\n"""
3865,1,  But the exact method is difficult to piece together from what is written.
3866,1," The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example."
3867,1, Is this update rule\nguaranteed to produce convergence of \\theta?
3868,1, \n\nThe proposed technique sounds.
3869,1, \n? p.4: The word embeddings for the CNN are pre-trained word2vec/Glove/xyz embeddings?
3870,1, \n\nThey focus on linear MLPs in the paper for computational simplicity.
3871,1,. It could be beneficial to do an intermediate experiment (a handful of attributes on a middling task)
3872,1," Especially in Figure 6 (right), the original Hyperband algorithm ends up higher than f-Hyperband."
3873,1, \nThe experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames.
3874,1,"  On SQuAD dataset, their results show some small improvements using the proposed augmentation technique."
3875,1,\n\n# Quality\n- The experimental result is not much comprehensive.
3876,1,\n\nOne thing that is currently not very clear to me is about the rotational symmetry.
3877,1,"\n2) The scaling rules described in section 5 could help practitioners use much larger batch sizes during training, by simultaneously increasing the learning rate, the training set size, and/or the momentum parameter."
3878,1, It also produces meaningful node embeddings with semi-interpretable latent spaces.
3879,1,\n\nThe shift variables seem only useful when they are not shared for different pixels.
3880,1," However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture."
3881,1,"  Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported."
3882,1," The 3 ConvNets all have a different goal:\n1. an Open Classification Network (per class sigmoid, trained 1vsRest, with thresholds for rejection)"
3883,1," \n\nThe dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the Hessian is y_i y_j <x_i, x_j>, thus highlighting the pairwise similarities between the points x_i & x_j; here the labels represent whether the point x comes from the samples A from the target distribution or B from the proposal distribution."
3884,1,"""The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations."
3885,1,"\n- page 8: (laf, 2008): bad formatted reference"""
3886,1,\n\nQuality: The method appears to be technically correct.
3887,1, I believe the primary difference (other using an LSTM instead of a convnet) is to replace max-pooling with softmax-pooling. Do these two architectural changes matter?
3888,1, The experiments show also that in on of the considered case the generalization accuracy is better for the proposed method.
3889,1,\n\nThere are a number of minor issues as well. In language modelling we do not compute normalisation term during NCE training or testing as explicitly stated by the authors you are referring to (Chen 2016) - that is the whole point of using NCE.
3890,1,\n\n5. The parametrization nu of \\hat{Q} is never specified in Section 3.6.
3891,1,"""This paper studies the critical points of shallow and deep linear networks."
3892,1, but the impact may be.
3893,1, The key innovation here is to use pixel saliency as a channel for the adversarial example detector.
3894,1," Same comment in Atari, but there it\u2019s not really obvious that the proposed architecture is helping."
3895,1, Any insights or results on the optimization performance vs. the number of latent tensors?
3896,1, I thus appreciate the authors\u2019 effort.
3897,1,\n\nThe approach obtains strong results on the CNN/Daily Mail and NYT datasets.
3898,1, There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3).
3899,1,"""This paper presents a lifelong learning method for learning word embeddings."
3900,1,\n\nCons:\nThe idea is rather incremental compared to FractalNet.
3901,1,\n- Figure 1 is hard to understand.
3902,1,"  Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n"""
3903,1,"  \n\n3.  In the experiments: \n(1) The authors didn\u2019t compare the proposed method with existing state-of-the-art one-shot learning approaches, which makes the results not very convincing."
3904,1,"\n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models."
3905,1,"  Compared to prior work (hierarchical latent tree model), this work introduces skip-paths."
3906,1,\n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants.
3907,1, However it would be nice to see the difference to other related methods more clearly.
3908,1,"  Is syntax checking used at each step of token generation, or something along these lines?"
3909,1," \n\nAs stated in the introduction, the authors think there are several drawbacks of existing methods including \""training instability, lack of topology generalization and computational complexity."
3910,1,"\nAnd just because something works on MNIST, does not mean it works in general."
3911,1," Many sentences are poorly written making the paper hard to read, especially when not familiar with the presented methods."
3912,1,\n-This is not clear what is novel here since ACOL and GAR already exist.
3913,1,"Negative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C)."
3914,1,"  Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN."""
3915,1," Without a consistent objective function, the algorithm seems somewhat heuristic."
3916,1,\n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.
3917,1,"""The paper describes some interesting work"
3918,1,\n(1) only small networks on relatively small datasets are tested.
3919,1," Basically, I'd love to see the trends for how these types of tuning relate to each other over the whole populations: those trends could then be tested against experimental data (possibly in a future study)."""
3920,1,\n-Unprecedented visual quality as compared to other generative models has brought the GAN to prominence and yet this is not really a big factor in this paper.
3921,1, Posterior weights are sampled to select actions during execution (Thompson Sampling style).
3922,1,"\n\n\""The adversary would select a random subset of anomalies, push them towards the normal data cloud and inject these perturbed points into the training set\"" -- This seems backwards."
3923,1,\nFig 2 caption: through bottom-up -> through a bottom-up\n3: Let S be a set of state -> Let S be a set of states\n3: form of task graph -> form of a task graph
3924,1, This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint.
3925,1,"\n5. I did not understand the paragraph beginning with \""This poor estimation\""."
3926,1,"""The paper presents an interesting spectral algorithm for multiscale hmm"
3927,1,.  \n\nOriginality and significance:
3928,1," This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting. "
3929,1," To generate point clouds, they sample a latent code and pass it to the decoder."
3930,1,"\n* Section 2.3: What is meant by \""reconstruction data\""?"
3931,1," The paper would be a much stronger contribution, if the experiments could be improved."
3932,1,"\n\nFinally, what are the savings from reducing this time complexity?"
3933,1,how does this change depending on the dimensionality of the latent codes?
3934,1,\n\nThe paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet.
3935,1," Figure 2 is almost identical, again a reference to the original would be better."
3936,1, This is used for learning initial feature representation of the student model.
3937,1,"\n\nOTHER NOTES\n\n- This paper needs serious proofreading---just in the first few pages the errors\n  I noticed were \""in 2D environment\"" (in the title!), \""such capability\"", \""this\n  characteristics\"", \""such language generalization problem\"", \""the agent need to\"",\n  \""some early pioneering system\"", \""commands is\""."
3938,1," Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?"
3939,1,"\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network."
3940,1,\n\n- Please add citations for Figures 1a-1b.
3941,1," It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct."
3942,1,"\n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n"""
3943,1,\n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al.
3944,1,"  For instance, the proposed method should be compared  with the discriminative approaches including VAT and [1] in terms of the training efficiency."
3945,1,"""This paper introduces a new architecture for end to end neural machine translation."
3946,1," There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers."""
3947,1,"\n\nOverall, I found the paper deeply uninspired."
3948,1," Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME)."""
3949,1,"  There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. \""universal schema\"" [Riedel et al, 2014]."
3950,1,"Otherwise, the objective won't be a proper probability distribution."
3951,1,"""The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution."
3952,1,"  Overall, the paper is mathematically sound,"
3953,1,"   Specifically:\n\n    1. Target aspect vectors live in R^V, but it's not clear to me what V represents; this is the number of distinct aspects?"
3954,1, The idea to represent task graphs are quite interesting.
3955,1," \n\nMore comments:\n\n- Some figures could be more complete: to see more examples in Fig 1, 2, 3 would help to understand better the dataset and the challenges."
3956,1,\n-\tSection 4 Models \u201cThe parameters of the attention modules are either shared across sensors (STAN-shared) or not shared across sensors (STAN- default).
3957,1," Even from a computational perspective, \""SDG schemes aim for computational efficiency\"" and \""stochastic makes the convergence slow down\"" are not a causality dilemma."
3958,1, The theory part of the paper is reasonable and quite well written.
3959,1,. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.
3960,1," \n\nFor claim (i), the comparison to related work is far from sufficient to demonstrate its utility as a graph generation model."
3961,1,\n\nThe exposition leaves ample room for improvement.
3962,1," \n\nThere have been several works that have noted that lambda can and perhaps should be changed as a function of state (Sutton and Barto, White and White [1], TD-Gammon)."
3963,1, \n\nThe main concern is that this looks like a class project rather than a scientific paper.
3964,1,\n\nExperiments on generative optimal transport are interesting and probably generate more discussion/perspectives
3965,1, The auxiliary softmax layers take different views of the input for prediction.
3966,1,"\n- For instance, the example in 4.2 only works because the optimal solution is to go \""right\"" in every timestep... if you had the network parameterized in a different way (or the actions left/right were relabelled) then this parameter noise approach would *not* work..."
3967,1, I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should.
3968,1,"\n\nIn the model section, the paragraphs \""notation\"" and \""objective function and discussion\"" are clear."
3969,1,"""This paper shows a simple method for predicting the performance that neural networks will achieve with a given architecture, hyperparameters, and based on an initial part of the learning curve."
3970,1," This identity connection acts as a \u201csurrogate memory\u201d component, preserving hidden activations over time steps."
3971,1, This might be a relevant comparison to add to establish more clearly that it is the implicit step that yields the improvement.
3972,1,\n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).
3973,1,"\n\n* \""Note here that, although we explicitly input an occupancy map to the master agent, the actual infor-\nmation of the whole system remains the same."
3974,1," Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR)"
3975,1,"  First, I'd suggest acknowledging these works and discussing the differences to your work."
3976,1," Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice."
3977,1," For example, what are the necessary and sufficient conditions for an attacking method to be undetectable?"
3978,1," It might be my mis-understanding but from my reading a prescriptive procedure for universal perturbation seems not attained from the results presented."""
3979,1," On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU"
3980,1,\n4) There is no direct comparison with other state-of-the-art in the paper.
3981,1,"\n\nFor generative modeling on MNIST, \""784d vector\"" is less clear than \""784-dimensional vector\""."
3982,1," Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression."
3983,1," I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6)."
3984,1,"\n- please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017) \nshould be\nGraph Neural Network (Nowak et al. (2017))"
3985,1," To start with, GAN is already training the generator G to match the induced P_G(x) (from P(z)) with P_d(x), and hence by definition, under the generator G, there should be no better prior than P(z) itself (because any change of P(z) would then induce a different P_G(x) and hence only move away from the learning target)."
3986,1,"""Pros: \nThe paper proposes a \u201cbi-directional block self-attention network (Bi-BloSAN)\u201d for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient."
3987,1,\n\n- The adaptive component seems to provide improvements for small dataset sizes
3988,1,"\n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342"
3989,1,\n2. Arrow in Figure 3 are not quite readable;\
3990,1,"\n\nDetailed comments:\n\""For all tasks, the number of batch per training epoch is set to 100."
3991,1," The lack of labels on the plots makes them hard to understand at a glance, and the overlapping lines make finding certain algorithm\u2019s performance much more difficult."
3992,1, This Lipschitz property has already been proposed by recent methods and has showed some success.
3993,1,"Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU."
3994,1, \nThis is not the first attempt of doing these encoders and decoders.
3995,1, The assessment of the method is incomplete and not convincing.
3996,1," but the paper suffers from a lack of technical novelty in the model and limited experimental validation."""
3997,1, Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature.
3998,1,\n\nThe discussion of dynamic programming: the dynamic program is also only available because the attention pattern is limited in a way that self attention is not.
3999,1,"\n\n\""However in practice the operations were incredibly slow, taking up to 30 minutes in some cases.\"" It is unclear what operations are referred to here."
4000,1," The baseline model lacks both \""batch normalization\"" and \""dropout\"", which I guess is because otherwise the proposed method would under-perform against the baseline."
4001,1," With its poor exposition of the technique, it is difficult to recommend this paper for publication."""
4002,1," They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables."
4003,1,\n\nPositive aspects:\n+ The idea of using GANs for this goal is smart and interesting
4004,1," Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right."
4005,1,"\nii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC."""
4006,1, In terms of the model this is a relatively small extention of Raffel et al 2017.
4007,1,"  After equation (17) it is stated that \""the left-hand side is independent of x_{k, m, l}\"" which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term). "
4008,1," Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference."
4009,1, Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework?
4010,1,- A detailled discussion of the relation to previously existing very similar work is missing (see above)
4011,1," Also, try to move Figure 1 so that it appears closer to its inline reference in the text."
4012,1,"\n- The ability to use state restoration for Path function learning is actually introducing a strong extra assumption compared to standard A3C, which does not technically require it."
4013,1,"\n\nThe authors take a binary-circuit approach: they represent numbers via a fixed point binary representation, and construct circuits of secure adders and multipliers, based on homomorphic encryption as a building block for secure gates."
4014,1,"\n- a module transforming the image into a log-polar representation according to the predicted origin,"
4015,1, Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data.
4016,1,"""The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations."
4017,1,". More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails."
4018,1,\n\nThe approach raises the natural questions of where the tasks and the task graphs come from.
4019,1,\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition.
4020,1,"""The authors suggest using a variational autoencoder to infer binary relationships between medical entities."
4021,1,"""This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE."
4022,1," Where the label 'semi-supervised' is used (page\n4) is actually wrong: yes the labels are used, but of course it is the\n*gradients* which show up in the update, not the labels themselves\ndirectly.[[CNT], [EMP-NEU], [DIS], [MIN]] It's also not true that there is little research in understanding\nthe formation of internal representations.[[CNT], [EMP-NEU], [DIS], [MIN]] There is a whole subfields of\npapers trying to interpret the features learned by deep networks, and much\nwork designing learning frameworks and objectives to achieve better\nrepresentations, e.g, to better disentangle the underlying factors."
4023,1,\n- The goal of the paper is to address automatically the learning of regularization parameters.
4024,1,"\n-- Section 4, importance of large datasets."
4025,1,\n\n* Figure 1: What is the x-axis here?
4026,1,   In other words samples in the same category will have the same weight \n\nError bound is derived.
4027,1, The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class.
4028,1,\n\nPros and Cons\n============\n+ good results
4029,1," However, given how little we know about the behavior of modern generative models, it is a good step in the right direction."
4030,1," \n\nThat said, the paper has several positive aspects in all areas:\n\nOriginality - the paper presents first combination of DenseNets with LSTM-based output factorization,"
4031,1,"\n\nThe lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds."
4032,1," It appears that the iterative method should result in \""direct improvement with additional samples and inference iterations\""."
4033,1," What do you do when you go back to the task that doesn't have the input, feed 0?"
4034,1,"""The authors proposed a graph neural network based architecture for learning generative models of graphs."
4035,1,"""My review reflects more from the compressive sensing perspective, instead that of deep learners."
4036,1, Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper.
4037,1, Logic flows naturally within the paper.\n\n
4038,1,\n\n4. The hypothesis are not correctly specified.
4039,1,"\n\n(The transfer from positive->negative on an ambiguous example was interesting: Original \""service is good but not quick\"" -> \""service is good but not quick, but the service is horrible\"", and \""service is good, and horrible, is the same and worst time ever\""."
4040,1," Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper."
4041,1, \n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original.
4042,1, Or am I missing something?
4043,1,"\n\nOverall, the paper provides limited technical novelty."
4044,1,"  ( If the goal state s\u2019 was just the next state, then this would just be a dynamics model and this would be model-based learning?"
4045,1,"""The paper claims to develop a novel method to map natural language queries to SQL."
4046,1,"""This paper proposes to improve time complexity of factorization machine."
4047,1," The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient."
4048,1,  You also need to define the evaluation metrics used.
4049,1, It would be more clear if you can provide a complete pseudo-code of the learning procedure.
4050,1,\n\nThe main positive point is that the performance does not degrade too much.
4051,1,"""Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same."
4052,1, The proposed method doesn\u2019t seem always outperforming the baselines.
4053,1,"\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. "
4054,1, I would like to see results from a few different random seeds.
4055,1,".\n\nThe work focuses on studying \""non-saturating\"" GANs, using the modified generator\nobjective function proposed by Goodfellow et al. in their seminal GAN paper, and\naims to show increased capabilities of this variant, compared to the \""standard\""\nminimax formulation."
4056,1,"\n\nSo, while I cannot vouch for the correctness, I think it can and should go through a serious revision to make it succinct and that will likely considerably help in making it accessible to a wider readership and aligned to the expectations from a conference paper in the field. """
4057,1," However, I find there are some limitations/weakness of the proposed method:\n1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer)\n2."
4058,1, New method proposed and shown to work well in one case.
4059,1," Interestingly, these include representations that have been observed in mammals and that have attracted considerable attention, even honored with a Nobel prize. "
4060,1,"\n\nOverall I think this paper addresses an important problem in an interesting way,"
4061,1," Also, the results section seems to switch off between calling the method CCN and LCO interchangeably."
4062,1,"""Clarity \nThe paper is well-written and clear."
4063,1,These are minor annoyances; there were some typos and a strange citation format.
4064,1,"""The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs)."
4065,1,. What is the main benefit of the proposed mechanism compared to the existing ones?
4066,1," It would also aid readability if the various models were described more clearly in this section, with an emphasis on structure, output targets, what LMs are used, how are the LMs pruned for the embedded-size models, etc."
4067,1,"  Then to make it fast, the implicit step is approximated using conjugate gradient method because the step is solving a quadratic problem."
4068,1,"\n\nOverall, this paper is more suitable for the workshop track"
4069,1,"""Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic."
4070,1,"Namely, the author train a generative adversarial network (GAN) to adversarial examples for a target network."
4071,1," Then, a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribution."
4072,1, The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect.
4073,1, The authors evaluate on cifar-10/100 and several out of distribution datasets and this method outperforms the baseline by significant margins.
4074,1," \n\n- \""Table 4.1 compares these log likelihoods, with VHE achieving state-of-the-art."
4075,1, \n- The presentation of the results is not very clear.
4076,1,"""In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework."
4077,1," If the claim from the work is true, that model should be just as bad as a regular VAE and would clearly establish that using language is helping get better image samples."
4078,1,"n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm."
4079,1, Is there a real reason for that?
4080,1," \nIn experiment 3 instead, the defined setting is supposed to give imporance to colors, as stated at the end of page 4."
4081,1," \n\n- Is there anything special about the GAN approach, versus other generative approaches? "
4082,1, The authors generalize this idea in a nice  way and present results on 1 experiment.
4083,1," Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders."
4084,1," It is not clear how it is used. In particular, it is not clear how it is used to \""update the task weights\"
4085,1,"""This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task."
4086,1," The idea is tested on very small data sets (80 and 50 examples, respectively)."
4087,1," However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue."
4088,1,"  \n\nSome minor comments: \n\n1.\tIn page 2, 6th line after eq (1), \u201c\u2026 these two problems\u201d --> \u201c\u2026 these three problems\u201d \n2."
4089,1," Second, it is unclear how close the proposed method is to finding the optimal regularization parameter \\lambda."
4090,1, There is also no evidence on whether adaptive learning on the fly is needed or not.
4091,1,  The paper also provides numerical results to support their theoretical findings.
4092,1, The objective function is a multi-class classification problem (using softmax loss) and with linear model.
4093,1,"  They also evaluate realism images using AMT fooling - asking turks to chose the fake between between real and generated images, and obtain substantial improvements on this metric as well."
4094,1," In some cases the results are similar to pix-to-pix (also in the numerical evaluation) but the method allows for one-to-many image generation, which is a important contribution."
4095,1," For example on ImageNet, 14.2% of the innocent images are mistakenly rejected as malicious to get 90% detection rate."
4096,1,. Is the point that GloVe is a bad algorithm?
4097,1,\n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow.
4098,1,"\n\nSignificance: although the application of L2S to RNN training is not new,"
4099,1," \n3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods."
4100,1,\nThe hidden representation obtained by the encoder should already capture information about the relation.
4101,1," Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question."
4102,1,\nThe benefits of not estimating an explicit action model are not really demonstrated in a clear way.
4103,1, \n\nOther comments and questions:\n\n1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel.
4104,1, This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard.
4105,1," Even if the authors released their code used for training (which is not mentioned), I think the authors should aim for a more self-contained exposition."
4106,1, The manuscript motivates the necessity of such technique and presents the basic intuition.
4107,1, It might be good to emphasize that you don\u2019t train on the IWAE bound in any experiments.
4108,1," Intuitively, if the repetition issue is prominent to having decent summarization performance, it might affect our judgement on the significance of using intra-attention or combined RL signal."
4109,1,  There is no way to discover a network outside of the principled design space articulated in point (1) above.
4110,1,   \n\nPros:\n+ The paper is written clearly and easy to read
4111,1, Why would that result in zero training error?
4112,1,  It may also lead to simpler solutions.
4113,1,"  As far as I know, symmetric LDS models are not common in the controls community."
4114,1,\n2. Missing some investigation of the properties of the estimator on simple problem to be compared to standard methods. 
4115,1," This is a very practical idea, well-explained with a thorough set of experiments across three different tasks."
4116,1,"""Summary:\n The paper presents an unsupervised method for detecting adversarial examples of neural networks."
4117,1,\n\n3. Increasing k also comes at a computational cost.
4118,1," As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded."
4119,1," I think a good paper should investigate this fact more."""
4120,1,"  The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior."""
4121,1,"  With real data, acquired from humans, the training is likely to end up in a very different minima."
4122,1," Two more papers from the same group which use also make similar observations - tweaking the image representation makes image captioning better: (1) Fang et al., 2015: Multiple-instance Learning using bag-of-objects helps captioning (2) Misra et al. 2016 (not cited): label noise can be modeled which helps captioning. This claim has been both made and empirically demonstrated earlier.\n\nMetrics for evaluation\n- Anderson et al., 2016 (not cited) proposed the SPICE metric and also showed how current metrics including CiDER may not be suitable for evaluating image captions."
4123,1, I am not convinced that the proposed bound will have low enough variance.
4124,1,"IMO, this leads to unnecessary confusion and does more harm than good."
4125,1,"  However, the authors do not seem to have considered this straight-forward approach."
4126,1," If I understand it correctly, the model needs to compute O(N^3) LSTM compositions."
4127,1,"  They also propose a new metric, motion accuracy, which uses the accuracy of the predicted position of the object instead of conventional metrics like PSNR, which is more relevant for robotic motion planning."
4128,1,"\n\nMoreover, I have the following comments:\n\n(1) Theorem 3.3 is currently not self-contained."
4129,1, However I have some concerns to this paper.
4130,1," Such results are hardly convincing, since the tuning of the parameter lambda plays a crucial role in the performance of the method."
4131,1,"\n\n- Page 4- sec 3.2- last paragraph.[[CNT], [CNT], [DIS], [GEN]]\nThis claim lacks scientific support, otherwise please cite proper references."
4132,1," Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all."
4133,1,\n\n4) What happens with ongoing training?
4134,1," Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)?"
4135,1,"  I was also confused by the notation $D_\\phi^q$, which was described but not defined."
4136,1," \n\n2. In B.1 the authors use an increasing number units in the hidden layers of the GRUs as opposed to a fixed size like in Deep Speech 2, an obvious baseline that is missing from the  experiments is the comparison with *exact* same GRU (with  768, 1024, 1280, 1536 hidden units) *without any compression*."
4137,1,"\n\n2). In Algorithm 1, how do you deal with vocabulary items in the new domain that do not exist in the previous domains i.e. when the intersection of V_i and V_{n+1} is the null set."
4138,1," Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy"
4139,1," \n\nSecondly, paper has a lack of novelty."
4140,1," The new model \""EEN\"" is compared to a deterministic model and conditional GAN."
4141,1,"   The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?)."
4142,1, This argument is questionable.
4143,1,"""The paper present online algorithms for learning multiple sequential problems."
4144,1,\n\n- Learning translation without parallel corpora is a useful task and leveraging a pragmatic reference game to induce additional semantically valid samples of a source language is an interesting approach to do so.
4145,1," Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this?"
4146,1,"\n- MMI was also widely used in HMM/GMM systems, not just NN systems"
4147,1, I was assuming that those are the same system but did not see the numbers match each other.
4148,1,"\n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence."
4149,1,"\n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations."
4150,1,\n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel.
4151,1," Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition."
4152,1," Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones."
4153,1," According to Lemma 1 and its finite\nsample version in Theorem 1, the risk on the target domain can be upper bounded\nby the combination of 1) the re-weighted empirical risk on the source domain;"
4154,1," \nRelated to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%."
4155,1, I especially liked the general idea of using multiple modalities to improve embeddings of relational data.
4156,1,"\n\n\n* \""Therefore they tend to work more like a commentator analyzing and criticizing the play, rather than\na coach coaching the game."
4157,1, This is an important question that has not been addressed in the literature and is clearly a pro of the paper.
4158,1," There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL."
4159,1, How statistically valid are the results?
4160,1,"\n\nWhile the proposed objective is interesting and meaningful for several conversational applications, as well as sentence modeling, the presented experimental results are not convincing."""
4161,1,\n-The large scale aspect lacks of thorough analysis
4162,1,"\n\n2. The paper reads like a collection of lemmas, with no verbose connection."
4163,1,"""This paper includes several controlled empirical studies comparing MC and TD methods in predicting of value function with complex DNN function approximators."
4164,1," This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A."
4165,1," They iteratively explore all\npossible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive\noracle ensures that the procedure converges."
4166,1," The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models."
4167,1," I.e. 1 or 3 feed-forward networks for age, zip code, and release dates?"
4168,1," \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables."
4169,1,The idea is quite simple as pruning using the connection in the batch norm layer
4170,1, Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word?
4171,1, What components precisely does an element of U have?
4172,1," This is good work: it is well written, the experiments are thorough and the proposed method is original and works well."
4173,1,"Overall, the results are a bit mixed."
4174,1,"  I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here."
4175,1,\nThis paper is much related not to representation learning but to user-interface.
4176,1, The targets\u2019 update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation).
4177,1,  In Advances in Neural Information Processing Systems (pp. 503-511). (see Figure 5A for the circuit implementing a Predictive Sigma-Delta encoder discussed by you)\n\nb.
4178,1," \nBased on the experiment, the step in the implicit direction seems to decrease faster the objective, but the paper does not make an attempt to explain why. "
4179,1, Thus this space may constitute a better candidate for performing data augmentation by small perturbations or by nearest neighbour search around the given vector since 1) the augmented data is more likely to correspond to features of similar images as the original provided image and 2) it is more likely to thoroughly capture the intra-class variability in the augmented data.
4180,1, That seems important to explain more thoroughly than is done in the current text.
4181,1,\n\nThe paper is clearly written.
4182,1, It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal.
4183,1," The main technical contribution is the search procedure to find minimal training sets and pare down the observation size, and the empirical validation of the idea on several algorithmic tasks."
4184,1, \n(2) The overall objective in Section 5 is broken.
4185,1,"\n\n2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)?"
4186,1,\n- The proposed memory architecture is new. 
4187,1,"  \n\nOriginality: The ideas do not seem to be novel, and are mostly (trivially) using existing work as different components of the proposed technique."
4188,1," However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. "
4189,1, The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.
4190,1,"  The instability of a gradient method on non-strongly convex-concave saddle point problems (like the bilinear form of Figure 1) is a well-known property, and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this paper."
4191,1," The parameters \\theta are updated just after equation 10 by\nfollowing the gradient of the loss in which the weights of the last layer are\nfixed to a posterior sample, instead of the posterior mean."
4192,1,"\n\nThe image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach."
4193,1," The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed."
4194,1, The authors also provide some visualisation of the parameters of their model.
4195,1,"  It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them."
4196,1,"""The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly."
4197,1, The type of model used is counterintuitive for me; why use a SVR model?
4198,1, It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.
4199,1," ICLR does not have a strict page limit, and the figures you have are hard to impossible to read."
4200,1,\n\n\n===============================================================\n\nRevising my review following the rebuttal period and also the (ongoing) revisions to the paper.
4201,1,". Also, the caption for Table 2 could contain more information regarding the network outputs."
4202,1,"\n\n\nCOMMENTS\nThe j-step returns TD error is not written correctly[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nIn Figure 1 it is not obvious how the confidence of the values is estimated."
4203,1,"\n\n* Section 3: It would be good to explicitly draw out the graphical model for the proposed approach and clarify how it differs from prior work (Eslami, 2016)."
4204,1," I am also skeptical about the computation efficiency of a model that scores all spans in a document (which is O(N^2), where N is the document length)."
4205,1,"  The regularizer is specifically nice in this setting, as it suffices to have the Kronecker factors be unitary."
4206,1," However, I'm a little unsure about these conclusions, what is the unregularized version exactly doing, how is it different from a standard softmax?"
4207,1,"\n\n \""3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance."
4208,1, It would be nice to understand/visualize what information have been extracted in the representation learning phase.
4209,1, These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution.
4210,1,"\n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments."
4211,1," This is presented in the same figure and paragraph as the CIFAR results,"
4212,1,\n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W.
4213,1," \n\nThe results are interesting, but more explanation is needed for the main message to be conveyed more clearly."
4214,1, The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.
4215,1," It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques."
4216,1," The model is based on tensor factorization which extends GloVe to higher order co-ocurrence tensors, where the co-ocurrence is of words within subgroups of the text data."
4217,1, This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations.
4218,1,"\n- It would be worth explaining, in a sentence, the approach in Shen et al for\n  those who are not familiar with it, seeing as it is used as a baseline."
4219,1,\n\n\nTypoes:\n\nLegend of Figure 2: red lines are error -> red lines are accuracy\nTable 1: test accuracy -> test error\nBefore 6.2: architecture effects -> architecture affects
4220,1," Overall,  the study is interesting and contains some new idea."
4221,1," I'm not very familiar with SQuAD dataset, but the results seems worse than \""Reading Wikipedia to answer open-domain questions\"" Table 4 which seems use a vanilla LSTM setup."
4222,1," \n\nThe methods reviewed prior work which the authors refer to as \u201cparallel order\u201d, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn\u2019t be the case."
4223,1,.\nUPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf\nEfficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf\nA bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf\n\n4.
4224,1,   but could be expanded into a nice contribution eventually. 
4225,1," In AAAI, Austin Texas, USA, Jan. 25-30 2015. \u2028\n2. Y. Engel, S. Mannor, and R. Meir."
4226,1,"  \n\n(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2)"
4227,1," \n\n3. For clarity, the authors should express equation 5 in terms of Y_1, Y_2, Y_3, and Y_4."
4228,1," The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition."
4229,1, \n\nQuality: The paper is well written.
4230,1,"  Granted, most world model also try to predict the reward."
4231,1,"\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations"
4232,1,Many different evaluation measures defined to measure different properties of the project\n\t\u2022\t
4233,1,"\n\n4). Typos: In Section 3, \""...is depicted in Figure 1 and Figure 3\"". I think you mean \""Figure 1 and Figure 2\"" as there is no Figure 3."
4234,1," Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints."
4235,1, This phenomenon definitely goes against the intuitions about developing most of the conventional machine translation models.
4236,1," How does this compare to the near-identity constraints in resnets in Shaham et al. ?\n\n"""
4237,1," The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work."""
4238,1, \n\nMain issues:\n- It's not generally scalable to build a neural network whose size scales with the number\nof possible inputs.
4239,1," Works such as [2,3] have used this objective function for single step and multi-step frame prediction training, respectively."
4240,1," \n\nOnly one set of data is used throughout the paper: the Cifar10 dataset, and the architecture used is only a 100 layered MLP."
4241,1," This is supported by the literature on \""data cleaning\"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition."
4242,1, \n\nMy biggest concern is about Figure 6a.
4243,1,"\n\nTherefore, the claim that \n\n\""Since we have established that MCBN performs on par with MCDO, by proxy we\nmight conclude that MCBN outperforms those VI methods as well.\""\n\nis not valid."
4244,1, Ho and Ermon 2016 extensively study the fact that imitation is not possible in stochastic environment without the knowledge of the actions.
4245,1,"\n\nI would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.)."
4246,1,"  While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters."
4247,1, The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem.
4248,1," As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015."
4249,1," Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm."
4250,1,\n- Figures 3 and 4 illustrate some oscillations of the proposed approach.
4251,1,"\n\nNotation and Typos:\n- Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify."
4252,1," The first paragraph of page 5 uses q(\\theta | \\phi, (x, y)), but y is not known at test time."
4253,1," They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks."
4254,1," In Section 3, a few methods from the literature are classified according to the proposed taxonomy."
4255,1," The presentation of the paper, however, should be improved significantly before publication."
4256,1, What input is injecting?
4257,1, This decision is in order but needs to be stated in order for the reader to form a balanced view of methods at her disposal.
4258,1,  The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. 
4259,1,The paper is well organized and clearly written.
4260,1,"\n\n3. Based on the experiment results, this proposed method outperformed previous methods (TD-GAN, IcGAN)."
4261,1," \nSecondly, there are no other state-of-the-art baselines are used."
4262,1, \n-\tCan the authors address the earlier comment on how their approach provides \u201cguarantees for preserving neural net functionality approximately\u201d?\n\n
4263,1,"\u201d => This is simply incorrect. """
4264,1,  The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net.
4265,1," \n\n3. In Section 4, it would be great to have more in-depth simulations (e.g., multi-task learning in various settings)."
4266,1,\n2) What can be said about rate of convergence in terms of network parameters?
4267,1, but at this point it is not clear to me why multiplicative is better than additive in either performance or in giving meaningful interpretations of the model.
4268,1," Also after reading the abstract of this paper, one may think that this is the first paper discussing the SOM / supervised learning combination."
4269,1," For instance, it only became clear at the end of the section that E was learned."
4270,1," Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon."
4271,1, \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).
4272,1,"""It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing"
4273,1,"\n1) It would be nice if the paper were to explain, from a theoretical perspective, why large evidence should correspond to better generalization, or provide an overview of the work which has shown this (eg, Rissanen, 1983)."
4274,1,"""The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric."
4275,1," It is riddled with grammar, choice of word, and spelling mistakes."
4276,1,"  Probabilistic models of string distance have been common since Ristad and Yianlios in the late 90s, and there are proper probabilistic models that would work with your same dynamic programming algorithm, as well as improved models with some modest state splitting."
4277,1,\n\nThere are some papers that could be connected.
4278,1, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.
4279,1," For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation."
4280,1,  This means that skill composition that is proposed in this paper might be quite far from the best policy that could be learned directly from a single conjunctive TLTL formula.
4281,1, RL results are reported with only the best-performing attention setup for each dataset.
4282,1," The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top."
4283,1," If the authors release their code as promised, the off-the-shelf tool would be a very valuable contribution to the GAN community."
4284,1," \n\nSome other issues regarding quantitative results:\n- In Table 1, there are 152 clusters for 10-d latent space after convergence, but there are 61 clusters for 10-d latent space in Table 2 for the same MNIST dataset."
4285,1, \n\nPros: \n+ The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel.
4286,1," In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement."
4287,1,"\n\nIn the problem set-up section, it is not immediately clear what the distinction between s, r, and t is."
4288,1,"""This paper proposes a new space for reasoning about human identity."
4289,1, While I agree that a robust and fine-tuning-free model is ideal 1) this has to be justified by experiment. 2) showing the experiment with different parameters will help us understand the role each component plays.
4290,1,\n\n- The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there.
4291,1," If S is diagonal then the S generation methods a) b) c) in the end of section 3.1 will make sure that S is PSD, I do not think that this is the case with d) though."
4292,1, Any reasonable model does more than just reproducing the data points.
4293,1,"\n\nThis paper has some very good ideas, and asks questions that are very much worth asking."
4294,1,"""This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural."
4295,1, The key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the training.
4296,1,\n\nSignificance/Conclusion: The idea of meta-learning or learning to learn is fairly common now
4297,1, Their technique can be applied on top of existing GANs and can address issues such as mode collapse.
4298,1," Lastly, they incorporate attention for large visual inputs."
4299,1," My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation."
4300,1,"  They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be."
4301,1, many design choices of the system is questionable.
4302,1, This is a novel use of existing methods.
4303,1, This is important to compare properly with previous work.
4304,1," However, it seems to me that this goal is not fully implemented."
4305,1, Why training a network to predict 2D spatial location from velocity inputs?
4306,1," \n\n[1] Kulesza, Alex, and Ben Taskar. \""Determinantal point processes for machine learning.\"" Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012): 123-286."""
4307,1,"  \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low,"
4308,1,\n\n\n* Unclear relation with other papers:\nWhat part of the derivations of this work are novel?
4309,1,  parameter equalization is not the only approach to fair evaluation
4310,1," With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful."
4311,1," As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models."
4312,1," I found the undefined citations in the middle of page 2 frustrating, since I'd have liked to follow up on those citations as comparison points for this work"
4313,1,"""The main contribution of the paper seems to be the application to this problem, plus minor algorithmic/problem-setting contributions that consist in considering partial observability and to balance multiple objectives."
4314,1,"\n\n[R1] \u201cVideostory: A new multimedia embedding for few-example recognition and translation of events,\u201d in ACM MM, 2014\n\n[R2] \u201cTransductive Multi-View Zero-Shot Learning\u201d, IEEE TPAMI 2015\n\n[R3] \u201cVideo2vec embeddings recognize events when examples are scarce,\u201d IEEE TPAMI 2014\n"""
4315,1, The robot then uses the learned parametric skill functions to reach goal states (images) provided by the demonstrator.
4316,1," However, the results show that the deeper self-attention layers are indeed useful (but still not improving a lot, about 0.7-0.8%)."
4317,1," In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n"" "
4318,1,\n\n-ves:\n1. My main concern with the paper is the novelty of the contribution to the techniques.
4319,1,  They empirically verify that the features they learned lead to good quality SVM classifiers.
4320,1," I would expect the notation style to be consistent within a single equation (i.e., use ||w||_2^2, ||w||^2, or ||w||_{l_2}^2)\n\n3.)"
4321,1,  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.
4322,1," It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function."
4323,1,"\n\n(2) A similar concern about the baselines: the paper did not compare with ANY previous work on speeding up RNNs, e.g. \""Training RNNs as Fast as CNNs\""."
4324,1,"\n\nEven though the introduced dataset in this paper is interesting, there are some issues:\n- On the model side, this paper used the same architecture as the Karpathy & Fei-Fei (2015)."
4325,1,". The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters."
4326,1, How much does that impact the overall compute?
4327,1,"""# Update after the rebuttal\nThank you for the rebuttal."
4328,1,"\""  I'd like to see work in this area encouraged."
4329,1," There are many graph models that are superior to DC-SBM, including KPGMs, BETR, ERGMs, hierarchical random graph models and latent space models."
4330,1, I don\u2019t think this is a good fit for ICLR.
4331,1," The authors state that WAE generates \""samples\nof better quality\"" (than VAE) without any condition being put on when\nit does this."
4332,1, The experimental results also sufficiently demonstrate the proposed advantages of the model.
4333,1,\n3) Validation on real-world software data
4334,1, The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision.
4335,1,". They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time."
4336,1,"\n\nWhile it is clearly written, my main concern is whether this model is significant enough."
4337,1,"- Appendix A: \n    - \""Let's\"" -> \""Let\""\n"
4338,1,"The key distinction being that in this gamified setting, the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth."
4339,1, The way the authors propose to do it is to have the agent follow a random policy.
4340,1, The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty.
4341,1," Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance."
4342,1," \n\nWhile most of the paper is well written,"
4343,1,"""The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor."
4344,1,The authors re-invent this and find it works better than randomly choosing a gold token or taking the max.
4345,1,\u201d  I am not sure what is meant by this since in this paper the authors never test their algorithm on real systems and in real systems it is not possible to completely eliminate collisions.
4346,1, The literature review seems to cover and categorize well the field.
4347,1,"\n\nThe authors say of the proposed TwinStream dataset that it \""may not be\nrepresentative of real use-cases\"". It seems odd to propose something that is\nentirely artificial."
4348,1,  I realize that SQUAD is not explicitly multiple choice and that this is a challenge for an answer elimination architecture.
4349,1,"The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset."
4350,1,". It definitely does not preserve phase, like modReLU would."
4351,1," In the latter case, a stronger parameter is applied, followed by reduced regularization parameter."
4352,1,"""The paper presents some conceptually incremental improvements over the models in \u201cNeural Statistician\u201d and \u201cGenerative matching networks\u201d."
4353,1,"\n\nTypos:\n- Equation 14: In the first term (target loss), theta should have an index t (I think).\n- Bottom of page 6: \""... and that as our validation set\"" (missing word).\n"""
4354,1," It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines."
4355,1, Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. 
4356,1," Further, is the auto-encoder trained separately or jointly with the training of the one-shot learning classifier?"
4357,1,    I don't think it can just be assumed a priori that humans would be super good this form of generalization.
4358,1,"\n\nThe paper builds upon approximate n-step discrete-action Q-learning \nwhere the Q value for an action is a linear function of the state features:\n\n    Qp(S,a) = Wa S + Ba\n\nwhere parameters p = ( Wa, Ba )."
4359,1,\n\nQuestions:\n\n- Why not compare against Seq2Seq + Search?
4360,1,"\n[4] M. L\u00e1zaro-Gredilla. Bayesian warped Gaussian processes.\n"""
4361,1,"   \n\nWith that being said, there is some work that needs to be done to make the paper clearer."
4362,1, Is there good reason to think RL agents will need to contend with time-limited domains in the future?
4363,1, The example in Table 1 is very good; but more examples (especially involving the quantitative comparison) are needed to demonstrate the claimed advantages.
4364,1,\n\n(1) The scope is a bit limited. 
4365,1," I'd recommend the just remove the pivot objective, or at least not call it a loss."
4366,1, Given that the authors\u2019 algorithm does not reproduce the sign-flip in the STDP rule I would suggest revise the corresponding part of the paper.
4367,1, \n* Related work.
4368,1," The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels."
4369,1," Pixel GAN rather then only giving a global score for the whole image? """
4370,1,"\n\nOn the whole I appreciate the novelty of the task and dataset,"
4371,1, Did you consider doing that here too?
4372,1," The paper's particular application to learning word embeddings (PPMI factorization), however, is new although perhaps not particularly original."
4373,1," It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b."
4374,1, How does that compare to the assumptions needed for standard inverse probability weighting?
4375,1,\n\nCons:\n\nWhile we liked both the challenge posed and the idea to solve it we found several major issues with the work. 
4376,1,"""The principal problem that the paper addresses is how to integrate error-backpropagation learning in a network of spiking neurons that use a form of sigma-delta coding."
4377,1,"\n\n\n## Significance\n\nMany recent previous efforts have looked at the importance of batch sizes\nduring training, so topic is relevant to the community."
4378,1, They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements.
4379,1,.\n3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank.
4380,1,"Minimizing a quadratic f(x) = .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE)."
4381,1,"""The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein\u2019s identity and control functionals."
4382,1," What if content(x_t) = W_1 . h_{t-1} + W_2 . x_t? """
4383,1, I was not convinced it is.
4384,1,"""This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner."
4385,1,  It's not clear if an evaluation on the MNIST data set is particularly meaningful. 
4386,1,  \n4. The paper needs some careful editing both for language (cf. following point) but also notation.
4387,1,"  how the \""good emulation property\"" is exactly measured ?)."
4388,1, But the future work on bandits is already happening:
4389,1,"""In Bayesian neural networks, a deterministic or parametric activation is typically used."
4390,1," Now let's assume we insert a scaling constant after each residual block, say c = 0.5."
4391,1,"\n\nA bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful."
4392,1," Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed."
4393,1,1\n\nMajor:\n\n- The size of the generated images is up to 26x31x22 which is limited\n(about half the size of the actual resolution of fMRI data).
4394,1,"\"" This is true of any autoencoder."
4395,1,  What architecture did you use in your experiments?
4396,1, It seems that authors spend extra space creating problems and then solving them.
4397,1,"\nThe tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets."
4398,1, A quick search reveals [1] (probabilistic modeling of dependency parses to create Bayesian topic models directly) and [2] (creating a semantic vector space from a dependency parse) I suspect there are others
4399,1," More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights."
4400,1," \n\nUnfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not."
4401,1," \n\n- \""In a VHE, this recognition network takes only small subsets of a class as input, which additionally ...\"": And that also clearly leads to loss of information that could have been used in learning."
4402,1, It combines the following ingredients:\na) a population-based setup for RL\nb) a pair-selection and crossover operator\nc) a policy-gradient based \u201cmutation\u201d operator\nd) filtering data by high-reward trajectories\ne) two-stage policy distillation;
4403,1," Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice."
4404,1,"\n\nThe paper is mostly clear and well-presented,"
4405,1,"\n\n The paper proposes \""FastNorm\"", which is a way to implicitly maintain the normalized weight matrix using much less computation."
4406,1,\n\n\n4) Conclusion:\nThis paper seems to contain very minimal changes in comparison to the baseline by [1].
4407,1,"\n\nStrengths:\n- The complementary kernels come at no loss compare to standard ones\n- The resulting wider networks can achieve better accuracies than the original ones\n\nWeaknesses:\n- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions\n- The improvement over the baseline is not very impressive\n- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)\n\nDetailed comments:\n- The separation into + and x patterns is quite clear for 3x3 kernels."
4408,1,\n- it suggests an interesting connection between a traditional model and Deep Learning techniques\
4409,1,"\n\n2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration."
4410,1,"""This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN)."
4411,1,"\""\n\nIn the conclusion, \""optimal for deterministic objective\"" should be \""deterministic objectives\"""""
4412,1,"\n\n3) It took me a while to understand the authors' subtle comparison with target propagation, where they say \""Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization."
4413,1,\n-The authors tackle a very challenging subject
4414,1," Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks."
4415,1, It's not clear which two baselines are depicted in 5b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baseline?
4416,1,"\n\nMinor comments:\n- The justification of why OptStrong is missing from Table2 (last three sentences of 3.3) should be summarized in the caption of table 2 (even just pointing to the text), otherwise a first reader will mistake this for the omission of a baseline."
4417,1,"\nQuality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done,"
4418,1,\n\n* The claim that the paper works with natural language should be toned down and clarified.
4419,1, although it does not seem to have a main focus (exponential gaps vs. optimisation vs. universal approximation).
4420,1," \n\n4. The Experiment section is not well structured, at least for me, I cannot understand it well."
4421,1, but some experimental comparisons and claims are not very convincing.
4422,1,"\n\nOverall the paper is a descent one, but with limited novelty."
4423,1,"\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning."
4424,1, The paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other's actions.
4425,1,"""This paper proposes a self-normalizing bipolar extension for the ReLU activation family."
4426,1,"  As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning)."
4427,1, and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels.
4428,1, The paper is not talking about the weaknesses of the method at all.
4429,1,"\n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model."
4430,1, The authors' use of 'sparse' when\n  they mean 'zero' is really confusing.
4431,1,\n\nHow sensitive are the results to hyperparameters?
4432,1," Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains."
4433,1, The main reason is that the original face mesh graph that goes into the convolution/downsampling\n operations is topologically preserved through the upconvolutions.
4434,1," Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model."
4435,1,\n-\tSection 2: better to explain about how to obtain attention scores z in more details.
4436,1," The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.\n\nI think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.\n\nFirst, Assumption 4 seems a bit too abstract."
4437,1,"""  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss."
4438,1, but I do not understand a few points which make it hard to judge the contribution.
4439,1," The first 8 pages mostly constitute a literature review and experimental section provides no insights about the performance of the TAGCN besides the slight improvement of the Cora, Pubmed and Citeseer benchmarks."
4440,1,  It was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human readers.
4441,1,"n\nIt would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.\"
4442,1, What is the reconstruction error during the second CP decomposition in 2?
4443,1,  \n\nI found it that the contribution of the paper is very limited.
4444,1,"\n-\tExperimental results are partial: results are not presented for multiple defenders, no ablation experiments"
4445,1,"\n\nI have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted."
4446,1, Do other categories have the similar results?
4447,1,\niii) The model generalizes well in terms of link and node classification
4448,1,\n  Putting these in a table along with the results would improve readability.
4449,1,". I don't see the usual two lines under the title (\""Anonymous authors\"", \""Paper under double-blind review\"")."
4450,1," However, Gal and Ghahramani\n(2015) actually follow Hern\u00e1ndez-Lobato and Adams, 2015 so the correct\nreference should be the latter one."
4451,1, Empirical results are presented for several benchmarks.
4452,1,"\n\nf) It is supposed that the L_1 regularization motes the weights to be informative, this work is doing something similar."
4453,1," Note that there is a difference between batch methods in stochastic optimization where batches are composed of a subset of observations (which then leads to an approximation of desirable quantities, e.g. the gradient, in expectation) and the current approach where subtensors are considered as batches."
4454,1, Contextualization with respect to previous work is adequate.
4455,1, I also find the empirical results encouraging.
4456,1," It's unclear to me how effectively accuracy and precision are balanced by this training strategy, and there isn't any discussion of this point either."
4457,1,]\n- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.
4458,1,"  In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs."
4459,1," Experiments are presented on MInst, Cifar10, and ImageNet."
4460,1," However, I am not familiar enough with the literature on Bayesian evidence, or the literature on sharp/broad minima, and their generalization properties, to be able to confidently say how original this work is."
4461,1,The method is to prune the network\u2019s activations at each layer and renormalize the outputs.
4462,1," I strongly suggest that these results be submitted to a more suitable venue.\n\n"""
4463,1," They might be true for a narrow field of application. But in general, I think they are not quite correct."
4464,1," It is unclear to me whether the paper has value in its current form, that is without experimental results for the task it achieves."
4465,1,"""Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs"
4466,1, One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.
4467,1,"\n\n\n4. 1/2 sentence summary\n\nWhile the authors did an extensive job evaluating different settings of their technique I have serious doubts about it as a privacy-preserving method."""
4468,1, In general I found this paper clearly written and technically sound.
4469,1,\n\nPros:\n1. Very well written paper with good theoretical and experimental analysis.
4470,1,"\n\nHowever, there are also a few things to be cautious of... and some of them serious:"
4471,1,  This is partly because many non-trivial results are mentioned with little or no explanation.
4472,1," How well does the framework scale to more complex scenarios, e.g., multiple types of manipulation together?"
4473,1,. \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks
4474,1,\nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n
4475,1,"  \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models."""
4476,1,"""This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust."
4477,1,"\n\nWhile the strength of this paper is clearly the good writing as well as rigorous experimentation,"
4478,1," In other words, a sparse reward setting is only \""difficult\"" if getting into a good neighbourhood requires long term planning and cannot be resolved by a (pseudo) blind random walk."
4479,1,"\"" arXiv preprint arXiv:1707.05589 (2017)."
4480,1,\n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications?
4481,1,"\n\nClarity: The clarity of the text is fine, though errors make things difficult sometimes."
4482,1,"\n- What is a \""sequence-level variant of CTC\""?"
4483,1,"""This paper presents a novel application of machine learning using Graph NN's on ASTs to identify incorrect variable usage and predict variable names in context."
4484,1,"\n* \""unless an approximate ... is provided\"" (the use of the subjunctive here is definitely dated :-) )"
4485,1," Authors are suggested to discuss in more detail.\n"""
4486,1,. \n\nI have found it hard to understand what table 3 in section 4.2 actually means
4487,1, Is it the case that the model learns to attend to last sentences for all the questions?
4488,1, This seems like a good algorithm in many ways.
4489,1," How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL."
4490,1,\n\nThe paper should discuss the assumptions needed for classifier accuracy to be a good proxy for the quality of a generative model that generated the classifier's training data.
4491,1," Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n"""
4492,1," On one hand, fleet management is an interesting and important problem."
4493,1," \n\nOverall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings."
4494,1,"\n\n5) It would be nice to more quantitatively map out the relation between speed tuning, direction tuning, and spatial tuning (illustrated in Fig. 3)."
4495,1,\n- Discussion of results relative to baselines is somewhat lacking.
4496,1, \n\n2. The claim of cross-view for sequence tagging setting is problematic.
4497,1," First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.)."
4498,1,"\n\n1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read."
4499,1,"\n\nFigure 3, contrary to text, does not provide a visualisation to the sampling mechanism."
4500,1,\n\n* Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info.
4501,1, This is hard to do in practice due to jointly learning both the forward model as well as the state mismatches.
4502,1, They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks.
4503,1,\nb) the consequence of the approximation errors on the general convergence of the proposed method (consistency and rate)\n\n-
4504,1,"""This paper proposes the use of an ensemble of regression SVM models to predict the performance curve of deep neural networks."
4505,1,"\n\nThe second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return."
4506,1,"Cons:\nNot very well setup experiments\nPerformance is lower than you would expect just using supervised training\nNot clear what parts are working and what parts are not\n\n\n"""
4507,1," I thought, that it should model the joint (empirical) distribution over the labels, and this is part of the dataset."
4508,1, \n\nI am genuinely curious and would love the authors' comments on this.
4509,1," I expected that some observations about this point will be unveiled in the paper, but unfortunately, the paper described only a few BLEU scores with different window sizes which have not enough information about it."
4510,1, \n\nDetailed Comments:\n(*) Pros\n-The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST.
4511,1, The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs.
4512,1, This text generation task is traditionally done using recurrent neural networks.
4513,1,"""This paper introduces a graph neural net approach to few-shot learning."
4514,1,\n\nOverall this is an interesting paper
4515,1,"\n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations."
4516,1," It relies on the logistic regression model as the discriminator, and the dual formulation of logistic regression by Jaakkola and Haussler."
4517,1," 2) The writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (What is 'Golden Centroid Augmentation\"" or \""Model Centroid Augmentation\""?"
4518,1," In addition, I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases."
4519,1," In section 4, the authors define objectives, but do not clearly describe how these objectives are optimized, instead relying on the read to infer from context how REINFORCE and beam search are applied."
4520,1,".\n\nIn section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables."
4521,1, The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy.
4522,1,"""This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples."
4523,1," However, it is not clear to me what contribute to the massive improvement of speed."
4524,1,"\"" Why do the features human cognition uses give an optimal predictive accuracy?"
4525,1,"\n\n* Comparison to baselines: \n  1. How well does this model do against a baseline discriminative image caption ranking approach, similar to [D]?"
4526,1," Also, as the authors note the method seems to be limited to conditional sequence generators."
4527,1," This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions)."
4528,1, However producing marginal distributions that look reasonable is at least a *necessary* condition and without seeing those plots it is hard to rule out that the model may be producing highly unrealistic samples.
4529,1,Could the authors explore other quantitative measure?
4530,1, The authors empirically demonstrated the gated fast weights outperforms other baseline methods on the associative retrieval task.
4531,1,"\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions."
4532,1,"\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?"""
4533,1," even though some parts deserve more discussion/clarification (algorithm, experimental evaluation)."
4534,1,"\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?"
4535,1, The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.
4536,1," \nIn particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential."
4537,1," I am not convinced that this particular method is showing much information or highlighting anything particularly interesting, but could be refined in the future to do so."
4538,1,"""Summary: The paper proposed a two-dimensional approach to lifelong learning, in the context of multi-task learning."
4539,1," Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting."
4540,1," If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well."
4541,1,\n\n- The use of the time series is the main novelty.
4542,1,"\nSpecifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy."
4543,1," If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version."
4544,1,  Why not learning the representation using an unsupervised learning method (unsupervised pre training)?
4545,1,"\n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\n"
4546,1, I suspect this lower number of particles might be model-dependent.
4547,1,"\n\n\nUnconditioned generation experiments:\nIn these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help."
4548,1," For image classification, the performance of proposed method is below its predecessor Xception network."
4549,1," By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent."
4550,1," \n\nThe experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017)."
4551,1, \n\n- The positional attention is rather unclear and it would be better to revise it.
4552,1,It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem.
4553,1," However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy."
4554,1,\n2. Clipping is supposed to help with the exploding gradients problem.
4555,1, The paper is very well written and easy to follow
4556,1,\n\nOverall I think this method is inventive and shows promise for probing invariances.
4557,1,"""The paper presents a Depthwise Separable Graph Convolution network that aims\nat generalizing Depthwise convolutions, that exhibit a nice performance in image\nrelated tasks, to the graph domain."
4558,1, It\u2019s also difficult to compare the learning curves among the different models (Fig 1) as they are in separate plots with differently scaled axes.
4559,1,  I think another way of teasing apart such results would be recommended.
4560,1,"It seems that the proposed method works well for heat sink problem and the steady flow around airfoil, both of which do not fall into the more complex physics regime."
4561,1, though the authors mostly fail to explain how this analysis could provide further work with means to create new defenses or attacks.
4562,1," \n* Section 3. As said before, a general description of the learning framework should be given."
4563,1," Somewhat related to this point, it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity"
4564,1, No empirical results on the effect of the parameter are given.
4565,1," ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!)."
4566,1, There are many multi-agent techniques that can be applied to the problem that would have served as a better baseline.
4567,1," \n- the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for \""ord\"" the input is a WxHx1 matrix, but for \""ord\"" and \""cifar\"" the input is a \""WxHx3\"" matrix."
4568,1," These two papers need to be cited:\n\nRudolph et al., NIPS 2017, \""Sturctured Embedding Models for Grouped Data\"":"
4569,1," Also, the experiments are not clearly explained."
4570,1,".\n- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted."
4571,1," Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6."
4572,1,"\n\n2. In order for the paper to be more self-contained, maybe list at least once the formula for \""rectifier net\"" (sth. like \""a^T max(0, wx + b) + c\"") ?"
4573,1,\n\nThe node classification experiment could use a bit more refinement.
4574,1, and several ablation experiments show the effectiveness of these two improvements.
4575,1," However, is this statement enough to cross the acceptance threshold of ICLR?"
4576,1, Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes.
4577,1," This should be explicitly shown, at least in the appendix."
4578,1," (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically."
4579,1,"\n- Figure 4: Gain -> Relative performance\n- The batch size must have a great influence on the functioning of the regularisation (especially when there are many classes, in that case just a single example counts for the class mean). This is not explored in the paper."""
4580,1,"\n-- Overall it\u2019s unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance."
4581,1," However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing."
4582,1,\n\n* I\u2019m confused about the statements made about the \u201cconstant uncertainty\u201d baseline.
4583,1,\ne)\tCan this approach be used with multiple i.i.d. graphs?
4584,1," The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced."
4585,1," Calling these \""popular architectures\"" is misleading."
4586,1," I enjoyed reading the paper,"
4587,1,  Are there any theoretical results which can be leveraged from the stochastic processes literature?
4588,1," The results are fine when the idea is applied to the BiDAF model,"
4589,1,"  It feels to me that these tend to be sparse,"
4590,1, \n\nTarget-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning by Zhu et al.
4591,1, The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN.
4592,1,"\nWith systematic comparisons like that, it would be easier to understand where the gains in performances are coming from."
4593,1," Using GAN, the architecture allows for model-agnostic learning, controllable fitting, ensemble graph generation."
4594,1,  The choice to have one model seems especially peculiar given the authors say they couldn't get one set of weights that works for both their classification and regression tasks
4595,1," The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency."
4596,1, \n\n\n\nPros - \n* This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets.
4597,1,  It could use a couple more passes with an editor.
4598,1, Layers alternate between convolutional and sigmoidal.
4599,1,\n \n \nWhat is the relation between \\rho_jit and q_it ?
4600,1,\n\nOriginality:\n\nThe proposed contribution is original. 
4601,1,"""Overall I like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image, super-resolution, etc."
4602,1," From my understanding, the idea of subgoal learning boils down to a non-parametric (or kernel) regression where each state is mapped to a subgoal based on its closeness to different states in the expert's demonstration."
4603,1,\nThey present results on MNIST aiming to demonstrate that using PIBs improves generalization and training speed.
4604,1, Exp1 seems to indicate that the new method does not converge to the correct solution.
4605,1,\n- In caption of Fig. 5 25K should be 250K
4606,1,"  The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality."
4607,1,  What hyper-parameters are used.
4608,1,"\n2) The experiments are restricted to a single dataset - MNIST.  The authors mention that \u201cthe test accuracy obtained by following the above procedure is of 0.982, against a test accuracy of 0.971 for the real CNN\u201d - these are very poor accuracies for MNIST. So even the MNIST results do not seem convincing."
4609,1, This seems to mean that we should not expect for q -> p when K increases?
4610,1,"\n\n----\nAfter reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper."
4611,1, The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms.
4612,1,"\n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove"
4613,1," however I recommend the rejection of this paper."""
4614,1,\n\nSignificance \n\nThe approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models.
4615,1, Are you saying the encoder produces it using a fully-connected layer?
4616,1," \n\nThe solution proposed in this work is to have past activations decay exponentially, to reduce this problem."
4617,1,\n2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression).
4618,1, So what's the authors' contribution?
4619,1,"\n\nThe authors' findings are intuitively obvious, and certainly not surprising."
4620,1,"\n\n3. It is not clear from the text whether the setting is already considered in Brutzkus and Globerson, 2017."
4621,1," However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck?"
4622,1," e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n"""
4623,1, Superior performance to recent baselines (e.g. EWC) is reported in several cases.
4624,1,\n\nThe paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF.
4625,1," Also, corrupting actions with noise in the replay buffer is not simulating correctly what would happen in reality."
4626,1," I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner."
4627,1," Furthermore, images with even higher resolutions, e.g. 512x512, which is quite common in ImageNet, are difficult to synthesizes using current techniques."
4628,1,"\n\nIt seems like a good strategy is to subsample, perform Hadamard rotation, then quantise."
4629,1," Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units."
4630,1, but it is significant given the lack of theory and understanding of deep learning.
4631,1, The authors provide both theoretical and experimental validation of their idea.
4632,1," In their generative approach they say they find outputs by \""nearest neighbor search.\"""
4633,1,".\n\nSecond, the navigation task used needs to be better justified."
4634,1,\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do.
4635,1, It is confusing why permuting these filters make sense.
4636,1," You cite Nocedal & Wright, but could you please provide a page number (or at least a chapter)?"
4637,1," Secondly, more datasets including imagenet needs to be tested."
4638,1," It appears to be an overstatement to claim that the approach \""nearly-optimally\"" transports one distribution to the other (Cf e.g. Conclusion)."
4639,1, \n- Fig 1: which model is used to generate the conditional sample? 
4640,1," Therefore, I am currently leaning towards rejecting this paper."
4641,1, It seems to me the variances \\nu^2_i shall be directly estimated from \\hat{z} as is.
4642,1," The paper proposes a \""Q-masking\"" strategy that reduces the action space according to constraints or prior knowledge."
4643,1,"\n     - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2=5?"
4644,1," Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? "
4645,1,\n\n- Numerical study shows some promise of the proposed method.
4646,1, It motivates the method by examining the form of the log density ratio in the continuous and discrete cases.
4647,1,  Why did the authors make this choice?
4648,1,"\n\nIn section 2.1, the authors claim \""Spectral techniques are non-parametric in nature\""; this is wrong again."
4649,1," While I think that this experiment is well done, it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of tasks."
4650,1,"\""On Valid Optimal Assignment Kernels and Applications to Graph Classification\"", NIPS 2016.\"
4651,1, More datasets should be tested for evaluation.
4652,1," The numerical experiments show a significant improvement in accuracy of the approach."""
4653,1,"\n\nThere are 5 terms in the proposed objective function. There are also several other parameters associated with them: for example, the label temperature of z_2\u2019\u2019 and and parameter alpha in the second last term etc."
4654,1," I would also like to see a discussion (and perhaps experimental comparison) to standard preprocessing techniques, such as PCA-whitening."
4655,1, They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack
4656,1,"  Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems."
4657,1,"\nThe heuristic reward is proportional to the cosine distance between the learner and expert \u201csubgoals\""\n\n   Rh = B  <   Wv LearnerDirectionInStateS,  "
4658,1,"\n\n2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing."
4659,1," The first three results are on small datasets/tasks, O(10) feature dimensions, and number of\n  tasks and O(1000) images; (i) distinguish two MNIST digits, (ii) 10 UCI tasks with feature sizes\n  4--30 and number of classes 2--10, (iii) 50 different character recognition on Omniglot dataset."
4660,1,What would happen if Y is random or the activation is ReLU?
4661,1, (Finn et al.)\n- Prototypical Networks for Few-shot Learning (Snell et al.)\n- Matching Networks for One-shot Learning (Vinyals et al.)
4662,1, The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training.
4663,1," In general, questioning the reliability of the visualization techniques is interesting."
4664,1,"\n\n- It's odd to write \""we do not suggest a specific neural network architecture for the\nmiddle layers, one should seelect whichever architecture that is appropriate for the domain at\nhand."
4665,1,\n\n- The extrapolation of the value function approximator can also contribute to why the limited horizon MC method can see beyond its horizon in a sparse reward setting.
4666,1," Journal of Memory and Language, 59(4), 434-446.)."
4667,1,"""The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space."
4668,1,"""Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks."
4669,1,"\n\nCons\n\nDoes not provide new theory but combines existing ideas in a new manner."""
4670,1," Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000."
4671,1, The experiments offer no empirical comparison.
4672,1,\n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?
4673,1," With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger."
4674,1, I found the paper relatively creative and generally well-founded and well-argued.
4675,1, Did you run any statistical significance test on the evaluation results?
4676,1,\n\nSome more detailed comments:\n\n- The argument for evaluating visual realism never quite gels and is not convincing.
4677,1,\n\nThis is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea.
4678,1," The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention."
4679,1,"\n\nThe first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights."
4680,1, \n\nThe experiments are also not enough.
4681,1, It is difficult to understand the benefit of this technique if no other baseline is benchmarked.
4682,1, I have a few things listed in the weaknesses section that I found unclear or think would make for a stronger submission.
4683,1,. The node embeddings are then projected into a 2-dimensional space by PCA.
4684,1," The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer."
4685,1,. The word-pair was generated by the Standford dependency parser
4686,1," \n(2) The authors state \u201cIn back propagation, the gradient from z2 is kept from propagating to h\u201d.  This makes the learning process quite arbitrary under the objective in eq.(14). "
4687,1, This should be added.
4688,1," Specifically, the paper lacks of justification on why adjusting the learning rate based on the class labels are crucial to improve training FCNs, more specifically, how does it help resolving the exploding and vanishing gradient problems?"
4689,1,  It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix.
4690,1,"\nCons: no evidence that this is a \""break-through\"" idea"
4691,1,"  Sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselines,"
4692,1,"  If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative."
4693,1, It would strengthen the paper significantly if the experiments presented on WSJ in the first section were also conducted on the \u201cinternal\u201d task with more data.
4694,1," Different from typical auto-encoders, this work does not require another reconstruction network, but instead uses the \""derivative\""."
4695,1, \nPage 4: \n-\tStep 3 of the algorithm is not clear:\no\tHow exactly does HDDA model the data (formally) and how does it estimate the parameters?
4696,1," \n- The claim \""The bag of objects model clusters these group the best\"" is not supported by any evidence or metric."
4697,1," The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states."
4698,1,  The authors evaluate their proposed methods on one toy problem and two real-world problems.
4699,1,"Hence, the title seems a bit misleading."
4700,1," The use non-parametric approaches to the action-value function go back to at least [1] (and probably much further). So the algorithms themselves are not particularly novel, and are limited to nearly-deterministic domains with either single sparse rewards (success or failure rewards) or introducing extra hyper-parameters per task."
4701,1,"  Additionally, the images do not seem to add very much to the classification."
4702,1,\nIs this a standard assumption in the literature?
4703,1, What will be the total required training time to reach the same performance compared with single branch model with the same parameter budget?
4704,1, There is no indications on the extra computations required for handling this modification compared to standard training.
4705,1, This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur).
4706,1,"\n\n4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size?"
4707,1,\n\nTable 4 is unclear.
4708,1,"""The main result specifies a (trigger) strategy (CCC) and corresponding algorithm that leads to an efficient outcome in social dilemmas, the theoretical basis of which is provided by theorem 1."
4709,1,  This is achieved via an attention mechanism.
4710,1,"\n\nTypos etc.:\n\n* \u201clearn a particular series intermediate\u201d missing \u201cof\u201d.[[CNT], [CLA-NEU], [CRT], [MIN]]\n\n* \u201cTo do so, we generate on sequence y1:T\u201d s/on/a/, I think?"
4711,1,\n(4). What is Theorem 2 trying to convey?
4712,1, it does not prove its effectiveness in general or its elimination of the gradient vanishing problem.
4713,1, \n\nThe work itself is interesting and can provide useful alternatives for the distribution over the latent space.
4714,1, the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens. 
4715,1," that addresses three important problems simultaneously: \n(a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT)."
4716,1, This stochastic version obviously requires a step size; so it would have been proper to state the stochastic version of the algorithm instead of the batch algorithm in Algorithm 1.
4717,1,"\n\nOverall, while I like the and think the goal is good,"
4718,1,\n\n* Technical Novelty\nOne main limitation of the paper is the lack of technical contribution.
4719,1,  This would help to address the contribution of Q-masking vs. simply abstracting the action space.
4720,1," Would AE-k beat NATAC with a different dimensionality of latent space and k?"""
4721,1, Addressing each and every concern is quite important:\n\n1) Speed.
4722,1,"\n\nThe distance based loss is novel, and significantly different from prior work."
4723,1,  How do you\n  explain that it converges to a an objective value that is so much worse?
4724,1,\n\nCONS\n- Some of the figures are hard to read (in particular Fig 1 & 2 left) and would benefit from a better layout.
4725,1," \n\nThe conclusion is speculative:\n\""Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it\nis key to suppress this subspace of shared positive directions, which can possibly be done through\nregularization of the objective function."
4726,1, Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains
4727,1, I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates.
4728,1, In this paper there is no generative model for the data and the\ndata obtained is not actually sampled from the model.
4729,1,"""Overview:\nThis paper proposes an approach to curriculum learning, where subsets of examples to train on are chosen during the training process."
4730,1, (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.)
4731,1,  but I have some misgivings about accepting it in its current state.
4732,1,\n\nThe idea is interesting and trendy.
4733,1,\n\npros\n- the paper is written in a clear and concise manner
4734,1,"  How much of the ideas offered in this paper would then generalize to non-resnet settings?\n\n"""
4735,1," Furthermore, the work is not compared to the appropriate baselines."
4736,1," The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel"
4737,1," It seems you are scaling to mini-batrch gradient to be in expectation equal to the full gradient (not normalized by N), e.g. it scales ~N."
4738,1, Does the KL term help prevent overfitting at some stage?
4739,1," The authors also introduces \""Ensemble voting\"" facilitate exploitation"
4740,1," Here, the generalization error is measured instead, which is heavily influenced by regularization."
4741,1,  I think this should have been one of the baselines to compare to for that reason.
4742,1,"\n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l}"
4743,1, \n\nCons:\n- I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others.
4744,1,"""This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185)."
4745,1,  I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness.
4746,1, The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.
4747,1," Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods."
4748,1, What is the problem the authors are trying to solve with AESMC (over existing methods)? 
4749,1,"\n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point."
4750,1,\n\nI think the idea is interesting and novel..
4751,1,"\n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory."
4752,1,"""The paper proposes and evaluates a method to make neural networks for image recognition color invariant."
4753,1, None of these works have been compared with.\n   b. All the baseline methods use 8 bits per value.
4754,1,\n\nThe authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539)
4755,1, it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights.
4756,1, Figure 2 is sufficient to illustrate the model to readers familiar with the literature.
4757,1," Specifically, they observe that:\n\n(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet."
4758,1," There are more useful approaches to this task, such as the quality framework [*]."
4759,1, Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly.
4760,1,How much longer does it takes to train the model with the ISTA based constraint
4761,1,\n\n+ The proposed approach has a very strong relations to the recently proposed regularized OT (see [1] for a long list of regularizations) and more precisely to the euclidean regularization.
4762,1,"\n1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order."
4763,1, And I love the results. 
4764,1," It shouldn't be, but the authors should spend a little on this."
4765,1," This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision."
4766,1," DC-SBM, while a generative model, inherently can only capture first order random walks with target degree biases, and generally over-fits into degree sequences."
4767,1," Specifically, the authors propose to learn an autoencoder model, where the encoder translates image data into the lower dimensional subspace of semantic representation (word-to-vec representation of image classes), and the decoder translates semantic representation back to the original input space."
4768,1," The authors claim to improve the state-of-the-art, but fail to mention and compare with the state-of-the-art, such as [1]."
4769,1,"\n\nThe proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state."
4770,1," DAuto does seem to offer more boost in domain pairs that are less similar. """
4771,1,"Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling?"
4772,1," The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications."
4773,1, I recommend a \u201cSOFT\u201d REJECT.
4774,1, I will stress though that the statement about Newton in the paper is not justified. Newton method does not converge globally with linear rate. Cubic regularisation is needed for global convergence. Local rate is quadratic.
4775,1," For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient? "
4776,1," This paper shows that these representations can be used as exemplars by test images, in the same vein as bag of words used word exemplars to create the bag of words of unseen images."
4777,1,\n- the labels selected by the authors for the icu example are to forecast the next 15 minutes and whether a critical value is reached.
4778,1, Why is this the case?
4779,1,"\n \n\u201cit uses \\epsilon-greedy as a policy, \u2026\u201d Do you mean exploration policy?"
4780,1, Why not\n  just using better enough basic English and the text of the target user?
4781,1, (How does it relate to graphs?
4782,1,"  \n\nPage 2: calling additional instances of the same person \u201ccounterfactual observations\u201d didn\u2019t seem consistent with the usual definition of that term\u2026 maybe I am just missing the semantic link here, but this isn't how we usually use the term counterfactual in my corner of the field."
4783,1,"\n[5] Hausknecht, M., Lehman, J., Miikkulainen, R., & Stone, P. (2014). A neuroevolution approach to general atari game playing."
4784,1," The proposed approach is evaluated on few shot learning tasks, on omniglot and timit."
4785,1, My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector.
4786,1, But I have some concerns about this paper:\n\n- The derivations of the paper are unclear.
4787,1,\n\nIt will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearby.
4788,1,"""ORIGINALITY & SIGNIFICANCE\n\nThe authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network. "
4789,1, The authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the parameters.
4790,1,. Authors formulate the active learning problem as core-set selection and present a novel strategy.
4791,1,\n\nSignificance: Moderate.
4792,1, Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning.
4793,1,"  For example, there is a lot of math, but in Section 4.5 the authors seem to hedge and say \""We give an intuitive explanation ... and leave the rigorous analysis to future works.\""  Please clarify.\n\n"""
4794,1, It is quite unclear how one could generalize the Theorem 1 to arbitrary activation functions phi given the crucial use of the homogeneity of the ReLU at the beginning of p.4.
4795,1,"""Paper is well written and clearly explained."
4796,1," Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too."
4797,1," In fact, Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping."
4798,1,"""The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming."
4799,1,"\n5) I would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale, as this would have practical implications.."
4800,1, I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract.
4801,1,"\n \n\u201c,which we propose in a later part of this paper\u201d -> which we propose in this paper"
4802,1, \n\nOriginality: This is an original approach.
4803,1," I'd highly recommend the authors to cut a third of their text, it would help focus the paper on the actual message: pushing their new algorithm."
4804,1, First the lambda return definition lends itself to online approximations that achieve a fully incremental online form with linear computation and nearly as good performance of the off-line version.
4805,1, It allows the subgoal to adjust the value \nof the underlying model?
4806,1,"\n\n---\n\nReferences \n\nCai, J., Shin, R., & Song, D. (2017)."
4807,1,"  For example, analyzing whether the latent stochastic variables may shown to actually help with generalization of composition of primitive commands. \n\n """
4808,1,"\n\n3. The current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques (e.g., Zoph & Le, ICLR 2017)."
4809,1,\n3. It is unclear how the proposed method outperforms other methods based on fine-tuning.
4810,1,  Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets.
4811,1,"\n\nClarity\nThe paper is mostly clear,"
4812,1,No example sentences are provided for a qualitative comparisons.
4813,1, The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed.
4814,1,"\n\nThe performance comparison to the algorithm of Genevay et al. is somewhat limited: it is only on one particular problem, with three different hyperparameter settings."
4815,1,\nThe experimental setup is not convincing.
4816,1," Details below. \n\n1. As much as I enjoyed reading Section 3, it is very redundant."
4817,1,"\n\n- The DA results are shown with a linear classifier, for the comparison to the baselines to be fair, which I appreciate."
4818,1," However, I would not worry too much about that issue, as the same techniques presented in this paper apply to any weighted linear averaging algorithm."
4819,1,"""The paper proposes another training objective for training neural sequence-to-sequence models."
4820,1,\n\nThe description of the empirical setup could be more detailed.
4821,1,"\n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs."
4822,1," How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does \u2018F\u2019 refer to? There is dependency of \u2018F\u2019 on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update?"
4823,1, It introduces only some small technical novelty inspired by soft-k-means\nclustering that anyway seems to be effective.
4824,1,\n\nImportance:\nSomewhat lack of originality and poor experiments lead to low importance.
4825,1, The loss on \\pi_S should be made explicit.
4826,1, The authors argue that the reason for those diminishing returns is that the agent is actually learning a trivial wall following strategy that doesn't benefit from more maps.
4827,1,"In addition, attention context is computed for each layer, then, combined together as a single context."
4828,1,"\n\nThere are two half-papers here, one on parameter pruning and one on applying insights from random matrix theory to neural networks, but I don't see a strong connection between them. "
4829,1, One requires the strict positivity of the densities (to properly define conditionals).
4830,1," If an optimization algorithm falls onto these solutions, it will be hard to escape."
4831,1,"""Quality\n\nThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data."
4832,1," Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case."
4833,1,"  How this \""most interpretable\"" were selected?"
4834,1," Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation."
4835,1," Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used)."
4836,1," Moreover, what is the form of f_beta and how beta is optimized?"
4837,1,"If it is done by hyperparameter tuning with cross-validation, the training cost may be too high."
4838,1,"""This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks."
4839,1,"\nThus its Lipschitz constant is different, potentially equal to\nL_q = max_{k} \\| L_q^k \\|, \nwhere L_q^k is the Lipschitz constant of f_c-f_k."
4840,1," For visual information, authors use a pretrained CNN (with fine tuning)."
4841,1, The regret of concave player can be bounded using existing result for FTRL.
4842,1, I think this is the main contribution of this work.
4843,1,"  While the argument makes sense, it is not clear to me why one cannot simply index the original text."
4844,1,\n\n2) p 3 center -- this seems to be reinventing non-maximum suppression
4845,1," \n\nMinor comments:\n\nIn the introduction, for VAEs, it's not the case that f(X) matches the target distribution."
4846,1," \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n"""
4847,1,  What does it represent?
4848,1, I would have expected the same encoder-decoder architecture to have been used for all the methods considered.
4849,1, the approach is only tested in a small 7x7 grid and 2 agents and in a 10x10 grid with 4 agents.
4850,1, The author's choice of tasks seem somewhat artificial in that they impose time limits on otherwise unlimited domains in order to demonstrate experimental improvement.
4851,1," If it were novel, it would be an incremental development."
4852,1, it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods.
4853,1," \n\nQuality: No obvious mistakes in the proposed method, but has very low novelty (as most methods follows existing studies in especially for online kernel learning)."
4854,1,\n\nSignificance: The problems the authors consider is worth exploring further
4855,1,  The input to each model is a sparse sum of the outputs of modules in the previous set.
4856,1, \n\n[Pros]\n- The overall direction toward more flexible/scalable memory is an important research direction in RL.
4857,1," As in, what should be the ground-truth answer against which the answers should such questions be evaluated."
4858,1,\n\n* This is quite an interesting paper with a sensible goal.
4859,1," However the ICU example has some pitfalls which need to be addressed."""
4860,1, This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum.
4861,1," The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well)."
4862,1," Crucially, the teacher model will also rely on these learned features."
4863,1," As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST)."
4864,1," This might give a better sense of: (1) how difficult the task is, (2) how much variation there is in the real data from patient to patient, and (3) how much variation we see in the synthetic time series."
4865,1,". \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank,"
4866,1,\n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks.
4867,1,"\n- L_h f = f(h^{-1}), p. 4\n- \""coordiantes\"", p. 5"""
4868,1," In this paper, bipolar activations are used to train very deep stacked RNN. "
4869,1," The proposed architecture could identify the \u2018key\u2019 states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory."
4870,1, in Vendrov work they defined a partial ordering.
4871,1," Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed."
4872,1,"\n\nSoundness: As far as I can tell, the work is sound."
4873,1,\n\n\nPros:\n- The model achieves SOTA on SQuAD among published papers.
4874,1," The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem."
4875,1,"\n\n=Minor Comments=\n* \""We also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optima\""\nGenerally, people add noise to the gradients, not the values of the parameters."
4876,1,  The second problem of what tasks to evaluate on is a general problem with comparing RNNs.
4877,1,\na) It is known that unsupervised clustering methods can achieve 0.97 accuracy for MNIST.
4878,1, More details in the comments below.
4879,1," The paper demonstrates that it is possible to obtain very deep plain networks (without skip connections) with improved performance  through the use of constrained optimization that gradually removes skip connections, but the value of this demonstration is unclear because a) consistent improvements over past work or the \\lambda=0 case were not found,"
4880,1, However it looks like the task graph itself is still simple and has limited representation power.
4881,1,  Why not include Bayesian linear regression and\n  Gaussian process regression as baselines?
4882,1,"""This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses."
4883,1,"\n\nSecond, Cambridge dialogs: the dataset's metric is not accuracy-based (while the paper reports accuracy), so I assume some preprocessing and altering have been done on the dataset."
4884,1, It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks.
4885,1," Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well."""
4886,1,"  While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy."
4887,1, One option would be to run user studies where humans judge the quality of the matches.
4888,1,"\n\nThe paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional)."
4889,1,\n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9) 
4890,1,"\n\nGILAD-BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER, K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In Proceedings of The 33rd International Conference on Machine Learning (2016), pp. 201\u2013210."
4891,1, At each iteration the teacher generates  examples based on the students current concept.
4892,1," The authors claim that the previous SotA result was carefully fine-tuned with a low learning rate, and that in this paper they used only default fine-tuning with a high learning rate."
4893,1,"\nAppendix B: Heatmaps through the different stages of perturbation should be normalized using a common factor, not individually, in order to better reflect the change in the explanation[[CNT], [CNT], [DIS], [MIN]]\n\nConclusion:\nThe paper follows an interesting approach,"
4894,1,\n2.\tThe dataset is motivated as consisting of four challenges (described in the summary above) that do not exist in the existing RC datasets.
4895,1,"   As far as I can tell, this term is undefined."
4896,1,\n\nMy concern is that one-bit system is already complicated to implement.
4897,1," Beyond this, the paper does not provide any futher original idea."
4898,1,"\nIn terms of experiments, it is shown that the system is more effective than others but not so much *how* it achieves this efficiency."
4899,1," The (known) causal graph is only used to model the dependencies of the labels, which the authors call the \u201cCausal Controller\u201d."
4900,1,\n  - extensive experimental comparison and good experimental results.
4901,1," However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs."
4902,1,". Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions"
4903,1, I find this of limited usefulness.
4904,1, The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.
4905,1,\n\nThe authors may want to consider citing
4906,1, The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature. 
4907,1,"\n-\tFigure 3, experiments of Double audio/video clean conditions: I cannot understand why they are improved from single audio/video clean conditions.Need some explanations."
4908,1, The pairwise brain maps would support the interpretation of the generated data.
4909,1," \n\nIn section 2, it is not obvious to me that the regularizer (4) would not be ignored in absence of regularization on the output matrix."
4910,1," I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required."
4911,1, In Table 2 the baselines have a lower capacity.
4912,1," \n\nPhysical processes in Machine learning have been studied from the perspective of Gaussian processes. Just to mention a couple of references \u201cLinear latent force models using Gaussian processes\u201d and \""Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations"
4913,1, The covariate-dependent\nembeddings are diagonal scalings of the shared embedding.
4914,1," Specifically, the authors say that for each image they produce 5 additional \""virtual\"" data points, but when multiple methods are combined, does this mean 5 from each method? Or 5 overall? If it's the former, the increased performance may merely be attributed to using more data."
4915,1, \n\n\nMore comments:\nThe main problem with this paper is that the proposed systems is designed for a human interaction setting but no such experiment is done or presented.
4916,1,"""This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). "
4917,1,"""The paper presents an extensive framework for complex-valued neural networks."
4918,1,"""The authors propose a defense against attacks on the security of one-class SVM based anonaly detectors."
4919,1, This process is applied iteratively to different groups of layers.
4920,1, PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7. 
4921,1,"  It may very well end up that your very simple model does as well as a well estimated model, but that is something to establish in your paper, not assume."
4922,1, there is absolutely no detail on how do they train them.
4923,1," It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end."
4924,1,\n\nThe paper does not say anything about the convergence of the full algorithm.
4925,1," Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications."
4926,1," The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting."
4927,1," However, since the properties are measured based on a set of random walks it is still difficult to interpret the impact on the generated graphs (since an arbitrary node in the final graph will have some structure determined from each of the regions)."
4928,1," \n\nBelow are the points that I'm particularly confused about:\n\n1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me"
4929,1,"\n\nThe task of reducing computation by skipping RNN inputs is interesting, and the proposed method is novel, interesting, and clearly explained."
4930,1,"""The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report."
4931,1, RMN reduces the complexity to linear time for the bAbi dataset.
4932,1, This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).
4933,1, So the use of cross-task transfer performance and the task clustering approach can only capture positive correlations between tasks but ignore the negative task relations which are also important to the sharing among tasks in multi-task learning.
4934,1,  Set of experiments does not prove claims of the paper.
4935,1, The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.
4936,1,"""This paper focuses on accelerating RNN by applying the method from Blelloch (1990)."
4937,1,\nThe paper benefits from such a relationship and derives an actor-critic algorithm.
4938,1, It would be interesting to think about what further might be done here.
4939,1,"\n\nFinally it's worth noting that discarding patients with missing data is unlikely to be innocuous for ICU applications; data are quite often not missing at random (e.g., a patient going into a seizure may dislocate a sensor)."
4940,1," How does this particular model generalize to classes not seen during training?"""
4941,1, but lacks some important experimental comparisons.
4942,1," In particular, in Table 2, where the different types of augmentation are compared against each other, we observe similar results between augmenting only in the image feature space versus augmenting only in the semantic feature space (ie we observe that \""FeatG\"" performs similarly as \""SemG\"" and as \""SemN\"")."
4943,1, The authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from Wikipedia and IMDB.
4944,1, \nc)\tWould this approach be able to generate a lattice?
4945,1," \n\nGranted, being able to infer hidden states is of course an important problem,"
4946,1, It belongs to the recent family of papers based on GANs
4947,1," \nSignificance: medium-high\n\nReferences:\n[1] https://arxiv.org/pdf/1602.02867.pdf\n[2] https://arxiv.org/pdf/1612.08810.pdf\n[3] https://arxiv.org/pdf/1707.03497.pdf"""
4948,1,\n\nI don't completely understand the objective.
4949,1," However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step."
4950,1,"\n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines."
4951,1,"\n* Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear."
4952,1,"\nAuthors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines."
4953,1," Also, since Genevay et al. propose using SAG for their algorithm, it seems strange to use plain SGD; how would the results compare if you used SAG (or SAGA/etc) for both algorithms?"
4954,1," For the most paper, the paper is clearly-written, with each design decision justified and rigorously specified."
4955,1,  This seems to complicate the experimental setting.
4956,1," Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples."
4957,1," but the paper is not easy to read, the presentation is sparse \nand the bit and pieces of information do not allow to derive strong final conclusions.\n\n\n"""
4958,1,\n\nDespite of this limitation I think the paper's idea is OK and the result is worth to be published
4959,1," In some cases it is good to outline a powerful and generic framework (like the authors did here with defining \""teaching\"" in a very broad sense, including selecting good loss functions and hypothesis spaces) and then explain that the current work focuses on one aspect (selecting training data points)."
4960,1, The idea being to construct a superposition of Taylor approximations of the individual monomials.
4961,1, The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN.
4962,1,"\nEven the footnote has been copied & pasted: \u00ab\u00a0For convenience we assume that the system is k-observable: that is, the distribution of all future observations\nis determined by the distribution of the next k observations. (Note: not by the next k observations\nthemselves.)"
4963,1,\n\nHigh-level comments:\n\n- It's not clear why the optimization is done in 3 separate steps.
4964,1,".\nCons.\n- in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al),"
4965,1,  This makes the high level learning strategy more efficient because it does not have to explore these possibilities (Q-masking).
4966,1, This should be made more\nclear in the paper.
4967,1," However, in order to solidify the experimental validation, the authors could consider a broader range of experimental evaluations. "
4968,1,. It might be only complex when the number of dependency layer is large.
4969,1,\n\nI thought the paper was clear and well-motivated.
4970,1, This effect is not combated by batch normalization.
4971,1," The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. \n"""
4972,1, \n\nDoes selecting a projection based on compactness remove the randomness?
4973,1,\n\nThe experiments have serious issues.
4974,1,"\u201d  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across."
4975,1," The\nauthors basically refer to Appendix 6.4 for the case in which the weight decay\npenalty is not zero.[[CNT], [null], [DIS], [MIN]] The details in that Appendix are almost none, they just\nsay \""it is thus possible to derive the prior...\"".[[CNT], [SUB-NEG], [DIS], [MIN]] \n\nThe results in Table 2 are a bit confusing."
4976,1,"""Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games."
4977,1,"\n- It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups)."
4978,1," Also, could authors please throw some light on why this might be happening?"
4979,1," In particular, what makes cost-based optimization inapplicable?"
4980,1, This is demonstrated in comparison to weight normalization in Figure 4.
4981,1,The resulting log-polar image is analyzed by a conventional CNN.
4982,1,\n\nClarity: The paper is well-written.
4983,1," To be clear, I think that even if the proposed approach were to be slower than the state of the art it would still be very interesting."
4984,1,"Unless I have missed something completely, I did not see any novel idea proposed in this paper."
4985,1, \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n
4986,1,"\n4. Is the error in Table 2 averaged over multiple runs? If yes, how many?"
4987,1,\n-- the wealth of literature on combining autoencoders (or autoencoder-like\nstructures such as ALI/BiGAN) and GANs merits at least passing mention.
4988,1," Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space."
4989,1, An experiment illustrating this effect could be illuminating.
4990,1,"""This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning."
4991,1," Or conversely, GPO without high-reward filtering during crossover."
4992,1,"The clustering results of rejected\nexamples are still far from the ground truth, and comparing the result with\na total unsupervised K-means is a kind of unreasonable.\n"""
4993,1, Is its logit set to zero or -\\infty ?
4994,1, I believe this requires clarification in the manuscript itself.
4995,1," First, there is not much innovation in the model architecture."
4996,1, This paper establishes an interesting connection between least squares population loss and Hermite polynomials.
4997,1, Authors are suggested to involve more datasets to validate the effectiveness of the proposed method.
4998,1, \n\nWhat is the exploration strategy in the experiments?
4999,1, The sentiment that extending TS to larger spaces hasn't been fully explored is definitely valid... but this line of work should certainly be mentioned in the 4th paragraph. 
5000,1, It also is a bit simplistic (I had expected the agents to at least need to learn to move the customer to some square rather than get reward and move to the next job from just getting to the customer\u2019s square).
5001,1,"References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\n"
5002,1, the choice of baselines is convincing.
5003,1," This itself can be a small, nice contribution."
5004,1," Besides, the idea of using a rotation operation in recurrent networks has been explored before [3]."
5005,1,The experimental results show the benefits of this approach.
5006,1,\n\nMinor Asks:\n(1) Clarification on how the error rates are defined.
5007,1, How is k chosen? Is it fixed or dynamic?
5008,1,"\n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose \u201cstochastic lazy attributes\u201d, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid."
5009,1," As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic)."
5010,1,"   \n\n\n- \""Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate..."
5011,1," The proposed algorithm is described in excellent detail, which is essential to reproducibility\n3)"
5012,1," If we treat the value of S_T as zero or consider gamma on the transition into the time-out state as zero, then in cost to goal problems the agent will learn that these states are good and will seek them out leading to suboptimal behavior."
5013,1,"The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods."
5014,1,"  is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?"
5015,1," This study is (unfortunately) typical in that it focuses on and provides detail of the technical modeling issues, but ignores the medical applicability of the model and results."
5016,1," In the data collection stage, how were the points lists generated from pen strokes? "
5017,1," I would also relate your work to distillation/model compression which tries to get one network to behave like another. On that point, is it important to train the forward and backward network jointly or could the backward network be pre-trained?"
5018,1," The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach."
5019,1," Then, based on an arbitrary prefix of epochs y_{1:t}, a model can be learned to predict y_T."
5020,1,"\n\nIt is interesting how strong the denoising effect is, as simply a byproduct of the adversarial regularization."
5021,1, I do not believe it should be accepted for the following reasons.
5022,1,"The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution."
5023,1, My list of concerns are the following: \n\n    The authors state as their first contribution the presentation of a novel dataset.
5024,1, \n\nOriginality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new.
5025,1," The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks."
5026,1,"Variances should be added, and preferably more than 3 initialisations used."
5027,1," Could predicting presence or absence separately be a way to encourage sparsity, since absence of a unit is already representable as a count of zero?"
5028,1,I am wondering this method is trying to mixture several samples into one to generate adversary samples.
5029,1, The challenges in the proposed dataset as outlined in the paper seem worth pushing for.
5030,1,"  It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve."
5031,1," If what I think is correct, I think it would be important to show this."
5032,1,\n2) There are two flaws in the experimental validation:.
5033,1,\n- The proposed method and problem setting are not well-justified.
5034,1,"\n- For the simulated results, the comparisons seem unfair since the validation error is different"
5035,1," \n\nIn order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e. after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy)."
5036,1,"\n\nY. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432, 2013.\n\n"""
5037,1," Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes"
5038,1," It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function. "
5039,1,". To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables."
5040,1,\u201d Didn't see how the initial (well constant here) step size was tuned?
5041,1," but needs more work, and should provide clear and fair comparisons."
5042,1, \n\nBut let\u2019s assume that at some point they can be used as the authors propose.
5043,1," In particular, the latter problem is well posed and has good properties (see, e.g., Lim, Comon. Nonengative approximations of nonnegative tensors (2009))."
5044,1,"\n- Appendix D1: \""In this section is presented\"" : sounds odd\"
5045,1," This is a limitation, as the model space is not as flexible as one would desire in a discovery task."
5046,1, It\u2019s a bit suspicious that it doesn\u2019t achieve 20+ in Pong.
5047,1, \n\nIssues that I wish were addressed in the paper: \na)\tHow is the method learning a generator from a single graph?
5048,1," The number of layer as a complexity is not appropriate , as we need to take in account many parameters:  the pooling or the striding for the resolution, the presence or the absence of residual connections (for content preservation), the number of feature maps. More experimentation is needed."
5049,1,\n- connection to Rand-Walk (Aurora 2016) not stated precisely enough
5050,1," Statistics has for decades successfully used criteria for model selection, so what is this example supposed to proof (to whom?).[[CNT], [null], [QSN], [GEN]]\n\nChapter 4 takes the work of Mandt et al as a starting point to understand how SGD with constant step size effectively can be thought of as gradient descent with noise, the amplitude of which is controlled by the step size and the mini-batch size."
5051,1," The poor language and presentation does not help in clearing that, as it does not help in general. """
5052,1, I assume that the same is true for the evaluations of F on line 4 of Algorithm 1?
5053,1,"""Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning."
5054,1, This is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical arguments.
5055,1,"""The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets."
5056,1, It also happens a few times in Table 5. 
5057,1,"\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured."
5058,1," As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense,"
5059,1,"  \n\nPros:\n- The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights."
5060,1, both method and experiments are presented well. 
5061,1," The main limitation of the proposed method of analysis I think is that the two parts of the inference gap are not really separable: Because the VAE encoder is trained jointly with the decoder, the different limitations of the encoder and decoder all interact."
5062,1," 6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method."
5063,1," \n\nThe writing of the paper is also very unclear, with several repetitions and many typos e.g.:\n\n'we first introduce you a'\n'architexture'\n'future work remain to'\n'it self'\n\nI believe there is a lot of potential in the approach(es) presented in the paper."
5064,1, This new layer is referred to as \u2018pixel deconvolution layer\u2019 and it is demonstrated on two tasks of semantic segmentation and face generation.
5065,1,". Furthermore, it becomes clear, that without CNN structure no really good performance is achieved neither on CIFAR nor on ImageNet "
5066,1," This assumption seems reasonable if a central station broadcasts all agents' positions and customers are only allowed to stop vehicles in the street, without ever contacting the central station; otherwise if agents order vehicles in advance (e.g., by calling or using an app) the central station should be able to communicate customers locations too."
5067,1,"""Paper summary:\nExisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks."
5068,1," If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should."
5069,1," The title is misleading, this may be the direction where the authors of the submission want to go, but the title  \u201c.. with human interactions\u201d is clearly misleading.[[CNT], [CNT], [CRT], [MAJ]] \u201cModel of human interactions\u201d may be more appropriate.[[CNT], [CNT], [CRT], [MAJ]] \n\nThe technical idea of this paper is to introduce a separate score in the GAN training process."
5070,1, Existing methods are then compared based on how accurately they can produce the target action sequence based on the command input sequence.
5071,1, The origin of the last equality constraints on the sums of \\alpha_A and \\alpha_B is also unclear to me.
5072,1,"""This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs)."
5073,1, These are just two complementary lines of work.
5074,1, the paper lacks clarity and sufficient details.
5075,1," For instance, in the introduction, it is stated that previous attractor network type of models (which are also recurrent networks) \u201c[...] require hand-crafted and fined tuned connectivity patterns, and the evidence of such specific 2D connectivity patterns has been largely absent."
5076,1, Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy?
5077,1,"It may take more than one day on 320-640 GPUs for training (I did not find enough details in this paper, but the training complexity will be no less than the in [1])."
5078,1,\n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b].
5079,1,"  Heuristics for distributing connections among windows/groups and a measure called \""scatter\"" are introduced to construct the connectivity masks, and evaluated experimentally on CIFAR-10 and -100, MNIST and Morse code symbols."
5080,1,"\n2. Does the result implies that we should make the decision boundary more flat, or curved but on different directions? And how to achieve that?"
5081,1," in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples)."
5082,1,"\n- Below eq. (1), I am not sure what the V in P_V refers to."
5083,1,\n\nPros:\nThe paper is well written and easy to follow.
5084,1, This straightforward extension is claimed to improve image quality and shown to improve performance on a previously introduced image caption ranking task.
5085,1,\n- the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training.
5086,1,  The excellent Figure 2 supports this point.
5087,1,"\n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models?"
5088,1,"\nFor evaluation data was split randomly in 80% train, 10% test and 10% validation."
5089,1,  What are some architectures that might be appropriate for different domains?
5090,1," For example, what does the *convergence rate* mean (what is the measurement for convergence)?"
5091,1,"\n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications."
5092,1, There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future.
5093,1," Snell et al use a soft kNN classification rule, typically used in standard metric learning work (e.g. NCA, MCML), over learned instance projections, i.e. distances are computed over the learned projections."
5094,1, \n\nOverall I think this is a good paper.
5095,1," However, the authors show this per-step evaluation in the Amazon Mechanical Turk, and predicted object position evaluations."
5096,1,"\nHowever five of the words the authors retain: bored, annoyed, love, optimistic, and pensive are not in fact found in the PANAS-X scale:\n\nReference: The PANAS-X Scale: https://wiki.aalto.fi/download/attachments/50102838/PANAS-X-scale_spec.pdf Also the longer version that the authors cited: \nhttps://www2.psychology.uiowa.edu/faculty/clark/panas-x.pdf"
5097,1,"\n\nOverall, I am inclined to accept this paper on the basis of its experimental results."
5098,1,\n\n5.1: What dataset/model was this experiment done on?
5099,1, Can the authors show that the procedure will always converge?
5100,1,\n\nAdvGAN is a simple and neat solution to for generating adversary samples.
5101,1, It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results.
5102,1," \n\nThe predictions in Section 3 appear to be very good, and it is nice to see the ablation study."
5103,1," However, I found the empirical results to be a little underwhelming."
5104,1,"\n\nI ignore the ReLU here, but I assume that is operates element-wise and just clips negative values?"
5105,1," \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature."
5106,1,"\n\nQuality:\n\nI am very concern about the authors stating on page 1 \""we sample from the\nposterior on the set of Q-functions\""."
5107,1,\n\nThe authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*.
5108,1, It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings.
5109,1, I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper.
5110,1,\n- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well
5111,1,"Indeed, it is basically an application of the orthogonalized alternating least squares method of Sharan and Valiant (2017) and more generally of the tensor decomposition ideas of numerous papers of Anandkumar and colleagues."
5112,1, The \u201cview\u201d in this paper is corresponding to actually sub-regions in the images
5113,1," As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically"
5114,1,"?\n        * comparison to GloVe on the entire corpus (not covariate specific)\n        * no reference for the metrics used (AP, BLESS, etc.?)"
5115,1," As argued in Section 1, the ability of \nobtaining signals token-wise looks beneficial at first, but it will actually\nbreak a global validity of syntax and other sentence-wise phenoma."
5116,1," There are a few ideas the paper discusses:\n\n(1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not \""worth it\"" until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree."
5117,1,"\n\nIn addition to my concerns about the experimental evaluation, I have concerns\nabout the general approach."
5118,1,"\n- Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST."
5119,1,"\n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? "
5120,1,\n\n=Clarity=\nThe overall setup and motivation is clear.
5121,1,"\nSpecifically, we compare the performance among the CommNet model, our\nMS-MARL model without explicit master state (e.g. the occupancy map of controlled agents in this\ncase), and our full model with an explicit occupancy map as a state to the master agent."
5122,1,\n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets.
5123,1," This would be very interesting to have arguments on why being better than the \""Dirac estimation\"" in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation)."
5124,1," Especially with natural images,  the spacial location and the scale should be critical. "
5125,1," If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3."
5126,1," Even though the proposed method works better, the prediction accuracies of S are still high."
5127,1, \n\nThe experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks.
5128,1," The genetic algorithm is a black-box optimization method, however, the proposed method has nothing to do with black-box optimization."
5129,1," It would have been interesting to study more kinds of such relationships, such as equality up to translation or rotation, to understand the limitation of such networks."
5130,1, In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688).
5131,1,"\n  This isn't necessarily a problem, it's fine for models to not do everything well,\n  but it's a stretch for the authors to claim that these results are a positive\n  aspect of the model."
5132,1,"\n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner."
5133,1, \n\nThis is either a very important paper or the analysis is incorrect but it's not my area of expertise.
5134,1,\n\nIf the authors add FSGM to the batch of experiments (especially section 4.1) and address some of the objections I will consider updating my score.
5135,1, But how can it describe the robustness?
5136,1,"  Similary, you say \""On the other hand, the ensemble model can explain the performance improvement easily."
5137,1,"\n\nMinor comments:\n1.) The paper mentions making predictions from \u201cincomplete\u201d input several times, but in all experiments, the input is an edge map, normal map, or low-resolution image."
5138,1," The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms."
5139,1," While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky)."
5140,1," Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only?"
5141,1," What is done here is quite different, but I think it would be worth discussing these relationships in the paper."
5142,1, Not easy to understand nor concrete.
5143,1,"\n\n-\tThe authors bring a good point on the limitations of the SVRT dataset \u2013 mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations, which make it difficult to quantify the effect of image variability on the task."
5144,1,  They do not really provide any substantial theoretical justification why these heuristics work in practice even though they observe it empirically.
5145,1,"\n\nBeyond that, I only have a list of very insignificant typos:\n-p3, end of S3, \""this term correspond to minimizing\""\n-p3, S4, \""to approximate Wasserstein-1 term\"" --> \""to approximate the Wasserstein-1 term\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n-Figure 1, caption \""which is similarly decoded to $\\mathbf{\\~x}$\"" ."
5146,1, Extra evaluations of different relaxations would be appreciated.
5147,1, What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)?
5148,1,"\n\nIn the experiments section, can you provide a citation for ADAM and explain how the parameters were selected?"
5149,1, however the technique is different since this submission uses PAC-Bayesian analysis.
5150,1,"  \n\nIf the authors stated goal is \""different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment\"" they should be aware that this is exactly what PANAS is designed to do - not to infer the latent emotional state of a person, except to the extent that their affect is positive or negative."
5151,1," Additionally, the authors present evaluation based on PSNR and SSIM on the overall predicted video, but not in a per-step paradigm."
5152,1, Could the authors comment on this potential limitation?
5153,1, The lambda term would then serve as an indicator to how much entropy is necessary.
5154,1," This paper offers a new conceptual setup to look at the problem and consolidates different views (successor repr, proto values, eigen decomposition) in a principled manner."
5155,1, \n\nPros:\nThe question of internal representation is interesting.
5156,1,"\u2028First, authors do not formulate what exactly is the problem statement for MARL."
5157,1,"\n- Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i})."
5158,1,"""The authors present a method to enable robust generation of adversarial visual\ninputs for image classification."
5159,1,\n\n8th line of 5.1 you mention Nesterov momentum method -> a precise reference and precise equation to lift ambiguities might be helpful.
5160,1, (The comment above about re-invention is the most charitable intepretation -- the worst case would be using these ideas without citation.)
5161,1,\n- Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image.
5162,1," Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity."
5163,1," Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4."
5164,1," To speed up the search, they focus on finding cells instead of an entire network."
5165,1, \n\nPros:\n- The paper is well-written and clear.
5166,1,"On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers."
5167,1,"  Thus, Bayesian evidence prefers minima that are both deep and broad."
5168,1, These details are useful for judging technical correctness.
5169,1, This is a very strong assumption.
5170,1,"\n    -- \""Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\"""
5171,1, What is the graph \nin the time series or among the multiple time series?
5172,1, The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme.
5173,1,"\n\nA few additional small remarks and questions:\n- \u00ab Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet."
5174,1, This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. 
5175,1,"\n\nThere is an explanation indicating that switching from SGD to TR causes an uphill movement (which I presume, is due to the trust region radius r being large); but statements such as - this will lead to climbing over to a wide minimum etc. are too strong; no evidence is given for this."
5176,1,"""This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy."
5177,1," For me, it is a clear accept."
5178,1,. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format.
5179,1," \n- Typos in this sentence: \u201cLastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well."
5180,1," but it is not clear if those results are something difficult to find with other existing standard ways,"
5181,1,\n\n- A lot of important references  touching on very similar ideas are missing.
5182,1," It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded."
5183,1,"\n\nWeakness:\n- Although the paper shows that skip RNN worked well, I found the appropriate baseline is lacking here."
5184,1,"\nThe evaluation used in the paper don't correlate well with human ratings see (SPICE paper), therefore trying to improve them marginally doesn't make a difference."
5185,1," The other issue is with the baseline solver, which also seems to be broken since their solution quality seems extremely poor."
5186,1, \n1. I am not very familiar with the literature of few shot learning.
5187,1,"\n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs."
5188,1,"  Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities."
5189,1," Consider calling k a time index instead of t_k a time interval (\""subject x experiences cause m occurs [sic] in a time interval t_k\"")\n- Line after eq 8: do you mean accuracy term?"
5190,1, I suggest the authors at least discuss the empirical over-fitting problem with respect to ordering.
5191,1,\n\nThis paper targets at a potentially very useful application of neural networks that can have real world impacts.
5192,1, As is done in the baselines usually.
5193,1,\n \nDo you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues?
5194,1,"In this case we would be excluding a large set (about half!) of the gradient values, and it didn\u2019t seem from the context in the paper that this would be desirable."
5195,1,"\n\nReferences:\n[1] Gomez, F. J., & Miikkulainen, R. (1999). Solving non-Markovian control tasks with neuroevolution."
5196,1,"""This submission does not fit ICLR."
5197,1," To accomplish this, they use a generator LSTM that takes in a sequence of random noise as well as a sequence of conditonal information and outputs a sequence."
5198,1,"""The paper is written well and clear."
5199,1," For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?"
5200,1,"\n-\tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)\u2026\n"""
5201,1,"  \n\nOverall, I liked the paper."
5202,1," A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems."
5203,1,\n\nThere are several things to like about this paper:\n- The problem of efficient exploration with deep RL is important and under-served by practical algorithms.
5204,1," \n\nThe practical implementation section losses some of this clear organization, and could certainly be clarified each part tied into Algorithm 1, and this was itself made less high-level. But these are minor gripes overall."
5205,1," However, the related work discussion is significantly lacking."
5206,1, It is in line with current trends in the research community and is a good fit for ICLR.
5207,1,\n\nCons:\n* The novelty of this paper is marginal.
5208,1," \n\nContribution: \nAs discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units."
5209,1," Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer."
5210,1,\n\nAuthors do not provide enough citations.
5211,1, So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a).
5212,1, The paper first relies on convolutional neural networks to extract image features.
5213,1," so I would expect the map to be computed on a small sample of source data, and then all source instances to be projected on target with the learned map."
5214,1,\n\nThe experiments range over 4 video datasets. 
5215,1,The author categories the existing works int four categories.
5216,1, The authors then experiment with a variety of TD and MC based deep learners to explore which methods are most robust to increases in difficulty along these dimensions.
5217,1," In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents."
5218,1, It was an interesting read.
5219,1,"\n\n-For the DA application, the considered datasets are classic but not really \""large scale\"", anyway this is a minor remark."
5220,1," The first advantage of the proposed model is short response time due to parallelism of non-sequential output generation, proved by experiments on the SQuAD dataset."
5221,1," For instance, since the reward has been designed arbitrarily, it could have been defined as giving a penalty for those missing customers that are at some distance of an agent."
5222,1, \n- Comparison to VPN on Atari is not much convincing.
5223,1," The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10."
5224,1,"\n\nIt may be that the methods in this paper can outperform previous ones -- that would be interesting,"
5225,1, In the case of Mirowski et al. this means that the LSTM has somehow learned to do general SLAM in a meta-learning sense.
5226,1, \n- The results are not very convincing.
5227,1," \n\nThe paper is easy to read,"
5228,1,  The results are interesting
5229,1, The design of the empirical evaluation doesn't address the analysis of the impact of this new formulation.
5230,1,"""This paper explores a new approach to optimal transport."
5231,1,"While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets."
5232,1,"\n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required."
5233,1," Its core benefit is its adaptiveness towards diverse opposing player strategies (e.g. selfish, prosocial, CCC) while maintaining maximum reward."
5234,1," The author also addressed this point, and I changed my scores accordingly. """
5235,1," \n\nI think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance,"
5236,1," but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win StarCraft competitions, meaning that, at least as the paper is currently framed, the critical evaluation metric would be showing that a defogger helps to win games."
5237,1, The final dataset is also a good size (36M search queries).
5238,1," This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words."
5239,1," However, it is well-known that spectral algorithm is not robust to model mis-specification."
5240,1," Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise."
5241,1,\n(c) generation of high quality samples even with higher perplexity on ground truth set.
5242,1, I'm not convinced the current incarnation is showing anything insightful or useful.
5243,1, It is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baseline.
5244,1," Furthermore, the use of these replacement tables means that even when the noise is natural, it\u2019s still kind of artificial."
5245,1, Also some curves in the appendix stop abruptly without visible explosions.
5246,1," To my knowledge, copy task could be solved easily for super long sequence through RNN model."
5247,1," The approach is compared to Isola et al\u2019s conditional adversarial networks, and unlike the conditional GAN, is able to produce a diverse set of outputs."
5248,1,"\n\nAlthough I believe there is work to be done in the current round of RL research using nearest neighbor policies, I don't believe this paper delves very far into pushing new ideas (even a simple adaptive distance metric could have provided some interesting results, nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains....),"
5249,1," If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix."
5250,1," It's well accepted that in any partially observed domain, inclusion of the latent variable(s) as a part of the agent's observation will result in a fully observed domain, less state-aliasing, more accurate value estimates, and better performance."
5251,1," I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case."
5252,1, Arrows seem to have some different meanings.
5253,1," Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM? "
5254,1,"""This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks."
5255,1,"\n\nIn this regard, I would have expected comparison with other state-of-the-art data augmentation techniques."
5256,1,\nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem.
5257,1,\n3) It is easy to envision a (full knowledge) attack where the attacker knows the architecture and parameters of the detector and then devises an attack to fool both the detector and the original classifier. 
5258,1," Furthermore, a clear description of your \u201cpull\u201d configuration (such as in Figure 1) i.e."
5259,1," The Background section introduces notation for POMDPs, never to be used again in the entirety of the paper, before launching into a paragraph about apoptosis in glial cells."
5260,1,\n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written.
5261,1, \n                     Wv ExpectedExpertDirectionInStatesSimilarToS   >\n\nThe learner\u2019s direction in state S is just (S-S\u2019) in feature space.\n\nThe authors model the behavior of the expert as a kernel density type approximator\ngiving the expected direction of the expert starting from a states similar to the one the learner is in.
5262,1,\nThe experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.
5263,1," My chief criticism is a matter of research style: instead of this deluge of barely distinguishable least-publishable-unit papers on the same topic, in every single conference, I wish the authors didn\u2019t slice so thinly, devoted more time to each paper, and served up a more substantial dish."
5264,1,\n\n* I'd like to hear more about the effects of different parametrizations of the airfoil surface.
5265,1, Sample-based RL did not originate in 2008.
5266,1," The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins."
5267,1, \n\nCons:\nSome statements in this paper are not clear to me.
5268,1,"\n- As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments."
5269,1, I would put it at the front.
5270,1,"\"" It also proposes \""Generalized Binarization Transformation\"" for the first layer of a neural network."
5271,1,\nThe section should also emphasize that the models discussed in this paper are only applicable for early stopping in cases where the function evaluation budget N is much larger than 100.
5272,1, The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work.
5273,1,\n\n- How were the tasks selected?
5274,1, Can you post the gold German sentence?
5275,1," \n- If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds."
5276,1,"\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise."
5277,1,I think the paper could be significantly improved by providing more applications of this property in both theory and experiments.
5278,1,"\n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global."
5279,1,The proposed clustering method is problematic.
5280,1,\n- How does the fixed horizon interact with conjoining goals?
5281,1,"""This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases."
5282,1,"  The proposed dataset, called the SCAN dataset, is a selected subset of the CommonAI navigation tasks data set."
5283,1,"""The authors study the effect of label noise on classification tasks."
5284,1,"\n10. Also, table 3 could be greatly improved by providing more ablations such as results for (n=16, d=8), (n=4, d=4), etc."
5285,1, This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension.
5286,1,\n- Section 3.2 GRU citation should be Cho et al. [2].
5287,1," \n\nThe paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field."
5288,1," I liked the originality of this paper. """
5289,1,\n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...').
5290,1,\n\nFinally -- is it expected that the ordering of the factorization in Eq. 3 does not count much (results in Table 3)?
5291,1,\nA dedicated section on motivation is missing.`
5292,1,"The authors should look at several existing papers on stochastic trust region and stochastic quasi-Newton methods, e.g., papers from Katya Scheinberg (Lehigh) and Richard Byrd (Colorado)'s groups."
5293,1, It is interesting to see the impact of incorporating PCN into the training of OCN and encoder.
5294,1,"\n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity."
5295,1,"  There is a footnote on page 7 regarding Bayesian posterior sampling -- I think this should be brought into the body of the paper, and explained in more detail."
5296,1,"\n\nSimilarly, the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer, and show the form of the resulting kernel functions."
5297,1,"  The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it."
5298,1," The experimentation is only performed using small to medium datasets (< 80K instances), it will be good to show if the benefits of the proposed approach can also be present in the case of large datasets."
5299,1,"""Paper Summary:\nThis is a very long paper (55 pages), and I did not read it in its entirety."
5300,1, Does each component is related to a certain topic?
5301,1, THe additional experiments are a good addition.
5302,1,\n+ well written
5303,1, \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE.
5304,1, This widely accepted definition of navigation does not preclude being limited to known environments only.
5305,1,\n\nThe paper largely follows the work of Nachum et al 2017.
5306,1,\n- The technical novelty seems incremental (but interesting) with respect to existing methods.
5307,1," This suggests that those constraints (lower overall connectivity strengths, and lower metabolic costs) might play a role in the EC's navigation function."
5308,1, \n  Negatives:\n- the methodological contribution of the paper is minimal
5309,1,"\n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May."
5310,1," It is *not* true that given CBOW, it's not important to compare with SGNS and GloVe."
5311,1, The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.
5312,1,"""The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one \""slow\"" trained as usual and one \""fast\"" that gets updated in every time-step based on the slow network."
5313,1," The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3)."
5314,1,  Although I don\u2019t think this is the responsibility of this paper (although something that should be considered).
5315,1,"\nWhile the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive."
5316,1,"""The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only."
5317,1,"  However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root)."
5318,1," When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient."
5319,1," Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible."
5320,1," My rating changes to marginally above threshold for acceptance."""
5321,1, This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results.
5322,1," For the constraint structure you have, projection is trivial (just clip the values)"
5323,1, It wasn\u2019t clear what assumptions the authors make to exclude situations like this.
5324,1," First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset."
5325,1," The authors may hence consider\nimproving the LP relaxation, noting that the big-M constraint are notorious\nfor producing weak relaxations."""
5326,1,\n\nComments:\n1) This paper is well motivated.
5327,1,  My question here is \u2014 are gradients being re-used intelligently as suggested in Section 3.1?
5328,1, \n\nSpecific comments \n\n- Figure 2 is too small
5329,1,The main theoretical contribution stimulates and motivates much needed further research in the area.
5330,1," Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature."
5331,1, Why is the complexity O(r^3) independent of the parameter d?
5332,1,\n\nCons:\n- Requires a potentially costly search procedure to generate images.
5333,1,"(Indeed I have a typo in my previous review regarding \""w.r.t. k-th sample\"", which should be \""w.r.t. k-th update\"". )"
5334,1," \n- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees."
5335,1, At least the concept of stochastic predictions should be discussed\n* The rule-based baselines are not described in detail.
5336,1, The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed.
5337,1," Are the types of classes in the seen/unseen classes important, I'd expect at least multiple runs of the current experiments on (E)MNIST."
5338,1,"\nIn the paper, only attributes of objects are used which is not semi-natural languages."
5339,1,\n\nSection 5.4:\nUsing a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family.
5340,1," What happens when you try a bigger data set or a more complex problem?"""
5341,1," Could this be a result of suboptimal, possibly compromised training?"
5342,1,"\n\n\u2014\u2014\u2014\u2014\u2014-\nUpdate: I lowered my rating considering other ppl s review and comments. """
5343,1, The paper is also clearly written and the theoretical result is accompanied by some supporting experiments.
5344,1,"\n\nNegatives:\n\n(1) The state-of-the-art CNN architectures are not mysterious or difficult to find, despite the paper's characterization of them being so."
5345,1,"\n\n- The derivations in pages 4-6 are somewhat disconnected from the rest of the paper: the optimal baseline derivation is very standard (even if adapted to the slightly different situation situated here), and for reasons highlighted by the authors in this paper, they are not often used; the 'marginalized' baseline is more common, and indeed, the authors adopt this one as well."
5346,1, I suspect the proposed method only works in a more constrained setting (such as Multi-PIE where the images are all well aligned).
5347,1,"  While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble."
5348,1,\n* nice results and decent experiments (but see below)
5349,1,\nWould it be possible to empirically evaluate how the R3NN performs on this dataset?
5350,1,\n\nImportance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points.
5351,1, It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.
5352,1," It may help the readers better understand the values of this work.\n    """
5353,1, The results are visually compelling
5354,1, \n\nAlso the experiment is not very convincing.
5355,1,"""\nAs one can see by the title, the originality (application of DCNN) and significance (limited to ATM domain) is very limited."
5356,1, Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more.
5357,1,"\n\n\n\nSpecific comments:\n\npage 1:\n- \""different more recent contributions\"" -> more recent contributions\n- avoid double brackets \""))\""[[CNT], [PNF-NEU], [SUG], [MIN]]\n\npage 2:\n- Please rewrite the first sentence below Definition 1 in a meaningful way.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Section 3: if \\mu is an empirical distribution, it is customary to write it \\mu_n or \\hat \\mu_n (in a way that emphasizes the number of observations available)."
5358,1," but unfortunately the efficacy is not well justified in theory.[[CNT], [CNT], [CRT], [MAJ]] The empirical study is not always convincing, and did not compare with many state-of-the-art baselines."
5359,1, \n- The paper needs more discussion and comparison to relevant baseline methods.
5360,1,"""The paper addresses an interesting problem of DNN model compression."
5361,1,"\n\nIn both these cases the results for PPO and TRPO vary pretty significantly from what we see here, and an important one to look at is the InvertedDoublePendulum-v1 task, which I would think PPO would get closer to 8000, and TRPO not get off the ground."
5362,1," However, in the latter, the optimization objective is the f itself (sup E[f_1]-E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric)."
5363,1," Experiment in section 4.1.2 do not validate significant improvement, either."
5364,1,"\n\nAppendix A isn't referred to from the main text as far as I could tell.[[CNT], [null], [DIS], [GEN]] Just merge it into the main text?\n\n\n\n"""
5365,1,"""This paper introduces a fairly elaborate model for reading comprehension evaluated on the SQuAD dataset. "
5366,1,"  Is each feature being normalized to be zero mean, unit variance, or is each training example being normalized?"
5367,1, \n\n2. Experiments are comprehensive.
5368,1," See [B] for an earlier treatment, and an extension by [A]. See also the Bivcca-private model of [C] which has \u201cprivate\u201d latent variables for vision similar to this work (this is relevant to Sec. 3.2.)"
5369,1,\n\n3) The key of this paper is to approximate the dynamics using neural network (which is a continuous mapping) and take advantage of its gradient computation.
5370,1,"\n\nPresentation/Clarity: \nTo the best of my understanding, the paper has some misconceptions."
5371,1," This is nice but doesn't really tell me much about overcoming catastrophic forgetting. \n"""
5372,1, This causes the method not to work.
5373,1,"\n\nIn the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation."
5374,1,\n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5.
5375,1,"""Overall, the idea of this paper is simple but interesting."
5376,1,"\nThe paper is overall well-written, and the proposed idea seems interesting."
5377,1,"\n\n* Section 3.2: The choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work, but there\u2019s only a very cursory discussion in the main text."
5378,1, Have the authors considered this issue?
5379,1," This limitation is from Hazan et. el., 2017, where the authors wants to predict the output."
5380,1, It also provides some sufficient conditions for the non-linear cases.
5381,1," However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution."
5382,1,\n\ncons:\n- doubts about the interest and originality\n\n\
5383,1,n3. The paper is over 11 pages.
5384,1,"\n\nAlso, is the setting the same as in Weston et al (2015)?"
5385,1,"  \nI find the experiments to be limited, only on two hand-written digits/letters datasets."
5386,1, In the first paragraph of Sec. 3 the competing deep networks are introduced.
5387,1,"\n\n\nComments:\n\n- Why use a model-free technique like Q-learning especially when one knows the model of the car in autonomous driving setting and can simply run model-predictive control (MPC) (convolve forward the model to get candidate trajectories of certain reasonable horizon, evaluate and pick the best trajectory, execute selected trajectory for a few time-steps and then rinse-and-repeat."
5388,1,"\n\nFor instance, eq. (3) proposes one model for p(F|X). Eq. (8) proposes a different model for p(F|X), which is an approximation to the previous one."
5389,1,\n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help?
5390,1,"\n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics."
5391,1," \nCombining self-organising with classification.[[CNT], [null], [DIS], [GEN]] \nComparing learned representations from different models."
5392,1, The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients.
5393,1,"  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server."
5394,1," \n\nIn conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time."
5395,1,\n\nStrengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations.
5396,1," \nThe transferrability of features described in Section 6 is interesting.[[CNT], [CNT], [APC], [MAJ]]\n\nCons:\n\n1. The title \u201cassessing the generalization capability of CNNs\u201d is too broad, as the paper is only studying a narrow aspect of generalization."
5397,1,"Nonetheless, there are some criticisms than can be made of both the method and the evaluations:"
5398,1," I would foresee more and more works will be devoted to this area, considering its close connection to our daily life."
5399,1,"""The authors propose the N-Gram machine to answer questions over long documents."
5400,1,"\n\n- in (2), the hat is missing over \\Psi\nin the definition of v_\\pi(s), r only depends on s'?"
5401,1," The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%)."
5402,1,  Some parts of the paper lack clarity and the empirical results need improvement to support the claims (see details below). 
5403,1," Without such experiment, it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernels."
5404,1," The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph?"
5405,1, The authors even call these methods 'recent'! Clearly not\nthe case. 
5406,1,"but the paper presents useful results,"
5407,1, Not a realistic setting.
5408,1," Some Theory and Empirics\"" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images."
5409,1,"\n\nFurther comments:\n* In Figure 1, the feed-forward network looks like an encoder-decoder network and it does not show the projection from r to R^d which is mentioned in the text."
5410,1,"\n-\tThe generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier)."
5411,1,"\n\nIn the experiments, does the \\varphi MLP explicitly enforce symmetry and identity or is it learned?"
5412,1,"""This paper introduces a new model to perform image classification with limited computational resources at test time. "
5413,1, Larger scale experiments on ImageNet is also recommended to show how general the conclusion of the paper is.
5414,1," This approach is useful even when there is only weak supervision to provide the \""similarity/dissimilarity\"" information."
5415,1, Have the authors been evaluating many times on the test data?
5416,1," It would be convincing if the authors could transfer an English paragraph into the style of a certain author, such as Shakespeare, which can be easily evaluated by a human instead of a trained classifier."""
5417,1,"\n\n\""hypothesis proposed by Santus et al. which says\"" is not a valid reference."""
5418,1,  The key technical limitation is the dependency of the local minima on the weight parameters
5419,1,"  As the graph embedding is used in the generating process and for learning, the functions must be defined and their choice explained and justified."
5420,1, What happens when it is set to 10^4 or 10^6?
5421,1, These are 9% and 6% higher than the reported numbers for adversarial GMemN2N. 
5422,1, An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance.
5423,1,"  However, I would have preferred to see more quantitative evaluation and less qualitative evaluation, but I understand that doing so is challenging in this domain."
5424,1, It seems to have a negative impact for the range of values that are discussed.
5425,1," This is somewhat odd: it is not clear why, e.g., garbled circuits where not used for something like this, as it would have been considerably faster than FHE."
5426,1,  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.
5427,1," Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion? "
5428,1,"\n\nOverall, I found this paper quite interesting."
5429,1," Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required."
5430,1,\n\nExperiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines.
5431,1,  The authors should not compare to the uniform prior as a baseline for entropy.
5432,1,"\n\nI believe the work is very promising,"
5433,1, Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.
5434,1,"\\n\nConclusion:\n- I feel that the motivation is good,"
5435,1,"\n\nReferences:\n- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit."
5436,1, I wonder whether the proposed method scales.
5437,1," On the global convergence of the BFGS method for nonconvex unconstrained optimization problems. SIAM Journal on Optimization, 11(4), 1054-1064."
5438,1, And the coreference experiment isn't that clearly described nor necessarily that meaningful.
5439,1,", and in the abstract generation when that is the only attribute specified, it is not clear how good the results are.\n"""
5440,1,"   From the cited paper by Posner et al : \n\""The circumplex model of affect proposes that all affective states arise from cognitive interpretations of core neural sensations that are the product of two independent neurophysiological systems."
5441,1," Especially the white boxes have no labels, the experiments are conducted only on one small-scale proof of concept dataset, several relevant references are missing, e.g. GAN, DCGAN, GAWWN, StackGAN."
5442,1," While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment."
5443,1," That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates."
5444,1,\n-- How is test set accuracy defined in section 5.3?
5445,1, The generation models are usually highly sensitive to details settings.
5446,1, \nThis paper presents a learning algorithm that can \u201coutperform a greedy baseline in terms of efficiency\u201d and \u201chumans driving the simulator in terms of safety and success\u201d within their top view driving game.
5447,1,"  Moreover, the evolved networks are woefully inefficient in terms of parameter count."
5448,1,"\nThis paper also uses a log-polar transform, but lacks the focal point prediction / STN."
5449,1, The authors should measure the actual speedup also on a single GPU.
5450,1, and 2) predicting a variable's name by consider its semantic context.
5451,1,"\n- There is a typo in your 3.7 \""level 1 complexity\"": there is an extra O inside the big O notation."""
5452,1," \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank"
5453,1,"\n\np 1: authors write: \""Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012)."
5454,1,"\n\nEvaluation:\nCurrently, only neural abstractive methods are compared."
5455,1, The kernel is fixed.
5456,1,"\n\nThe paper frequently refers to \""embedding\"" \""imaginary trajectories\"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me)."
5457,1,\n\n1. System identification.\nSubspace identification (N4SID) won't take exponential time.
5458,1," However, the questions themselves could have been formulated differently: \nQ1: the way Q1 is formulated makes it sound like the covariates could be both discrete and continuous while the method presented later in the paper is only for discrete covariates (i.e. group structure of the data)."
5459,1,"\n\n5) Experiments on synthetic datasets, where both the shift in policy and the\nshift in domain are simulated and therefore can be controlled, would better \ndemonstrate how the performance of the proposed approach (and thsoe baseline \nmethods) changes as the degree of design shift varies."
5460,1,"\n\nThe text refers to a Figure 3 which does not exist, probably means Figure 2."
5461,1, That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.
5462,1," I am wondering how this will be used and evaluated in medical imaging setting. \n"""
5463,1, \n\n3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections.
5464,1,  The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch.
5465,1,"\n\nOn the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2."
5466,1,\n\n- Are there plans to release the dataset?
5467,1,"\u2019, \u2018Our two-pass decomposition provides the better result as compared with the original CP decomposition\u2019."
5468,1,\n- The abstract should be self contained and should not contain citations.
5469,1, Was the model architecture tuned based on the proposed representations? 
5470,1,... but I'm left wondering how/why?
5471,1, None of this is discussed.
5472,1,".\n* The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners."
5473,1," The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on."
5474,1, What if I only have pure text document without these HTML structure information?
5475,1," To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem."
5476,1,"  Second, given its contribution, the manuscript is better suited for a conference specific to multi-agent decision-making."
5477,1,"\n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ? "
5478,1," Overall, I would like to vote for a weakly acceptance regarding this paper."
5479,1,"""# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP)."
5480,1," Or introducing noise \""on-line\"" as part of the training? "
5481,1, \n- interesting idea but I think it's more theoretical than practical.
5482,1,\n\nA major drawback of the evaluation of the different approaches is that\neverything was used with its default parameters.
5483,1, The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed.
5484,1," \n\niv. In part 1 of the proof of the main theorem, it seems unclear why the sign of the main inequality has changed after application of Cauchy convergence in probability (equation at the top of the page)."
5485,1,  Should one expect a certain trend in these sequences?
5486,1,"  The assumptions that large Fourier peaks happen close to origin is probably well-justified from the empirical point of view,"
5487,1, Did you use dropout?
5488,1," Altogether, it seems like this paper contains a significant amount of additional text beyond what other submissions enjoyed."
5489,1," The Greedy schedules are far inferior to hand-tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be \""seen\"" with a small lookahead."
5490,1,"  If this is true--that learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments."
5491,1,"  The paper is clearly written, and has useful ablation experiments."
5492,1,"\n\n(1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large."
5493,1,"The width of the network is bounded by the two input distributions, so is this network just incredibly deep?"
5494,1,"As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful."
5495,1, It is actually not very well explained what is the main\ndifference.
5496,1," The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed."
5497,1, The certificate is derived with rigorous and sound math.
5498,1, I\u2019m curious if this phenomenon still holds if authors use float64 in the experiments.
5499,1,\n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.
5500,1,"  What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods."
5501,1,Is the offline attention baseline unidirectional or bidirectional?
5502,1," Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community."
5503,1," \n\nAs weak points:\n1) The paper claims the selection of \\alpha is critical but then, this is fixed empirically without proper sensitivity analysis."
5504,1,"""The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations."
5505,1," The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on."
5506,1,"  \n- Influence of the clustering method on VCs, eg k-means, gaussian, von-mises (the last one is proposed)?"
5507,1, \n\nOriginality\nThe proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm.
5508,1,\n\nPros:\n- De-duplication modules of inter and intra object edges are interesting.
5509,1, The method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data.
5510,1,\n\nA key element in PSRNN is to used as an initialization a kernel ridge regression.
5511,1,"\n\nOriginality\nThe theoretical results are original, and the SGD approach is a priori original as well."
5512,1, \n- The notation changes between E and N and Z for the noises.
5513,1,"\n(3) detailed information are provided about the experiments, such as data, model and inference."
5514,1,"\n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way."
5515,1,"\n\nI don't understand why (1) differs from other approaches, in the sense that one cannot simply reduce the number of epochs without hurting performance."
5516,1," However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \""wide\""-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected."
5517,1, The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel.
5518,1,\n\nThe baseline methods result in rewards much lower than those in previous experimental papers.
5519,1,"  And note that the \""w(u)\"" defined in this reference is the lambda*w*(alpha) optimal relationship defined in this paper (but without the 1/lambda factor because of just slightly different writing; the point though is that strong duality holds there and thus one really has equality)."
5520,1, The paper should also discuss this method in Section 4.\
5521,1," For example, the entire network receives an external reward (R_t^{ex}), presumably for its performance on some task."
5522,1,\n\nThis is particularly called into question due to the lack of assumptions about the function class for value functions.
5523,1, In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.
5524,1," Second, \""it is easy to imagine a hybrid system where a network is trained on a simulation and fine tuned ...\"". Implementing such a hybrid system is nontrivial due to the reality gap."
5525,1,\n  Is beam search performed in the case of sequences?
5526,1," VI is a much older\tfield, and Kingma and Welling proposed a very special form of VI, namely amortized VI with inference networks."
5527,1,"  In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch."
5528,1, it will be good to test directly on real long-tailed datasets.
5529,1," Like the anonymous commenter, I also initially thought that the proposed \""spectral normalization \"" is basically the same as \""spectral norm regularization\"", but given the authors' feedback on this I think the differences should be made more explicit in the paper."
5530,1," More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z)."
5531,1,\n\n- The results for the perceptual complexity experiment seem contradictory and inconclusive.
5532,1, Optimal hyperparameters are usually model-specific.
5533,1,"\n\nI'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images."
5534,1, I would like to see the same spectra included.
5535,1,. \nThe paper compared the vanilla version of BiVCCA but not the one with factorized representation version
5536,1," Perhaps I am mistaken, but it seems to me that the proposed model cannot seem to handle negation, can the authors confirm/deny this?"
5537,1,"""This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule."
5538,1,The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior.
5539,1,"""Thank you for the submission. "
5540,1, And PCA as a non-random projection would a nice baseline to compare against.
5541,1, The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.
5542,1," Given that there is also no theoretical argument why an ensemble approach is expected to perform better,"
5543,1,  except for the \u201cfuture updates\u201d meaning which probably deserves a clearer explanation.
5544,1,"""This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training."
5545,1, \n\nThe paper attempts to make progress in the region between deep learning and functional data analysis (FDA). This is interesting.
5546,1, Deciding on these targets is formulated as a combinatorial optimization problem.
5547,1,"\n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks."""
5548,1, Is it for space? for speed? for expressivity of hypothesis spaces?
5549,1, \n\n1. Can the observations be used to explain more recent works?
5550,1, The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label.
5551,1,"\n\n- For results in Table 1 and Table 2, how are the confidence intervals computed?"
5552,1,"  The network for the MNIST dataset is similarly only 3 layers deep (input, hidden, output)."
5553,1,\n9. The examples provided in the appendix are great.
5554,1,\n\n\nDetailed comments:\n\n- I think the novelty of the attack is not very strong.
5555,1," I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on)."
5556,1, I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve.
5557,1," These tasks are clearly different, as nicely shown by the authors' example of \""do(mustache = 1)\"" versus \""given mustache = 1\"" (a sample from the latter distribution contains only men)."
5558,1,"""This paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation games."
5559,1,"The approach is empirically tested on the following data: MNIST, CIFAR, and SVHN."
5560,1, Is this really worth doing? 
5561,1,"\n- I think it's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model, see Fig 16."
5562,1," \n\nThe introduction is fairly strong, but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy \""1) train S on random exs; 2) train T to pick exs for S\"" makes sense."
5563,1, but it is not clear at all how this particular connection is more than a curiosity.
5564,1,"\n\nSecondly, in terms of the development of reading comprehension models, I don\u2019t see why we need to care about eliminating the irrelevant options."
5565,1, It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Appendix G in https://arxiv.org/pdf/1706.04987.pdf)
5566,1," I would still use the test as a part of a suite of measurements, but I would not solely rely on it."
5567,1,How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs? 
5568,1,\n- It was not clear to me why we should use a Gaussian approximation for the \\theta_NN parameters?
5569,1, I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.
5570,1,\n\nNegative aspects:\nMy main criticism to the paper is that the task learning achieved through self exploration seems relatively shallow.
5571,1," At least in \""standard\"" compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without).\"
5572,1," However, the results are more focused on compute cost."
5573,1, I would strongly urge the authors to improve the method description in the camera read version though.
5574,1," \n\nIt is not easy to read the figures in the experimental section, no walkthrough of the results are provided."
5575,1," Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?"""
5576,1,\n- The title seems misleading. 
5577,1,\n2. The batch size for MNIST classification is unusually low (8) .
5578,1, I would say that the topic is an important one.
5579,1,\n\n2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results.
5580,1,"  If not, why?\n\nWhen contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on?"
5581,1, It investigates what sort of image representations are good for image captioning systems.
5582,1,                      \n- The results are otherwise convincing and clear improvements are shown with the proposed method.
5583,1," Rather, removing the distributional and cycle constraints changes the overall objective being optimized."
5584,1, I agree with the second reviewer that the approach is interesting.
5585,1," However, the paper is not flawless."
5586,1," However, I have the following concerns for this draft:\n\n1. The technical contribution is not enough."
5587,1,"\""\n- Targets an important problem\n\nCons:\n- Related work seems inadequately referenced"
5588,1,"""**I am happy to see some good responses from the authors to my questions."
5589,1, Why authors choose different set of benchmarks?
5590,1," \n\nFinally, the experimental results are somewhat uninspiring."
5591,1, \n\nPros: \n\n- The network compression problem is of general interest to ICLR audience.
5592,1, \n\nThe results in Baird\u2019s counter example are discouraging for the new constraints.
5593,1," 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design."
5594,1," As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples."
5595,1," This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time."
5596,1,The reward is +1 when the agent collects a health kit and 0 otherwise.
5597,1," Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML."
5598,1," More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces."
5599,1,"\n\nAlso, the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained. "
5600,1,"""The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states."
5601,1,"  Only two examples are not convincing.[[CNT], [EMP-NEG], [CRT], [MAJ]] \n3.\tIn section 3, the authors claimed that (5) models the target and context independently."
5602,1,\n\nIn my opinion the main weakness of the paper is the focus on the RACE dataset.
5603,1,"Note\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say."""
5604,1," It seems that effectively, the  introduce their additional softsign in the process."
5605,1, \n8. The performance on One-Layer Subspace Network (with only the input features) could be added.
5606,1, An autoencoder objective is used to produce meaningful tuples.
5607,1,\n\n2) Report results on well-known CF datasets.
5608,1, I thus believe the overall contribution is not sufficient for ICLR.
5609,1, \n\nThe unimodal claim for distribution without randomness is weak.
5610,1, Follows a section of experiments on variants of MNIST commonly used for continual learning.\n\n
5611,1,"  The other loss should remain what it is in the full \u201cOurs\u201d condition (i.e., l_1).[[CNT], [CNT], [DIS], [MIN]] \n\n- The last sentence in the caption of Table 1 -- \u201cSlight improvement in motion is observed by training with an adversary as well\u201d -- should be removed."
5612,1," On Page 3 we assume that K = 1, but Theorem 6 still maintains a dependence on K"
5613,1," \n2.\tIn Table 1, the results only report the standard deviation of AUC."
5614,1, So the paper feels directed to an audience with less background in neural net LMs.
5615,1,\n-The history of how to go from a Bellman equation to a sample-based update seems a bit distorted.
5616,1,  The authors devise a heuristic way (based on an innovated measure that combines computational complexity with performance) to select the tensor rank to be used.
5617,1,\n\n6. The manuscript is hard to understand and not written clearly enough.
5618,1,"  The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings."
5619,1, What happens if predicting the polar origin is not trivial and prone to errors?
5620,1,\n\nThe main weaknesses of the paper are in the soundness of some of its qualitative analyses and claims.
5621,1,\n  * s_j is undefined
5622,1," While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible."
5623,1, Therefore I does not seem pertinent here.
5624,1,\n\n\nThe paper could have gone farther experimentally (or theoretically) in my opinion.
5625,1,  \n\nThe motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network.
5626,1, What exactly makes them dynamic?
5627,1,"  Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward."
5628,1," However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n"""
5629,1," The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas."
5630,1, The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. 
5631,1,"  If so this seems like it may be a significant constraint that would shrink the application space and impact even further.\n"""
5632,1,\n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most).
5633,1,"\n\n- Given that the paper seeks to use uncertainty in estimates and the\n  entire regression setup could be trivially made Bayesian with no\n  significant computational cost over a kernelized SVM or OLS,\n  especially if you're doing LOOCV to estimate uncertainty in the\n  frequentist models."
5634,1, The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).
5635,1,"\n\nIn it's current state I'd say marginally above, but wouldn't be surprised if these changes turned it into an even better paper quite quickly."
5636,1," As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution"
5637,1,n\n3. Quite good results on several relevant tasks.
5638,1, \n\nThe authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps.
5639,1,"""The paper proposes to generate embedding of named-entities on the fly during dialogue sessions."
5640,1,"""This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network."
5641,1,  Or are they different (maybe arbitrary) groupings over the feature maps?
5642,1, To see the confusion matrixes could be useful.
5643,1," Finally, the authors propose a new method by using one unexplored combination of taxonomy features."
5644,1," Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance."""
5645,1,"\n2) Minor: some of the citations are a bit awkward, e.g. on page 7: \u201calgorithm from Williams Williams (1992)."
5646,1, For example the cascade adversarial training is buried deep inside the experimental section.
5647,1,"  However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area."
5648,1, and has just a few typos: 2.1: \u201can Gan\u201d.
5649,1,"\n\nA few suggestions for experiments:\nA. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric."
5650,1," What is \""performance\"" measured in? In general the Table captions need to be clearer and more descriptive"
5651,1,\n* The found hyperparameter of the grid-search would also be interesting to know.
5652,1," Also, please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic, optimally forming groups and placing them is learn-able."
5653,1,"This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help."
5654,1,"After all, the L2 one is just an approximation of the entropic one."
5655,1," Considering that it is part of the title, I would have expected a proper exposition of the idea in the technical section (before any results are presented)."
5656,1," My intuition is that if the learner algorithm is convex, then ultimately they will all get to the same accuracy level, so the task is just to get there quicker."
5657,1," The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets."
5658,1,"  If the learning rate were optimized together with batch size (eg, keeping aN/B constant), would the generalization gap still appear?"
5659,1,"  Not the interpretation itself is fragile, but the model."
5660,1," Finally, they show that singular perturbations can be easily detected."
5661,1,"\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012."""
5662,1,"In the experiments with real data, it's unclear how good the uncertainties produced by the model are."
5663,1," Adadelta, RMSprop, and ADAM are mentioned explicitly, but what exactly are differences and similarities?"
5664,1, The paper explores in particular:\n1.) Model generalization to unknown data similar to the training set.
5665,1,"\nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer."
5666,1," When you train the base learners, their loss functions will become weighted."
5667,1,"\nAuthors use the dataset provided for Starcraft: Brood war by Lin et al, 2017."
5668,1, Is it just that JCP-S also incorporates 2nd order embeddings?
5669,1," \n\nFirst, wrt claim (i) the problem of generating \""sibling\"" graphs is ill-posed."
5670,1, It was also mentioned that increasing the regularization parameter size increases the convergence rate.
5671,1," \n\nSussillo, D., Churchland, M. M., Kaufman, M. T., & Shenoy, K. V. (2015)."
5672,1," While condensing the paper, consider presenting all technical material before evaluation."
5673,1,"""The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language."
5674,1,"   You state that they are extracted from  64 beam candidates, are they unique N-best lists?"
5675,1,\n\nCons:\n- Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers
5676,1,\n\nClarity:\nThe content of the paper is unclear in certain areas.
5677,1,"   \nThe idea is nice and simple,"
5678,1,\n\nMinor edits: Page 1. 'significantly match and improve' => 'either match or improve'\n\nAdditional notes:
5679,1, Many syntax errors/ format issues.
5680,1," As a result, the resulting ROC and AUC score are expected be better than PCL."
5681,1, and it is important to compare simple approaches on benchmark problems
5682,1," It is unclear from the text if this is the case.[[CNT], [CLA-NEG], [CRT], [GEN]] \n                   The authors should do a better job explaining and comparing the overall experimental results."
5683,1,"""I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP)."
5684,1,"\nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed."
5685,1,"\n\nThe paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Examples: foveate inspection the data (abstract), may allow to (motivation), tu put it clear (motivation), on contrary to animals retina (footnote 1), minimize at most the current uncertainty (perception-driven control), center an keep (fovea-based implementation), degrade te recognition (outlook and perspective)."
5686,1,"  Indeed, a skeptical reviewer may suspect that the authors needed to add perceptual realism to the evaluation because that\u2019s the only thing that justifies the adversarial loss."
5687,1,"  I am including this additional information in my review, for use by the area chair, but otherwise leaving my assessment as-is.\n"""
5688,1, The authors do seem to realize this and they provide experiments examining this interaction.
5689,1," Because the layer-wise decoupling, it can easily be applied for distributed training of the model."
5690,1,\nI would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with Astrid and Lee 2017 are answered.
5691,1,  I thought the main selling point of the method is the computational gain.
5692,1,"""The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that \""obfuscates\"" the document, i.e. changing words in the document."
5693,1," The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22."
5694,1,\n\nSome minor notes\n- 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question.
5695,1, Is there any justification for why this method of injecting noise was chosen ?
5696,1," \nHowever, if the next step in the expert\u2019s path was difficult to approximate, \nthen the reward for imitating the expert would be lower."
5697,1,\n\n+ves:\nExplaining the power of depth in NNs is fundamental to an understanding of deep learning.
5698,1," \nThe implicit are potentially one order of magnitude more costly than an explicit step since they require\nto solve a linear system, but can be solved (exactly or partially) using conjugate gradient steps."
5699,1, The motivation in the end of page 3 seems to be computational only.
5700,1,"\n\n\nIncomplete experiments:\nThe authors only show experiments on videos containing objects that have already been seen, but no experiments with objects never seen before."
5701,1," For example, I can't find the definition of D.[[CNT], [PNF-NEG], [CRT], [MIN]] From the context it seems to be the number of layers in the network (I shouldn't need to guess)."
5702,1," In the revised manuscript, this bullet point is now removed."
5703,1,   It might help to show the distance between the actual model parameters that those algorithms converge to.
5704,1," Looking at figure 1, it appears that you start out with some alpha < 1 but then immediately alpha converges to 1, i.e. the skip connection is eliminated early in training, because wx is further away from y than tanh(wx)."
5705,1,"  However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency."
5706,1,"""Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \""luckiness\"" of the license plate number."
5707,1, Performance on MNIST in the 7-11% test error range is comparable to a simple linear logistic regression model; for a CNN that is extremely bad.
5708,1,\n\n2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2.
5709,1," On page 2, the authors claimed that \""These are arguably one reason why deep learning requires large sample sizes as large sample size is clearly not per se a guarantee that the confounding effect will become weaker."
5710,1,"""The authors address two important issues: semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image."
5711,1,"""This paper proposes a direct way to learn low-bit neural nets."
5712,1,"\nAn algorithm that can work with off-policy data has also been developed by [Nachum, 2017] (in the form of a Bellman residual minimization algorithm, as opposed to this work which essentially uses a Fitted Q-Iteration algorithm as the critic)."
5713,1," \n- While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN)."
5714,1,\n* The much better performance of ConvE is worrying there.
5715,1,\n- Most of the results are qualitative
5716,1," Moreover, The definition of autonomous agent is not clear, do you mean learning agent or policy execution agent?"
5717,1,"\n\nExperiments\n- Many of the previous comments still hold, please proofread"
5718,1, \n- I believe Prop 1 (as it is stated) is wrong.
5719,1," but this approach is not standard for speech recognition,"
5720,1," Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. \""two of the predicted titles for Die Hard have something to do with dying and being buried\"")."
5721,1,"\nNevertheless, the additional experiments and clarifications are very welcome."
5722,1, Or is only the final image and description optimized.
5723,1,- I think this is an important conceptual paper.
5724,1, This is nice
5725,1," The baselines chosen are 1). no embeddings 2). generic embeddings from english wiki, common crawl and combining data from previous and new domains."
5726,1,n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.
5727,1," On the other hand, I think the ICLR community would benefit from about the opportunities to work on problems of this nature."
5728,1," \n\nOverall, I like this paper and think the underlying group-wise posterior construction trick is worth exploring further."
5729,1,\n(c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation.
5730,1, \n- Some unclarity in the description of the method; see below.
5731,1, In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance.
5732,1," This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community."
5733,1,\n\nYou have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet
5734,1,"\n\nAs a purely empirical study, it poses more new and open questions on GAN\noptimization than it is able to answer; providing theoretical answers is\ndeferred to future studies."
5735,1," Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs?"
5736,1,"\n\n\n[1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux."
5737,1, but in my opinion the significance of the results as they stand is low.
5738,1,"\n\nEquation (2) shows that the learnable filters g are operating on the k-th power\nof the normalized adjacency matrix A, so when K=1 this equals classical GCN\nfrom T. Kipf et al."
5739,1," We postulate this is because SGD is easy to overfit training data and \u201cstick\u201d to a solution that has a high loss in testing data, especially with the large batch case as the inherent noise cannot push the iterate out of loss valley while our TR method can."
5740,1, This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions.
5741,1,\nIt seems that the authors have made an effort to accommodate reviewers' comments.
5742,1,"\n-\tPag5 \u201cone-hot\u201d or \u201conehot\u201d;[[CNT], [null], [DIS], [GEN]] \n-\tin the inline equation the sum goes from n=1 to S, while in eq.(8) it goes from n=1 to N;"
5743,1,"\n\n(b) A locally positively curved model, in which there is a positively curved outer bound for the collection of points which are assigned a given label."
5744,1,. \n\nPros:\n\nThe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. 
5745,1, They also claim that their method is reaching a new milestone in evolutionary search strategies performance.
5746,1,"""This paper dives deeper into understand reward augmented maximum likelihood training."
5747,1,  I do think the paper would benefit from experimental results
5748,1,"""Summary: The paper studies the problem of effectively training Deep NN under the constraint of privacy."
5749,1,"  My scores were changed accrodingly.\n"""
5750,1," \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines."
5751,1,n\nCons:\n- Limited technical novelt
5752,1,\n- interpretation of embedding dimensions as topics not convincing
5753,1," \n\nOne has to wait till we go into the experiments section to read something like:\n\""Lastly, although in theory, we need full gradient and full Hessian to guarantee convergence, calculating them in each iteration is not practical, so we calculate both Hessian and gradient on subsampled data to replace the whole dataset\""\nfor readers to realize that the authors are talking about a stochastic mini-batch method."
5754,1," An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter."
5755,1,"""= Quality = \nOverall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well."
5756,1,"\n\nAnd for this same reason, I find the multiple references of \""a generative model P(x|z)\"" in this paper inaccurate and a bit misleading."
5757,1, Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms.
5758,1,"\nHowever, there is not enough analysis on showing whether and how the identified\nhidden units help \""interpret\"" the model.\"
5759,1,"\n\nOverall, I think it is a good paper and deserves to be accepted to the conference."
5760,1," In summary, a very nice paper."
5761,1,\n\nORIGINAL REVIEW BELOW\n\nThe paper introduces an architecture that linearly interpolates between ResNets and vanilla deep nets (without skip connections).
5762,1,\n\nEq. (1) is incorrect
5763,1,".\n\nThere are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).\"
5764,1,"""This paper induces latent dependency syntax in the source side for NMT."
5765,1, But it would be good to know whether the approach works well across games and is competitive against other stronger baselines.
5766,1," Then, the model generates a program, based on the extracted tuple collection and the question, to find an answer."
5767,1,"\n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337\u2013348, 199.\u2028"
5768,1,"\nDumoulin, Shlens and Kudlur (2017)\nhttps://arxiv.org/abs/1610.07629"
5769,1, Multi-layer representations are mostly interesting because each layer shares hidden basis functions.
5770,1,\n- The decoder model has randomness injected in it at every stage of the RNN.
5771,1,The authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching.
5772,1," To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet."
5773,1," Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved."
5774,1, This part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization? Doesn\u2019t this prevent exploration? What are the network parameters (e.g. size of layers) etc.
5775,1, It also bears the questions that how important is the entropy regularization vs. the reconstruction loss.
5776,1, \u201c - The paper you give exactly solves for the nonlinear function approximation case.
5777,1,"""This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling."
5778,1," I suppose you use psi for L and L', but this is not very clear."
5779,1," At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed."
5780,1, \n\nCons:\n\nThe motivation (Section 2) needs to be improved.
5781,1," The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level."
5782,1,"  Why? \n\n- Surely \""d_t = Dt\"" should be \""d_t = D v_t\"" in the \""Interaction Term (IT)\"" subsection?"
5783,1,"\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs."
5784,1, \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5).
5785,1,  The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters.
5786,1, The paper should at least outline the strategy.
5787,1, This model shows the rather strong performance of spelling models \u2013 at least on this task \u2013 which he again benefit from training in the context of the end objective.
5788,1,"\n\n2. One of the conclusions of the paper is: \u201cAlthough CNNs do not intrinsically classify objects based on their shapes, they can learn to do so when trained with enough number of images with the same shape and different colors."
5789,1,n2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones but many questions are still unanswered:
5790,1," Unlike in this previous work, the approach proposed here induces this behavior though a multi-agent reference game. "
5791,1,\n\nThe second alternative for training the goal function tau seems confusing.
5792,1,"The approach proposed by the authors is an excellent first step in this direction, and they provide a convincing argument for why a previous approach (joint optimization) did not work."
5793,1,"\n\nThe training data is synthetic, allowing for arbitrarily large training sets to be generated."
5794,1, however it would have been\ngreat if the authors provide other kernel functions with closed-form convolution \nformula that may be relevant for learning.
5795,1, The results are not as good as the state-of-the-art.
5796,1,"\n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC."
5797,1, I found no technical\nerrors.
5798,1,"\n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning."
5799,1,"\n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective."
5800,1, since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.
5801,1,\n- legend/sizes are not readable (especially in printed version).
5802,1,"\n\n- In Table 1 the linear SVM uniformly outperforms the RBF SVM, so why\n  use the RBF version?"
5803,1,   It should at least be evaluated.
5804,1, Are the cells found interpretable?
5805,1,"  How\u2019s the score of the proposed model compared with the above paper as well as [Tang et al. 2016]?\n"""
5806,1," In general, the writing and presentation of the model seem highly fragmented, and it is not clear what the specifics of the overall model are."
5807,1, Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features.
5808,1,"\n\nIt is also known that these benchmarks, while being widely used, are small and\nresult in high variance results."
5809,1," if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes."
5810,1, Experiments show that ISRLU is promising compared to competitors like ReLU and ELU.
5811,1,"\n\nThe paper may have a valid point that differential privacy is hard to work with, in the case of Deep NN."
5812,1," \""Recurrent orthogonal networks and long-memory tasks."
5813,1,"\n \nOriginality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable. "
5814,1," For the convolutional layers of VGG19, they observe that the sum of IDs for each feature map is roughly equal to the ID of the matrix formed by concatenating the vectors over all feature maps."
5815,1,\n- \n\ncons:\n\n- some experiment choices do not appear well motivated / inclusion is not best choice
5816,1,The task itself is interesting and novel.
5817,1,"\n\nDespite this enthusiasm, I am doubtful whether this is a good paper for ICLR."
5818,1,"Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates."
5819,1," While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network."
5820,1, Can you show some analysis of your model results that confirm/deny this hypothesis?
5821,1,. The main object of study is to quantify the benefits of overlap in convolutional architectures.
5822,1,  \n\n(3) It is not clear how will the vocabulary-information be exploited?
5823,1, Even RL methods with linear function approximations use abstractions.
5824,1,\n\nI wonder whether this approach could be generalized to other classes of words or morphemes.
5825,1," The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task"
5826,1," Overall, I think this is interesting work,"
5827,1," \n - Thanks for the clarification and adding this citation. """
5828,1,  Basically how hand picked were the options?
5829,1," The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited)."
5830,1, \n(4) the idea of using surrogate labels to learn representation is also not new.
5831,1,"\n\n    Loss = E[    R + MAXa\u2019 Qp(S\u2019,a\u2019)    -   Qp(S,a)   ]  \n\nThe authors suggest we can augment the environment reward R \nwith a heuristic reward Rh proportional to the similarity between \nthe learner \u201csubgoal\"" and the expert \u201csubgoal\"" in similar states."
5832,1, This is worked around by updating the parameter using the sign of its gradient.
5833,1," \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence."
5834,1," However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven."
5835,1," I think it is crucial to highlight that the deficiencies of DIB matter in practice, and are not simply a theoretical consideration."
5836,1,"\n2. If linked words does improve topic modeling, why does it do so?"
5837,1, Based on how\n  the authors use 'sparse' I think that Table 1 shows the fraction of zeros in\n  the learned embedding vectors.
5838,1," how well can it be\nintegrated into a distributed workflow?), and included some comparisons with\nother recent recommended ways to increase batch size over time."
5839,1,"""Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage."
5840,1,. It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.\n\n12
5841,1," The authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity; however, this doesn't fully address why non-BC SbT-NMF can strongly outperform the BC method in certain tasks (e.g. the step of light, Fig. 3b)."
5842,1," however, I found the presentation lacking."
5843,1, The evaluation shows that the approach performs better classifying MNIST digits than another approach.
5844,1," \n\nGeneral Comment:  \nThe technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs."
5845,1,"If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution)"
5846,1," An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques). \n\n"
5847,1, but that\ntheir analysis still my provides some additional leverage for such\nlayers.
5848,1, The approach is methodological similar to using expected likelihood kernels.
5849,1, Do you have any intuition for what kind of features or information the networks are capturing?
5850,1, This should at least be verified and compared appropriately.
5851,1, Why is this a reasonable assumption to make?
5852,1,"\n\nSince these earlier results existed, and approximation-amortization decomposition is fairly simple (although important!), the main contributions of this paper are the empirical studies."
5853,1,\n\nThe proposed method is evaluated on just one dataset.
5854,1," \n\nIt is nice that the authors use \""counterfactual regularization\""."
5855,1, It would be more useful to predict links that are removed with an unknown bias.
5856,1,"\n9. Experiments are unconvincing. First, only one hyperparameter is being optimized and random search/grid search are sufficient for this."
5857,1,\n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix
5858,1, It would be necessary to see the tensor reconstruction error during the following 2 scenarios:\nWe apply the CP decomposition to a pretrained network
5859,1," In that sense, to me this work is more of a negative result informing that to be successful, deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime."
5860,1, \n\nIn the last paragraph of Section 3 ``m = w^k-1'' This is a very big first layer.
5861,1, While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary.
5862,1,\n- Experiments on two different network architectures showcasing the generality of the proposed method.
5863,1,  This interpretation plays a role in the proposed training method by informing how the \u201cstep sizes\u201d in the Euler discretization should change when doubling the depth of the network.
5864,1," Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched."
5865,1,"\n\nHow can variational inference be attributed to (again) a survey paper on the topic from 2008, when for instance [2] appeared in 2003?[[CNT], [CNT], [QSN], [MIN]]\n\n\n- Correctness of the approach"
5866,1," The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below)."
5867,1,"\nIn particular one sentence that can be misleading:\n\u201cGiven a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient\u2026\u201d\nYou should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient. "
5868,1,".\n\n5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space."
5869,1,"\n\nFor table 1, please use decimal points instead of commas.\n"""
5870,1,\n\n\n2) Pros:\n+ New quantitative evaluation criteria based on motion accuracy.
5871,1,"\""\n\nExplanations of better generalization properties of TR over SGD are important."
5872,1, How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables?
5873,1,"\""\nFrom my reading of this paper, it is clear to me that the authors do not have a clear understanding of the current state of psychology\u2019s view of emotion representation and this work would not likely contribute to a new understanding of the latent structure of peoples\u2019 emotions."
5874,1,"""The paper develops an interesting approach for solving multi-class classification with softmax loss."
5875,1,"\nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \""at a large venue\"" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example. """
5876,1, It would be interesting to show results on sensitivity to the number of updates (p).
5877,1, but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.
5878,1," However, it does not mean training the wider networks is more efficient than training the deeper ones."
5879,1,\n\n- Missing details.
5880,1, Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.
5881,1,"""Summary\n========\n\nThe authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point."
5882,1,"""This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature."
5883,1," The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices."
5884,1,\n\nThe presentation of the paper is poor.
5885,1, Only a tiny subset of words have intrinsic and context-free affect.
5886,1, \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary.
5887,1," Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling."
5888,1,\nSignificance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.
5889,1," \n\n(1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions."
5890,1," the problem is described neatly in conjunction with the past work,"
5891,1,"""This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives."
5892,1,"\n- the language at the end of section 3 is very vague and noncommittal -- maybe just state what you did, and separately give future work suggestions?"
5893,1, What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?
5894,1,"""This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation."
5895,1, \n-\tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features.
5896,1,I think this only contains a single round of optimization.
5897,1, I get that tau is going to be constrained by whatever representation PATH function was trained on and that this representation might affect the overall performance - performance.
5898,1," Therefore I have modified my rating accordingly."""
5899,1, They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training.
5900,1," In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks."
5901,1, Should ALS be applied repeated (each round solves d problems) until convergence?
5902,1,"\n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features."
5903,1,  (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?
5904,1, The resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning tasks.
5905,1,\n\n1. The paper needs a lot of editing.
5906,1, but the details of the paper seem hastily written.
5907,1,\n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.
5908,1, Each extension to the Dual Actor-Critic is well motivated and clear in context.
5909,1," I am recommending acceptance,"
5910,1,\n\nSome open questions that I find crucial:\n\n* How exactly is the \u201cstochastic forward-pass\u201d performed that gives rise to the moment estimates?
5911,1, \n- experimental results are encouraging
5912,1,\n\nClarity\n\nThe rationale is clear and the results are straightforward to interpret.
5913,1, This could help validate the hypothesis: how does the estimated rank vary with the number of components?
5914,1,"\n\nIn terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why."
5915,1, The authors compare only the prediction time of SRM with them which is really fast.
5916,1," In fact, it is not clear if the model generates full sentences or attribute phrases."
5917,1,"""Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion. "
5918,1," I would use the \\citet command from natbib for such citations and \\citep for parenthesized citations, e.g. \u201c... incorporate dark knowledge (Hinton et al., 2015)\u201d or \u201cThe MNIST (LeCun et al., 1998) dataset...\u201d"
5919,1,"n\n- I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these \""clouds\"" are hard to interpret)."
5920,1,\n\nThe paper studies text embeddings through the lens of compressive sensing theory.
5921,1,"""The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn."
5922,1,\n\nAlmost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes.
5923,1,".\nC. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words."
5924,1," The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching."
5925,1," In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method."
5926,1, I believe that such material is more appropriate to a focused applied meeting.
5927,1,  \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA.
5928,1, The reason is that the label already appears in the loss of the nodes  in 5.1. Isn't using the label also as input redundant?
5929,1," The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods."
5930,1, The weights within a depth channel is shared thus maintaining the stationary requirement.
5931,1,"Hopefully other expert reviewers will be able to provide constructive feedback."""
5932,1,"\n- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima."
5933,1,\n\t\u2022\tExplanations are exceptionally well done: terms that might not be familiar to the reader are explained.
5934,1,"""NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset?"
5935,1,\n\n2. Why is the crash car dataset used in this scenario?
5936,1,"\n\nAn interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks."
5937,1," \n\nOverall, I think the paper proposes some interesting ideas,"
5938,1,\n\nIt's also difficult for me to understand how this interacts with the other terms in the objective (quantization error and loss).
5939,1," However, in this paper, the way it is added is simply by updating word representations based on this extra text."
5940,1," I'd encourage the authors to do a more detailed experimental study with more tasks,;"
5941,1, The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image. 
5942,1,"\n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications."
5943,1,  This should be at least one of the baselines.
5944,1, It is not clear whether the difference in the evaluation measures is related to the greater number of examples or by the specific generative model.
5945,1,\nIt would be interesting to see whether the idea can be applied to more recent GAN models and still perform better
5946,1, This probably leads to more parameters in the convolutions.
5947,1, The theory part focuses on effectiveness of drawing samples from the reservoir.
5948,1,"""The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect."
5949,1, The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors.
5950,1," Frankly this is not very surprising; matrix factorization is very powerful, and these simple word similarity tasks are well-suited for matrix factorization."
5951,1," This is a limitation, as the model space is not as flexible as one would desire in a discovery task."
5952,1," \n\nI think the writing of the paper is also misleading in several places. \n"""
5953,1,"\n\n--Main comment\nAbout the deep network case in Theorem 1, how $L$ affects the bound on ranks?"
5954,1," There is also the issue of keeping tracking of a large number of demonstration states in memory. This technique reminds me of some common methods in learning from demonstrations, such as those using GPs or GMMs, but the novelty of this technique is the fact that the subgoal mapping function is learned in an IRL fashion, by tacking into account the sum of surrogate rewards in the expert's demonstration."
5955,1, These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.
5956,1," From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified."
5957,1,\n\nThe paper is clear and very well written.
5958,1, You say that test set also contains obfuscated documents.
5959,1,\n\nMinor Comment:\nThis paper needs more proper proof-reading.
5960,1," More importantly, The heuristic proposed in the paper is interesting and promising in some respects"
5961,1,"""This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings."
5962,1,"  Even if you assume that you use the simplest possible acoustic model and/or an open source tool kit for the decoder,  these error rates are high (WSJ error rates are lower than 10%, not 16.7%)."
5963,1,\n* Where is Table 1?
5964,1, In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). 
5965,1,\np3. Table 1. Where do the choice of CL Junction densities come from?
5966,1,"""The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem."
5967,1, Please define what semantic attention is.
5968,1, \n\nDETAILED COMMENTS\n1. The mapping function $phi$ appears in Eq. (1) without definition.
5969,1, This could be problematic if the risks are unbalanced.
5970,1,". I believe this is true, but there is a lot of prior work on these, such as adding a temperature to the softmax, or using distillation, etc. None of these are discussed appropriately in the paper."
5971,1,"\n- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt."
5972,1,"  When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful."
5973,1, \u201cGen Image Encoder\u201d and repeat the rest of the proposed pipeline.
5974,1, Are you sure that you've run enough iterations to fully converge?(Fig 4 was still trending up for b1=64).
5975,1,"\nThough not technically difficult, the extension of GloVe to covariate-dependent\nembeddings is very interesting and well motivated."
5976,1,"\n- Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017?\n"""
5977,1,\n  The last task is real world -- 40 attribute classification on the CelebA face dataset of 200K\n  images.
5978,1, In Section 4.2.1 statements on resemblance and closeness to mean faces could be tested.
5979,1,"\n \nThe last paragraph of introduction discussed the possible applications of the proposed methods without any substantiation, especially neither citations nor any related experiments of the authors are provided."
5980,1,"\nOverall, although the result are not very surprising, the approach is well justified and extensively tested."
5981,1,"\n- The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text."
5982,1,"\n\nAlso, it's not clear from section 3.1 what inputs are used to generate the adversarial examples."
5983,1, Have the authors experimented with other variants (dropping the weight sharing in either or both of these steps)?
5984,1,"\n4) about section 3.2, again I didn't get whether the model needs RL for training."
5985,1,\n\nOriginality\nTo my knowledge the method proposed in this work is novel.
5986,1," In the paper, the authors concerned \""more accurate gradients\"" and \""faster convergence\""; their causality is very clear (the first leads to the second), and there is no causality dilemma."
5987,1,\n\nProposal: The first motivation is not clear.
5988,1, \n\n- This work also assumes that other cars in the vicinity can be simply observed without any perception uncertainty or even through occlusions.
5989,1,\ncons:\n 1. SVD is a standard tool for subspace and manifold analysis for decades of years.
5990,1,"\n1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?)."
5991,1," Also, without knowing intricate details about each of 17 tasks you mentioned it is really hard to make any judgement as to how significant is improvement coming from your approach."
5992,1,"\""\nThis is a somewhat peculiar statement."
5993,1, It is a reasonable idea and authors indeed show that it works.
5994,1," \n\nI like this work,"
5995,1,"""This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm."
5996,1,\n\nI think this paper would improve by demonstrating how time-aware policies can help in domains of interest (which are usually not time-limited).
5997,1,"\n6. In section 4, the authors talk about estimating the model uncertainty in the stopping point and propose a way to estimate it."
5998,1,"\n\nComments:\n1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer?"
5999,1,"\n\n- The variable \""Evidence\"" in equation (4) is never defined."
6000,1," However, the paper falls short in lacking of theoretical justification and convincing empirical results."
6001,1," Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice."
6002,1,I do not see why there's need for a proof for the matrix completion result. 
6003,1," for example, the DiscoGAN results have some artifacts but capture the texture better in row 3."
6004,1,"In my mind this creates a chicken-and-egg issue: how to get the data, to learn the right Path function which does not make it impossible to still reach optimal performance on the task at hand? "
6005,1, You have no obligation to use this notation but it does make reading of your paper much easier on others in the community.
6006,1," For certain transfer functions, the authors perform a numerical integration to compute the resulting kernels."
6007,1, Theorem 3 provides conditions under which the multiplication X*Y is a locally open map.
6008,1,"\n- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described."
6009,1, We refer to this game as the Pong\nPlayer\u2019s Dilemma (PPD).
6010,1," the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al., "
6011,1,"""This study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a 3-way factorization thereby identifying spatial and temporal modules as well as their signed activation."
6012,1,"\n\nThe proposed architecture combines a few recent DL techniques: residual networks, dilated convolutions and gated linear units."
6013,1," However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic."
6014,1," However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function."
6015,1," However the exposition needs to significantly improve for the paper to be ready for ICLR."""
6016,1," Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%)."""
6017,1, The authors further propose model-pruning optimizations which are aware of the persistent implementation.
6018,1,"\n\nIn summary, I d like to encourage the authors to further investigate into their approach, but I am not convinced by the manuscript in the current form."
6019,1," Similarly, parts of Section 4.2 seem to follow directly from the previous discussion."
6020,1," \n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality."
6021,1,\n\nA couple of other questions:\n- I couldn't find any mention of eligibility traces - why?
6022,1," \n\n============================================================================================\n\nThis paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning."
6023,1," In particular, the authors adopt the Wasserstein distance to define the ambiguity sets."
6024,1,"""This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)."
6025,1,"\n\n- I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt."
6026,1, \n\nI like the idea of trying to formulate the feature learning problem as a two-player min-max game and its connection to boosting.
6027,1, Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10.
6028,1, The fact that additional conceptors can be trained does not appear new for the approach described here.
6029,1," Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm). "
6030,1,"""After reading rebuttals from the authors: The authors have addressed all of my concerns."
6031,1," Besides, what is the effect when having more and more support images?"
6032,1," This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks."
6033,1," \u201cSemi-Supervised Learning with Deep Generative Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.\n\n[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. \u201cDeep Variational Canonical Correlation Analysis.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454."
6034,1,\n[e] \u201cDo DRL-based navigation algorithms really 'learn to navigate'?
6035,1,\n\nOverall the paper feels a little bit incomplete .
6036,1,"""This paper proposed a class of control variate methods based on Stein's identity."
6037,1,\n\nPro:\n-            The topic is timely and of relevance to the ICLR community continuing a current trend in building meta-learning system for few-shot learning.
6038,1," Say, show the affinity matrix between the words and the objects to indicate the correlations."
6039,1,"\n- In Theorem 2, when does the exponential tail bound assumption hold?"
6040,1,\n\n\n* The claim that this paper is about learning from demonstration is a bit questionable.
6041,1,"\nDean et. el., 2017 also pursued this direction by combining system identification with robust controller synthesis to handle estimation errors in the system matrices (A, B) in the state-feedback case (LQR), and I can see that Dean et. el. could be extended to handle observer-feedback case (LQG) without any restriction."
6042,1," It's surprising that it works at all, but ultimately doesn't reveal a big scientific finding that could be re-used."""
6043,1,"In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms."
6044,1,  it'll interesting to see how the proposed approach gets around this issue.
6045,1,"""The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of \u201chard\u201d functions that are not easily representable by shallower networks."
6046,1,"""This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a \u00ab backward \u00bb update (i.e. from end to start of episode)."
6047,1," \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction."
6048,1, The encoder is then used as a feature extractor for classification and regression of targets (e.g. heard disease).
6049,1," So, I wish to see a section on testing with Resnet and GoogleNet."
6050,1," Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level."
6051,1, This focus is mainly on medical image classification but the approach could potentially be useful in many more areas.
6052,1,"\n\nTo be more specific, the choices of \\hat{Q} in (15) and \\hat{V} in (19) are not clear."
6053,1,\n \n- Theorem 2.2 there is no discussion about the nature of the saddle point is it strict?
6054,1, It looks as if the improvement comes from adding additional capacity to the model.
6055,1,"\n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work."
6056,1, The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. 
6057,1,"\n\nOverall, I would like to see if the paper could have been stronger empirically."
6058,1, I feel that the authors should give a more prominent disclaimer to potential users of the test.
6059,1,"\n\n- How the MDP M and options are defined, e.g. transition functions, are tochastic?"
6060,1," The authors propose a differentiable counting component, which explicitly counts the number of objects."
6061,1, but not quite convincing.
6062,1," \nMoreover, in the experimental setting, the games are not similar but simply the same."
6063,1," The original min/max formulation of the WGAN aim at minimizing over all measures, the maximal dispersion of expectation for 1-Lipschitz with the one provided by the empirical measure."
6064,1, I recommend the paper for acceptance and expect it will garner interest from the community.
6065,1," If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)"
6066,1,\nThis is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision.
6067,1, Is it because the total number of training images drops?
6068,1,\n\nOverall: The challenges proposed in the DuoRC dataset are interesting.
6069,1,. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers
6070,1,"  That has have to be named after the EM process, given that EM is unsupervised."
6071,1, They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT.
6072,1,"\n\nThe main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results."
6073,1," However, all compression methods such as pruning and quantization also have this concern."
6074,1,"\n\nIn addition, I don't see at all why this discrepancy is a discrepancy between training and testing data."
6075,1, It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.
6076,1," Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C]. "
6077,1,  The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle.
6078,1, \n\nThe proposed DAuto is essentially DANN+autoencoder.
6079,1," Moreover, the experiments are weak in two senses: (i) there are lots of obvious open questions that should have been explored and closed, see below,"
6080,1," Although the focus is apparently different, these methods are clearly closely related"
6081,1, I had to read a lot of sections more than once and use details across sections to fill in gaps.
6082,1,  Or is it designed to still classify adversarial examples in a robust way?
6083,1,"\n\n4 In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting.[[CNT], [null], [DIS], [MIN]] How is this happening?[[CNT], [null], [QSN], [MIN]] Can you elaborate more?[[CNT], [SUB-NEU], [QSN], [MIN]]\n\n5. The learning framework is not explained in a precise way."
6084,1, Additionally for their experiment authors use the SUMO top view driving simulator. 
6085,1,  Combining policy gradient and Q-learning.
6086,1," Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out."
6087,1, 2) Uniform convergence of the empirical risk to population risk.
6088,1," \n\n\nGenerally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations."
6089,1," However, I was hoping to see a direct comparison between FP16 and INT16. "
6090,1,"  However, the metric that using a pre-trained style classification network has issues in terms of evaluating how well the original content is preserved."
6091,1," Additionally, there is a paragraph of \u201carchitecture improvements\u201d which also are minimal changes.[[CNT], [EMP-NEG], [CRT], [MAJ]] Based on the title of section 3, it seems that there is a novelty on the \u201cprediction with flow\u201d part of this method."
6092,1," For example, \""Transferability analysis\"" in section 1 is barely understandable;"
6093,1,"If there's a gap, it suggests that your NN approximation might have not been that accurate."
6094,1,"\n- What is the average number of sentences per document?[[CNT], [CNT], [QSN], [MIN]] It's hard to get an idea of how reasonable the chosen truncation thresholds are without this."
6095,1, \n\nThe paper is hard to evaluate.
6096,1,\nImprovements on top of Dual-AC with ablation study show improvement.
6097,1,The mode collapse issue is never discussed elsewhere in the paper. \
6098,1,"  \n- Top of page 5, when you describe the checkerboard BFS: please include a visualization somewhere, it could be in the Appendix.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Section 6: there is lots of math here, but the main results don't obviously stand out."
6099,1,"\n2. generate new synthetic data, assume this data does not leak private information"
6100,1,  \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization.
6101,1,"\"" Psychometrika 23.2 (1958): 111-136.\n[4] Zhang, Cheng, Hedvig Kjellstr\u00f6m, and Carl Henrik Ek. \""Inter-battery topic representation learning.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n\n"""
6102,1,"t.\n\nAlso, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean."
6103,1, but it doesn\u2019t explore them yet in detail.
6104,1," However, the motivation of the analysis of this paper is unclear."
6105,1,"  Secondly, it views SGD with different settings as introducing different levels of noises that favors different minima."
6106,1," However, the practical utility, significance, and novelty of this work -- in its current form -- are questionable, and the related work sections, analysis, and experiments should be significantly extended."
6107,1, Where does this ground truth come from?
6108,1,"""Overall strength:\nIn this paper, the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions."
6109,1, Or how to figure out a coordinate group?
6110,1, Which versions of the algorithm (explicit/implicit) were used for which experiments ?
6111,1,"The authors should use the same hyper-parameters for all methods (Jaderberg, Zhang, Rank selection)."
6112,1,"\n\nSecond, from a purely methodological point of view, STANs boil down to learn the optimal linear combination of the input sensors."
6113,1," They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. "
6114,1," However, I wish the authors would have done it more properly."
6115,1,"\n\n(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs."
6116,1,"\n\nCons:\n- Methodology of the paper is very incremental compared with previous models.  \n- Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper."
6117,1, \nHave you checked your model using different length vectors?
6118,1,\n- Paper is clearly written
6119,1," 2) Although the problem itself is interesting,"
6120,1, How good are the trust region updates based on q_t given the huge variability associated with the mini-batch operation? 
6121,1,"  They observe better performance for their approach in comparison to ADDA, improved WGAN and MMD, when restricting the discriminators to be a linear classifier."
6122,1,"  \n\n- The authors write \""\\alpha_i W c_i does not explicitly depend on the target word t."
6123,1,?\n- Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)?
6124,1, The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output.
6125,1,". \n\n * \""The gradient decent approach required roughly 150 iterations to converge where as the simulated annealing approach needed at least 800.\""\nThis is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach."
6126,1,"\"" Nevertheless, I find the use of these terms confusing in the context of the proposed approach, since they are commonly used to denote concepts different from those represented here. "
6127,1," Second, the latter half of the paper is concerned with details of the experimental results, without offering any insights as to the implications for deep learning."
6128,1," Of course having reduced number of\nparameter updates is useful, but it's difficult to tell how big of a win this\ncould be."
6129,1," Captions also run into the text in a few places.\n"""
6130,1," As a consequence, those two methods need to take as input a new reward function for every new map."
6131,1," For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation. "
6132,1," Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better."
6133,1, Is it to learn a continual learning solution?
6134,1," Hence, the novelty of this paper is limited."
6135,1,\n\nWhy were so many of the chosen datasets have so few training examples?
6136,1,"\n\nOn the negative side: \n\nThe manuscript is not written in a way that is suitable for the target ICLR audience which will include, for the most part, readers that are not expert on the entorhinal cortex and/or spatial navigation."
6137,1," The contribution for ICLR is rather minimal, unfortunately."
6138,1, \n\nThe authors do not discuss the gains of their best low-precision regime in terms of computation and memory.
6139,1,"""This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments."
6140,1," Since depth is the meaningful quantity here, CrescendoNet does not have an advantage over FractalNet in terms of layer number."
6141,1,  Observation 3 states some property of non non-deterministic coupling but the concept itself seems somehow to appear out of the blue.
6142,1," However, two such patterns would not be sufficient for 5x5 or 7x7 kernels."
6143,1,  How the off-policy version is implemented is missing.
6144,1,\n- We know finding options is the hard part about options\
6145,1, Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence.
6146,1,\n\nThe contributions of the paper are in parts surprising and overall interesting.
6147,1," Word Semantic Representations using Bayesian Probabilistic Tensor Factorization, EMNLP 2014\n\nMo Yu, Mark Dredze, Raman Arora, Matthew R. Gormley, Embedding Lexical Features via Low-Rank Tensors\n\nto name a few via quick googling."
6148,1,"\n\n\nAdditional comments:\nThe method is shown to converge faster compared to the baselines, however, it is possible that the baseline may finish training faster (the authors do acknowledge the additional computation needed in the backward RNN)."
6149,1, \nThe proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network.
6150,1, \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results?
6151,1, How about A and A2 here?
6152,1, The novelty would be improved with clearer differentiation from the Hazan 2017 paper.
6153,1, \n--I liked the uncertainty analysis.
6154,1," Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model. "
6155,1," As a user, I would be interested in the typical computational cost of both \""MCMC sampler training\"" and MCMC sampler usage (inference?), compared to competing methods."
6156,1,"""This paper presents an simple and interesting idea to improve the performance for neural nets."
6157,1," \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search"
6158,1,   \n\n    3. What is u is equation 3?
6159,1, \nFirst relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking.
6160,1,. Why are the cost functions non-stationary?
6161,1, It would it make more sense if the comparison was between VGG and ResNet or other\ndifferent deep structures.
6162,1,"\u201d. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf."
6163,1, They suggest that training on such signal can be beneficial when training deep models on complex perceptual input spaces.
6164,1,"Distinguishability in high dimensions is an easy problem (as any GAN experiment confirms, see for example Arjovsky & Bottou, ICLR 2017), so it's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries."
6165,1,\u201d\n[f] \u201cwe are the first to evaluate any DRL-based navigation method on maps with unseen structures
6166,1,I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)?
6167,1,\n\nThe evaluation is extensive and mostly very good.
6168,1, but this is an obvious missing step.
6169,1," Perhaps for these tasks, multi-class classification is not the most correct objective, and maybe the proposed regularization can help, but the motivations are not given. \n\n\n\n"""
6170,1," This is the core claimed contribution:\nempirical evidence that these strategies are \""equivalent\""."
6171,1,"""This paper deals with early stopping but the contributions are limited."
6172,1, This is an interesting property and it seems to be the major strength of TR over TT. 
6173,1," Yet, the authors demonstrate their approach in environments where the controlable dynamics is mainly deterministic (if one decides to turn right, the agents indeed turns right)."
6174,1,"\n- Uses same hyperparameters as original training, making the process of using this much simpler."
6175,1," But it\u2019s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR."""
6176,1,"Rather than jointly optimize the student and teacher (as done previously), they have form a coupled relation between the student and teacher where each is providing a best response to the other. "
6177,1," \nIn addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively."
6178,1,\n\nIn reviewing the paper the following questions come to mind:\n1) Is the false positive rate too high to be practical?
6179,1," \n\nSignificance \u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n"""
6180,1, The authors do not provide any analysis about what can be learned from this learning procedure.
6181,1,\n\n9. I do not understand author's\u2019 justification for figure-3.
6182,1, \n\nI would additionally like to see a few examples of the time series data at both the 5 minute granularity and the 15 minute granularity.
6183,1, One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the \nassigned pseudo-labels.
6184,1," The paper also states (Secs. 1 and 2) that the the network studied here is based on Hartono et al, 2015, with the main difference of the sigmoidal ouput layer being replaced by a softmax layer."
6185,1, \n\nOriginality:\nI'm pretty sure this is the first paper to tackle this problem directly in general.
6186,1, \n\nSummary:\nThe goal of the technique to involve human interaction in generative processes is interesting.
6187,1,"  In comparison, MagNet has shown to be effective against different confidence parameters."
6188,1,Please first of all make the figures much larger.
6189,1, One reason is convexity in W of the problem (2). Any other?
6190,1,"However, the results do not convince this reviewer to switch to using 'post-training'."
6191,1,"Unfortunately, there isn't much insight to be gained from them."
6192,1," More importantly, MagNet is able to defend the adversarial examples very well (almost 100% success) no matter the adversarial examples are close to the information manifold or not."
6193,1,\n\nUsing the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monotone.
6194,1," I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation)."
6195,1,"""This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation."
6196,1,\nIt is also not clear how exactly the equation 7 is evaluated.
6197,1,"  For instance, Borsa et al 2017 doesn't do inverse RL (as said in the related work section) but learn to perform a task only from the extrinsic reward provided by the environment (as said in the introduction)."
6198,1," Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text)."
6199,1,\n\nAnother issue is the discussion of the entropy regularization in the objective function.
6200,1, and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.
6201,1,The paper is well written and easy to follow.
6202,1," This notion does not seem to protect the privacy of every individual training example, in contrast to notions like differential privacy."
6203,1,"  \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n"""
6204,1," In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen."
6205,1," Also, the authors should provide a clear and standard description of the experimental setup for each experiment (e.g. which network, which dataset, which task/loss, which measure, etc.)."
6206,1, This is further conflated by fig7 which attempts to illustrate the quality of the learned value functions.
6207,1," Regarding the technical details, the reviewer has the following comments: \n\n- What's the limitation of this attack method?"
6208,1,"\"" Is this a known fact, or something that you observed empirically?"
6209,1,"\n\nOverall the paper appears rushed -- the acknowledgements section is left over\nfrom the template and there is a reference to figure \""blah\""."
6210,1,\n\nThe paper is very clear and easy to follow.
6211,1," I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures."
6212,1, Is it data dependent?
6213,1," By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space)."
6214,1, The SVM is trained for the final classification task at hand using the last layer features of the deep network.
6215,1," The CIFAR-10, STL-10, and SVHN results are disappointing."
6216,1, Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward?
6217,1,"  Even then, the authors approach selects the highest number of\nexamples (figure 4). "
6218,1,"""This paper introduces a number of different techniques for improving exploration in deep Q learning."
6219,1,They also analyze the effect of various span-identification steps and preprocessing steps on the performance.
6220,1," 2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. """
6221,1,\n\nSome minor points that I wonder about:\n - The heuristic against repeating trigrams seems quite crude. 
6222,1, Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem.
6223,1," Which is the real issue, noise or curvature?"
6224,1, but there seems many other things to play with.
6225,1," Unsupervised learning for physical interaction through video prediction. In NIPS, 2016."
6226,1,"  \n- To claim \""task-agnostic\"", you need to try to apply your method to other NLP tasks as well."
6227,1,"   For reference,  please see papers from Saon et al., Seide et al, Povey et al, Yajie Miao et al in various ICASSP, Interspeech and arXiv papers. Comparisons with weak baselines can significantly color the conclusions."
6228,1,"""The paper proposes a \u2019Cross View training\u2019 approach to semi-supervised learning."
6229,1, The experiments show the proposed approach is effective on 14 UCI datasets
6230,1, The idea seems interesting.
6231,1," There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) method of IWAE."
6232,1," This paper does not bring completely new perspectives for the task, but the contribution is valuable to the community."
6233,1,Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT).
6234,1,"  In effect, you have two view of a point in time, its past and its future and you force these two views to agree, see  (Blum and Mitchell, 1998) or Xu, Chang, Dacheng Tao, and Chao Xu."
6235,1,  An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym.
6236,1,"\n\n[1] Zoph, Barret, and Quoc V. Le. \""Neural architecture search with reinforcement learning.\"" ICLR (2017).\n[2] Baker, Bowen, et al. \""Designing Neural Network Architectures using Reinforcement Learning.\"" ICLR (2017).\n"""
6237,1,"  \n\nOne can imagine there might be scenarios where the local guidance rewards of this \nform could be problematic, particularly in scenarios where the expert and learner are not identical\nand it is possible to return to previous states, such as the grid worlds the authors discuss:"
6238,1,"\n\nQuestions and comments:\n\u2013 While an 85% compression rate is significant, 88% accuracy on MNIST seems poor."
6239,1,"\n\nRelated work: many related work in robotics community on the topic of task and motion planning (checkout papers in RSS, ICRA, IJRR, etc.) should also be discussed."""
6240,1, The paper also needs more clarifications in the writing.
6241,1, \na) How to set/learn the scaling parameter \\lambda_y and \\beta_y
6242,1, I wonder though if such a transformation could not be learned by the vanilla prototypical networks simply by learning now a projection matrix A_z as a function of the query point z.
6243,1," Similarly, since the KL loss is the same as XENT, why give it a new name?"
6244,1,\n\nWeaknesses:\n- Some aspects of the paper are not clear and presentation needs improvement.
6245,1," When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent."
6246,1, It also make some connections to random matrix theory.
6247,1,"\nIf rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper."""
6248,1," The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq. 2)."
6249,1, I think the goal is to identify which of the graphs a subgraph belongs to?
6250,1, \n--The experiments were well carried through and very thorough.
6251,1, Figure 3 illustrates the idea nicely.
6252,1, The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions.
6253,1," When positive initializations are enforced, the network can more or less mimic the BFS behavior, but never when initializations can be negative."
6254,1," Now, if we think of a mini-batch as being a batched version of single pattern updates, then clearly the effective step length should scale with the batch size, which - because of the batch size normalization with N/B - means \\epsilon needs to scale with B."
6255,1,  With high-dimensional hidden layers a Monte-Carlo estimate on the minibatch can be very noisy and the resulting estimation of MI could be poor.
6256,1,Is that Fig 8 @0%?
6257,1,"\n\nMinor comments:\n\n- 'in order of increasing abstraction', does the order of gender-> smiling or not -> hair color matter? Or, is male, *, blackhair a valid option?\n\n- "
6258,1,"  At the moment in the last section you mention \""We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication."
6259,1,"\n\nWhile the learned representations are successful for the two main performance tasks discussed above,"
6260,1," What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS."
6261,1," The proofs are clear, and seem correct on a superficial readthrough; I have not carefully verified them."
6262,1," From the adversary's standpoint, it would be easier to manipulate inputs than latent variables."
6263,1,  The main equation of Algorithm 2 merges into Algorithm 3.
6264,1, This is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach.
6265,1, More information on the experimental design would help.
6266,1,"\n- In the abstract, the authors should emphasize that the PIR model used in this paper is based on VGG features."
6267,1,\n\nThe architecture was described but not really motivated.
6268,1," \n- Top of page 8: \""it also have a\"" -> \""it also has a\"", \""when encountering larger dataset"
6269,1, Why is nodes at different stages with the same initialization problematic?
6270,1," As I suggested above, authors need more experiments to show the effectiveness of their approach.\n"""
6271,1, Results on this data might be more convincing.
6272,1,"\n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model)."
6273,1,"\nFinally, it would be nice to see where the algorithm falls short, and where there is room for improvement."
6274,1, Something is wrong unless the authors are using different beam widths in the two settings.
6275,1,"\n\nI have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper."
6276,1, At the very least a comparison to concrete would be nice.
6277,1," But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius?"
6278,1,\n\n1.  The result in Section 4.3 empirically showed that Trust Region Method could escape from sharp local minimum.
6279,1," In the latter case, how long were models trained?."
6280,1, \n\n- Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications.
6281,1," Some questions are not clear from section 3.4:\n1.[[CNT], [CNT], [CRT], [MIN]] Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it?"
6282,1," As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting."
6283,1," Also, the proposed methods aim to preserve the \""colorfullness\"" of a color."
6284,1,\n\n\nAdditional comments:\nThe paper needs a considerable amount of polishing.
6285,1, This seems similar to the visual domain where edge detectors emerge easily when trained on natural images with sparseness constraints as in Olshausen&Field and later reproduced with many other models that incorporate sparseness constraints.
6286,1,   I don't doubt that the authors saw the results they report.
6287,1," So I increased my score. \n"""
6288,1," For example, it\u2019s unlikely that the defender would ever know the attack network utilized by an attacker."
6289,1," A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations.\n\nDetailed:\n-\n"""
6290,1, Some of the experimental results\ndo a good job of demonstrating the advantages of the models.
6291,1,"""The paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consistent."
6292,1,"  Thus, the model also have a vector representation for pair of word and position in the region."
6293,1,And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space?
6294,1," To apply BFGS, one might have to replace the RELU function by a smooth counterpart.."
6295,1," In particular, there is no clear definition of problem formulation, and the algorithms are poorly presented and elaborated in the context."
6296,1," The relevance of the method to achieve a deeper sense of learning and performing more complex tasks is however unclear to me."""
6297,1," network morphism is not novel,"
6298,1," Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper."
6299,1,"\n\nCons:\n- Overall, the approach seems to be an incremental improvement over the previous work ResNeXt."
6300,1, The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems.
6301,1,"\n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques, "
6302,1, Why should we introduce the convolution stochastic layers?
6303,1,"  \nFinally, there are some minor issues here and there (the authors show quite some lack of attention for just 7 pages):[[CNT], [CNT], [CRT], [MIN]] \n-\tTwo times \u201cget\u201d in \u201cwe get get a decoding scheme\u201d in the introduction;"
6304,1,\n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice.
6305,1, Presumably phi is not actually 1-dimensional?
6306,1,\n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated.
6307,1, I am guessing the random walk transitions are the ellipsoids?
6308,1, 1) The arguments for using clusters instead of single sentences are questionable.  
6309,1,"  Besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures."
6310,1,"  This is an interesting objective but since no interactive experiments presented in this paper,"
6311,1,\n  My experience has been that most of the search space has very poor\n  performance and the objective is to find the small regions that work\n  well.
6312,1, I believe the following things would be interesting points to discuss / follow up:
6313,1," Though [Gupta et al.] proposed an ego-centric neural memory in the RL context, the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory, whereas [Gupta et al.] designed the memory specifically for predicting free space."
6314,1, The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy.
6315,1,"\n\nThe contribution from the RL perspective is limited, in the sense that the authors simply applied standard models to predict a bunch of labels (in this case, emotion labels)."
6316,1," As such, I am borderline on its acceptance."
6317,1,"\n\nThe work is described in sufficient detail including the experimental setups, data set, neural networks, and results."
6318,1,"\n- Figure 2: can you plot the various transition types (normalized, un-normalized, ...) in the plots?"
6319,1, The experimental results on counting are promising.
6320,1,"  If it is no longer a lower bound, what is the rationale behind maximizing it?"
6321,1," Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution."
6322,1," The paragraph on p. 2 is now written without do-notation (\""intervening Mustache = 1 would not change the distribution\"").[[CNT], [PNF-NEG], [CRT], [MIN]]  But this way, the statements are at least very confusing (which one is \""the distribution\""?).[[CNT], [CLA-NEG], [CRT], [MIN]] \n- I would get rid of the concept of CiGM.[[CNT], [null], [DIS], [MIN]]  To me, it seems that this is a causal model with a neural network (NN) modeling the functions that appear in the SCM."
6323,1,"\n* I wish the authors would show that they get a *useful* model eventually - for example, can this be used to denoise other images from the dataset?"
6324,1,"""The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training."
6325,1, The update steps describe gradient ascent.
6326,1, ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9)
6327,1,"""This paper proposes to re-formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear)."
6328,1,"""A large margin , end to end language model that uses a discriminative objective function is proposed."
6329,1,"""The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update."
6330,1," Therefore, it cannot contribute positively to the performance or ResNet."
6331,1, The discriminator then needs to decide whether this is a real corrupted measurement or a generated one.
6332,1,"\n\nWhile these simple tasks are useful for diagnostics, it is well-known that these tasks are simple and, as the author's suggest \""more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization-based policy algorithms.\"" "
6333,1," I could not understand in what sense is\nthis the center, and of what ? It seems that the max value has been subtracted\nfrom all the logits into a softmax (which is a fairly standard operation)."
6334,1,The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem. 
6335,1,\n1. Figure 1 could be improved using a concrete example like in Figure 6.
6336,1," \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table."
6337,1,"""This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices."
6338,1, It is hard to understand sufficiently well what the formalism means without more insigh
6339,1, For example the ordering in Figure 6 would be inverted if plotting as a function of sample complexity?
6340,1," Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences."
6341,1,n\n6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?\n\n7
6342,1,\n2. The proof part(Section 2.2) can be extended with more details in Appendix.
6343,1,", but don't actually show any results on such tasks.)    \n\n"
6344,1,\n\nThe architecture is competitive on SVHN and CIFAR 10 but not on CIFAR 100.
6345,1,\n\nI don\u2019t understand why the authors say the PATH function can be viewed as an inverse?
6346,1,  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.
6347,1," In this work, they explore the effect of using partial natural language scene descriptions for the task of disentangling the latent entities visible in the image."
6348,1," In this model, self-organising is a property of the hidden neurons' activation (eq. 1-3), and the training procedure is entirely supervised."
6349,1,"""Summary: This paper studied the conditional image generation with two-stream generative adversarial networks."
6350,1, Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction.
6351,1,  The method is evaluated only against their own VAE-based alternatives. 
6352,1, The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer.
6353,1,\n\n2. 'patters' -> 'patterns'\n\n
6354,1, I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.
6355,1,"""This paper builds on Zhang et al. (2016) (Understanding deep learning requires rethinking generalization)."
6356,1,\n\nThe paper  comes with a series of experiments to empirically \u00ab\u00a0demonstrate\u00a0\u00bb the conjectures. 
6357,1," Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish."
6358,1,"\n\nSecond, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network)."
6359,1,"""Hi, \n\nThis was a nice read."
6360,1," The overall framework could be relevant to multiple areas in graph analytics, including graph comparison, graph sampling, graph embedding and relational feature selection."
6361,1," \n\nThe spectral convolution methods have been applied to mesh data structures for about 5 years now, as stated in the paper as well [Bruna et al. 2013], [Defferrard et al. 2016], [Bronstein et al. 2017], [Li et al. 2017] ..."
6362,1,"\n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features."
6363,1," \n\nSecondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:"
6364,1,  This contribution is added to the logits coming from each agent.
6365,1," Intuitively, (1) is an easy result."
6366,1,"\n\nAt times, the experimental details are a little unclear."
6367,1,"""This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs)."
6368,1,\n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.
6369,1,"\"" \n\nNot true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct.(The term \""deep learning\"" was introduced to ML in 1986 by Dechter for something else.)"
6370,1,"\n\nIn addition, the experiments are too limited to support the claimed benefits from encoding uncertainty."
6371,1,"\n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control)."
6372,1,"\n\n7. The experiments in Section 3.2.3 were interesting, since they demonstrate that the model can be warm-started from a model that hasn\u2019t fully converged."
6373,1," \n(a)In section 4, how to use formal method (Ledig et al., 2016) to enlarge the portrait from 64x64 to 512x512 is unclear."
6374,1," It is a good incremental research,"
6375,1,"\n\nSecondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models."
6376,1, My main concerns are:\n\n\t- The adversarial objective and the stability objective are potentially conflicting.
6377,1, It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better)
6378,1," (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)"
6379,1,"  The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation."
6380,1,"  The manuscript would be much stronger if the authors compared their method to a more sophisticated baseline, for example having each agent be a simple Q-learner with no centralization or \u201cdeepness\u201d."
6381,1," For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling."
6382,1," Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge."
6383,1, How is the sequence length determined for reinforcement learning tasks?
6384,1,"\n\nSignificance: I believe that this work is quite significant in two different ways:\n1) \""Bayesian evidence\"" provides a nice way of understanding why neural nets might generalize well, which could lead to further theoretical contributions."
6385,1, and (ii) the results just aren\u2019t that good.
6386,1, If not we have to count the distillation frames as well.
6387,1," however, some the results do not clearly back up the claim yet."
6388,1,\n\nPros:\n-- SDR characterisation of the convolutional filters is interesting
6389,1, But all the other components seem to have been demonstrated previously in other papers.
6390,1," Because there are more than two lines in some plots, it can also hurt people that can\u2019t distinguish colors easily. \n\n"""
6391,1, What type of distributions of X will be a good example?
6392,1,.\n- Some of the used metrics can detect mode collapse.
6393,1, Please include your description of how your method can be extended to networks which do allow for skip connections.
6394,1, I would like to see the results on more datasets.
6395,1," There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies."
6396,1, I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset.
6397,1,"  It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator)."
6398,1," \nWhen used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label."
6399,1,"""In the centre loss, the centre is learned."
6400,1,"\n7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.\n\nOverall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). "
6401,1, It is useful for all followers of this paper to provide some observations about this point.
6402,1," Rather use something more standard, with well-known baselines, such as the taxi domain."
6403,1," However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise."
6404,1,"\n- In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control."""
6405,1, The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter.
6406,1," Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant."
6407,1,\n\nAnother concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported.
6408,1, Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR.
6409,1, The approach is illustrated by numerical experiments.
6410,1, The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples.
6411,1,  \n\nReachability: authors show that different ways of abstracting the state s into a vector encoding affect the performance of the system.
6412,1, \n\nThe paper has little to no content.
6413,1," By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework."
6414,1,"""This paper focuses on solving query completion problem with error correction which is a very practical and important problem. "
6415,1,"  \n- Since filter weight prediction forms the central contribution of this work, I would expect some ablation studies on the MLP (network architecture, placement, weight sharing etc.) that predicts filter weights."
6416,1," As usual, the label information was added as the input of generator and discriminator as well."
6417,1, \n\nThe writing of this paper seriously needs more work.
6418,1," \n\nThe citation (E, 2017) seems to be wrong, could the authors check it?\n"""
6419,1, A major example is the mathematical description of the methods.
6420,1," In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components)."
6421,1," However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used."
6422,1,The authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pairs.
6423,1,\n\nI am not convinced by the presented results for the following reasons:\n1) the paper introduces two concepts - the dense skip-connections and the multi-head attention.
6424,1,\n- Eqn 11 should use \\simeq.
6425,1,\n\n =Quality=\nThe authors seem to be experts in their field.
6426,1, Is any qualification task used?
6427,1," We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters."
6428,1,"   In Eq. 2, below, it actually seems words are embedded separately via C, although again the dimensions are not provided."
6429,1," Also, it could be interesting also to discuss how the results in Table 2 and 3 compare to human classification capabilities, and if that performance would be already enough for building a computer-aided diagnosis system."
6430,1,\n\nThe main problem with this paper is that the connections between the TLTL formulas and the conventional RL objectives are not made sufficiently clear.
6431,1, Do the \\rho values\nsomewhere treat these two quantities differently?
6432,1,"""The paper introduces a novel method for modeling hierarchical data."
6433,1, \n \nMy only concern is that the parameter delta (threshold used to determine in/out distribution) is not discussed much.
6434,1," But isn't this exactly what neural networks do \u2013 learn intelligent combinations of features optimized (in this case, via GA) for a downstream task?"
6435,1, but I think it would be good to also report WERs on the WSJ set in either case.
6436,1, It seems to rely on some mixing RW conditions to model the distinct graph communities.
6437,1, Is there any mistake in the description of the abstract?
6438,1, \n How is it implemented in practice? 
6439,1," In particular, the SRM method, which is the core of the paper, is not described properly, I am not able to make sense of the description provided in Sec. 3.1."
6440,1, I see this work as adding communication to improve the translation learning.
6441,1, I suspect that the model is also computationally efficient: can the authors report training time for different datasets?
6442,1, Surely one could\ndevise situations where VAE outperforms WAE.
6443,1,"\n5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by \""efficient\""?"
6444,1, This and / or the defense of training to adversarial examples is an important experiment to assessing the limitations of the attack.
6445,1," If I were a medical expert, I would not have a clue about how these results and models could be applied in practice, or about what medical insight I could achieve."
6446,1,\n - why is your definition of generalization that the test set distance is strictly less than training set ?
6447,1,\n\n+ The numerical experiments are encouraging
6448,1, In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al. and Naesseth et al.
6449,1, But I still wonder the effect of permuting the layers.
6450,1, Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs.
6451,1,  Is this where you artificially return an agent to a state that would normally be hard to reach?
6452,1,"\n\nGeneral Comments:\n- I think Sections 2, 3, 4 are too long, we only start getting to the results section at the end of page 4."
6453,1,"\n\n4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase \""computational chicken-and-egg loop\"", the organization and presentation of the whole manuscript are poor."
6454,1," But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative?"
6455,1, What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?
6456,1, There is some experimental evidence presented on how to resolve the tradeoff between too much noise (underfitting) and too little (overfitting).
6457,1," When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input."
6458,1," However, I have three main concerns:\n1) Presentation. The organization of the paper could be improved. "
6459,1,"\n\nI don't see how the German verb \""orders\"" inflects with gender..."
6460,1,"\n\n* Experimental Results\nFrom what I understood from the experiments, it seems that using the \u201ctwo-pass decomposition\u201d (i.e. projected gradient descent) is better than CP-ALS (gradient descent ended with a single projection step)."
6461,1,\n\nClarity:\nThe paper is clearly written
6462,1,  The size of the set of nodes might be meant.
6463,1,The paper is missing further centralized baselines.
6464,1,\n\nPros\n\nUseful extension of an important technique backed up by behavioural experiments.
6465,1,"\n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases. "
6466,1,"\n\nThe paper has some interesting contributions and ideas, mainly from the point of view of applications, since the basic components (convnets, graph neural networks) are roughly similar to what is already proposed."
6467,1,"\n \nTo conclude, while the general direction is interesting and the proposed method might work, the experimental evaluation is very poor, and the paper absolutely cannot be accepted for publication."""
6468,1, One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work.
6469,1,.\nThe key point is the definition of the transformations
6470,1,"  Like, what does it mean when we say, \u201cenvironment that the multi-agent system itself touches\u201d?"
6471,1,.\n\nstrength\n\n* The paper is mostly clear and easy to follow
6472,1," \n\n4. The \""CVT, no noise\"" should be compared to \""CVT, random noise\"", then to \""CVT, adversarial noise\"". The current results show that the improvements are mostly from VAT, instead of CVT. \n\n\n"""
6473,1," \n\nOverall, the paper is well-written, clear in its exposition and technically sound."
6474,1,\n- Writing seems to be rushed.
6475,1, The authors mention that word dropout can be considered as its special case which randomly drops words without any prior.
6476,1,"    \n \n\nUnfortunately, At this point, I do not see a sufficient contribution to warrant publication in ICLR."""
6477,1,"\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and J\u00fcrgen Schmidhuber."
6478,1, I still can not see how previous work by Balestriero and Baraniuk 2017 motivates and backups the proposed method.
6479,1," but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done."
6480,1,"\n\n3. Only one attribute can be \""manipulated\"" each time?"
6481,1," Both are based on the claim that adversarial examples lie outside a certain sub-space occupied by the natural image examples, and modeling this sub-space hence enables their detection."
6482,1,\n* On implementation - the authors mention using Tensorflow\u2019s auto-differentiation.
6483,1," Ideally, one should see the effect of learning with options (and not primitive actions) to fairly compare against the proposed framework."
6484,1,"  Some interesting results are obtained, such as \""enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures\"" ."
6485,1, Do they (partially) share the incentive or may have completely arbitrary rewards?
6486,1,"  \n\nSpecific comments\n---\n- I had some trouble following the notation in places, and I think this is due to a bit of sloppiness."
6487,1, I would like to see a plot of the sample energy as a function of the number of data points.
6488,1," Since there are only two baselines are compared to the proposed method, this claim seems too general to be true."
6489,1,\nEquation (3): I couldn\u2019t find the definition for H anywhere.
6490,1,  Is this a substantial fraction of the time of the games studied? 
6491,1," \n\nHowever, the formulation is very similar to \""[1] Semi-supervised Question Retrieval with Gated Convolutions\"" 2016 by Lei, and \""Deriving Neural Architectures from Sequence and Graph Kernels\"" which give theoretical view from string kernel about why this type of networks works."
6492,1, This is much smaller than the gap in Figure 5(a).
6493,1,"\n\nOn the negative side, from what I can tell, the authors don't seem to have introduced any fundamentally new architectural choices in their neural network, so the contribution seems fairly specific to mastering StarCraft, but at the same time, the authors don't evaluate how much their defogger actually contributes to being able to win StarCraft games."
6494,1," Further, Q-masking largely amounts to simply removing actions that are infeasible (e.g., changing lanes to the left when in the left-most lane), but is seems to be no more than a heuristic, the advantages of which are not evaluated."
6495,1," Specifically, a simple method such as \""background subtraction\"" can easily infer the mask (the outlying pixels which correspond to moving objects)  while simple tracking methods (see a huge literature over decades on computer vision) can allow to track these objects across frames. "
6496,1," Methods like neelakantan et al's multisense embedding, for example, which the work cites, can be used in some of these evaluations, specifically on those where covariate information clearly contributes (like contextual tasks)."
6497,1,"  Toward this end, the authors adopt a memory-network based approach."
6498,1, The PIB objective is new and different to the other objectives.
6499,1," On the other hand in sea quest and space invaders, where your method does worse, the l2 error is better."
6500,1,"  It is unclear if the simulator extends beyond a single straight section of highway, as shown in Figure 1. "
6501,1,"""This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length."
6502,1," Take for example the case where a particular transition (s,a,r,s\u2019) gets replayed multiple times in a row, the posterior about Q(s,a) might then become overly confident even though no new observation was introduced."
6503,1," but the paper has little scientific value.[[CNT], [CNT], [CRT], [MAJ]]  Below are a few suggestions to make the paper stronger."
6504,1,".\nB. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both"
6505,1," \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity."
6506,1,It is my understanding that Sens-FGSM is not trained to a particular iteration of the \u201ccat-and-mouse\u201d game.
6507,1, \n1) Is the network given a sequence of program/solution input?
6508,1," Also, it is always possible to simply add more paths to FractalNet if desired without increasing depth."
6509,1," even though the potential impact of this paper is limited because the training strategy proposed is (by deep learning standards) relatively complicated, requires tuning two additional hyperparameters in the initial value of \\lambda as well as the step size for updating \\lambda, and seems to have no significant advantage over just using skip connections throughout training."
6510,1,\nI find the paper interesting
6511,1," I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC."
6512,1,  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets.
6513,1,"\n   - It is mentioned that \""While their model unit-normalizes the output of every encoder, we found this to consistently hurt performance, so do not use normalization for fair comparison with our models."
6514,1," Inherently, the objective is discontinuous since # of bits is a discrete parameter."
6515,1, Why not adopt it in CrescendoNet? 
6516,1, The essential problems here are how to identify which states should be stored and how to retrieve memory during action prediction.
6517,1," Also, a comparison against typical auto-encoders (which uses another decoder networks, with weights possibly tied with the encoder networks) is missing.\n\n"""
6518,1,n- good problem for FA algorithm / well motivated
6519,1," Showing results with multiple hops (1,2,..) would be useful here."
6520,1,\n \n\u201cThis paper unifies both issues\u201d sounds very weird.
6521,1,"\n\n- The format of reference should be fixed in this paper."""
6522,1,"If fixed, what sizes were tried?"
6523,1," Herewith some important missing papers, which are the previous or current state-of-the-art."
6524,1, Authors perform experiments on two datasets: Asian Faces Dataset and ImageNet Large Scale Recognition Challenge dataset.
6525,1," However, there should be some discussion and reporting of that aspect as well."
6526,1, Can anything be said about this?
6527,1, I could imagine a network learning to ignore features of objects that tend to wander over time.
6528,1,\n\nClarity\nSome crucial aspects of the paper are unclear as mentioned above.
6529,1,"\n\nCons:\n\nI have some questions on the shallow/deep in the context of CNN, and to what extent the cyclic cost is not needed, or it is just distilled from the shallow training: \n\n- Arguably the shallow to deep distillation can be understood as a reconstruction cost , since the shallow network will keep a lot of the spatial information. "
6530,1,". \n\n- The experimental results are not properly presented, with many overlapping figures"
6531,1," This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world."
6532,1," A recent work, presented at CVPR this year also does multi-frame prediction featuring an adversarial loss and explicitly models and captures the full dense optical flow (though in the latent space) that allows non-trivial motion extrapolation to future frames."
6533,1, This makes training of such networks cumbersome.
6534,1, Sometimes I am very confused about what is being described.
6535,1, Humans and animal are not given task ID and it's always clear distinction between task in real world.
6536,1," However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states."
6537,1,"\n3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious."
6538,1, \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks.
6539,1, The experimental section could be organized better.
6540,1,"\n\n5. The paper, in general, needs to be polished."
6541,1,"\n\n* Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high."
6542,1, Please do not use color as the only cue to identify a curve.
6543,1,\n\n[Pros]\n- Interesting problem
6544,1, I have not seen this particular setup for processing meshes with neural networks in an autoencoder setting.
6545,1, The experiments demonstrate that the proposed algorithm are competitive with standard backpropagation and potentially faster if code is optimized further.
6546,1,"\n\n- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation?"
6547,1," However, the authors do not explain how this architecture can be used to do the domain adaptation."
6548,1,"  The algoroithm of STB should be briefly explained in Section 4.2."""
6549,1, Did the baseline (original model) reported here also use 50k? 
6550,1,\n\u2028Conclusion: I would suggest that the authors address the concerns mentioned above.
6551,1,"""The main idea is to use the accuracy of a classifier trained on synthetic training examples produced by a generative model to define an evaluation metric for the generative model. "
6552,1," However I have a few concerns:\n1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017)."
6553,1,"""This paper examines sparse connection patterns in upper layers of convolutional image classification networks."
6554,1,  The method is demonstrated to the generate better results than the baseline on a variety of datasets and noise processes.
6555,1, These priors are indirectly induced from the data - the example discussed is via an empirical diagonal covariance assumption for a multivariate Gaussian. 
6556,1,\n\nThe paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming.
6557,1, I won't repeat it here.
6558,1,"In summary, the proposed method is lack of novelty compare to existing methods."
6559,1, The baseline performance in MT is too low.
6560,1,"\n* For Table 8, the similarities are not striking."
6561,1, The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right. 
6562,1," \n\n** DETAILED REVIEW **\n\nOverall, this is a good paper."
6563,1,"""The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community."
6564,1," For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. "
6565,1,"/data with similar co-occurrence statistics, in order for your method to be appropriate?"
6566,1, The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption.
6567,1,  This problem does not seem to be really solved by this method.
6568,1, I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships.
6569,1, The other proposed components contribute less significant.
6570,1,"""Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks."
6571,1," The slight improvement may be achieved only by chance and be due to computational inefficiency, or changing a seed."
6572,1,"  For example, if it is due to overfitting, is there a correlation between performance and size of FNNs? "
6573,1, The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.
6574,1,\n\n\n4) Conclusion\nThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse.
6575,1,"""This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function."
6576,1, \n\nA few more minor comments:\n(i) How are you training a GP exactly on 50k training points?
6577,1, It is novel and should generate further research with respect to understanding its vulnerabilities more completely.
6578,1, It is not clear whether the GA optimisation takes place on the level of cross validation error estimation or within an internal validation set as it should have been the case.
6579,1,\n\nSome examples of specific issues:\n- the abstract is almost incomprehensible and it is not clear what the contributions are
6580,1,"\n\nI overall like the paper's theoretical results,"
6581,1,\n3. Experiments performed on public datasets.
6582,1," \n- Furthermore, I would reformulate the theorem."
6583,1,".\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach."
6584,1,\n2) The inclusion of ablation studies to strengthen the analysis of the proposed technique
6585,1, A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training.
6586,1,  What will happen if we restrict the copy mechanism to only copy from SQL table.
6587,1," Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections."
6588,1, It means that we multiplied by K the number of parameters in our model (K is the number of classes).
6589,1,"""This paper applies several NN architectures to classify url\u2019s between benign and malware related URLs."
6590,1," One aspect does does strike me as novel is the \""gated composition module\"", which allows differentiation of messages to other agents based on the receivers internal state."
6591,1,"  \n\n* The proof, despite being a trivial application of existing work, has obvious flaws."
6592,1,". I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent."
6593,1,\n\nI found it is very interesting that the emergence of these representations was contingent on some regularization constraint.
6594,1,  There should also be a comparison to a k-means solver in the k-means section as an additional baseline.
6595,1," This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue."
6596,1,\n\n- The computational complexity of this model shouldn\u2019t be neglected.
6597,1,\nThey show that it can be included as additional term in cost functions to train generic models.
6598,1," Comparable baselines, I believe, are regular LSTM/GRU whose inputs are randomly dropped out during training."
6599,1," \""One-hot\"" is the encoding of a categorical label."
6600,1,"\n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task,"
6601,1,"\n\n* The trade-off parameter \\gamma is a \""fiddle factor\"" -- how was this set for the lung image and MNIST examples?"
6602,1," They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. "
6603,1,"  I would have felt better about the results if there were reported results from other papers included here, instead of the authors' attempt to create a baseline from the given data, which may or may not (as we have seen) represent a strong enough baseline from which to draw conclusions."
6604,1, Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning.
6605,1,\n\nPost-rebuttal comments:\n\nI have revised my score after considering comments from other reviewers and the revised paper.
6606,1," In fact, the claim that deep layers don't learn useful representations is directly contradicted by the paper."
6607,1," By varying the prior distribution, the framework can incorporate both generative and discriminative modeling."
6608,1," The assumption given in the introduction is that softmax would not yield such a representation, but nowhere in the paper this assumption is verified. "
6609,1," \n\nOverall this feels like an cute hack, supported by plausible intuition but without deep theory or compelling results on real tasks (yet)."
6610,1, It is never explained how this reward is allocated even in the authors\u2019 own experiments.
6611,1, It seems this has not been implemented for comparison and that overfitting may come from this method missing.
6612,1," The neural networks in the experiments are shallow. \n"""
6613,1," but I don't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence."""
6614,1,.\n\n(1)\tThe SW-SC kernel (Figure 2 (a)) is an extension of the existing shaped kernel (Figure 1 (c)).
6615,1,"\n\nThe paper appears to be original, and the related work section is quite extensive."
6616,1,"   \n\n\nAnyway, I believe the paper is interesting and the authors are exposing interesting facts that might be worth to spread in our community, so I rate the paper as slightly over the acceptance threshold."""
6617,1," The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation."
6618,1, \n\nThe description of the neural net in Section 3.3 (bottom of page 5) is hard to follow on first read-through.
6619,1," The studied problem is interesting, and the paper is well-written."
6620,1, the main concern I have with this paper is novelty.
6621,1," Otherwise, it does not seem to be a practical application."
6622,1," From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods."
6623,1,"""In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods."
6624,1, The statement lacks scientific support.
6625,1,\n\n5) Argument on top of page 6 is incorrect as the global optima is data dependent and hence lemma 4.1 (which is for a fixed matrix) does not apply
6626,1,"\n\nSHOKRI, R., AND SHMATIKOV, V. Privacy-preserving deep learning."
6627,1,"\n\nThe first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge."
6628,1," On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further."""
6629,1," Moreover, the authors filter the \u201cnormal\u201d samples using those (p.7 top), which makes the entire exercise a possible circular argument."
6630,1,  Does this mean that the network can only be trained when a complete set of input-output examples is available (i.e. outputs for all possible inputs in the domain)?
6631,1,"\n\nThe results of MOS is very good,"
6632,1," In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal."
6633,1,", which is already quite popular by now and should be compared with."
6634,1," Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?"
6635,1,  and large number of classes.
6636,1,"\n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm."
6637,1," Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures."
6638,1," The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one."
6639,1,"\nThis is an interesting extension of the open-world paradigm, where at test time, the classifier has to identify images beloning to the C seen classes during training, but also identify (reject) images which were previously unseen."
6640,1,\n2. The authors made an essential assumption that all target samples have the same style embedding y*.
6641,1, Suppose SGD leads to a local minimum of the empirical loss.
6642,1," Seem to be rather orthogonal.\n"""
6643,1," Their architecture is a deep feedforward network where each layer takes as input two images: one corresponding to the original maze (a skip connection), and the output of the previous layer."
6644,1,"\n\nThe method seems to give significantly lower kernel approximation errors,"
6645,1," A possible improvement is to try other means for the embedding instead of the Euclidean one.\n\n"""
6646,1," Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions."
6647,1,  I apologize if some of\nthese comments have already been addressed in replies to other reviewers.
6648,1,"\no\t It is not clear what the \u2018four-layers strided CNN\u2019 is: its structure, its role in the system. How is it optimized?"
6649,1,  The authors present their approach and evaluate it empirically.
6650,1,"""(Score before author revision: 4)\n(Score after author revision: 7)\n\nI think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly."
6651,1,"\n\ne) En the definition of R_AC, I denoted by <k,l> the pair of nodes (k \\ne l)."
6652,1," For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x."
6653,1," \nFinally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix."
6654,1,\n\nThe main takeaway from the entire paper is not clear very much.
6655,1," These scaling rules provide guidance on how to increase the batch size, which is desirable for increasing the parralelism of SGD training."
6656,1, I'm concerned since the evaluation is only done on 100 points.
6657,1," However, this form is not appropriate for the case presented at this paper, since the method presented only models one of these PDFs (Specifically p(x | W1)  - there is not generative model of p(x|W2))."
6658,1," The appendix are very useful, and tutorial paper material (especially A)."
6659,1,"This low-dim latent space is first predicted from the residual of the (deterministic prediction - groundtruth), and then the low-dim encoding goes into a network that predicts a corrected image."
6660,1, The main framework is based on the Dawid-Skene model.
6661,1, There is no mention of computational cost.
6662,1,"Notably, the question of transfer\nand generalization is of high relevance here."
6663,1," However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date."
6664,1,"\n\nArguments in section 2.3 are weak because, again, all other grapheme-based end-to-end systems have the same benefit as CTC and ASG."
6665,1," Shortly\n  thereafter we are told that the generator quickly learns to produce norm 1\n  outputs as evidence that it is matching the encoder's distribution, but this\n  is something that could have just as easily have been built-in, and is a\n  trivial sort of \""distribution matching"
6666,1,"""This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision."
6667,1," While the approach (as has been shown in the past) is very reasonable,"
6668,1, This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map.
6669,1, The ideas and formalism of the merge and partition operations are valuable contributions.
6670,1,\n\nThis paper was quite interesting and clearly written for the most part.
6671,1, I regrettably cannot recommend this paper for acceptance owing to these concerns.
6672,1, The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem.
6673,1,"""\n-I think title is misleading, as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to \u2026 deep LINEAR networks"
6674,1, A very simple queue for the latter is shown to do quite competitively in practice.
6675,1,"\n\nThe present paper extends the above work to include the learning of a Mahalanobis matrix, S, for each instance, in addition to learning its projection."
6676,1,\n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance
6677,1," \n\n- It is not entirely clear to me how the proposed metrics, whose definitions all reference magnitudes of weights, generalize to the case of random forests. """
6678,1,"\n\nFinally, the task (bAbI) and baseline models (LSTM from a Keras tutorial) are too weak."
6679,1," It is well known that CNN features are much better if enough data is available.\n """
6680,1,"  Some explicit discussion of convolutional layers may be\nhelpful.  """
6681,1,  Please comment in the rebuttal and I would appreciate if the details of the synthetic PIR values on the training set could be explained.
6682,1,\n\nExperiments a are satisfying and show good performance when compared to other methods.
6683,1," In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate."
6684,1, The authors should also define (maybe in appendix) the conditional and unconditional inception scores and why they are important (and why only some of them are computed in Table 1).
6685,1,"\nMoreover, this paper is only cited in the experimental part, while the\ncontribution should be far more emphasized by the authors."
6686,1,". The proposed combination is straightforward,"
6687,1,  Such datasets are too simplistic.
6688,1," Finally, the authors\nsay toward the end of Section 2 that \""A careful comparision shows that this\napproximation is precisely that which is implied by equation 4, as desired\"". "
6689,1,"\n1. It is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  Euclidean metric (e.g. the problem depicted in Figure 1),  because they are not trained to do so: their purpose is just to predict the correct label."
6690,1," \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem,"
6691,1," Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \""a significant improvement over previous work\"" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors)."
6692,1," \n\nAlso, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading."
6693,1," Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied."
6694,1,. It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately
6695,1,"""The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs."
6696,1," To combat this problem, the authors suggest modifying the state representation of the policy to include an indicator of the amount of remaining time. "
6697,1," However, the relative entropy term added seems like a marginal modification."
6698,1," From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form."
6699,1," While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm."
6700,1,"\n\nOriginality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model."""
6701,1,   - eq12 should be x''=...\n-
6702,1, How does the proposed method handle large images?
6703,1," but it's also really valuable, because it's much more close to real world usage of language models."
6704,1," \n\n# Conclusion:\nIn conclusion, while interesting, for me the paper is not yet ready for publication."
6705,1,\n\nMain Comments:\nThe paper is very well written and clearly states and explains the contributions.
6706,1, It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method.
6707,1,"\n\nHowever, there are also some issues, many of which have already been raised:\n- The poor performance of the DDQN baseline is concerning and does not seem to match the behavior of prior work (see Pong for example)."
6708,1,"""This paper misses the point of what VAEs (or GANs, in general) are used for."
6709,1," Performing such an analysis with this model is challenging (i.e. retraining a GAN) and it is not clear if a given image generated by a GAN will always achieve a given epsilon perturbation/\n\nOn a more minor note, the authors suggest that generating a *diversity* of adversarial images is of practical import. "
6710,1," Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow."
6711,1,"""Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc."
6712,1,  only a few works tackle the problem of learning with spikes showing the potential advantages of such form of computing.
6713,1,"\n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1)."
6714,1, Here the embedding of word i is combined with the elements of the context units of words in the context.
6715,1, But what's the regularizer that's vanishing?
6716,1,\n+/-  this work builds largely on previous work.
6717,1,"  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features."
6718,1,"  It seems that this training process follows a projected gradient descent procedure, where the filter weights of the network are iteratively updated using regular (stochastic) gradient descent and then they are projected onto the set of rank-R tensors."
6719,1, \nf)\tIsn\u2019t learning the random walk sample path a much harder / higher-dimensional task than it is necessary?
6720,1,"  The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c."
6721,1,\n\nOverall the paper was well written.
6722,1,"  Hence, using ensembling in deep networks is not a significant contribution."
6723,1,"\n\n\n[1] Thoma et al: \""Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics\"", 2017."
6724,1, Does the model try to reconstruct the state of the current time-step or the future?
6725,1, The learning objective is not clearly defined.
6726,1,"\n\nComparison with predicted parses by Spacy are by no means \""gold\"" parses..."
6727,1, The paper uses this view to improve the MAML algorithm.
6728,1," \n- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.\n"""
6729,1,"  \n\nOverall, this paper does seem to identify a concrete problem, and I liked the use of explicit aspect embeddings for sentiment analysis."
6730,1,"\n\n- To achieve the goal above, huge data (not the \""portion of the general English\"") should be communicated over the network."
6731,1,\n\niii. Although the formal discussion is concerned with Markov games (i.e. repeated games with stochastic transitions with multiple states) the experiments (particularly the PPD) appear to apply to repeated games (this could very much be cleared up with a formal description of the games in the experimental sections and the equilibrium concept being used).
6732,1," Then, this graph is processed with an graph neural network framework that relies on modelling the differences between features maps, \\propto \\phi(abs(x_i-x_j))."
6733,1," Why a diagonal covariance has been estimated, and not a full covariance one?"
6734,1, Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place).
6735,1," From what I can see, there is barely new that is proposed by the paper."
6736,1,  This could be a way to decrease the number of CG iterations that must done at each step.
6737,1, What happens if you set the threshold using one out of distribution dataset and then evaluate on a different one?
6738,1,"""[Reviewed on January 12th]\n\nThis article applies the notion of \u201cconceptors\u201d -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks."
6739,1," \n\nPros: \n\n- The paper is clearly written, self-contained and a pleasure to read."
6740,1, The example images look convincing to me.
6741,1," \n\n** REVIEW SUMMARY **\n\nThe paper reads well, has sufficient reference."
6742,1,"\n\nOne empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3])."
6743,1,"Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? "
6744,1,"\n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected."""
6745,1,"\n\nThe experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment."
6746,1," The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution."
6747,1," In this vein, the experiments with DP-\u000fSGD are more interesting,"
6748,1,"\n\nIn summary, I'm both excited about the dataset and new architecture,"
6749,1,"\n\n\nReferences:\n[1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192).\n[2] Pad\u00f3, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199."
6750,1, The approach is thoroughly validated using two online behavioural experiments.
6751,1," This sort of sentence is clearly wrong and for many separate reasons:\n    - Parameter noise exploration is not a separate/new thing from RL... it's even been around for ages! It feels like you are talking about DQN/A3C/(whatever algorithm got good scores in Atari last year) as \""RL\"" and that's just really not a good way to think about it."
6752,1,"\n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? """
6753,1,  \n\nThe idea is interesting and to my knowledge novel.
6754,1,"\n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part."
6755,1,"""The paper presents a new CNN architecture: CrescendoNet."
6756,1," \n\nExperiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain."
6757,1," The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods."
6758,1,   My intuition is that this simpler approach would work better.
6759,1," At some point, we will have to use insights from this search to stop early, when no improvement is expected."
6760,1, There is too much pre-training. I find this less elegant. 
6761,1," Also, authors claim that their method consume less computation time than reinforcement learning."
6762,1, but why is it worse on the other 6 categories?
6763,1,"""This paper proposed an approach for detecting adversarial examples using saliency maps."
6764,1," Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment."""
6765,1,"\n3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters."
6766,1,"\n\nOne fundamental limitation of the proposed approach is that although it is invariant to global translations, it does not have the built-in equivariance to local translations that a ConvNet has."
6767,1,"\n\nSecondly, the motivation of adding the rotation operation is not properly justified."
6768,1,  Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.
6769,1,"  The paper states the purpose of the meta learning is \""to learn a general word context similarity from the first m domains\"", but I was never sure what this meant."
6770,1,"""This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models)."
6771,1,"""The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM)."
6772,1,\nThe authors should mention this point.
6773,1,"\n4-\tDue to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting."
6774,1,"\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case."
6775,1,"\n\nOriginality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work."
6776,1, Does the discretization into strokes matter?
6777,1,"""This paper proposes a variant of hierarchical hidden Markov Models (HMMs) where the chains operate at different time-scales with an associate d spectral estimation procedure that is computationally efficient."
6778,1,"  The main contributions of the paper are to provide a method that converts a TLTL formula that specifies a task into a reward function for a new augmented MDP (that can be used by a conventional RL algorithm to yield a policy), and a method for quickly combining two such formulas (and their policies) into a new policy."
6779,1,\n\nTable 1 needs more discussion in terms of retained edge percentage for ordered stages.
6780,1,"""Summary:\n\nThis paper presents a new network architecture for learning a regression of probability distributions."
6781,1," Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known."
6782,1,\n\nI found confusing how to use the proposed method to obtain estimates of\nuncertainty for a particular test data point x_star.
6783,1,\nThey propose to have a predictor for deterministic information generation using a standard transformer trained via MSE.
6784,1, They describe their training procedure and their sampling approach for the gate weights.
6785,1," However, I still have concerns about this method not being shown to improve the best methods presented in Table 3 which would give a more solid result."
6786,1," For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer."
6787,1," In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results."
6788,1,"""The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd-order) PPMI tensor factorization. The paper chooses symmetric CP decomposition so that word representations are tied across all three views."
6789,1,"   \nSection 5.2 was nice and so was 5.3.[[CNT], [null], [APC], [MAJ]] However, for the covariate specific analogies (5.3.) the authors could also analyze word similarities without the analogy component and probably see similar qualitative results."
6790,1," \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined."
6791,1,"""My main concern for this paper is that the description of the Visual Concepts is completely unclear for me."
6792,1,\n\n-- Comparison with existing work: There has been a lot of work recently on one-shot and few-shot learning that would be interesting to compare against.
6793,1,\n\nGraph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter.
6794,1,". Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper,"
6795,1,"\"" It is not at all\n  clear to me why this would be the case."
6796,1,  How important are these two methods to the success of GTI?
6797,1," In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83). "
6798,1,".\n- Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way. "
6799,1, These works can benefit from the uncertainty analysis scheme introduced in this paper.
6800,1,  How do we treat the output of Tau as an action?
6801,1," My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]."
6802,1,"\n\nTwo propositions are made, (although I would argue that their derivations are trivially the consequence\nof the model structure and inference scheme defined), and experiments are run which compare the approach to maximum likelihood estimation for 'Y' using an equivalent stochastic network architecture."
6803,1,\n3. Section 4.1: You say that you don't train the recurrent matrix in the KRU version.
6804,1, The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step.
6805,1, The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution.
6806,1," The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs."
6807,1," \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results."
6808,1," In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper."
6809,1,"""The paper studies the theoretical properties of the two-layer neural networks."
6810,1," If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication."
6811,1,"\n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \""standard\"" or \""conventional\"" LSTM implementation (e.g., as provided in optimized GPU libraries)."
6812,1,"\n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial."
6813,1,"  This is fine, but could perhaps be pointed out if that is indeed the case."
6814,1,"\n\n1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples)."
6815,1, I don\u2019t see much novelty.
6816,1," Alternatively, some evidence that interesting exploratory behavior is obtained (in Atari or even smaller domain) would help convince the reader that the approach does what it claims in practice."
6817,1,"The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv"
6818,1, \n\nOverall I think that this paper is decent.
6819,1,"\n \n2 Related Work\n \n\u201cPOSG, a class of reinforcement learning with multiple ..\u201d -> reinforcement learning framework"
6820,1,"\nThey use a variational bound to deal with the relevance term, I(Z_l,Y), and  Monte Carlo sampling to deal with the layer-by-layer compression term, I(Z_l,Z_{l+1})."
6821,1,"\n\n   Q( s,a,g ) = g(S) Wa S + Ba\n\nThe subgoal is the same as the first part, namely a linear transform of the expected expert direction in \nstates similar to state S.\n\n    g(S) =  Wv   SUMj  < Wk S, Wk Ej >  ( Ej - Ej\u2019 ) \n\nSo in some sense, the Q function is really just a function of S, as g is calculated from S."
6822,1, \n\nComments on the Assumptions:\n- Please explain the motivation behind the standard Gaussian assumption of the input vector x.
6823,1, The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit.
6824,1,"\n\nBottom line: The paper may contribute to the current discussion of the Zhang et al 2016 paper, but I feel  it does not make a significant contribution to the state of knowledge in machine learning."
6825,1,\nThe idea is simple and it seems to work for the presented examples.
6826,1,"""The paper presents a multi-modal CNN model for sentiment analysis that combines images and text."
6827,1," The learning dynamics defined for the network results in specific update equations of the weights W (Eqn. 14), which combine elements of supervised learning and self-organizing maps (SOMs)."
6828,1,"""This paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next state\u2019s value."
6829,1,"\n\nMinor comments:\n- Figure 1: what is the difference between \""cost-sensitive loss\"" and just \""loss\""?"
6830,1, A rudimentary  experimental evaluation with small networks is provided.
6831,1,\n\nI will adjust my score based on the answer to these questions.
6832,1,\n\n- paper is overall well written/clear
6833,1,"""\nThe authors describe how to use Bayesian neural networks with Thompson sampling\nfor efficient exploration in q-learning."
6834,1," The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator."
6835,1, This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach.
6836,1," I think it's correct but may not scale well with dimensions."""
6837,1," The \nempirical validation, while being limited in some aspects, is largely convincing."
6838,1,. It is clearly written (there are some typos / grammar errors).
6839,1, It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI.
6840,1,"  It uses support vector regression to show that a\nrelatively small number of samples of hyperparameters, architectures,\nand validation time series can lead to reasonable predictions of\neventual performance."
6841,1, The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.
6842,1," \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and"
6843,1, I feel this part is badly done in the paper.
6844,1,"  As in the example that follows, if the adversary wants to make anomalies seem normal at test time, it should move normal points outward from the normal point cloud (eg making a 9 look like a weird 7)."
6845,1,"""The papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density function."
6846,1,\n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al.
6847,1,"\n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss)."
6848,1, The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.
6849,1," IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I\u2019d appreciate a few examples where this is a case to better justify the method."
6850,1,  \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept.
6851,1," I suspect the results will still be meaningful, but the appropriate analysis is essential to be able to interpret the human results."
6852,1," The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM."
6853,1,\n\nThe paper is well written overall and relatively easy to understand.
6854,1," Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent."
6855,1,\n- Fairly good experimental results
6856,1,\n- No comparison with state-of-the-art techniques on the experimented tasks and datasets.
6857,1," If the error rates are different for different tasks, it is not sensible to measure raw accuracies."
6858,1,"""\n- Paper summary\n\nThe paper proposes a label-conditional GAN generator architecture and a GAN training objective for the image modeling task."
6859,1," However, the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantageous."
6860,1," Rainforth et al. suggest this approach, as well as the approach of averaging K^2 noisy estimates, which the theory suggests may be more appropriate if the functions involved are sufficiently smooth, which even for ReLU networks that are non-differentiable at a finite number of points I think they should be."
6861,1, Could the authors also report these?
6862,1,  The authors test their method on various adversarially constructed inputs (with varying degrees of noise).
6863,1,  And even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model.
6864,1,"\n\nWith regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks."
6865,1," With various agreement measures, it removes or merges edges and count the final nodes."
6866,1," \n- In table 4, for example, it would be nice to see the performance on the different emotion categories."
6867,1," \n\nIn Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem."
6868,1," This is typically described as finite horizon MDP planning and learning and the optimal policy is well known to be nonstationary and depend on the number of remaining time steps.[[CNT], [null], [DIS], [GEN]] There are a number of papers focusing on this for both planning and learning though these are not cited in the current draft."
6869,1, Does the average iteration error behaves differently in case of a tanh-RNN ?
6870,1," It seems like a more likely explanation is that the decoder doesn\u2019t have to work as hard to memorize the training set, so it has some extra freedom to make the true posterior look more like a FFG."
6871,1,"\n\nAccording to the authors the core claims are:\n\""1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation."
6872,1," Baring theoretical results, you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directly."
6873,1," In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time."
6874,1," By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies)."
6875,1,"\n* The very recent paper by Krishnan et al. (posted to arXiv days before the ICLR deadline, although a workshop version was presented at the NIPS AABI workshop last year; http://approximateinference.org/2016/accepted/KrishnanHoffman2016.pdf) examines amortization error as a core cause of training failures in VAEs."
6876,1,\n- Best to clarify what the weights in the weighted sum of Natarajan are. 
6877,1,"   Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient"
6878,1,. It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening.
6879,1,"(Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD."
6880,1,"\n- cramer->Cramer\n- wasserstein->Wasserstein (2x)\n- gans-> GANs\n- Salimans et al. is provided twice, and the second is wrong anyway."
6881,1,\n\nBiggest question:\n - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge?
6882,1, \nThe method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer. 
6883,1, The authors also present an analysis of the data by applying one of the SOTA techniques on SQuAD to this data. 
6884,1, This is not because the proposed model is technically difficult to understand.
6885,1," But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up."""
6886,1," which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases."
6887,1,\nThe paper already mentions about this direction and it would be interesting to see the experimental results.
6888,1," The right part of the figure should be better described in the text or in the caption, I don't understand well what this illustrates."
6889,1,"  \n\nAlthough the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks."
6890,1, (or show that it can be ignored?)\n\nIt appears that \\hat{Q} and \\hat{V} are parameterized independently from Q (which is a function of theta).
6891,1," Intuitively, the authors' method should scale\nbetter, but they fail to show this -- a missed opportunity to make the paper\nmuch more compelling."
6892,1," \n\u2028(TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader."
6893,1,\n\nPros:\n- Jointly optimizing forming of groups and placing these seems to have merit\
6894,1, \n\nIt would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.
6895,1," Given this conclusion, I would have expected a discussion about \nthe difference between learning with all data at the same time or with fine-tuning in two different \nsteps."
6896,1," They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results."
6897,1," The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network."
6898,1,"  To solve this problem, the authors proposed five formulations in the final prediction layer."
6899,1,  Is that a general suggestion or a data-set specific recommendation?
6900,1,"\n\n* \""...self-consistent equations are highly non-linear and still too abstract to be used for many...\"", presumably what was implied was that the original solution to the information bottleneck as expressed by Tishby et al is non-analytic for most practical cases of interest?"
6901,1, What is the variance in the numbers for Table 1?
6902,1," More evaluations are needed to verify the method, especially with natural images."
6903,1,"\n - what is a \""pushforward measure\""?"
6904,1,\n\nI am quite a bit concerned about the experimentation protocol as well.
6905,1,"  Identifying some weakness in the formulation, the authors propose 5 solutions."
6906,1, \nHowever I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved).
6907,1,"  For example, adding PWL to Theorems and Corollaries in Section 3.1 will help. "
6908,1,"\n\nAlthough performance is still reduced compared to single task learning in some cases,"
6909,1,"""This paper addresses the problem of one class classification."
6910,1, The good results obtained support the design decisions made.
6911,1,. I don't see any reason to exlude these
6912,1," For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported."
6913,1, What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted?
6914,1, The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions. 
6915,1, So I will focus on the omniglot experiments.
6916,1," Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions."
6917,1, The authors suggest computing a similarity matrix amongst the tasks.
6918,1," Therefore, empirically, it is really hard to justify whether this proposed method could work better."
6919,1," \n\nPage 5: \u201cnote that we do not suggest a specific neural network architecture for the middle layers, one should select whichever architecture that is appropriate for the domain at hand\u201d - such as?"
6920,1,"  On these insights, the paper felt\nincomplete."
6921,1,\n\n3. The authors did not perform any ablation study.
6922,1," \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric."
6923,1,"\nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained."
6924,1," To evaluate the first two, the authors use estimates (lower bounds of log p(x)) given from annealed importance sampling and the importance sampling based IWAE bound (the tighter of the two is used)."
6925,1,"\nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016)."
6926,1,  So I'm not really sure what is being added here.
6927,1," As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda."
6928,1,"\n* Perhaps counterintuitively, experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training."
6929,1," The weight matrix is maintained normalized, which helps accuracy."
6930,1,  The intuition given\nis not sufficient to substantiate some of the claims on generality and understanding\nof graph based DL.
6931,1, but the significance of the performance difference in downstream ML tasks is unclear
6932,1," A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. \n"""
6933,1,"""This paper surveys models for collaborative filtering with user/item covariate."
6934,1,\n\nOriginality:\nThe originality is weak.
6935,1," Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina."
6936,1," But the main interest in matrix completion is in the undersampled case \u2014 in the full observed case, there is nothing to complete."
6937,1,"\n\n\nExperimental evaluation\nThe experimental evaluation uses 2 datasets, MNIST and EMNIST, both are very specific for character recognition."
6938,1,". Unfortunately, this statement raises more questions than it answers"
6939,1, Maybe you could motivate your choice at this point.
6940,1, It would be good to show whether part of its efficiency comes from effective image-guided navigation: does a partial image match entail with targetted navigation (e.g. matches in the right side of the image make the robot turn right)?
6941,1,"\n\n(extremely minor typo: \""One popular possibility from L2S is go the full reduction route down to binary classification\"")"""
6942,1,  Some interesting results on the development set show the importance of things like warm starting on large language model training data.
6943,1," The new section \""intuitive analysis\"" is very nice."
6944,1,"\n\nFinally, I found the experimental evaluation to not thoroughly demonstrate the\nadvantages and disadvantages of the proposed algorithm."
6945,1," The proposed addition of a new loss function for this purpose is an obvious choice, not particularly involved."
6946,1,"  \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling."
6947,1, They additionally use their embeddings to decrease the search time for the Sarfgen program repair system.
6948,1, Besides L_1 is not well defined.
6949,1,"\n\nMy concerns are as follows:\n1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task."
6950,1," The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results."
6951,1," While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline."
6952,1, I have a number of objections to it.
6953,1," More state-of-the-art baselines are needed, e.g. [1] and [2]."
6954,1,"  Of course, negative results are of value, but it doesn't seem like much is at stake in this work to begin with."
6955,1,". If not, section 2.1 should contain several other works (as in Table 1). \n\n4"
6956,1," The paper does a good job of explaining the connection,"
6957,1," Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?"""
6958,1," There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller \""effective\"" d', you only have to figure out a generating system for this subspace and carry out optimisation inside)."
6959,1," For example, Entnet is able to reason as the input is fed in and the decoding costs are low. "
6960,1, The paper is clearly written and easy to understand
6961,1,\n4) The performance of the technique is reasonable enough to actually be used.
6962,1,\nI also have some concerns regarding the claim that \u201cWe confirm that optimization with the framework of NaaA leads to better performance of RL\u201d.
6963,1, Would be good to know if the authors have any intuition why is that the case.
6964,1,"\nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers."
6965,1, My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way.
6966,1,Mentioning of failures and limitations demonstrates a realistic  view on the project\n\t\u2022\t Complexity and time analysis provided\n\t\u2022\t
6967,1,"   In Hazan's paper they mention that the system id portion, at least, seems to work with non-symmetric, and even non-linear dynamical systems (bottom of page 3, Hazan 2017)."
6968,1,"n\n2. Regarding the partial observability, each agent knows the location of all agents, including itself, and the location of all obstacles and charging locations; but it only knows the location of customers that are in its vision range."
6969,1,"\n\nNor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix. "
6970,1,"\n\nMore generally, putting the biological arguments aside, why would a 2D neighborhood relationship be helpful?"
6971,1," A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor."
6972,1, Another problem is of course the assumptions required for the proof that typically don\u2019t hold in practice (see comment below).
6973,1," Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm."
6974,1,Is it scalability?
6975,1,\n\nSome minor comments:\n- 2.1: We introduce --> We discussion\n- Pieczak 2015a did not propose the extraction of MFCC.
6976,1,"  In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1."
6977,1," \n\nEmpirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks."
6978,1,The present paper proposes to obtain mixed strategy through an online learning approach.
6979,1,. This is not a very challenging task.
6980,1," Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n"""
6981,1, GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.
6982,1,"\u2028In addition to that, one could fine-tune the representation during forward model training."
6983,1, \n\nA first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in W distance rather than just trying to minimize the desired f-divergence.
6984,1," For example, this wouldn't work for linear value functions!\n- I think the original bootstrapped DQN used \""ensemble voting\"" at test time, so maybe you should change the labels or the way this is introduced/discussed. "
6985,1, The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties.
6986,1,"\n\nOverall, I quite like this line of work,"
6987,1,"""The authors propose to speed up RL techniques, such as DQN, by utilizing expert demonstrations."
6988,1,"  E.g., we would also like to learn synonymy with light verb like \""take note\"" or \""pay attention\"" means roughly \""notice\"" or \""observe\""; or the widely studied SVO triples like <rock,sank,ship> would also seem to cry out for a tensor decomposition."
6989,1,\n\nI have one question about baselines: is the proposed approach better than training to forward generators and force an agreement between them (in the spirit of the concurrent ICLR submission https://openreview.net/forum?id=rkr1UDeC-)?
6990,1,"\nHowever, experiments are insufficient to determine whether it is this novelty\nthat contributes to improved performance or just the gating."
6991,1,"""* Paper Summary\nThis paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs)."
6992,1," The authors position the approach well within contemporary literature, both conceptually and using experimental evaluation, and are explicit about its strengths and limitations."
6993,1,"   \n\nIn addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet."
6994,1,"  What the paper really lacks in my opinion is a closer analysis of *why* the proposed approach works, i.e., a qualitative empirical analysis (toy experiment?)  or theoretical justification."
6995,1, In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.
6996,1," Overall,  the study is interesting and contains some new idea."
6997,1," In my opinion, this is a red flag."
6998,1,"n\n- The paper could possibly be clearer by integrating more of the \""background\"" section into later sections."
6999,1, I think matching the number of hidden units could be helpful.
7000,1, but the proposed model is too hand-crafted.
7001,1, \n\nThe high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge (though I am not an expert in this space).
7002,1," Secondly, the approach parses this \u201csynthetic\u201d language into structured tuples which makes it even less natural."
7003,1," Moreover, the curves of TRPO is so unstable, which is a bit uncommon."
7004,1," This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments."
7005,1,.\n\nThe selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries?
7006,1," Unfortunately, this is mostly a negative result, with gains over \u201cflat attention\u201d being relatively small."
7007,1," The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call \u201cstatic maze\u201d), and in fixed mazes with changing goal environments (what they call \u201cenvironments with dynamic elements\u201d or \u201crandom goal mazes\u201d)."
7008,1, \n\nI must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task). 
7009,1," If the goal is to generate inductively, over the full distribution of graphs, then it would be better to (i) assess whether the sampled graphs are isomorphic, and (ii) compare more extensively to alternative graph models (many of which have been published since 2010).  \n"""
7010,1," \u201d  The surprise would be the initial reaction of \u201cwhat\u2019s that on my car?  Is it dangerous?\u201d but after identifying the object as non-threatening, the emotion of \u201csurprise\u201d would likely pass and be replaced with appreciation."
7011,1,"  I suspect their upper bounds will be loose,"
7012,1,\n\nThere are also some  typos.
7013,1, I feel the discussion to be too much obsessed by the claims made in Zhang et al 2016 and in no way suprising.
7014,1," But in either case, it will be good to discuss the connections."
7015,1,"\n- writing could be cleaned up for spelling / grammar (e.g., \""last 70 stories\"" instead of \""last 70 sentences\""), currently the paper is very hard to read and it took me a while to understand the model"""
7016,1," \n\nPage 8:\n-\tThe results in general indicate that the method is much better than chance,"
7017,1," However, I do not see it being the case here."
7018,1, This paper has some interesting insights and a few ideas of how to validate an evaluation method.
7019,1," As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task."
7020,1," Are the\nproperties that are being verified different properties, or the same property on\ndifferent networks?"
7021,1,"""Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay."
7022,1, The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion.
7023,1,"  \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance."
7024,1, Please add information about how this critical value was generated.
7025,1,"""This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. "
7026,1,"""The authors propose a probabilistic framework for semi-supervised learning and domain adaptation."
7027,1," Additionally, standard RNNs (non-gated versions) have an ill-conditioned recurrent weight matrix, leading to vanishing/exploding gradients during training."
7028,1," From the title a Bayesian approach is suggested, while in practice a rather standard softmax classifier is learned, albeit with a different regulariser (last layer is regularised towards batch class means)."
7029,1," A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure."
7030,1,"A well-written manuscript,;"
7031,1," Given the labels, the rest of the architecture are extensions of conditional GANs, a causalGAN with a Labeller and an Anti-Labeller (of which I\u2019m not completely sure I understand the necessity) and an extension of a BEGAN."
7032,1, How about the performance and pairwise KL divergence?
7033,1, So imperfect demonstration have very typical distributions.
7034,1,. \n\nThe problem with the approach is that it is very ad-ho
7035,1, This is a key point not discussed in the manuscript.
7036,1, Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers.
7037,1, The analysis suggests a practical (heuristic) algorithm incorporating two features which emerge from the theory: L2 regularization and keeping a history of past models.
7038,1,"\n\nA super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models (Table 1 row 8),"
7039,1,"  In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly."
7040,1,\n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.
7041,1,"\n+The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical?"
7042,1," Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work."
7043,1, \n\nSignificance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.
7044,1, (a) To retrieve the key (a vector) given the value (a string)  as the encoder input.
7045,1," In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets."
7046,1," Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable. "
7047,1,"\n\n* Novelty. The network design builds heavily on the work of [Finn et al., 2106]."
7048,1,\n\n2) Predicting the noise has no guarantee that the data items are better clustered in the latent space.
7049,1," The choice of simulation constants (% delayed, and delay time) seems somewhat arbitrary as well."
7050,1,  This should also work for (5).
7051,1,"\n - qualitative analysis could be extended\n - writing could be improved  """
7052,1,"\n\nOther Comments:\n\n- In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text."
7053,1, I do not understand how the \u201crepresentative units\u201d are selected and where the \u201clate\u201d selectivity on the far right side in panel a arises if not from \u201cearly\u201d units that would have to travel \u201cfar\u201d from the left side\u2026 
7054,1," Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field."
7055,1, but not the architecture of choice.
7056,1," They show that their idea is effective in reducing private information leakage,"
7057,1, \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks.
7058,1,"  Therefore, it is not clear to me why the proposed updating framework for the embeddings allow to generate decision functions adapted to the graphs to be learned."
7059,1,"\n\nComments:\n1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other."
7060,1," As far as I see, section 2 is somewhat detached from the rest of the paper."
7061,1, n\n4) It is also unclear how to compare Tables 4 and 5.
7062,1, The clarity is poor enough that the paper might not be ready for publication. 
7063,1," At training time, the generator's encoder computes a context representation using the masked sequence."
7064,1, The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods.
7065,1," I can't really support the conclusion \""RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually\""."
7066,1," However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models."
7067,1, however this paper is difficult for me to follow.
7068,1," The focus of the paper is confusing: while the introduction discusses about works on CNN model interpretability, the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier, without analyzing anything in relation to interpretability."
7069,1," First of all, I find the execution poor in the details: \n(i) Why is \\omega limited to a scalar?"
7070,1,\n\nMost of the originality comes from integrating time decay of purchases into the learning framework.
7071,1,"\n\nHowever the new bound is an \""upper\"" bound of the worst-case performance which is very different from the conventional sampling based \""lower\"" bounds. "
7072,1,. The derivation and analysis seems correct.
7073,1,"\n\nThe one layer analysis in sections 2.1, 2.2 and 2.3 is simply an explanation of graph polynomial filters, which were previously proposed and analyzed in cited work of Sandryhaila and Moura (2013)."
7074,1," \n\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary."
7075,1," If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS."
7076,1,".\n- The experiments reported in Table1 are way below state-of-the-art results, there a tons of previous work with much better results, see https://competitions.codalab.org/competitions/3221#results"
7077,1," \""This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting."
7078,1, How can such a limited technique be generalizing?
7079,1,"\n-The paper presents 2 contributions but at then end of the day, the development of each of them appears limited"
7080,1, Is there any justification for such a claim?
7081,1,  A general kernel method should address this issue (the authors just claim in the conclusions that it would be interesting to explore the NN context in more detail).
7082,1," The definition of intervention in the Background applies only to do-interventions (Pearl 2009) and not to general interventions (e.g. consider soft, uncertain or fat-hand interventions)."
7083,1," \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database."
7084,1,"\n3. train a supervised classifier on the private data\nso that the GAN training-sampling loop basically functions as an anonymization procedure. For this to pan out, we'd need to see that the GAN samples are a) useful for a range of supervised tasks, and b) do not leak private information."
7085,1," \n\nThe paper proposed a possibly interesting approach,"
7086,1, It is not quite clear from the text what the resulting complexity would be.
7087,1,"""The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches."
7088,1,"\n\nTypos: page 2, second-to-last paragraph: firs -> first, page 7, second to last paragraph: and and -> and."
7089,1," Specifically, the authors develop a Fourier-based generalization bound."
7090,1, \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions.
7091,1,"""The paper intends to show that complex and real valued neural network are different and lead to different results on similar tasks, the complex valued network being more appropriate to 'difficult' problems and datasets."
7092,1, The proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribution.
7093,1,"\n- When introducing CNNs, please also cite Waibel and TDNNs - they are *the same* as 1-d CNNs, and predate them."
7094,1,Are these really \u201cadversaries\u201d?
7095,1," For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration."
7096,1," It would be better if an example is given, which is verified to satisfy the assumption."
7097,1,"\n\nI wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations."
7098,1,  Bold everywhere or nowhere
7099,1,\n- What is the error of the raw h-predictions?
7100,1,\n- The structure of the network is clearly justified in section 4.
7101,1, I have not checked the proofs in detail but the general strategy seems sound.
7102,1,"\nThis can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012)."
7103,1,"\n* In Eq. 2, it should be \\alpha_i \\alpha_j instead of \\alpha^T\\alpha\n*"
7104,1,"all the new terms should be explained\np. 3:\ndefinition of T and R \nshouldn't V_{ij}^k depend on Q_{aij}^k?\nT_{::aij} should be defined\nIn the definition of h_{aij}, should \\Phi and b be indexed by a?"
7105,1," If the authors feel otherwise, please comment on why this is the case."
7106,1,"However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\"
7107,1,"\n\nIf I am not mistaken, the Vendrov WordNet test set is a set of positive pairs."
7108,1,\n\nFigure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case? 
7109,1, The Bayesian neural networks are only\nBayesian in the last layer. 
7110,1," But don\u2019t you need a softmax before applying the NLL loss mentioned in equation 1? In current form of equation 1, I think you are not including the distractor images into account while computing the loss? Please clarify."
7111,1," The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible)."
7112,1,\n\nMinor comments:\n- I believe you have the incorrect reference for conditional batch normalization on Page 5.
7113,1,"""The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks.Each word is represented  by a diagonal Gaussian and the KL divergence is used as a directional distance measure."
7114,1, it lacks theoretical justifications why the method could mix well. 
7115,1, Have the authors considered lifting this restriction for classification and if so does performance improve?
7116,1, but a bit short.
7117,1,\n\nSignificance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings.
7118,1," But I think some more work is needed in this work: comparing to the right current state of the art, and show that in principal (by demonstrating on other simpler simulations domains) that this method is better than other methods."
7119,1, There is no reason to believe that ASG can be faster than CTC in both training and decoding.
7120,1, \n\nSuggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning.
7121,1,  Then f_v is one of the centroids (named VCs).
7122,1,\n\n- The idealized case of quadratic function the analytical solution offers a good way to understand how T-step look ahead can benefit the meta-algorithm.
7123,1, \n- Equation (7): phi and psi seems inverted
7124,1," \nWithout extra justifications, it seems that the theoretical result only holds for an artificial problem setting."
7125,1," If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case."
7126,1," With this in mind, it is of course completely fine that the results are not better than for real-valued networks.\n"""
7127,1, The reader has to find out the connections between the textual description of the model and the figure themselves due to no reference to particular aspects of the figure at all.
7128,1,\n\nThe choice of early stopping is a very interesting problem especially for the EO-creitenrion.
7129,1,"The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training."
7130,1, The method is trained and evaluated in a multi-lane simulator and compared against a baseline approach and human drivers.
7131,1,"  \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent."
7132,1," The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task."
7133,1," Firstly, it\u2019s not clear to me what this would add, since the method is already interpolating in some sense between vanilla and resnets."
7134,1," Moreover, outliers are artificially generated in these datasets, hence there is no evaluation on pure real-world datasets."
7135,1, Given the ROC curves in figure 8  and table 1 it is redundant.
7136,1, \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.
7137,1,\n\nI did not fully understand the 'retrofitting' idea. 
7138,1,"  \n\nSignificance: There is no theoretical guarantee on the performance, despite the author\u2019s claiming this as a goal in the introduction itself (\u201cgoal of lifelong learner \u2026 computation\u201d)."
7139,1,  though the presented results are a bit on the thin side.
7140,1,"\n\nPart of the motivation for this paper is the goal of scaling to very large sets of examples.[[CNT], [null], [DIS], [GEN]] The proposed neural net setup is an autoencoder whose input/output size is proportional to the size of the program input domain."
7141,1,"\n3. You could consider giving the Discriminator, real data etc in Fig 1 for completeness as a graphical summary."
7142,1, This is inline with TD-lambda analysis in previous work.
7143,1, According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet.
7144,1, The paper only provides results on the bAbI task.
7145,1,\n\nThe experiments are only on toyish and small scale tasks.
7146,1," I would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data. """
7147,1,\n- Notation: I believe the space U is never described in the main text.
7148,1,"\n\nBased on the arguments above, I think this paper is valuable at least\nconceptually, but doubt if it is actually usable in place of ordinary LSTM\n(or RNN)-based generation."
7149,1,"\n- many realizations = one sample (not samples), I think."
7150,1,n\nOriginality\nThis approach to finding important hidden units is novel
7151,1, There are no new things in such kind of reviews.
7152,1,"\n\nOverall, I had a very confusing feeling when reading the paper."
7153,1,"\n\n\n4. Low level technical\n\n- The end of Section 2 has an extra 'p' character[[CNT], [CLA-NEG], [DFT], [MIN]]\n- Section 3.1: \""Here, X and y define a set of samples and ideal output distributions we use for training\"" this sentence is a bit confusing.[[CNT], [CLA-NEG], [CRT], [MIN]] Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3.[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Section 3.1: \""W is the learnt model...\\hat{W} is the final, trained model\"" This is unclear: W and \\hat{W} seem to describe the same thing.[[CNT], [EMP-NEG], [CRT], [MIN]] I would just remove \""is the learnt model and\""[[CNT], [CLA-NEG], [SUG], [MIN]]\n\n\n5. Review summary\n\nWhile the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read."
7154,1,\n\nThis is a fairly strong paper.
7155,1,  The proposed method is focused on discovering sparse neural networks.
7156,1,"\n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines."
7157,1," The algorithm operates in discrete time horizon, and continuous time is not used anywhere."
7158,1," For the TE task, the proposed method does not perform better than the state-of-the-art systems."
7159,1," E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax."
7160,1,  The forward RNN predicts forward in time and the backward RNN predicts backwards in time.
7161,1, The review of the literature is inaccurate.
7162,1, While the authors may claim it's necessary to use that much space to make their point I will argue that this length is uncalibrated to standards.
7163,1, It also outperforms existing methods in terms of coverage and compositionality.
7164,1," The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors."
7165,1, The approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is proposed.
7166,1,\n\nAnother important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2).
7167,1, The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end).
7168,1,"  I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n"""
7169,1," Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?"
7170,1,"  but I think authors should improve the aspects I mention for its publication.\n"""
7171,1,  The authors should validate these claims with an ablation study that compares performance with and without masking.
7172,1,  The main idea is to apply CNN on Hilbert maps of the data.
7173,1, It could have been interesting to know if there are more insights / lessons learned in this process.
7174,1," If I optimized a method using this synthetic data, I would still need to assess the result on real data."
7175,1,\nThe main concern is the motivation of the two-pass decomposition.
7176,1," It is straightforward if two policies are to be mixed. Although the mixing method is more reasonable than the genetic crossover operator, it is strange to compare with that operator in a method far away from the genetic algorithm."
7177,1," E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS)."
7178,1," Especially for this problem domain, drop-out and data augmentation should be investigated."
7179,1,\n\nI will refrain from adding additional detailed commentary in this review because I am unable to judge this paper fairly with respect to other submissions owing to its large deviation from the suggested length limits.
7180,1,"  Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily.\n"""
7181,1,"\n\nMore detailed comments:\n- On Page 1, \u201cthe optimal strategy for an arbitrary range of tasks\u201d lacks definition of \u201crange\u201d; also, in the setting in this paper, these tasks should share \u201csimilarity\u201d or follow the same \u201cdistribution\u201d and thus such \u201carbitrariness\u201d is actually constrained."
7182,1,"""The paper proposes a method which jointly learns the label embedding (in the form of class similarity) and a classification model."
7183,1,"\n\n\nFew comments:\nThe authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general."""
7184,1,"  However, this submission needs to be improved in terms of clarity and its experiments."
7185,1,"\nThe authors claim that the source of objective mismatch comes from n-step Q-learning, and their method is well-justified in 1-step Q-learning"
7186,1, The authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for \u2018Rank selection\u2019.
7187,1,"""The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row."
7188,1, A hard constraint is imposed during decoding to avoid trigram repetition.
7189,1," It has been improved since, through different sets of experiments and apparently a clearer presentation,"
7190,1, Entire technique is explained in a short section-3.1 with many important details missing.
7191,1,"\n\nThe only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain."
7192,1,"\n7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections."
7193,1, It will be interesting to compare more DFM with its discrete counterpart.
7194,1, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned).
7195,1,  It is interesting that the Task function is able to encode the higher level structure of the TAXI problem\u2019s two phases.
7196,1,"\n\n\nOverall, I see this as a paper which with improvements could make a nice workshop contribution, but not as a paper to be published at a top-tier venue.\n\n"""
7197,1,  How sensitive is the network to errors in this model?
7198,1,"""The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs."
7199,1,"\n\n+ The paper presents a promising application in police composite sketching, which can significantly improve human-in-the-loop search in face modeling."
7200,1,"n\n\nThe author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series."
7201,1," most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase."
7202,1," The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation)."
7203,1," However, this does not appear to be used anywhere else in the paper."
7204,1,  This is contrary to what is observed in the literature.
7205,1,"\n\n(B) A further difference to Hartono et al, 2015, are comparisons with multi-layer networks, and the presentation and discussion of this comparison is my strongest concern."
7206,1,"\n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training)."""
7207,1,"\n - p.5. Notation seemed a bit non-standard. I thought most people use \\sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid.[[CNT], [CLA-NEU], [DFT,DIS], [MIN]]...\n - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common."
7208,1," Third, the paper is not well presented and organized: the introduction is scant; the notational formulism is not at all clear, rigorous, or consistent; the paper overall lacks polish, with many grammatical errors."
7209,1,"""This paper proposes a model for solving the WikiSQL dataset that was released recently."
7210,1,  This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy.
7211,1," Additionally, the functional\nform chosen for f() in the objective was chosen to match previous work but with no\nexplanation as to why that's a reasonable form to choose."
7212,1,"""This paper explores the idea of adding parameter space noise in service of exploration."
7213,1,The two methods are used separately.
7214,1, The Related work part of this paper makes a good description of both topics.
7215,1," Some of the figures do not have the notations of curvey, making people hard to compare."
7216,1,"\n\n- In the study on reward sparsity, although a prediction horizon of 32 is less than the average steps needed to get to a rewarding state, a blind random walk might be enough to take the RL agent to a close-enough neighbourhood from which a greedy MC-based policy has a direct path to the goal."
7217,1,\n5. One solution is to approximate the solution F^{-1} J using gradient descent.
7218,1," \n\n* Further comments:\nThe paper contains a number of broken sentences, typos and requires a considerable amount of polishing prior to publication.\n"""
7219,1,\n\nThis is a well-written paper with interesting (and potentially useful) insights.
7220,1, At inference time y can be optimized by gradient descent steps.
7221,1," I would recommend borderline leaning towards reject."""
7222,1, I can appreciate though that this a fine line to walk.
7223,1,"\n\nIn section 2-3, the problem setting is not suitably introduced."
7224,1,"\n\nCompared with GAN, traditional graph analytics is model-specific and non-adaptive to training data."
7225,1," \n - Qualitative examples of intermediate values in counting component--adjacency matrix (A), distance matrix (D) and count matrix (C)--need to be presented to show the contribution of each part, especially in the real examples that are not compatible with the strong assumptions in modeling counting component."
7226,1," the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random."
7227,1,"""\n* In the \""flat vs sharp\"" dilemma, the experiments display that the dilemma, if any, is subtle."
7228,1,"  However, it does not adequately motivate the skip-path connections or applications of the method to supervised tasks."
7229,1,"\n\nEVALUATION\n\nIs it really the case that no results are presented for the QA task, or am I\nmisreading one of the charts here?"
7230,1," \n\nBut I am not sure if the novelty is strong enough for an ICLR paper. \n"""
7231,1," This type of stuff smells like overfitting!""."
7232,1,"\n3) \""simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging"
7233,1,The DReLU is supposed to remedy the problem of covariate shift better.
7234,1," \n-\tI did not fully grasp the details in the first \""Solution\"" paragraph on P5."
7235,1,\n\nii) What is the standard for creating the questions?
7236,1," It is easy to train since its low resolution, but also means a lot since it a relative complicated scene."
7237,1,"\n\np 1: authors write: \""Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989)."
7238,1," In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. "
7239,1," \n\nAdditionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3."
7240,1," According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit."
7241,1, The model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed before.
7242,1,  It would be more informative to explain how the baselines contribute to the solution instead of just citing them and highlighting their differences.
7243,1," Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training)."
7244,1," Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines."
7245,1,\nThis would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution.
7246,1," For example, in Eqn. 7, it would be clearer to write out the min over optimization variables."
7247,1,"\n\nAdditionally, some statements made by the authors are demonstrably untrue.[[CNT], [null], [CRT], [MIN]] First, the authors claim that they are the first to train DRL agents in all random mazes [f], but this has been already shown in at least two publications (Mnih et al, 2016 and Jaderberg et al, 2016)."
7248,1, They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.
7249,1,"\n- In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful."
7250,1, These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets.
7251,1,"\n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances."
7252,1,"\n\nminor comments: \n- some figures with just two parts are labeled \""from left to right\"" - it would be better to just write left: ... right: ...\"
7253,1,\n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence. 
7254,1,\n - Table 2: These results are interestingly different.
7255,1, While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features.
7256,1, The experimental analysis of the approach is very convincing and confirms the author\u2019s claims.
7257,1,"\n- are the training and testing sets all disjoint (sec 4.3)?\n- at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg \""tested on nored-test\"")"
7258,1,"""This paper presents a method for image classification given test-time computational budgeting constraints."
7259,1,"""This paper proposes a ranking-based similarity metric for distributional semantic models."
7260,1,"\n\n* Equation 3: Should there be a (1/K) in Z?"""
7261,1," Besides, it seems that the proposed path-wise training procedure also leads to significant performance degradation."
7262,1,"""This paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and document."
7263,1," however, I am not very convinced by its motivation, the proposed model and the experimental results."
7264,1," However, I do have concerns including two major concerns: (A) delimitation of results from earlier work; (B) numerical results (especially Tab. 1)."
7265,1,"  Also, it overall rather appears short.\n"""
7266,1,\n\n1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. 
7267,1," First, the definition of parameters should not include the word parameters."
7268,1,  I have a few suggestions along the text but nothing major.
7269,1," 4.2, the only quantitative analysis was missing some details."
7270,1,"  There are theorems,  but they are trivial, straightforward applications of importance sampling."
7271,1,\n\n\nGeneral Comments:\n\nThis paper proposes to use the rotation matrices with LSTMs. 
7272,1,"\n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling."
7273,1, There are typos and\ngrammatical mistakes throughout the paper.
7274,1,\n\nThe proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies.
7275,1," What is the \""combined\""?"
7276,1,"""This paper argues about limitations of RNNs to learn models than exhibit a human-like compositional operation that facilitates generalization to unseen data, ex. zero-shot or one-shot applications."
7277,1," What is the citation of this in the kernel approximation context?"""
7278,1,"  I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence."
7279,1," \nIn figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case?"
7280,1," The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked)."
7281,1," Perhaps Godel could be a good inspiration here, with a 21 page PhD thesis that fundamentally changed mathematics."
7282,1,\n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).
7283,1,"""The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights."
7284,1,n\nThe method seems to work well based on the samples and ROC curves presented.
7285,1,"""This paper is proposing a new formulation for regularization of Wasserstein Generative Adversarial models (WGAN)."
7286,1,\n- The extensive experimentation shows that the proposed network is better than previous approaches under different regimes.
7287,1," It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their \""continuous-propagation\"" factorization?"
7288,1," Here are a few comments:\n\nI think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix."
7289,1," The DuoRC dataset offers the following challenges compared to the existing reading comprehension (RC) datasets \u2013 1) low lexical overlap between questions and their corresponding passages, 2) requires use of common-sense knowledge to answer the question, 3) requires reasoning across multiples sentences to answer the question, 4) consists of those questions as well that cannot be answered from the given passage."
7290,1,"\n\u2013 The choice of activations is not motivated, and performance on variants is not reported."
7291,1," The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence."
7292,1, How good are the low-precision models trained by the authors at transferring to other tasks?
7293,1," Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow."
7294,1, The teacher model learns via reinforcement learning which items to include in each minibatch of the data set.
7295,1, the so called overlapping convolutional architectures
7296,1,". Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time"
7297,1,\n\n---\n\nSome questions/comments:\n- Do we need to add the print statements for any new programs that the students submit?
7298,1," My concern with that is that the authors might be using the test set for model selection; It is not a priori clear that the setup that works better for ML should also be better for RL, especially as it is not the same across datasets."
7299,1," The idea does not seem to be novel, technical novelty is low, and the execution in experiments does not seem to be reliable."
7300,1," In addition to further discussion on this point, this result also suggests evaluating other recently proposed \""minor changes\"" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016)"
7301,1," If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected)."
7302,1,\n\nThe authors also demonstrate that using multiple agents with different policies can be used to collect training examples for the PATH function that improve its utility over training examples collected by a single agent policy.
7303,1, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.
7304,1," The theorems (Theorems 1~3) are trivial, with loose relations with LSH."
7305,1,\n\n5) It is not clear how to set the number of clusters.
7306,1,"\n\nCon:\n- The experiments performed are interesting directions, although unfocused and rather limited in scope."
7307,1," \n\nFinally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here."
7308,1," The results are neat, but \nI couldn't tell why this approach was better than others."
7309,1, (Figure 4 provides qualitative examples from ImageNet but no quantitative assessment.
7310,1," The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences."
7311,1," From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder $f_\\beta(x)$ which gaussianizes the input (thus emulating the explicit transform) ?"
7312,1,"""In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle."
7313,1," It would be optimal if the authors can use their training method on such baselines and show improved performance, or explain why this cannot be done."
7314,1,"\n\nDiscussion:\n\n+ The paper spends a lot of time justifying the proposed method by discussing the limits of the \""Improved training of Wasserstein GAN\"" from Gulrajani et al. (2017)."
7315,1,  Nice ablation studies.
7316,1," For example, you could evaluate the deep-net-predicted drag ratio vs. the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shape. "
7317,1," The application domain is interesting,"
7318,1,". On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT."
7319,1, 2) auto-encoder that is trained on data for one-shot learning;
7320,1,  Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights).
7321,1,"""This paper can be seen as an extension of the paper \""attention is all you need\"" that will be published at nips in a few weeks (at the time I write this review)."
7322,1, The skip connections are penalized by Lagrange multipliers that are gradually phased out during training.
7323,1,"\n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so."
7324,1, This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network.
7325,1," The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic."
7326,1,\n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.
7327,1,"""This paper investigates the impact of character-level noise on various flavours of neural machine translation."
7328,1,"?\n\n2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree."
7329,1,\n\n=================\n\nThe authors describe a new variant of a generative adversarial network (GAN) for generating images.
7330,1,\n- Figure 8: scrambled caption.
7331,1," \n\n= Clarity = \n\nOverall, the exposition regarding the method is good."
7332,1,"  Is that what is meant by the \""defense network\"" (in experiments bullet 2)?"
7333,1," Three data sets are considered (Swimmer,  Mocap and  Handwriting)."
7334,1, They extended the training objective functions of Word2vec and Glove with the affect information.
7335,1,\n\nc)  It is not clear is the softsign is used besides the activation function: In page 5 is said \u201cR_BRE can be applied on ant rectified layer before the nolinearity\u201d 
7336,1, Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152?
7337,1, \n\nPros:\n1) The general task of learning distributions over network weights is interesting
7338,1, The difference with the original task seems to be that the network is also trained to predict a \u2018blank\u2019 symbol which indicates that no prediction has been made.
7339,1," It is sometimes difficult to communicate ideas in this area, so I appreciate the author's effort in choosing good notation."
7340,1, Results are examined mainly by looking at the first two PCA components of the data.
7341,1, The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose.
7342,1,"  Are the synthetic time series clearly multimodal, or do they display some of the mode collapse behavior occasionally seen in GANs?"
7343,1," Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all."
7344,1," Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity)."
7345,1," Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs."
7346,1,"\n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods."
7347,1,\n\nCons:\n\nThe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods
7348,1, Including videos of failure cases is also appreciated.
7349,1," Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.\n\n"
7350,1,"\n\nWhile the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:\n\n- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners."
7351,1," \n2) Batch size is scheduled to change between B_min and B_max\n3) Different setting of B_min and B_max>=B_min are considered, e.g., among [64, 128, 256, 512, ...] or [64, 256, 1024, ...] if it is too expensive."
7352,1," The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient."
7353,1," Again, use [*].\n\n[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.\n"""
7354,1,"""The paper proposes a budgeted online kernel algorithm for multi-task learning."""
7355,1," The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017),"
7356,1,"\n\nIn table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io."
7357,1," For the videos, it may be useful to indicate when the goal is detected, and then let it run a couple more steps and stop for a second."
7358,1,"\n\nThe approach, and its constituent contributions, i.e. of using RL for program synthesis, and limiting to syntactically valid programs, are novel."
7359,1,"  The presentation should be made consistent, either with dev/test or -retuning/+retuning as the top level headers.\"
7360,1,"\n\nGeneral Comments:\n\nThis paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address."
7361,1, This reduced form of the LSTM is shown to perform comparably to \u201cfull\u201d LSTMs.
7362,1,"\nThe motivation has certainly been clarified, but in my opinion it is still hazy."
7363,1, The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-
7364,1," everything is very clearly explained,"
7365,1,"\n* acronyms should be expansed at their first use"""
7366,1,\n\nThis paper would be stronger if it compared with Rainforth et al.\u2019s proposed approaches.
7367,1,"\n\nUnder these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution."
7368,1,I buy the premise and think the work addresses an important issue.
7369,1," \n[2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces."
7370,1,\n\nNovelty: This is in my opinion the weakest point of the paper.
7371,1,"""In this paper, the authors consider symmetric (3rd order) CP decomposition of a PPMI tensor M (from neighboring triplets), which they call CP-S."
7372,1," It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet."
7373,1, I do not really see enough novelty here.
7374,1,\n5. In Table 2 and Figure 3 the results are reported with percentage of using the learning curve. 
7375,1, \n5. Can the authors show convergence of the teacher parameter \\theta?
7376,1," I think it is important to see how fast the teacher model converges, too."
7377,1,"\n\nOverall, this seems to be a nice paper to me."
7378,1,"  As a second contribution, the authors show experimentally that the random matrix theory can provide a good model of the spectral behavior of the weight matrix when it is large."
7379,1,"\n\nThe difference from the original AAE is rather small and straightforward, making the\nnovelty mainly in the choice of task, focusing on discrete vectors and sequences."
7380,1,\n- Is there a typo in the bound given by eq. (17)?
7381,1,  Adam White and Martha White.
7382,1, It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs.
7383,1," I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper."
7384,1,. The authors filtered the number of words and word-pairs to very small
7385,1,"\n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)"
7386,1,"""The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs)."
7387,1, It is more like a dependency-based version skip-thought on the sentence level.
7388,1,"\n\nThe results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6)."
7389,1," However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n"""
7390,1,is there a replay memory? How are the samples gathered?). 
7391,1,"""The authors investigate knowledge distillation as a way to learn low precision networks."
7392,1,\n\n4) One extra line of derivation would be helpful for the reader to rederive the bound|w|^2/2sigma^2  <= O(...) just above equation (4).
7393,1,"\n\nOverall, this paper feels like a small improvement over RN."
7394,1, \n\nClarity:  The paper is fairly clearly written. 
7395,1," This variant is of course very well suited to the proposed method, and a bit artificial."
7396,1,"\n\n\nMODEL & ARCHITECTURE\n\nThe PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A)."
7397,1,"""This paper is concerned with both security and machine learning."
7398,1," However, I am not satisfied with the results in the current version."
7399,1," I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it."
7400,1, This search enables the authors to come to convincing conclusions regarding the shortcomings of current models.
7401,1,"  The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well. "
7402,1,"\n\nInstead of the longer intro and related work discussion,;"
7403,1, It seems to me that this paper is a significant contribution to the field of question answering systems.
7404,1, In my opinion both contributions suffer from some significant limitations.
7405,1," The \""novelty\"" of this paper is a bit hard to assess."
7406,1," I do agree now on Figure 5, which tips the scale for me to a weak accept. "
7407,1," but I got a little lost in the 3 cases on page 4.[[CNT], [null], [CRT], [GEN]] For example, what is r?"
7408,1,\n\nPros:\n1. The topic is interesting.
7409,1, \n\nThe main results include:\n(1) In a rollout update a mix of MC and TD update (i.e. a rollout of > 1 and < horizon) outperforms either extreme. 
7410,1, This does not address the issue of understanding why the\nnetwork makes a certain prediction for a particular input.
7411,1," The authors should include the exact model specification, including for the HRED model."
7412,1, There are large sections of blank page throughout.
7413,1, The method is validated on simulations as well as in cfDNA and is s\nhown to provide increased precision over competing methods.
7414,1," Does the learnt model performs well on other dataset (for instance,\nacquired on a different region or at a distant time). "
7415,1,"\n\nMoreover, it really felt like the biased importance sampling approach should be \ncompared to a formal inference scheme."
7416,1,\n\nI also think the experiments on VAE are not conclusive.
7417,1," The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9.[[CNT], [EMP-NEG], [CRT], [MAJ]] For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4?"
7418,1,"\n\nI like the weak convergence results, but this is just weak convergence."
7419,1,"\n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading."
7420,1,"\n* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not."""
7421,1,"\n\nA few comments:\n\nThe paper is easy to read, and largely written well"
7422,1,But it is not clear to me why would that be an important contribution.
7423,1," Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1)."
7424,1," Did the authors try truncating after more words (e.g., 10k)?"
7425,1, \n\n* The network takes as input a 2.5m (this is large) occupancy grid representation of the local environment.
7426,1," Instead of listing each step in each subsection, a general introduction picture should be introduced first. More intuition is also needed for each step."
7427,1, Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).
7428,1,"  However, this equation is the key proposal of the paper. "
7429,1,"\nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well."
7430,1,"\n\""We carry out this exercise 4 times and set n to 8, 16, 32 and 64 respectively."
7431,1, The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix.
7432,1,"""In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model\u2019s runtime."
7433,1, The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft.
7434,1," Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures."
7435,1," The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere."
7436,1," Since there are multiple customer and taxi agents, there is a multi-agent coordination problem."
7437,1, \n\nStrengths:\n- The derivation of the dual formulation is novel
7438,1,"\n\nMoreover, the proposed method failed to give any performance boost, but resulted in a big performance drop on the better-performed DrQA system."
7439,1,\n\n# Quality\nThe experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the 3D environment.
7440,1,\n \ne_j and 0 are not defined in equation 8
7441,1,\n\nThe paper is well-written and easy to follow
7442,1,The experimental issues (especially regarding the baseline) raised by the anonymous comments below were rather troubling; it\u2019s a pity they were left unanswered.\n\n
7443,1," I\u2019d like to see far more results, and some attempt at a metric."
7444,1,\n+ Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem
7445,1,\n\nSignificance: Average. 
7446,1,"""This paper propose a simple method for guarding trained models against adversarial attacks. "
7447,1, \n\n- The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity.
7448,1,\n\nCons:\n-The paper addresses a problem with an issue with RWAs. 
7449,1,  The concatenation does fairly comparably to LL in Tables 3&4.
7450,1,  What is the variance between the seeds.
7451,1,"  This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n"""
7452,1," At best, GANs generate reasonable images that are lower resolutions (e.g. < 128x128)."
7453,1," Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems?\n"""
7454,1,"""The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations."
7455,1, Or maybe the average is a bad performance measure due to outliers?
7456,1, That will show the generality of the proposed architecture.
7457,1," \n\nThis paper touches on many interesting issues -- deep/recurrent models of time series, privacy-respecting ML, adaptation from simulated to real-world domains."
7458,1,"  If we\nscale up to batch sizes of ~ N/10, we can only get 10x speedups in\nparallelization (in terms of number of parameter updates)."
7459,1, The authors evaluate AdvGAN on semi-white box and black box setting.
7460,1,  \n\nThe results on the natural images are not complete.
7461,1," I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016)."
7462,1," Some qualitative analysis, or even feature ablation study would be helpful."
7463,1," As the author say, learning the actions from state transitions in a standard stochastic MDP would require to learn the model."
7464,1," The model is based on the SWAN architecture which is previously proposed, and an additional \""local reordering\"" layer to reshuffle source information to adjust those positions to the target sentence."
7465,1," I believe that N is supposed to be the noise in the SCM, but then maybe it should not be called E at the beginning."
7466,1,"""Summary: the paper proposes a tree2tree architecture for NLP tasks."
7467,1, They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory.
7468,1," However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d."
7469,1," \n\nI encourage the authors to develop the problem and method further, as well as the analysis and evaluation. \n"""
7470,1,"\n5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation."
7471,1, Can you give more details or refer to where in the paper it is discussed/tested?
7472,1," The proposed metric is interesting,;"
7473,1,"\n\nI don\u2019t don\u2019t feel like the clause between equations (17) and (18), \u201cwhen sharing attention weights from the decoder with the encoder\u201d is a good description of your clever \u201cshared attention\u201d idea."
7474,1,". Several existing models are discussed to some length, while the benchmarks are introduced quite shortly."
7475,1,It seems that much less would be sufficient from figure 4?
7476,1,"\n\nWhen presenting Eq 11, the definition of w_j^t elides a lot of complexity."
7477,1,"  \n\nDespite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.\n"""
7478,1," \n* A related issue, is that the definition of metrics is very informal and, again, does not use the already defined notation."
7479,1,. Why is it important to separate class from style?
7480,1," A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity."
7481,1,If that are you reusing the frames you used during training to distill?
7482,1,"\n\nEven on the SQuAD data, the sentence-level processing seems sufficient: as discussed in this paper about Table 5, the author mentioned (at the end of Page 7) that \""the Conv DrQA model only encode every 33 tokens in the passage, which shows that such a small context is ENOUGH for most of the questions\""."
7483,1,\n\n(3) About the speedup: it could be imaged that the speedup from the usage of dilated CNN largely depends on the model architecture.
7484,1, Change your claim that VAN is equivalent to PReLU.
7485,1,\n\n- lack of discussion about distributed/asynchronous SGD
7486,1," Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly."
7487,1,  \n2)\tFrom modifying learning rates to weighting samples.
7488,1," It is shown that through combining the generator architecture and the GAN training objective function, one can learn a foreground--background decomposed generative model in an unsupervised manner"
7489,1,The recommended length is 8 pages + 1 page for citations.
7490,1, If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead.
7491,1,  The improvement is in the noise.
7492,1, and the proposed algorithm is supported by experiments.
7493,1,"Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100."
7494,1,"\n\n3. The authors demonstrated that, given the same resource budget, the wider networks with the proposed method are more efficient than the deeper networks due to the nature of GPU parallel mechanism."
7495,1,"\n\nAs a summary, this paper would benefit significantly with a more extensive overview of the existing relevant models, clarification on the model details mentioned above and a more through experimental evaluation with more datasets and clear explanation of the findings."""
7496,1," In the experimental parts, the authors claim they use both ReLU and Sigmoid function, but no comparisons are reflected in the figures."
7497,1,"\n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n"""
7498,1," The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on."
7499,1,\n\nMy concerns:\n1. There is no analysis on what the regularization effect is.
7500,1,\n- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity
7501,1," Where is your background material?[[CNT], [EMP-NEU], [CRT], [MAJ]] I do not understand why we would like to assume (1), (2), (3)."
7502,1, \n- The F1 performance increases are somewhat small.
7503,1," Unfortunately, the experiments is Section 5 reveal that the proposed method yields results that are at most comparable with the existing methods."
7504,1," Moreover, the method also yields new algorithms for learning decision trees."
7505,1,This paper should at least cite those papers and qualitatively compare against those approaches.
7506,1,\ \n- The proposed method is not compared with other methods that combine text and image for sentiment analysis.
7507,1," I would like the authors to have provided results on more than the current three datasets, as well as an explanation of how meaningful the MSEs are in each dataset (is a MSE of 0.2 meaningful for the Swimmer Dataset, for instance? the reader does not know apriori)."
7508,1, First learns a deterministic mapping from x to y.phases.
7509,1,"\n- The connection to RLSVI is definitely understated, since with a linear architecture this is precisely RLSVI."
7510,1, Not much insights were given.
7511,1,This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability.
7512,1," The main idea is essentially learning the manifold of the data distribution and using Gaussian mixture models (GMMs) and dictionary learning to train a \""reformer\"" (without seeing adversarial examples) to detect and correct adversarial examples."
7513,1,"\nFor those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not."
7514,1, \n- It would be interesting to see if the DA results with a kernel classifier are better (comparable to the state of the art)
7515,1," but there is a number of ways in which it can be improved, detailed in the comments below."
7516,1, Most of the techniques used are well-known in the tensor community (outside of machine learning).
7517,1," Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset."
7518,1, \n===========================================================\n\nThis paper presents an algorithm for few shot learning.
7519,1,\n\nOne claim of the paper is that a generative model of fMRI\ndata can help to caracterize and understand variability of scans\nacross subjects.
7520,1," However, the experiments only focus on identifying important hidden\nunits and fall short of actually providing an interpretation using these hidden\nunits."""
7521,1," Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation."
7522,1,"  This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.)."
7523,1, It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy.
7524,1,This paper would be much stronger if the authors can include these experiments and analyses.
7525,1,\n\nPros:\n- Important starting question
7526,1,".  It may be possible to motivate and demonstrate the methods of this paper in other domains, however."
7527,1,"\nCons: Clarity """
7528,1,"  Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns."
7529,1,"\n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \""inlier accepted fraction\"" is not well motivated in my opinion."
7530,1,\n\nMINOR COMMENTS\n\nAbstract: \u201cLittle studies\u201d -> \u201cFew studies\u201d
7531,1,"""In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\"
7532,1," The distance function (eq. 4) is d Mahalanobis distance, instead of \""linear Euclidean distance\""."
7533,1,\n\n4. A wall-time experiment is needed to justify the speedup.
7534,1," To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images."
7535,1," They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016."
7536,1," For example, in Figure 1, the answer is solely obtained using the second last passage."
7537,1," The paper is well-written, reasonably scholarly, and contains stimulating insights."
7538,1," the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. \n"""
7539,1,  The author extend the  original algorithm from two-folds:\n\n1. A style discrepancy loss that pushes the style vector of the sentences away from target style
7540,1,"\n\nMinor issues:\n* The last paragraph of Section 5 refers to a figure 8, which appears to be missing. "
7541,1, \n\n\nMinor comments:\n- Some citations have a strange format: e.g. \u201cin Hubara et al. (2016); Restegari et al. (2016)\u201c would be better readable as   \u201cby Hubara et al. (2016) and Restegari et al. (2016)\u201c.
7542,1," However, in this experiment, the features are handcrafted before they are fed into the models."
7543,1, It would be interesting to see further elaboration of the pros and cons of such problem formulation.
7544,1,"  \nExperiments on more complex data is desired, for example on Imagenet classes."
7545,1,"\n\nRevision:  Having read the author response for this paper, I am encouraged by the updated baseline for WSJ and the additional explication about the competing Fisher systems."
7546,1,"""This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015). "
7547,1, Your work falls under a similar category.
7548,1,\n\nPros:\n- The paper is nicely written and good to follow.
7549,1,"  Then for out-of-sample prediction for x*, the paper appears to propose sampling a z uniformly from the training data {z_i}_i (it is not clear from the description on page 3 that this uniformly sampled z* = z_i depends on the actual x* -- as far as I can tell it does not)."
7550,1, Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments
7551,1,\n\nThe paper is well-organized and easy to follow.
7552,1,\n--The experiment section is thorough.
7553,1," \n- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN."
7554,1, Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright.
7555,1," The link seems to be purely intuitive (SGD should converge to minima with high evidence, because its updates are noisy)."
7556,1,  They do not cite the extensive literature on KB completion.
7557,1," There are many related works concerning adaptive batch sizes, such as [1] (a summary in section 3.2 of [2])."
7558,1," It appears to be motivated by improving training dynamics, which is understandably a significant concern."
7559,1,  The results and analysis are informative.
7560,1," In particular, evaluating on random instances is not a good measure of performance,  as has already been pointed out."
7561,1, More info on this throughout the paper would be a valuable contribution. 
7562,1,  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings.
7563,1,"I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed, or compare them experimentally."
7564,1, \n- It seems from Figure 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end.
7565,1,\n\nMajor comments\n=============\n1. The authors authors showed that their method enables a higher speedup and lower drop in accuracy than existing methods when applied to VGG16..
7566,1,"  In addition, I wonder whether it will help to fine-tune the model on the graph classification data."
7567,1, \n- difficulty generalizing to longer sequence (compared to training sequences) of commands.
7568,1," Moreover, the introduction should be rewritten, and the the background section of VCs (Sect 3) should be clarified."
7569,1,"\n\nOverall, while the paper is well written and makes some interesting points,"
7570,1," but not good enough.\"""
7571,1,\n\n- Are the checkpointing modules designed to only detect adversarial examples?
7572,1,"\n\nThe authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. "
7573,1, The proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the imposter.
7574,1," In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA)."
7575,1,"\n\n* The noise model experiment in Appendix D is commendable, "
7576,1,"\n\nSection 3 could use some more signposting.[[CNT], [SUB-NEU], [SUG], [MIN]] Especially for 3.3 it would be good to explain (either at the beginning of section 3, or the beginning of section 3.3) why these measures matter and what is going to be done with them.[[CNT], [SUB-NEU], [DIS], [MIN]]\n\nIt's good that LEAR is mentioned and compared against, even though it was very recently published."
7577,1, The question is if it is worth adding so many pixel-wise parameters.
7578,1," Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well."
7579,1, Fair comparison of the data is a serious concern.
7580,1," It is more appropriate to refer to encoding a combination of the current value and the increment as a version of predictive coding in signal processing rather than the proportional derivative scheme in control theory because the objective here is encoding, not control."
7581,1,  de-identifying the 4 telemetry measures is extremely easy and there is little evidence to show that it is even possible to reidentify individuals using these 4 measures.
7582,1, The nice part of doing RL is that it provides ways of actively controlling the exploration.
7583,1," It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ?"
7584,1," The authors chose to study only random instances which are known to be bad representatives of real-world problmes, instead of picking a standard benchmark problem. "
7585,1,"  If it were, this would be a much sought breakthrough."
7586,1, The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.
7587,1,"\n\nIn the text discussing Figure 4 (middle of pg. 8) , \""which is obtained by using...\"" should be \""which are obtained by using..."
7588,1,  I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.
7589,1," Given that the results are very close to other approaches, it\nremains unclear whether they are simply due to random variations, or whether the\nproposed approach actually achieves a non-random improvement."
7590,1, The second one is a cycle consistency loss to make sure that the the original sentence can be recovered from the content of the transferred sentence and the style of the original sentence.
7591,1,\n\nI suggest that the authors start from a loss function and clearly derive all necessary steps.
7592,1,\n\nQuality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth. 
7593,1,"\n\nThe key idea is to reformulate the problem as a convex minimization of a \""double-sum\"" structure via a simple conjugation trick."
7594,1, Therefore the impact or actual contribution to the ICLR community is very limited.\n\n
7595,1,\n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.
7596,1, Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison.
7597,1,\n\n- How useful is r-squared as a measure of performance in this setting?
7598,1, The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.
7599,1,"Either the models are not properly trained, or the models are heavily tuned on the test set."
7600,1,"  The authors also don't show any results on previous datasets, which would allow for a more objective comparison to existing state of the art."
7601,1," In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission,"
7602,1," To me, the word2vec addition paraphrases also look quite good."
7603,1,"\n\nThe experiments are very convincing, both numerically and visually."
7604,1, \n\nQuestions:\n1. The approach of sequential regression SVM is not explained properly.
7605,1,  The proposed algorithm is tested on 3 data sets and compared with several baselines.
7606,1,"\n\nThe authors conjecture that several of the above observations can be explained by the fact that the training target in MC methods is \""ground truth\"" and do not rely on bootstrapping from the current estimates as is done in a TD rollout."
7607,1," This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models."
7608,1,. The word pairs are extracted using Stanford parser
7609,1,  \n\n\n** REVIEW SUMMARY **\n\nThe paper is readable
7610,1,"n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments)"
7611,1," I agree with authors that this is an attempt to combine system identification with generating control inputs together, but I am not sure how to remove the restriction on A."
7612,1,"The idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis."
7613,1,"\n\n* p 3 bottom -- give size of dataset\n\n* p 5 AUC curve -> ROC curve\n\n* p 6 Fig 4 use text over each image to better specify the details given in the caption.\n\n\n\n"""
7614,1," For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap."
7615,1," Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown."
7616,1, For example have you tried learning proposals for the LG-SSM in Section 5.1 using the IS objective as proposed in 4.1?
7617,1,  \n\nI believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed.
7618,1,". Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium"
7619,1," \n\nMinor comments: on the upper-bound of the distance, alpha_i instead of alpha^\\top, and please label the axes in your figures. """
7620,1," For example, the Gradient Scale Coefficient appears to be motivated because (bottom page 3), with other norm measurements, we could take any architecture and rescale the parameters, and inversely scale the gradients to make it \""easy to train\""."
7621,1, It is therefore not clear to me that any claims can be made about its performance without additional assumptions.
7622,1,\n\nClarity: I had trouble understanding some of this paper.
7623,1,"""1) This paper proposes a new dataset for Reading Comprehension (RC)."
7624,1,"\n\nSection 4. 1 \u201c Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle."
7625,1," \n\nPros:\n1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community. "
7626,1, In the experiments they only consider training the model with the context selection variable and the data variables observed.
7627,1,"""Summary:\n\nThis paper proposes generative models for point clouds."
7628,1,\n- why cite Anonymous (2018) instead of Appendix...?
7629,1, The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.
7630,1,\n- Are numerical instabilities making this completely unfeasible?
7631,1,"\n- Since this paper's main proposal is a methodological one, I would make the publication conditional on the fact that code is released. \n\n\n"""
7632,1,\n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?
7633,1,n\nIt looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct? 
7634,1," As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate)."
7635,1,\n- The visualizations of universal perturbations as they change during AT are nice.
7636,1," For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk."
7637,1, The idea of visual concept has been proposed in Wang et al. 2015.
7638,1, The Figure could have been more informative if it was averaged over different starting states.
7639,1," Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline."
7640,1," Instead of using 1 long paths, one can simply use 2, 3, 4 etc."
7641,1, What is the cost function and what is the strategy to train on multiple tasks ?
7642,1,"\n\nThe proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not."
7643,1,\n+ good experimental evaluation
7644,1," If we add another class, no matter how do we define the borders, there will be one pair of classes for which the transition from one to another will pass through the region of a third class."
7645,1," It looks like Theorem 2 has already been shown in \""Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d."
7646,1,\n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments?
7647,1," but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs."
7648,1,\n\nThe title and the motivation about the genetic algorithm are missing leading and improper.
7649,1,"This effect is well known, but it can easily be remedied."
7650,1, The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation.
7651,1, though it did take two read-throughs before the equations really clicked for me.
7652,1,. There are a number of interesting and intriguing side-notes and pieces of future work mentioned.
7653,1," \n- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks."
7654,1,"""The paper consider a method for \""weight normalization\"" of layers of a neural network. "
7655,1,"""The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices."
7656,1," \n\nStill, many of the design choices appear quite arbitrary and can most likely be improved upon."
7657,1, So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play?
7658,1,"  Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max."
7659,1,\n3. Experimental results.
7660,1, \u201cDeep neural networks without training deep networks\u201d.
7661,1, In particular it targets\nGraph Convolutional Networks.
7662,1," The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015)."
7663,1,- Model is novel and interesting
7664,1," While the authors already identify that this might be a restriction, it still does not lessen the fact that the configuration considered is a really specific one."
7665,1,"""Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems."
7666,1,"\n\n4. There is no theoretical novelty and the empirical one seems to be very limited, with less convincing results."""
7667,1, It is\na shallow architecture with a single RBF-like hidden layer.
7668,1," In this case, the problem could become less interesting, as no real analysis is required to do well here."
7669,1,\n\nThe paper begins by stating that the exponential weighting of lambda returns is ad-hoc and unjustified.
7670,1,\n\n7. Why don\u2019t you treat number of hops as a hyper-parameter and choose it based on validation set?
7671,1," \nFurther do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model."
7672,1,The authors have substantially updated their manuscript and improved the presentation.
7673,1,\nIt would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).
7674,1,\n- pivot loss seems hacky
7675,1," This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis."""
7676,1,"\n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost."
7677,1," This is not necessarily a bad thing, since the\nextensive experiments (both \""toy\"" and \""real\"") are well-designed, convincing and\ncomprehensible"
7678,1," \n\nThe existence of amortization error is often ignored in the literature, but (as the authors point out) it is not negligible."
7679,1," They show that by bootstrapping from the final state of the time-limited domain, they are able to learn better policies for the time-unlimited case."
7680,1,  Great to see that the method is fast---it seems fast enough to use in practice in a real IDE.
7681,1,"""The paper is well-written but is lacking detailed information in some areas (see list of questions)."
7682,1," b) which action to perform given the previous action and the world state,"
7683,1," The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL."
7684,1, \n\nOne concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice.
7685,1," I understand the authors are trying to make a very serious claim about much of the common wisdom, but again, having reviewed papers for many years, this is highly unusual and it is questionable whether it is necessary. "
7686,1, Did authors also experiment with gradient normalization in the intermediate CNN layers?
7687,1," This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s)."
7688,1,"\n\nFig. 5, bad readability of axes labels."
7689,1," In my opinion, the paper is in a preliminary stage and should be refined."
7690,1,\n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem.
7691,1, it outperforms when some of the attributes are missing (Figure 4a).
7692,1,Semi-supervised means that one combines\nlabeled and unlabeled data.
7693,1," The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis,"
7694,1,"\n\nFor all the experiments, the same set of parameters are used, and it is claimed that \u201cthe method is robust in our experiment and simply works without fine tuning\u201d."
7695,1,\n5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)?
7696,1,. \n\nPros:\n- The proposed method leads to state of the art results .
7697,1, \n\nI would like to see comparison experiments with voxel based approaches in the next update for the paper.
7698,1,"  I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded."
7699,1," Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial."
7700,1," Prior techniques which can address some of these aspects do not necessarily work with deep learning, which is a key focus of the paper."
7701,1,"   \nSecond, in non-convex problems, one can expect curriculum learning approaches to also perform better, not just converge faster. "
7702,1,  I think I mostly followed it.
7703,1," However, your response to one of the sibling\ncomments suggests that it's still a \""mixed\"" training setting where the sampled\nQA and NAV instances happen to cover the full space."
7704,1," If so, this should be stated explicitly."
7705,1, This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.
7706,1,  The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting.
7707,1,"""This paper examines ways of producing word embeddings for rare words on demand."
7708,1,\n\nIt is not clear why the equation 6 holds.
7709,1," \n\np1: authors write: \""Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015)."
7710,1, The experimental evaluation presents indeed interesting results.
7711,1,"The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution."
7712,1,"\n- The paper focuses fairly heavily on speech recognition tasks, and I wonder if it would be more suited to a conference on speech recognition."
7713,1,"\n- There is discussion as to what i-vectors model (speaker or environment information) - I would leave out this discussion entirely here, it is enough to mention that other systems use adaptation, and maybe re-run an unadapted baselien for comparsion"
7714,1," However, the work is not mature enough for publication."
7715,1,"\n3. Design, solving inverse problem using Deep Learning are not quite novel, see\nStoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017)."
7716,1,"  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention."
7717,1," I find it nice how they benefited from context (left context and right context) by solving a \""fill-in-the-blank\"" task at training time and translating this into text generation at test time."
7718,1,"""Summary: \n\nThis paper proposes a multi-agent communication task where the agents learn to translate as a side-product to solving the communication task. "
7719,1," If space allowed, an example of different ordering leads to the same graph will also help."
7720,1,"\n\nThe idea proposed in the paper is quite interesting,"
7721,1,"\n\nand citations within, plus quick googling for more recent works."
7722,1, Authors present a set of experiments designed to support this observation.
7723,1," Moreover, this manuscript contains many grammatical errors."
7724,1,n\nThe proposed model can maintain performance of single-task models and in some cases show slight improvements.
7725,1," However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values."
7726,1,"Some incomplete technical details and non-standard benchmarks makes this not completely ready for publication."""
7727,1,"\n\n\nPaper Strengths:\n- Despite being simple technique, the proposed pixel deconvolution layer is novel and interesting."
7728,1,"  The authors describe two methods to obtain this structure: (1) enforced during training, (2) enforced through regularization and pruning."
7729,1,. This makes it even likely that the same subject with the same disguise appears in the training and test set.
7730,1, This is true for mathematical aspects as well as program generating specific terms
7731,1," If the theorem remains unchanged after this modification, you should clarify the proof."
7732,1," \n\nAlso, would using the backward RNN, e.g. for rescoring, bring another advantage?"
7733,1," The revised paper includes many of the additional references that I suggested, and the author response clarified my confusion over the Charades experiments; their results are indeed close to state-of-the-art on Charades activity localization (slightly outperformed by [6]), which I had mistakenly confused with activity classification (from [5])."
7734,1," However, I don't think the authors explore the idea well enough -- they simply appear to propose a non-parametric way of learning the stochastic model (sampling from the training data z_i's) and do not compare to reasonable alternative approaches. "
7735,1," Especially it does not provide insight into which types of problems (small/large, linear/ non-linear) the method is applicable to."
7736,1,"""This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal."
7737,1," Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one."
7738,1,\n\nThe paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques
7739,1,\n- Maybe provide a reference for HMM/GMM and EM (forward backward training)
7740,1,\n\nThe skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) .
7741,1," The kernel recursive least-squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275\u20132285, 2004."
7742,1,\n\nNegatives:\n- The math is fudged around quite a bit with approximations that are not always justified
7743,1," \n\nGiven the facts above, I am having a hard time to understand the novelty of this paper?"
7744,1, Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined.
7745,1,"?\n-\tCan the authors address the earlier comment on \u201chow to set thresholds for weights across different layers\u201d, by providing motivation for choice of penalty for each layer?"
7746,1,"\n+ The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative."
7747,1, the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robustness.
7748,1, It this the case?
7749,1," The high-level idea is similar to the evolution method of [Real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch."
7750,1, Deep reinforcement learning that matters.
7751,1," \n\nThe network architecture is suitably described and seems reasonable to learn simultaneously similar games, which are visually distinct."
7752,1,"Also, the recently proposed continuous relaxation of random variables seemed relevant."
7753,1,"\n- Was any kind of regularization used, how does it influence task performance vs. transfer?"
7754,1, \n- The impact of the proposed approach on medical diagnostics is unclear.
7755,1,\n\nThe finding of the effectiveness of idea (1) seems to be significant. 
7756,1," I could not tell from the text but I assume, the next transition to the start state is fully discounted to zero, otherwise the value function would link the values of S_T and the next state, which I assume you do not want."
7757,1," I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission.\n"""
7758,1, The authors propose to introduce a set of latent variables to represent the fertility of each source words.
7759,1," Otherwise, the model can't learn anything."
7760,1," Intuition and implications of Theorem 1 is not sufficiently discussed: what do you mean by optimal DNN, what is the criteria for optimality?"
7761,1,". Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n"""
7762,1,"""This paper proposes a feature augmentation method for one-shot learning."
7763,1,\n\nCasting MAML as HB seems a good idea.
7764,1," If that's not the motivation, then what is it?"
7765,1,"\n\n\nB/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework."
7766,1,"\nSpecifically, results were presented for multiple defenders and some ablation experiments were highlihgted"
7767,1,"     Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered."
7768,1,"""Quality: The work has too many gaps for the reader to fill in."
7769,1," As a final point, the authors state, \""as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L."
7770,1,"  Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly."
7771,1,\n\nPros\n-Several interesting ideas for evaluating evaluation metrics are proposed
7772,1," \n=======\n\nThis article examines the two sources of loose bounds in variational autoencoders, which the authors term \u201capproximation error\u201d (slack due to using a limited variational family) and \u201camortization error\u201d (slack due to the inference network not finding the optimal member of that family)."
7773,1,"  While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions."
7774,1,  but I feel that the experimental section\ndoes not serve to show its merits for several reasons.
7775,1, The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion.
7776,1,"  Furthermore, the approaches did not seem particularly well motivated in my view; inconsistent and seemingly erroneous notation in places complicate the picture."
7777,1,n\nClarity\n\nIn its present form it is hard to assess if there are benefits to the current formulation compared to already existing formulations.
7778,1," Of course, that survey paper correctly attributes the origin to [1]."
7779,1, \n\nThe idea is original to the best of my knowledge and is presented clearly.
7780,1, \n-- How to interpret the isolated components condition in Theorem 4?
7781,1, It uses this attention as a\ncommon bottleneck for downstream predictors that select actions and answers to\nquestions.
7782,1, The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective.
7783,1,"\n\nRegarding the experimental setup, how are the hyper-parameters for the baseline tuned?"
7784,1,\n-- Figure 8 failed to show for me.
7785,1,\n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear.
7786,1,\n\nThe experiment section definitely demonstrate the effort put into this work.
7787,1,"  I would add Spirtes et al 2000, Heckermann et al 1999, Peters et al 2016, and Chickering et al 2002."
7788,1, It does not seem right at all.
7789,1," We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context."
7790,1,. For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case. 
7791,1," The authors demonstrate that this change results in significantly improved BLEU scores across a number of translation tasks. Furthermore, increasing the number of agents/languages in this setting seems to \n\nOverall I think this is an interesting paper"
7792,1, Results are reported on about 50 UCI datasets with different topologies.
7793,1,It may be interesting to explain the meanings of individual components.
7794,1,"""This paper proposes a tensor factorization-type method for learning one hidden-layer neural network."
7795,1,"\n\n\nComments:\n\n- The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work."
7796,1, The relationship between reward and ability to do general SLAM is not clear.
7797,1," The paper presents a neural task graph solver (NTS), which encodes this as a recursive-reverse-recursive neural network."
7798,1," So all classes, popular and rare, have equal weight for Bayesian classification."
7799,1, \n\nPros:\n- Good solution to an interesting problem.
7800,1," \n\n-\tThe authors experimented one trained CNN and tested on images on only three categories (Persian Cat, Container Ship, and Volcao)."
7801,1," \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance."
7802,1,"""This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy."
7803,1,"\n2. Regularizing the objective with KL div. seems promising, but expensive."
7804,1,Results are well discussed with reasonable observations.
7805,1," \"" Based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not. "
7806,1," By \""do not anchor a specific meaning into the disentanglement\""?"
7807,1," The second advantage is its potentially better representation, proved by better results compared to models using recurrent networks on the TriviaQA dataset."
7808,1," The authors survey different methods from the literature,\npropose a novel one, and evaluate them on a set of benchmarks."
7809,1,  This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly.
7810,1, Did the model used for drawing Fig. 7 converge?
7811,1," \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU."
7812,1,"\n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified."
7813,1,"""Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation."
7814,1,"  Moderate novelty,"
7815,1, This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?
7816,1,  But in fact later they admit that the problem of optimizing the alignment is a non-convex problem and the authors end up with a couple of heuristics to deal with it.
7817,1," \n\nFirst, the contributions need to be more clearly spelled out."
7818,1,"\n\nFigure 6a shows visualisations by different techniques and is evaluated \""by looking at it\""."
7819,1,"\n\n(2) that therefore, block diagonal layers lead to more efficient networks."
7820,1," For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs."
7821,1,\n\nPros: The paper is proposing a simple yet effective method to predict accuracy.
7822,1,\n- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder)
7823,1,\n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari.
7824,1,\n\n- Another aspect of the previous points is that it\u2019s not clear if stacking KRU layers will work well.
7825,1," \n\nOverall, I don\u2019t think this paper meet ICLR\u2019s novelty standard, although the authors present some good numbers, but they are not convincing. "
7826,1," Together with the above concerns, it makes me doubt the motivation of this work on reading comprehension."
7827,1," Many meanings of the equations are not stated clearly, e.g., $phi$ in eq. (7)."
7828,1," It is investigated how several RL strategies perform on a large, standardized data set."
7829,1,\n\n# Clarity\n- I did not fully understand the learning objective.
7830,1, Can you please explain if you use obfuscation when you report the final test performance too?
7831,1,"\n\nIn the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified."
7832,1,\n\nMoreover sometimes the descriptions are a bit imprecise and unstructured.
7833,1, I am not sure I see any reason why the vanilla prototypical networks cannot learn to project x directly to A_z z and why one would need to do this indirectly through the use of the Mahalanobis distance as proposed in this paper.
7834,1,\n\nThe presentation of the paper can be improved a bit.
7835,1, The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework.
7836,1,The fix is very specific and restricted.
7837,1, How do BDQN and DDQN\ncompare when one takes into account running time and not episode count into\naccount?
7838,1," In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters"
7839,1," In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore."
7840,1," Similarly, 40% error on CIFAR10 is worse than what some very simple fully connected models can achieve."
7841,1, Are the negative images messing up the training when the number is between 10^1 and 10^2?
7842,1," Indeed, when the model goes deeper, the receptive field becomes very close to that of the proposed method."
7843,1,"\n3) The connection between the work on Bayesian evidence, and the work on SGD, felt very informal."
7844,1," \n\nOther than that, I have some other comments"
7845,1,\n\n[Cons]\n- The proposed method has a technical issue.
7846,1, \nPage 6:\n-\tHow is equation 7) optimized?
7847,1," Such contribution can, in my opinion, be summarized  in a potential of the form\n\nwith\n\n$$\nR_BRE = a R_ME+ b R_AC = a \\sum_k  \\sum_i s_{ki}^2   +  b \\sum_{<k,l>} \\sum_i \\{ s_{ki} s_{li} \\}   \n$$\n(Note that my version of R_ME is different to the one proposed by the authors, but it could have the same effect)\n\nWhere a and b are parameters that weight the relative contribution of each term  (maybe computed as suggested in the paper)."
7848,1,"  However, I don\u2019t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn\u2019t really gain you much more than policies that are biased towards particular actions."
7849,1, Also more baselines are needed.
7850,1,"""The paper proposes an additional transform in the recurrent neural network units."
7851,1," \""Adversarial agents\"" is also misleading because those agents act like automata."
7852,1,"\n\nOverall, pretty interesting result and solid contribution."
7853,1, \n\nChapter 3 constructs a simple example with synthetic data to demonstrate the effect of Bayes factors.
7854,1,"  \n\nAs pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings."
7855,1," Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand."
7856,1," See below for details.\n\nReview:\n\u2014\u2014\u2014\u2014\u2014\n1. Experiments are only provided on very small datasets. According to my opinion, this isn\u2019t sufficient to illustrate the effectiveness of the proposed approach."
7857,1," There needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics."
7858,1," Additionaly, it's not clear why learning seems to completely \""fail\"" without the pre-trained policy."
7859,1,"\n\nIn general, the paper proposes an idea to tackle an interesting problem."
7860,1,\n- d is used as a discriminator and then as a distance. This is confusing...
7861,1,\n\n\nMajor Weaknesses:\n- The main weakness of this work is the unclear exposition of the proposed technique.
7862,1, \n\nSome of the experimental setting seem a bit haphazard and not very systematic.
7863,1," Indeed, adding Gaussian noise to a replicated input is too artificial to be meaningful."
7864,1," Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted."
7865,1,\n\nThe motivation is that the server may be a straggler.
7866,1,"\n\nOverall, the paper only has a few interesting observations,"
7867,1,  While the Ro->En results are goodG this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more.
7868,1,"""The authors propose a new network architecture for RL that contains some relevant inductive biases about planning."
7869,1,\n- an agent trained on random maps does much worse on fixed random maps that an agent trained on the same maps its being evaluated on (figure 4)
7870,1,. Given that in many applications such parent-class supervised information is not available
7871,1,"  The main observation is that static sigma-delta coding as proposed in OConnor and Welling (2016b), is not correct when the weights change during training, as past activations are taken into account with the old rather than the new weights."
7872,1,"\n\nThis comment \""variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty...\"" is not precise."
7873,1, How many such epochs are done?
7874,1," For example, see Miyashita, Lee and Murmann 2016, and Hubara et al."
7875,1,"  They are thus not meaningless, but are a measure to model reaction instead of an independent process."
7876,1, although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales.
7877,1," Though one could do\nthis, the authors do not adequately show that one should, or that one can do this\nwithout suffering a lot of error in the posterior approximation."
7878,1,   Two minor concerns are 1) what is the relationship between the anti-labeler and and discriminator?
7879,1, It should be supposed L is at least locally convex.
7880,1," If yes, how do you explain the differences?"
7881,1,"""The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the \""model capacity\"" of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator."
7882,1,\n\nReview:\n\nThe paper is clearly written.
7883,1, The concern is that the contribution is quite incremental from the theoretical side
7884,1, Will it converge faster?
7885,1, \n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences.
7886,1,It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score.
7887,1,"\n\n- From Eq. 4 to Eq. 5, the authors argue that the denominator does not depend on the model parameters and can be ignored."
7888,1,"\n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples."
7889,1,  For few-shot classification then the cross-entropy classification loss is used on the node.
7890,1,"  In this sense, the presentation should be re-worked."
7891,1," In fact, the limit of training with a fixed dataset is that the model \u2018sees\u2019 the data multiple times across epochs with the risk of memorizing"
7892,1," The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work."
7893,1," \n\n* From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables?"
7894,1,"\n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \""jumps\"" in the training curves as signs of learning rate anneal)."
7895,1," but, in my opinion, a description of the learning framework should be given in the paper.[[CNT], [SUB-NEG], [DIS], [GEN]] Also, a summary of the hyperparameters used in the proposed system should be given."
7896,1, Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process?
7897,1, The theorems provide exponential gaps for very simple polynomial functions.
7898,1," When combining different constituents, an energy function is computed (equation 6) and the resulting energies are passed through a softmax."
7899,1,"  The learned node embeddings appear to do significantly worse than node2vec, and the full model is worse than DC-SBM."
7900,1, (iii) \u201cLearning Scalable Deep Kernels with Recurrent Structure\u201d (JMLR 2017).
7901,1,"""This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks."
7902,1, The presentation may raise some misunderstanding. 
7903,1,\n\nNegative points:\n\n1 The motivation of the paper is not clear.
7904,1," My worry is that the method is more complicated and slower than existing methods, without significantly improved performance."
7905,1," Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet."
7906,1," The proposed meanChar architecture doesn\u2019t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn\u2019t extremely satisfying."
7907,1, One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected?
7908,1," Again, this seems a bit magical or even too good to be truth."
7909,1," However, it also has several limitations, as explained below.\"
7910,1, I don\u2019t see any convincing explanations here.
7911,1," Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2."
7912,1," \n\nConculsion\n- can you show how your approach is not so computationally expensive as RNN based approaches? either in terms of FLOPS or measured times\n"""
7913,1,\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments.
7914,1," EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014."""
7915,1,"\n \nIn light of the above, I have increased my score since I find this to be an interesting approach,"
7916,1, This is a very important point that should be made\nclearer.
7917,1, The baselines are various simiplied versions of the proposed model.
7918,1,"\n\nDetails about training, such as the optimizer, step size, and batch size, are missing."
7919,1, Are the graph nodes pre-ordered? 
7920,1,\n  because p() was used for the distributions in the model.
7921,1,Multiplicative update-rules are known to suffer from slow-convergence and I would suspect this also to be an issue for the semi-NMF update rules.
7922,1, This work should be discussed and compared.
7923,1," It is unclear whether this difference is substantial, and whether the proposed method is better than the architecture search method."
7924,1,\n\n\n- References\n\n[1] Edward Snelson and Zoubin Ghahramani.
7925,1, In the last paragraph of Section 3 ``To the best of our knowledge''.
7926,1,\n - Tables 2 and 3 are missing the original baselines.
7927,1," Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP."
7928,1, It is not clear how this method would generalize to new situations.
7929,1,"\n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed."""
7930,1,"\n\nCotterell et al., EACL 2017 \""Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis\"": This paper also derives a tensor factorization based approach for learning word embeddings for different covariates."
7931,1, Similar remarks perhaps apply to the NLI results.
7932,1," The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used,"
7933,1,\n- Why store rewards and gamma\u2019s at each time step in memory instead of just the total discounted reward?
7934,1,\n- What happens when we use adversarial attacks different from FGSM?
7935,1,"\n7) page 9, second par outlines alternative approaches but they are not presented as such. Confusing """
7936,1, The first baseline is a sanity check to ensure that you are not observing some random effect.
7937,1,\nA toy model with a single unit is used to illustrate the basic ideas behind the method.
7938,1, The novelty here how the problem is formulated.
7939,1," Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively."
7940,1,"\n\n2) More fatally, you have failed to compare your algorithm's performance against benchline implementations of similar algorithms."
7941,1,"\n- In the experiment section for Omniglot, when the authors say \""1200 classes for training and 432 for testing\"", it sounds like the authors are performing zero-shot learning."
7942,1,Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful.
7943,1,"\n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016."""
7944,1,". In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion."
7945,1," This is described in Veit et al, 2016."
7946,1, It\nwould be great to compare this model to a non-stochastic one which just has the\nmultiplicative effects applied in a deterministic way (during both training and\ntesting).
7947,1," Also, particularly given that the focus is on embedded speech recognition, of which the acoustic model is one part, I would like a few more details on how decoding was done, etc."
7948,1, They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\n
7949,1," however, the results are mixed, and the explanation given is plausible, but far from a clearly demonstrated answer."
7950,1," The one exception is when you use the tag language model. This is a good idea, but it is pretrained, not trained using the saddle-point objective you introduce."
7951,1,"""Summary of paper:\n\nThe paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks."
7952,1,"""The paper proposes a new neural network based method for recommendation.,"
7953,1,". \n\nThe theoretical results are presented in a bitmap figure and only referred to in the text (not explained),"
7954,1,"\n\n- The authors claim that the soft unitary constraint was key for the success of the network, yet no details are provided as to how this constraint was applied, and no analysis was made for its significance. \n"""
7955,1,\n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance.
7956,1, Some of the results are better
7957,1,"\n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions."
7958,1, This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method.
7959,1, Last sentences on page 7 are hard to parse.
7960,1, No comparisons to other gaze selection models or saliency models are given.
7961,1,\n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial.
7962,1,. Both quantitative and qualitative results are provided.
7963,1," Using SVM for regression in order to do accuracy curve prediction was for me an obvious approach, I was surprised to see that no one has attempted this before."
7964,1,"\nBased on the explanation of how these experiments are performed, the authors show individual images to mechanical turkers."
7965,1,(2) It refers to equations appearing much later.
7966,1,"- sec5: add reference for first mention of \""A NICE MC\""\n"
7967,1, \nPros: Good empirical results.
7968,1," There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy."
7969,1," In addition, the writing should be improved as it is often ambiguous."
7970,1,It raises questions on the usability of the method for small labs.
7971,1, \n\nA couple of details :\n\n- the length of a graph is not defined.
7972,1, The analysis of the approaches in terms of their ability to decode is also sound and interesting.
7973,1," However, overall, the paper is difficult to read and parse, especially since low-level details are weaved together with higher-level points throughout, and are often not motivated."
7974,1," Multiple data sets and network architectures are tested, and equally important, the effect of parameter settings is investigated."
7975,1,. The acronym pkeep in later Tables should be clarified.
7976,1," Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity."
7977,1,"\n\n* sec6 \""Machine learning and deep learning\"": I would definitely avoid this formulation, seems to tail in with all the media nonsense on \""what's the difference between ML and DL ?\""."
7978,1," In fact, this step can result in biased estimation because the replacement is inside the nonlinear function."
7979,1,"\n\n[1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016\n\n[2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017\n\n[3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014"
7980,1,\n\n- The competitive gain of the authors method in comparison with other competing methods is minor.
7981,1,Cons:\n* Paper suffers from lack of clarity (method and experimental section)
7982,1,"page 5: \u201cthe the degree\u201d , \u201cspecified as (\u2026.) followed by\u201d -> , \u201cas (\u2026.) followed by\u201d ?,\n- This notation probably stems from the code, but SAME and VALID could be nicer described as \u201c0 padding\u201d and \u201cno padding\u201d"
7983,1,(The authors reference some signal processing tasks in the introduction
7984,1,"\n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID)"
7985,1, I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference).
7986,1, How is the ensemble of (1-T) training models trained to predict the f(T)?
7987,1, The plots in Fig. 2 and the appendix are quite helpful in improving presentation.
7988,1,"\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs."
7989,1," For example, a counterfactual query is \u201cwe gave the patient a drug and the patient died, what would have happened if we didn\u2019t give the drug?"
7990,1,"""This paper proposes a new convolutional network architecture, which is tested on three image classification tasks."
7991,1," -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion."
7992,1,  The training data is significantly larger than the CNN/DailyMail single-document summarization dataset.
7993,1,"\n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas."
7994,1, The work is a useful addition to the communit
7995,1, The COCO leaderboard also uses this metric as one of its evaluation metrics.
7996,1,"""The paper falls far short of the standard expected of an ICLR submission."
7997,1, The paper is revised and I saw NMS baseline is added.
7998,1,"""This paper proposes to combine reinforcement learning with supervised learning to speed up learning."
7999,1,"""This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution."
8000,1, There should be some theoretical and/or empirical study of its effect on quality of the solution.
8001,1,"\n\nIn addition, your reported score on Atlantis of ~2M seems too big."
8002,1, \n\nQuestions/Comments:\n\n- How is the checkpointing module represented?
8003,1,\n* Section 3 should be further detailed (step 2 in particular).
8004,1,\n\n- Fig. 5 is not entirely clear to me.
8005,1, It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping.
8006,1, It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells.
8007,1,\n-            The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method (compared to e.g. MAML)
8008,1, but I can't recommend this paper's acceptance in its current form.
8009,1, The authors propose to train a forward and backward RNN in parallel.
8010,1," The idea is interesting, and overall introduction is clear."
8011,1,\n\nAnother concern that I have is that there are a lot of conditiona-conjugacy\nassumptions baked into the algorithm that the authors only mention at the end\nof the presentation of their algorithm.
8012,1, \n\nI am not sure I understand figure 3 in which the authors try to see what happens if instead of learning the Mahalanobis matrix one would learn a projection that would have as many additional dimensions as free elements in the Mahalanobis matrix.
8013,1,\nRemarks:\n- it is unclear if the ConvNet weights of the first layers are shared).
8014,1,"""It is rather difficult to evaluate the manuscript. A large part of the manuscript reviews various papers from the active vision domain and subsequently proposes that this can directly be modeled using Friston\u2019s free energy principle, essentially, by \u201canalogy\u201d, as the authors state."
8015,1, \n\nThey then overview the notion of effective depth for a residual network: a linear residual network can be written as a product of terms of the form (I + r_i).
8016,1,\n\nIt would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion. 
8017,1," \n- The term \u2019GradNorm\u2019 seem to be not defined anywhere in the paper.[[CNT], [PNF-NEG], [SUG], [MIN]]\n\n\nReview Summary:\nDespite promising results, the proposed technique is quite unclear from the paper."
8018,1,\n- Figure 2 is hard to read even on screen.
8019,1," Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on. IEEE, 2014."
8020,1, There is no indications on the extra computations required for handling this modification compared to standard training.
8021,1," On the other hand, if no communication with the central station is allowed, then positions of other agents may be also partial observable."
8022,1," However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past)"
8023,1," For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations."
8024,1," Also,  no details are given on how the mixing in the rollouts was tuned."
8025,1, \n(iv) How can one maximize a probability density function?
8026,1,   No comparisons with previous work are given (and perhaps cannot be given).
8027,1,"\u201d\n\nThe paper also conducts an extensive analysis of the performance of a different version of the NavA3C+D1D2L algorithm (without velocity inputs, which probably makes learning path integration much more difficult), in the same environments but by introducing unjustified changes (e.g., with constant velocities and a different action space) and with a different reward structure (incorporating a negative reward for wall collisions)."
8028,1, Proc R Soc Lond B Biol Sci 216: 427\u2013459. pmid:6129637
8029,1," In any case, the derivation for computing the covariance function (4) of a multi-layer network is a very simple reapplication of the procedure in Neal (1994)."
8030,1,\n- no evaluation of dataset
8031,1,"\n   d. No wall-time and real memory numbers are reported."""
8032,1, \n- Significant empirical advantage over TRPO.
8033,1, Collecting the user specific models and averaging them to form the next global model.
8034,1,\n- Why is the final Cross-Entropy Loss so high even though the accuracy is >99% for the MNIST experiments
8035,1," This paper also presents a method for learning embeddings specific for subgroups of the data, but based on hierarchical modeling."
8036,1,\n\nPros: \n - the crashed cars dataset is interesting.
8037,1," \n\nFurthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn't see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost)."
8038,1, What does it do that a usual LSTM cell could not learn?
8039,1," However, I had a very hard time understanding the paper."
8040,1,"\n6. Some recent results on WMT DE-EN are missing, such ashttps://arxiv.org/abs/1706.03762.\n"""
8041,1,\nThe design of the experiments on language model is problematic.
8042,1," I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful"
8043,1," They claim that:\n\""The above studies cannot be directly applied to the ASC task as they are not capable of mining finer-grained aspect dependent sentiments."
8044,1," For example: \n(1) The step number k is dynamically determined by a short line search as in Section 4 ``Dynamic Rollout\u2019\u2019, but later in the experiments (Section 6) the value of k is set to be 2 uniformly."
8045,1," Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature."
8046,1," The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed."
8047,1,"\n  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4"
8048,1, This is not supported by any strong theory or conceptual idea.
8049,1,"  I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.  \n"""
8050,1," With sufficient knowledge of related works from these areas, I find that the authors' proposed method lacks proper evaluation and sufficient novelty."""
8051,1," \n- Why are (1, 0) and (1, 1) not useful pairs?"
8052,1,"  Also the second experiment, which shows the spatial clustering for the \""car wheel\"" VC, is unclear, how is the name \""car wheel\"" assigned to the VC?"
8053,1," \n\n- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model.[[CNT], [EMP-NEG], [CRT], [MAJ]] The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance."
8054,1," It is interesting that the mixed objective (which targets F1) also brings improvement on EM. \n"""
8055,1,"""This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime."
8056,1, It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores.
8057,1,"""In their paper \""CausalGAN: Learning Causal implicit Generative Models with adv. training\"" the authors address the following issue: Given a causal structure between \""labels\"" of an image (e.g. gender, mustache, smiling, etc.), one tries to learn a causal model between these variables and the image itself from observational data."
8058,1," For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset."
8059,1,". The article is missing from the nouns quite often though so this is something that should be amended. There are a few spelling slip ups (\""to a certain extend\"" --> \""to a certain extent\"", \""as will see\"" --> \""as we will see\"")]"
8060,1," While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data."
8061,1,"   This should be clarified, ideally with concrete examples."
8062,1, but the tasks were not carefully explained for someone not familiar with that literature.
8063,1," \n\nThe authors claim that the hashtags represent self-reported emotions, but this is not true in the way that psychologists query participants regarding emotion words in psychology studies."
8064,1," So I suggest moving them to the Appendix and make the major focus more narrowed down."""
8065,1, The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance.
8066,1,"\n- in terms of considering a monotonic alignment, Hori et al, \""Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,"
8067,1,"""This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP."
8068,1," In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters."
8069,1, This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution.
8070,1, \n\nThe paper is well written and conceptually simple.
8071,1, It would be good to motivate (in addition to the paragraph in the intro) the cases where using the algorithm described in the paper may be (or not?) the only viable option and/or compare it to other algorithms.
8072,1, This evaluation includes training the agent on a set of training mazes and testing it's performance on a set of held-out test mazes.
8073,1, I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.
8074,1," The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job."
8075,1," Looking into the referece (to get the details of the dataset -  from a workshop of the IEEE International Conference on Computer Vision Workshops (ICCVW) 2017) reveals, that it has only 25 subjects and 10 disguises"
8076,1, I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.
8077,1, The result is yes and the authors show how differently the tuning can be compared to tuning the full run.
8078,1," The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance."
8079,1, but where are the scores for each span?
8080,1, and I think the paper would be much stronger if the authors can re-run these experiments were models are trained to convergence.
8081,1, the overall technical contribution of the paper that of widening the networks is fairly small.
8082,1,  This form of network compression has been worked on before.
8083,1," While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on."
8084,1,\n- simple merge or RL and SL in an end-to-end trainable model\n- improvements over previous solutions
8085,1,"""This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based)."
8086,1, This definitely would bring it closer to the way the VAE was originally introduced.
8087,1," Please add error bars (based on Figure 4, I would imagine they are quite large)."
8088,1,"""# Summary\nThis paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state."
8089,1, \n\nA more minor comment is that it is unclear that averaging is the right way to combine locally trained models for nonconvex problems.
8090,1," Because NAT is a fundamental starting point of the work, it will be nice to elaborate the NAT method to be more understandable."
8091,1," As shown in [Szegedy et al 2014, \""Intriguing properties of neural networks\""] adding an extra linear transformation does not change the expressive power of the representation. "
8092,1,"  Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task."
8093,1,"\n- In the experiments, why are the input images downsampled to 320x320?"
8094,1," In this paper, on the other hand, the clusters are obtained in a manner which only accounts for pairwise similarities of tasks, using a pairwise similarity metric which is quite different from how the cluster is eventually used."
8095,1,"  The symbol i is an index into $CV_{i}$, the cut-value of the i-th largest unique weight-value."
8096,1," In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm."
8097,1," For reader who does not already know this net, I'm not sure this is really clear."
8098,1, The former is the de-facto choice of variational approximation used in VAEs and the latter is capable of expressing complex multi-modal distributions.
8099,1, The model achieves good results on bAbI compared to memory networks and the relation network model.
8100,1,"  The proposed pipeline converts each character to an embedding with the only sentence of description being \""Each character is converted by a lookup table to a vector representation, known as character embedding.\"""
8101,1,\n- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach.
8102,1,"\n\nThe paper goes on to show that if the evidence is approximated using a second order Taylor expansion of the cost function around a minimia $w_0$, then the evidence is controlled by the cost at the minimum, and by the logarithm of the ratio of the curvature at the minimum compared to the regularization constant (eg, standard deviation of gaussian prior)."
8103,1,"   \n\nIn the numerical experiment, the parameter \\tau_\\theta is sensitive to the final solution."
8104,1," \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? "
8105,1," The term \""correlated\"" is a bit too broad, and it is difficult for a non-expert to know exactly how correlated this is."
8106,1," \n\nI have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection."
8107,1,  Experiments and analysis are both presented clearly.
8108,1, \n\nThe authors indicate that they do not need to compare to variational methods\nbecause Gal and Ghahramani 2015 compare already to those methods.
8109,1,"\n\nThe approach is interesting and the paper is well-written,"
8110,1," I found the discussion about rank to be very intuitive,"
8111,1," \nThe authors apply their different models on a small dataset (semeval 2014), they compare basic memory network implementations with their approaches."
8112,1, The fact that LastSeenValue\n  doesn't do as well as a linear model on TS alone would seem to\n  indicate that there are higher order autoregressive coefficients.
8113,1,\n\nAre you willing to release the code for reproducing the results?
8114,1, So my rating based on the message of the paper would be 6/10.
8115,1," \n\nThis paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results. "
8116,1,\n\nPositives: \n- The paper is clearly written and easy to follow.
8117,1,  The suggested method seems to work well on several document classification tasks.
8118,1," If so, how would one play with just a forward model?"
8119,1,"\nThe authors propose measures disentanglement, informativeness and completeness to evaluate the latent code c, mostly through learned nonlinear mappings between z and c measuring the statistical relatedness of these variables."
8120,1,"The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).\n\nThe paper explores multiple loss functions and techniques to train these models."
8121,1, The authors of the current submission extend the approach above in two ways: they show how to deal with stochastic dynamics and how to replace a linear model with a nonlinear one.
8122,1,"\nThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to\nnetworks and other irregular domains. IEEE Signal Processing Magazine, 30(3):83\u201398, 2013."""
8123,1,\n- structure could be improved
8124,1," If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?"
8125,1," Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient."
8126,1, More regularization required?
8127,1, Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space.
8128,1," Secondly, the paper introduces the model for generating sketch drawings."
8129,1, Is there one or are just making a conjecture?
8130,1, This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data.
8131,1,"\n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix.[[CNT], [SUB-NEG], [DIS], [MIN]] And do try to copy a bit more of them in the main text where reasonable.[[CNT], [SUB-NEG], [DIS], [MIN]] \n\nWhat is the role of PLAID?"
8132,1,\n\nThe methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment.
8133,1," Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible."
8134,1," Excluding a class from the\nsoftmax is equivalent to having a logit of -\\infty, not zero."
8135,1,".\nAlso Astrid and Lee 2017 do not seem to report the instabilities during fine-tuning of the decomposed layers, and argue that these layers should not be freezed."
8136,1," I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU."
8137,1,"\n\nIt seems a bit strange to say \""The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost\"" as if this was an advantage of the proposed method, since arguably there isn't a really natural cost in the generative modeling case (unlike in the domain adaptation case); the latent variable seems kind of conceptually distinct from observation space."
8138,1,\n\nI like the message conveyed in this paper.
8139,1," I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score."""
8140,1,"\u201d\nIn page 2, section 2.1, sentence starting with \u201cIndeed, a too low percentage \u2026\u201d needs to be fixed."
8141,1," However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. "
8142,1,\n(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes.
8143,1,\n\nPros:\nThe regularization of the Q-values w.r.t. the policy of another agent is interesting\n\n
8144,1," In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem). "
8145,1," Let\u2019s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, \u2026]."
8146,1,"""\nSummary of the paper\n-------------------------------\n\nThis paper proposes to factorize the hidden-to-hidden matrix of RNNs into a Kronecker product of small matrices, thus reducing the number of parameters, without reducing the size of the hidden vector."
8147,1," However, BN-LSTM outperforms BELU-RNN on Text8."
8148,1,"\n[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein"""
8149,1," In other words, the test set of a dataset that is used for evaluation might contain some classes that were also present in the training set that VGG was originally trained on."
8150,1," This typically yields a non-convex non-concave optimization problem.[[CNT], [null], [DIS], [GEN]] This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm."
8151,1,\n* I do not see how the single output layer is defined.
8152,1, The authors should consider exploring and discussing the effects of adding/moving/removing objects on the performance.
8153,1,\n\nCons:\n-- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs
8154,1," The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms."
8155,1, This is an important area and findings in this paper are interesting!
8156,1, the quality of the work is not of a high standard.
8157,1," It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs."
8158,1,"\n\nIn [1], it is argued that kernel machines with fixed kernels do not learn a hierarchical representation."
8159,1, Strong results in multiple settings indicate that the proposed method is effective and generalizable.
8160,1,"""Eigenoption Discovery Through the Deep Successor Representation"
8161,1,"\n\n- I\u2019m not sure if Section 5.5 strengthens the paper.[[CNT], [CNT], [DIS], [MIN]] Unlike the rest of the paper, it feels very \u2018quick & dirty\u2019 and not very principled."
8162,1,"  More sophisticated should have been employed."""
8163,1," By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics' convergence to a Nash equilibrium."
8164,1, The architecture is trained through n-step Q-learning with reward prediction loss.
8165,1,
8166,1,"\n\n  - Third, the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem."
8167,1,"\n\n- In Section 2, the authors review ideas of so-called random kernel sparsity."
8168,1,"\n\n* \""The generated bottleneck samples are then used to estimate mutual information\"" -> an empirical estimation of I(Z,X) would seem a very high variance estimator; the dimensionality of X is typically large in modern deep-learning problems---do you have any thoughts on how the learning process fares as this varies?"
8169,1, Also is the reference sentence for skateboard example typo-free?\
8170,1," However, it is not clear to me that the method is invulnerable to novel white-box attacks."
8171,1," Finally, a few of the experimental settings differ from their baselines in nontrivial ways."
8172,1,"\n\n====\n\nI appreciate the answers the authors added and I change the score to 6."""
8173,1, I might have missed something here.
8174,1, \n\nThe paper is clear and well written.
8175,1,"For example, the introduction is written rather \""arrogant\"" (not completely the right word, sorry for that), with a sentence, like \""we have only limited insights into why CNNs are effective\"" seems overkill for the main research body."
8176,1,\n \n\nMinor comments:\n- You should make sentences when using references with the author names format.
8177,1,"\n- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task."
8178,1,"\n\nThe model (GGNN) is not particularly novel, but I'm not much bothered by that."
8179,1,"  That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients."
8180,1," \n\nThe paper is well written,"
8181,1, In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.
8182,1,"\n\nFor future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable AI in the first place."
8183,1,nThe paper is generally easy to understand and clear in their results.
8184,1,"\nGiven the systematic parameter search, with reference to the actual validation\n(or test?) set I am afraid there could be some overfitting."
8185,1, Then make scatter plots of these quantities against each other.
8186,1," However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP."
8187,1,"""This paper proposes a supervised variant of Kohonen's self-organizing map\n(SOM), i.e., trained by gradient descent with gradient obtained by\nbackprop, using a grid of RBF neurons which respond to the input only if\nthey are in the grid-neighborhood of the 'winner' neuron (closest to the\ninput)."
8188,1,"""The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter."
8189,1,"\n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from."
8190,1,"\n\nIf the authors wish to push their dataset, it would help to first evaluate the quality of the dataset."
8191,1,\n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks
8192,1, The most interesting part is the Hermite polynomial expansion of the activation function.
8193,1,"   I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution. "
8194,1," Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain)."""
8195,1,It could have taken more spaces in the paper.
8196,1, \n\n== Novelty and Significance ==\nMulti-attribute image generation is an interesting task but has been explored to some extent.
8197,1,"""This paper proposes a new character encoding scheme for use with character-convolutional language models."
8198,1," To me, this approach is the right way of constructing control variates for estimating policy gradient."
8199,1," The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q."
8200,1,"\n-- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.Yan et al., In ECCV 2016."
8201,1, One could think that it makes a somewhat incremental contribution with respect to the more complete work (both theory and practice) from [Bartlett et al. 2017].
8202,1,\n- It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect.
8203,1,n- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature
8204,1, 2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST)
8205,1, The resulting method is\ncalled BDQL.
8206,1, G2 takes all the duty  can still obtain a lower L_ld.
8207,1," Prior work along this line includes [3, 4, 5, 6, 7]."
8208,1,\n\nIt seems that the generated images are not actually plausible images at all and so not many conclusions can be drawn from this method.
8209,1,\n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.
8210,1,\n\nOther comments:\n- Paper would benefit from writing improvements to make it read better.
8211,1,"\n- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers."
8212,1, \n\nPros:\n\n1. It is a novel approach which trains the placement end to end
8213,1,"\n* weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al."
8214,1, The authors further propose to maintain a coreset which consists of representative data points from the past tasks.
8215,1,. What conclusions should I make from it?\n\n* Why not use KL divergence as your \\Delta function?\n\n* Why are the results in Table 5 on the dev data?\n\n* I was confused by Table 4
8216,1, This paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f(.).
8217,1, The presentation could be improved.
8218,1," Isn't it still challenging to update them independently, given that at the beginning both components are probably not very accurate?"
8219,1,"""Paper summary:\nAuthors extend [1] to form an auto-encoder CNN network for face mesh representation."
8220,1,\n6) Fig 5. What does good performance look like in this domain.
8221,1,"\n\n* If I understand correctly, the only new part compared to Li & Malik (2016) is\nsection 3.5, where block-diagonal structure is imposed on the learned matrices."
8222,1," The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP)."
8223,1,"\n\n5 The authors should provide the experimental results on large-scale data sets (e.g. ImageNet) to prove the effectiveness of the proposed method, as they only conduct experiments on small data sets, including CIFAR-10, CIFAR-100, and SVHN."
8224,1,.   \n\nOriginality:  It seems very unclear to me what is added in this paper in comparison to works like (e.g.) Trabelsi (2017).
8225,1," On the one hand, I somewhat agree with the authors that \""while the running time is higher... we expect that it can be improved through further engineering efforts\"", but on the other hand, the idea of nested algorithms (\""matrix-free\"" or \""truncated Newton\"") always has this issue."
8226,1, Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research.
8227,1," Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81?"
8228,1,\n\nMinor:\n\nGTI was not introduced before it is first mentioned in the into.
8229,1,"The same holds for statements like \""... our method is a first step ...\"", which is very hard to justify. "
8230,1," However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with \""holes\"" in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc)."
8231,1,"\n\nThe first paper is the most related, also using homomorphic encryption, and seems to cover a superset of the functionalities presented here (more activation functions, a more extensive analysis, and faster decryption times)."
8232,1, There is no (multiplicative) gating as in Highway Networks.
8233,1," If this is still enough for ICLR, the paper could be okay."
8234,1,\n- Another nice thing about differential privacy and cryptography is that they are impervious to different algorithms because it is statistically hard or computationally hard to reveal sensitive information.
8235,1,\n\n* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks.
8236,1,"  Maybe I\u2019m showing my own lack of understanding here, but it\u2019s worrying that the actual sampling technique is not explained anywhere."
8237,1,"""This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting."
8238,1, The idea of using rank-1 tensor decomposition for training low-rank filtering operations in DNNs has already been proposed and used in several other work.
8239,1," \nIt employs a previously proposed system, Open Classification Network, for classifying instances into known classes or rejecting as belonging to an unseen class, and applies hierarchical clustering to the rejected instances to identify unseen classes."
8240,1, \n- How much data was actually used to learn the Path function in each case?
8241,1,"\n\nHowever, the method described is restricted in the following aspects."
8242,1," \n\n(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations"
8243,1,"  This provides a way of comparing models in a way which is independent of the model parametrization (unfortunately, however, computing the evidence is intractable for large networks)."
8244,1,"\n\n\nA lighter version of Algorithm 1 in Appendix F should be moved in the text, since this is the novelty of the paper.\n"""
8245,1," \n\nThe paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used."
8246,1,"\n\nPros:\nThis method combines the contributions of a few previous works, and obtains a stronger and more general model."
8247,1," So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory, nor it drastically improves practical results that it could become the standard technique in the literature."
8248,1,"""The authors propose a model for sequence classification and sequential decision making. "
8249,1, The approximated eigenoption was simply computed as a one-step greedy policy.
8250,1,"   For example the results in Figure 3 are unclear and need to be discussed in detail in the main text. """
8251,1, Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2.
8252,1,. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried.
8253,1,.\n\nI updated my scores based on the reviewers responses.
8254,1," Or is it joint learning and you learn all LSTMs and CNNs yourself? (Besides the reuse of VGG, I could not find this information explicitly stated within the paper.)."
8255,1,". If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention? "
8256,1," The formulation of the model is straight-forward,"
8257,1,\n\nMinor/typos:\n- what is G(j|G\\j) in eq. (9)?
8258,1, It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear.
8259,1," I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model?"
8260,1,\n- The use of MMD in the context of GANs has also been tried.
8261,1," \""The brain as an efficient and robust adaptive learner.\"" Neuron 94.5 (2017): 969-977."
8262,1," For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2?"
8263,1," Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be.[[CNT], [CNT], [DFT], [MIN]] A simple explanation in the introduction would improve the writing."
8264,1,\n\nThere are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy?
8265,1,\n\nMinor issues:\n\n(1) There are typos in Figure 1 regarding the notations of Question Features and Passage Features.
8266,1,"\n \n1. Introduction\nThere are several important concepts, such as reward distribution, credit assignment, which are used (from the very beginning of the paper) without explanation until the final part of the paper."
8267,1,"\n\nFirst of all, I have to say that the paper is very much focussed on the aforementioned paper, its experiments as well as its (partially speculative) claims."
8268,1,"  The first is that the index i appears twice (once in i=1..N and once in i \\in 1..C), the second is that \u03bb_r refers to an index that does not appear."
8269,1,  Please make this more clear.
8270,1,"\n- define V in Thm. 1.\n- in eq. (4) it may be clearer to denote g_k(w). Likewise in eq. (6) \\hat{g}_\\hat{A}(w), and in eq. (14) \\tilde{g}_{\\cal{A}}(w)."
8271,1, This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps.
8272,1,"\n\nFor the versions of the model that use beam search, what beam width was used?"
8273,1," At times it is detrimental to understanding.[[CNT], [CNT], [DIS], [GEN]] For example I had to read the text leading up to eqn (8) a number of times."
8274,1, \n\nb) the theoretical analysis might be misleading --- clearly section 6.2 shouldn't have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it's not a global minimum.
8275,1," Even if we ignore that for most tasks only the sparse reward (which favors this algorithm) version was examined, these author's only demonstrate success on 4, relatively simple tasks."
8276,1," However, they still work with less assumption, i.e., with no differential functions."
8277,1, This is not true.
8278,1," In particular, the\nselection probability of any particular example could be estimated through a\nheuristic, for example by simply counting the number of neighbouring examples\nthat have a different color, weighted by whether they are in the set of examples\nalready, to assess its \""borderness\"", with high values being more important to\nachieve a good program."
8279,1,"\n\nOriginality: \nThe paper heavily depends on the approach followed by Brutzkus and Globerson, 2017."
8280,1," Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps."
8281,1,"""The idea of using GANs for outlier detection is interesting and the problem is relevant."
8282,1,"\n\n# Minor\n\nmissing . after \""hypergradient exactly\"".[[CNT], [CLA-NEG], [DFT], [MIN]]\n\n\""we could optimization the hyperparam-\"" (typo).[[CNT], [CLA-NEG], [DFT], [MIN]]\n\nReferences:\n Justin  Domke.    Generic  methods  for  optimization-based modeling.  In\nInternational Conference on Artificial Intelligence and Statistics, 2012."
8283,1, Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations.
8284,1,"  Many claims are made without justification (e.g. 2.2. \u201cCavallanti 2012 is not suitable for lifelong learning\u201d\u2026 why?; \u201csimple removal scheme \u2026 highest confidence\u201d \u2013 what is the meaning of highest confidence?), etc."
8285,1, A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.
8286,1,\n\n+ Clarity: \n- Yosinski et al. 2014 citation should be Nguyen et al. 2015 instead (wrong author order / year).
8287,1,"  Then, the difference is crystal clear."
8288,1," These presumably lead to catastrophic failure of the post-transformer network, which is likely to be a problem in any real-world scenario."
8289,1, Is it only for the validation set?
8290,1,"\n\nTable 1 lists 5 models, while fig 3 contains 7, why the discrepancy?"
8291,1," A good historical hook to intelligent tutoring is Anderson, J. R., Boyle, C. F., & Reiser, B. J. (1985)."
8292,1,"\n\nExtractiveness analysis:\nI would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are."
8293,1,  This could help parallelize neural network training considerably.
8294,1,n\nCons\n- Sections 3.1 and 3.2 are hard to understand.
8295,1,\n\nMinor comments:\n\n- Theorems seemed reasonable and I have no reason to doubt their accuracy
8296,1,\n\n - The paper shows that the results are plausible using a neat trick.
8297,1," By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks."
8298,1,"\n\n\nSome specific comments by sections:\n\n2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates."
8299,1,.   The general approach of assigning a parameter budget to ensure fairness in comparison between complex and real-valued networks seems reasonable --
8300,1," Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper."
8301,1,"  In this work the authors provide a way to measure privacy but there is no guarantee that if someone uses this method their data will be private, by some definition, even under certain assumptions."
8302,1," The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized."
8303,1,"\n\nCompared to other papers on neural architecture search, the required computational resource is impressively small: close to state-of-the-art result in one day on a single GPU."
8304,1," This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity."
8305,1, The experimental results are similar to previously proposed methods.
8306,1, but isn\u2019t this just a deeper model with shared weights? And a max operation?
8307,1," In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced."
8308,1,"""Summary: The authors observe that the current image generation models generate realistic images however as the dimensions of the latent vector is fully entangled, small changes to a single neuron can effect every output pixel in arbitrary ways."
8309,1," So instead of showing a quick \u2018hack\u2019, I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided.\n"""
8310,1,"\n\nAmong the different solutions, IT and CI are very redundant."
8311,1," The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations. "
8312,1," \n\nThirdly, experiments on larger datasets, such as ImageNet, will improve the convincingness. \n"""
8313,1, The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.
8314,1,\n\n\nDetailed comments:\n- The upper bound used to derive the formulation applies to a logistic regression classifier.
8315,1," \n\nAlthough it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper."
8316,1,"\n[2] Suzuki, Masahiro, Kotaro Nakayama, and Yutaka Matsuo. \""Joint Multimodal Learning with Deep Generative Models.\"" arXiv preprint arXiv:1611.01891 (2016).\n[3] Tucker, Ledyard R. \""An inter-battery method of factor analysis."
8317,1," However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction."
8318,1, That makes  the contribution very difficult to evaluate.
8319,1, What do you indicate with the box and arrow?
8320,1,"\""\n\n7) Typo below (6): citetbarlett2017...\n\n8) Last paragraph p.5: \""Recalling that W_i is *at most* a hxh matrix\"" (as your result do not require constant size layers and covers the rectangular case). \n"""
8321,1, \n\nEVALUATION:\nCLARITY: I found the paper difficult to read.
8322,1,"""Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane."
8323,1," However, the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges."
8324,1,"""Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches."
8325,1," Why is Genz & Monahan 1998 better than other alternatives such as Monte-Carlo, QMC etc?"
8326,1," The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not)."
8327,1,"  Although the effect has been shown, the model is too specific to a narrow area, and is not general to be applied in a broad sense."
8328,1,\n\nNone of these ideas are new before but I haven\u2019t seen them combined in this way before.
8329,1, Should these parameters be take out of the n-step advantage function A?
8330,1," In other words, the authors oversell their own work, specially in comparison with the state of the art."
8331,1,\n\nIn terms of experimentation it would be interesting to see the reciprocal of the results between two datasets.
8332,1, Thus now the classification is based on the Mahalanobis distance: (z-c)^T S_c (z-c).
8333,1, The quantitative results also support the visuals.
8334,1,\n\nI like this paper.
8335,1, This ensures that VAN starts out as a regular ResNet.
8336,1, Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time?
8337,1," I.e. first doing the expansion keeping the beta terms and Frobenius norm sum, and then going directly to the current O(...) term."
8338,1, Note that a similar strategy has been used in the recent past under the name of stability training.
8339,1,". Moreover, the running of computing the node embedding must be emphasized: On page 2 the authors claim a \""constant time complexity at the instance level\"""
8340,1, Here it would be good to be more specific about the domain.
8341,1, the paper lacks in motivations\nin connecting to relevant previous works and in providing insights on why it works.
8342,1," Also, for each data set, which region size worked best?"
8343,1," \n\nOn the positive side, the task is a nice example of reasoning about a complex hidden state space, which is an important problem moving forwards in deep learning."
8344,1,   \n\nThirdly:  You need to better explain the propagation of uncertainly in section 3.2.2  and the central limit of distribution in section 3.2.1.
8345,1,  I think these contributions warrant publishing the paper at ICLR 2018.
8346,1, The ideas build up on each other in an intuitive way.
8347,1, The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference.
8348,1,"  From the experiments, we can see that the proposed method is effective. "
8349,1, This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework.
8350,1," The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters."
8351,1,. The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally
8352,1,"It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \""raw\"" bootstrapped DQN) and UCB still looks like it does better."
8353,1," For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results."
8354,1," I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution."
8355,1, Did you investigate this?\
8356,1," Despite the same level of clarification has not been reached for what concerns point 2, the proposed framework (although still limited in relevance due to the lack of more realistic settings) can be useful for the community as a benchmark to verify the level of disentanglement than newly proposed deep architectures can achieve."
8357,1,"The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension."
8358,1,"\n\nFor the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:\n\nPengfei Liu, Xipeng Qiu\u2217 and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015\n\nJingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo."
8359,1,\n- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel.
8360,1,\n\nThe paragraph after eq. 17 is duplicated with a paragraph introduced before.
8361,1,"\nDuring training, the agent explores by itself related (but different) tasks, learning a) how actions affect the world state,"
8362,1, Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest.
8363,1,"\n\nFinally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1. I believe this is just a typo."
8364,1,". Some specific comments by section:\n\n3. Rethinking Assumptions:\n-\tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily"
8365,1," \n\nOriginality \u2013 Fairly original, but it still needs some work to justify it better."
8366,1, The method is evaluated with Resnet -50 using synthetic delays.
8367,1," I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math."
8368,1," The model consists of three modules: encoder, interaction module and elimination module."
8369,1,". However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting \u2014 it can in fact magnify gradients of certain tasks and cause over-training and over-fitting."
8370,1," Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model."
8371,1, Figure 2 and 3 could be next to each other to save space.
8372,1, Essentially you perform a local exploration rule in parameter space... and sometimes this is great -
8373,1," The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'."
8374,1,It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower.
8375,1," As such, this may have very little relevance for the actual problem of cfDNA."
8376,1,This would require a smaller ratio of included parameters.
8377,1,"  I think that the paper would be made stronger and clearer if the questions I raised above are addressed prior to publication."""
8378,1, What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot?
8379,1, By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).
8380,1,\n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015).
8381,1,"""This paper presents a pixel-matching based approach to synthesizing RGB images from input edge or normal maps."
8382,1," However, to my opinion, completing it without any supervision is challenging and meaningless."
8383,1,"""This paper provides visualizations of different deep network loss surfaces using 2D contour plots, both at minima and along optimization trajectories."
8384,1,\nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus!
8385,1,"""This paper suggests a reparametrization of the transition matrix."
8386,1," So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough)."
8387,1,"I suspect this is the actual cause for it not converging to zero for Baird's, although please correct me if I'm wrong on that."
8388,1,\n\nThe cons of the approach may be the fact that it needs a pre-trained classifier for the style discrepancy loss.
8389,1,\n* At least one experiment should be carried out on a larger dataset such as ImageNet to further demonstrate the validity of the proposed method.
8390,1,". Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned. "
8391,1, The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption.
8392,1,"\n\n- The objective of the attention mechanism that sets the dropout mask seems to\n  be the same as the primary objective of classifying the input, and the\nattention mechanism is prevented from solving the task by adding an extra\nentropy regularization."
8393,1, Are you assuming isotropic diffusion? Is that realistic? 
8394,1,\n4. This would be a very useful tool for the community if open sourced.
8395,1, The situation worsens with more classes.
8396,1," First off, how is this (constant) width of the predictive region chosen?"
8397,1, It is hard to set the paramter alpha.
8398,1,"  Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation."
8399,1," It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value."
8400,1," Judging the content of the paper alone, it should be accepted."
8401,1,"\n1) you should include in your comparison Query-by- Bagging & Boosting, which are two of the best out-of-the-box active learning strategies\"
8402,1," by 2) reproducing the results in the same way as in the original work,"
8403,1,"  I see these results in Figure 6, however, the plots are condensed and it is difficult to see the exact number at the end of each cycle."
8404,1,"\n- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them."
8405,1," \n\nOverall this is a strong paper and I recommend to accept.\n \n\n\n"""
8406,1," The different ideas are clearly stated and discussed, and hence open interesting questions and debates"
8407,1," To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well."
8408,1," It uses two networks, parameterization network and prediction network to model the mapping from design parameters to fitness."
8409,1," The auto-encoder performs fairly well,"
8410,1, \nThe paper has extensive experiments in a variety of domains.
8411,1, Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network.
8412,1," Second, decades of empirical results illustrating good performance of TD compared with MC methods. "
8413,1," More profoundly, this impression comes from the nature of the investigation and results."
8414,1,"  Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim."
8415,1," This was not done systematically, but a broad general claim that the tensor multiplication models does the best cannot be verified."
8416,1," Authors present no clear explanation on why the shift should result in improved performance."""
8417,1, Experiments on MNIST using the proposed deep function machines (DFM) are provided.
8418,1, (end of page 2)\n- Is it correct that the same representation stored in the NE table is used twice?
8419,1, The features are selected such that ...  determines the distribution\nof future observations \u2026 Filtering is the process of mapping a predictive state\u2026\u00a0\u00bb
8420,1," A conv net on the image is combined with that on a state image, the state being interpreted as rechable pixels."
8421,1,"\nThe setup is not completely clear,"
8422,1,. Not clear how important they are\n- they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary).
8423,1,"  Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results."
8424,1,"\n2. Pairwise Classification Network, (binary sigmoid, trained on pairs of images of same/different classes)"
8425,1, The results in the paper are mostly qualitative and only on MNIST.
8426,1," As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery."
8427,1,"I\u2019m not saying those differences aren\u2019t there, but the paper simply didn\u2019t emphasize them very well and I had to reread the paper from Ba et al. (2016) to get the full picture. "
8428,1,\n\nThere exist a number of \u201cdrag and drop\u201d style UI design products (at least for HTML) that would seem to accomplish the same basic goal as the proposed system in a more reliable way.
8429,1,An further external reference \ncould be used to give an idea of what would be the experimental result at least in the supervised case.
8430,1,\n\nThis paper proposes a regularization strategy for autoencoders that is very\nsimilar to the adversarial autoencoder of Makhzani et al.
8431,1," \n\nI also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? "
8432,1,  \n\nQuality: The paper is good
8433,1," From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role. "
8434,1, It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort(given by the analytical solution)
8435,1," \n\nI find the problem of defogging quite interesting, even though it is a bit too Starcraft-specific some findings could perhaps be translated to other partially observed environments."
8436,1,\n\nWeak points:\n\n1. The novelty of this paper is limited.
8437,1,"""This paper introduces siamese neural networks to the competing risks framework of Fine and Gray."
8438,1,"""\n1) Summary\nThis paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably."
8439,1,\nI also cannot see the promising performance.
8440,1," It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change."
8441,1, The network learns unsupervised using a predictive coding setup.
8442,1,"\n- Page 2, line 12: \""prices."
8443,1," Furthermore, while their approach is sold as a general sensor fusion technique,"
8444,1,  What about comparison to other batch normalization methods in biology such as SEURAT? 
8445,1,"\n\nI'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?"
8446,1,"\n\nClarity: The use of the term \""adversarial\"" is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label (e.g. from \""church\"" to \""tower\"" or vice-versa)"
8447,1,"\n\nThe new evaluation metric proposed in Section 6.1.1 is confusing, and I do not understand what the metric is trying to capture. "
8448,1,"""The authors propose a method for performing transfer learning and domain adaptation via a clustering approach."
8449,1,   What would be useful would either be some form of positive control.
8450,1," Designing an attack for a specific defense is very well established in the literature, and the fact that the attack fools this specific defense is not surprising."
8451,1," On geometry images, in particular it would be possible to apply standard convolutional architectures without any special processing."
8452,1, This work makes a pretty significant contribution to such topic.
8453,1," Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes."
8454,1," \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n"
8455,1," Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community."
8456,1,"\n\n\n[1] Dessein, A., Papadakis, N., & Rouas, J. L. (2016). Regularized Optimal Transport and the Rot Mover's Distance. arXiv preprint arXiv:1610.06447."
8457,1,". The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features."
8458,1,  This paper defines and examines an interesting cooperative problem: Assignment and control of agents to move to certain squares under \u201cphysical\u201d constraints.
8459,1,\n\nMinor points:\n- Figure 1 does not seem to be referenced in the text 
8460,1,\n\nPros:\n- Many experiments which try to study the effect
8461,1,"\n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance."""
8462,1, Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians.
8463,1," The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]\n\nAnother way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2]."
8464,1," The authors do not comment on this.[[CNT], [CNT], [DFT], [MIN]] It would be nice for them to explain the circumstances under which the proposed method is best suited and any potential failure cases (e.g. cases when the low-rank decomposition leads to a significant decrease in performance)."
8465,1,"By the way, it will be better to compare to the snapshot of Oct 2017 as well, indicating the position of this work during the submission deadline."
8466,1," Again, the short walk may be capturing the communities but the high-dimensional random walk sample path seems like a high price to pay to learn community structure."
8467,1," Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram."
8468,1, So maybe one should not feature the term SDE so prominently.
8469,1, \nIn the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets.
8470,1,"\n\nThe best-claimed method of the method, called \""Hybrid method\"" is also mentioned only in passing, and that too in a scratchy fashion (see end of subsec 4.3):"
8471,1, \n\n- It is hard to map System-{ABCD} to the underlying proposed methods described in Table 2.
8472,1,I am wondering how a variety of multi-task settings can be handled by the proposed approach.
8473,1," \n3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss)."
8474,1,\nOriginality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces.
8475,1," In figure 8, my interpretation of (a) is\nthat the initial model has learned to be invariant to color and this remains true even if the fine-tuning data\ndo not contain any negative data."
8476,1, I would like to see more formal definitions of the methods presented.
8477,1, The downsampling experiment in Section 5.1 seems to indicate the contrary: downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location.
8478,1,\n\n- Stopping condition.
8479,1,\n\nSection 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }.
8480,1,\n\nWhat about distributed SGD or asyncronous SGD (hogwild)?
8481,1, The abstract and introduction should also be modified accordingly to more honestly reflect the current contributions.
8482,1, To address this issue a `loss aggregation\u2019 is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used.
8483,1," There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \""evolutionary\"" computing."
8484,1, It is not clear how the model\nlearnt on one particular type of data transpose to other data sequences. 
8485,1,\nThe model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities.
8486,1," If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still \""work\"" and the distribution of X can also change without affecting the accuracy of the predictions."
8487,1," By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin)."
8488,1," For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used."
8489,1,"\n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) ."
8490,1,"""In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data."
8491,1,"\tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?"""
8492,1,"\""  However, E can be intra-community because communities are partitioned by METIS."
8493,1,"""The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM."
8494,1," \n\nClarity: The paper is well structured and written, with a nice and well-founded literature review."
8495,1, Batch-norm?
8496,1," \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective."
8497,1, Is it the % of times that the classifier is confused?
8498,1, \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain.
8499,1, Both Ls are not cross-entropy because label and childnum are not probabilities.
8500,1,"""This paper adds source side dependency syntax trees to an NMT model without explicit supervision."
8501,1, Experiments on several real-world dataset reveal modest gains in comparison with the state of the art.
8502,1,"\n\n2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out. "
8503,1,\n\nThis work shows few interesting results
8504,1, The authors claimed to obtain efficiency improvement and better numerical stability.
8505,1,This seems very problematic if most of the entries are not observed.
8506,1, This is in general not true.
8507,1," From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information."
8508,1,"""Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output."
8509,1,"\n\nTo sum up, I think that the general idea looks very natural and the results are supportive."
8510,1,  I don't find the argument convincing.
8511,1, It has a very nice introduction and literature review of Prioritized experience replay
8512,1, Experiments are made in En-De and En-Ru.
8513,1," However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation)."
8514,1," \n\nOverall, the work is presented as a positive result in very specific conditions but it seems more like a negative result."
8515,1,"""The paper proposes a method  for learning object representations from pixels and then use such representations for doing reinforcement learning."
8516,1,"  However, there are a number of statistical issues that should be addressed."
8517,1,"\n4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up."
8518,1,\n\nProblem (2) is identical to robust PCA and Theorem 3.1 is common in matrix completion literature.
8519,1,"  The most novel contribution of this ICLR paper seems to be equation (4), where the authors set up an optimization problem to solve for optimal inputs; much of that optimization set-up relies on Hazan's work, though."
8520,1,\nThe basic form of SGD selects an example uniformly.
8521,1,".\n\n- Fully evaluating the \""claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\"" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization."
8522,1," The paper also lacks discussion with other models that use dilated convolution in different ways, such as WaveNet[1]."
8523,1,"""The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems"
8524,1,\n- Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term?
8525,1,"\n* Figure 4 misses the red trajectories, also it would be easier to have colors on the same (log?)-scale. "
8526,1, This paper simply presents the results of different parameter k on testing set directly.
8527,1,n\n- The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs.
8528,1,"I can't see where the number \""124x\"" in sec 5.1 stems from."
8529,1,"\n\nQuestions:\n* How did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc."
8530,1," They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \""context units\"" vector by a scalar."
8531,1,\n\nThe writing is ok.
8532,1,"   To paraphrase it: If the prior vanishes, so does the regularizer. Fine."
8533,1," Oh - now I get it.\nBecause it takes an extended n-step transition and generates an action.  \n\n\n\n\n\n"""
8534,1," There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution."
8535,1,  It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions. 
8536,1," \n\nTo analyze this specific property, the authors propose a concept called \""start-end rank\"" that essentially models the richness of the dependency between two disjoint subsets of inputs"
8537,1," However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU."
8538,1," Overall, also in the light of figure 4, the interpretation that the new algorithm results in better generalization seems to stand on shaky ground, since differences are small."
8539,1, The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks.
8540,1,\n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).
8541,1," \n\n\nIn a couple of places early in the paper, you mention that the neural net computes \u201cthe probability\u201d of examples."
8542,1,\n\nMinor:\n- It\u2019d be helpful to add the formulation of gated linear units and residual layers.
8543,1, Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one).
8544,1,\n\n- This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy.
8545,1," Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task."
8546,1,"\n\nAlthough the results are promising,"
8547,1, The results are not surprising:\n\n* NMT is terrible with noise.
8548,1, Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient?
8549,1," The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks."
8550,1,\n\t \n3. The LSH schemes are not correctly referred to.
8551,1,\n\nCOMMENTS:\n\nThe paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales.
8552,1,\n\nMinor points:\n- The definition of a stochastic policy (section 2) is unusual (it is defined as an unnormalized distribution).
8553,1,"\n\nAnother interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.\n\n\nThe proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be."
8554,1,"\nPros: interesting idea, relevant theory provided, high-quality experiments"
8555,1, How do we know if generalizing saturated STE is more worthwhile than generalizing STE?
8556,1," Finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years."
8557,1, \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.
8558,1," Also, the detailed specification of the VAE should be detailed."
8559,1,"""This paper considers the task of aspect sentiment classification, which entails categorizing texts with respect to the sentiment expressed concerning particular aspects (e.g., television resolution or price)."
8560,1," There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6)."
8561,1," Related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful."
8562,1," Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0)."
8563,1,"\n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased."
8564,1," (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize."
8565,1," I think it will eventually become a good paper, but it is not ready yet."
8566,1," Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training."
8567,1,"  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?"
8568,1,"\n\nIn the PCA result, it is not \""clear\"" that the first axis represents valence, as \""sad\"" has a slight positive on this scale and \""sad\"" is one of the emotions most clearly associated with negative valence."
8569,1," Currently, Section 3 is called \""Background\"" which does not say much.[[CNT], [null], [DIS], [MIN]] Section 4 contains CIGMs, Section 5 Causal GANs, 5.1. Causal Controller, 5.2. CausalGAN, 5.2.1. Architecture (which the causal controller is part of) etc."
8570,1,\n\nCons:\nNot clear justification and motivations
8571,1,"  But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about."
8572,1, Sparse Gaussian processes using pseudo-inputs.
8573,1,"\n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes"
8574,1,\n(3) what was the actual computational cost for the BBB RNN and the baselines?
8575,1,"\n\na useful reference:\n\nStrom, Nikko. \""Scalable distributed dnn training using commodity gpu cloud computing.\"" Sixteenth Annual Conference of the International Speech Communication Association. 2015.\n\n"""
8576,1,"""\nSummary:\n A method for creation of semantical adversary examples in suggested."
8577,1,  But the experiments are not convincing enough.
8578,1," To do something similar with GPs, we would need to learn the kernel."
8579,1, They claim their contribution is in the analysis of loss surfaces (true) and neural nets applied to graph-structured inputs.
8580,1, The best method finally outperforms the lea-3d baseline for summarization.
8581,1, There were a few things that jumped out to me that I was surprised by.
8582,1," Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? \n"""
8583,1," Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding."
8584,1,. It seems to say for instance that a model is trained on 2 and 3 layers then queried with 4 and the accuracy only slightly drops
8585,1, some of the algorithms tested are similar algorithms that have already been proven to work well in practice.
8586,1, \n\nClarity: Main ideas are clearly presented.
8587,1, This paper assumes the link function is invertible.
8588,1," \n\nFinally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images). "
8589,1,"\nAlso, how about using A^2 in GCN or making two GCN and concatenate them in\nfeature space to make the representational power comparable?"
8590,1,"\nWhile this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue."
8591,1,"  If such were present, I'd rate this paper significantly higher."
8592,1,"\n\nOverall, I believe this paper to be a useful contribution to the literature."
8593,1, What is your compression ratio for 0 accuracy loss?
8594,1, The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network).
8595,1,\n\nSignificance\nThe points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.
8596,1,"""The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks."
8597,1,"n\nThe comparison with other methods is very week, the authors compare their approach with two very simple alternatives, namely a first-order autoregressive mode and the Kalman filter."
8598,1,"\n\n2. In Lreconstruct, only min difference between A and A1 is considered."
8599,1," and  the results on datasets are not explained either (and pretty bad). A waste of my time."""
8600,1, The paper also did not report sample quantitative measures e.g. Inception scores / MS-SSIM.
8601,1,"\n\nThe data-generating process for the considered model, given in Figure 2, seems to be consistent with Figure 1 of the paper \""Domain Adaptation with Conditional Transferable Components\"" (by Gong et al.). Perhaps the authors can draw the connection between their work and Gong et al.'s work and the related work discussed in that paper."
8602,1," \n\nMinor comments: \nIn introduction, parameter with zero training error doesn't mean it's a global minimizer\nIn section 2, it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima."
8603,1,\n- Evaluation is not very convincing.
8604,1,\n\n2) This problem might actually matter.
8605,1," My best guess is that it is the same as p_u(), the underlying data distribution, but makes parsing the paper hard."
8606,1,". Thus although the \\|z_{0}\\|^{2} and \\|z_{1}\\|^{2} are chi-squared/gamma, I don't think \\|z^\\prime\\|^{2} is exactly gamma in general."
8607,1," Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example."
8608,1,"\n\nPresentation aspects:\n- Minor typo: Page 2, last paragraph of Introduction: `... will act act identically."
8609,1," If so your phrase \""is zero given a sequence of inputs X1, ...,T\"" is misleading."
8610,1, Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks.
8611,1, And what is the relation of the black/white and orange squares?
8612,1,"\n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models."
8613,1," Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration."
8614,1,"\n    -- \""if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \""."
8615,1,"The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. "
8616,1,"""The paper is a pain to read."
8617,1," By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility"
8618,1,"\n\nMinor comments\n-------------------------\nThe stochastic gradient equation in Sec 2.2.2 is missing a subscript: \""h_i\"" instead of \""h\""\"
8619,1," Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction."
8620,1," The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or \u201cstyle\u201d features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy."
8621,1,  \n\nTaxi: the authors train the PATH problem on reachability and then show that it works for TAXI.
8622,1," The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \""spectrally normalized\"" objective."
8623,1,\n\nThere are several drawbacks of the current format of the paper:\n1. The algorithm is vague.
8624,1, Or other component of DeepLab?
8625,1,"""OVERVIEW: The authors present results from several state-of-the-art generative models trained on a facial dataset for learning a general facial identity space."
8626,1,"""\nThe authors present a study that aims at inferring the \""emotional\"" tags provided by Thumblr users starting from images and texts in the captions."
8627,1,"\n\nIn contrast to the theoretical part, the experiments seems very encouraging."
8628,1,"""Pros\n- The proposed model is a nice way of multiplicatively combining two features :\n  one which determines which classes to pay attention to, and other that\nprovides useful features for discrimination."
8629,1," In order to make sure the second \nequality in Equation 2 holds, p_mu (y|x,t) = p_pi (y|x,t) should hold as well."
8630,1,"""The paper proposes a way of generating adversarial examples that fool classification systems."
8631,1,"""1. This is an interesting paper - introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm\n2."
8632,1," Within the experimental results, only two methods are considered: although Info-GAN is a reliable competitor, PCA seems a little too basic to compete against."
8633,1,.\nThe paper states three conjectures (predictions in the paper):
8634,1, The visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class.
8635,1," First, I found the cosine similarity scores in Table 1 largely uninterpretable."
8636,1, Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)?
8637,1, The derivation through Stein\u2019s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick.
8638,1," It adds insights that are of value even if the method should turn out to have significant overlap with existing work (see above), and perform \""only\"" on par with these:;"
8639,1,"""Overview\n\nThe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets."
8640,1,"n\nNote that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes."
8641,1,"\n\nThe main drawback of this paper is that there is no theory to suggest the ProxProp algorithm has better worst-case convergence guarantees, and that the experiments do not show a consistent benefit (in terms of time) of the method."
8642,1,"""This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer. "
8643,1,"\nIn the following, the authors explore the landscape of RNNs satisfying the necessary conditions."
8644,1,\n\n\nSome minor remarks :\n- p3: the following sentence is not clear  \u00ab\u00a0 Our hypothesis is that the lowest complexity small discrepancy mapping approximates the alignment of the target semantic function.
8645,1," I see several issues:\n(i) It seems that you are not doing much with a SDE, as you diredctly jump to the discretized version (and ignore discussions of it's discretization)."
8646,1," The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary."
8647,1, It may be more clear to change this phrase to \u201cevolutionary methods\u201d or similar.
8648,1,\n\n-- The rating has been updated.
8649,1," If so, it's never measured in this work."
8650,1,"\nThe subtleness of this work over most other video prediction work is that it isn't conditioned on a labeled latent space (like text to video prediction, for example)."
8651,1," While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like \u201c\\lambda_rec\u201d and \u201c\\lambda_norec\u201d"""
8652,1,"\n\n2) Even if for sin activation functions, the analysis is NOT complete. "
8653,1, How does thresholding mentioned in Figure 5 work?
8654,1,\n\nSignificance:  The paper lacks of theoretical justification as well as the experiments are not convincing.
8655,1," In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better."
8656,1,"Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea."
8657,1," This also is not in the spirit of the original VAE, where unconditional generation involves sampling from the prior over the latent space."
8658,1, I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).
8659,1,\n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input.
8660,1," For example, are there results which prove anything regarding the convergence of a stochastic process under different amounts of noise?"
8661,1,"\n\n\nReview update after reply:\n\nThe authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept. \n"" 001 101"
8662,1, \n\nAn important feature of this model is it is easier to parallelize and speed up the training/testing processes.
8663,1," Since you are not running experiments on Atari here, it is invalid to simply assume that such a second network is needed."
8664,1, I believe the paper would have been much stronger if either of the two above are further investigated.
8665,1,"\nCouldn't comparisons be made, in some way, to other multitask work?"
8666,1, From the paper it is very difficult to make out exactly what architecture is proposed.
8667,1, The main contribution of the paper is the introduce such techniques to the ML community and presents experimental results for support.
8668,1,"\n\n- The idea of sample reweighting within the MMD was in fact already used for DA, e.g., Huang et al., NIPS 2007, Gong et al., ICML 2013."
8669,1," While on MNIST and CIFAR, DTP and SDTP performed as well as backprop, they perform worse on ImageNet"
8670,1,\n-The evaluations rely on using a pre-trained imagenet model as a representation.
8671,1,"\nThe paper should present a comparison with such kinds of models.\n"""
8672,1,"\nUsing PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model."
8673,1, (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities).
8674,1,"""The approach solves an important problem as getting labelled data is hard."
8675,1," In the videos, it seems that the people and chairs are always in the same place."
8676,1," In this work, activation functions are considered random functions with a GP prior and are inferred from data."
8677,1, The experiment results seem solid and the proposed structure is with simple design and highly generalizable.
8678,1,\n\nAdditional experimental results would make it a stronger paper.
8679,1, It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good).
8680,1,\n--The experiments were well carried through.
8681,1,"\n- Strong potential impact, especially on constrained power environments (but not limited to them)"
8682,1,"""The paper proposes to add a rotation operation in long short-term memory (LSTM) cells."
8683,1,"""The paper describes a way of combining a causal graph describing the dependency structure of labels with two conditional GAN architectures (causalGAN and causalBEGAN) that generate  images conditioning on the binary labels."
8684,1,"\n\nThis aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset."
8685,1,"\n\nOn the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error)."
8686,1,"\n\n \""4/ We discuss how to use these insights to improve the design of WGANs more generally."
8687,1, The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest.
8688,1," \n\nFurther, I miss some baselines and ablation study."
8689,1,"""The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent."
8690,1, The paper is not surprising
8691,1," I would be curious if the same trends and conclusions can be drawn from a larger dataset -- e.g., the internal dataset that results are reported on later in the paper, or on a set like Switchboard."
8692,1,"""This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU."
8693,1," Since this is a major motivation of the paper, I actually do not see how the setup makes sense!"
8694,1, The 2-dimensional space is binned using an imposed grid structure.
8695,1," Then an algorithm is given to convert a generic plan into a Monge map, a deterministic function from one domain to the other, following the barycenter of the plan."
8696,1, but the work lacks maturity.
8697,1, Or it's trained jointly with PCN and OCN? 
8698,1,\n\nI addition I don't understand how learning confidence of the value function has a realizable target.
8699,1,"\n\n- In light of the time series aspect being the main contribution, a\n  really obvious question is: what does it learn about the time\n  series?"
8700,1,\n3.\tIt is not clear from the paper how should the questions which are unanswerable be evaluated.
8701,1," I guess the most accurate opposite would have been \""The service is quick but not good\""... )"
8702,1,"""The quality of the paper is good, and clarity is mostly good."
8703,1,"\n3. Some equations are labeled but never referenced, e.g., Eq. (4)."
8704,1, The paper also advances that GPU parallelization must be used to be able to efficiently train the network.
8705,1,"\n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more."
8706,1," Specifically in the PPD game, the use of CCC produces interesting results; when paired with cooperate agents in the PPD game, CCC-based players produce higher overall reward than pairing cooperative players (see Figure 2, (d) & (e))."
8707,1,"\n\n\n- Cons of this work\n\nAlthough this work introduces a new GP covariance function inspired by deep wide NNs, I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful."
8708,1," Would different communities have different choices of T?[[CNT], [CNT], [QSN], [MIN]] \nh)\tAnd a related question, how well can the method generate the inter-community links?"
8709,1, Figure 2 showed almost similar plots for all the varieties.
8710,1,"\n\nI think the separability of the filters in this case brings the right level of\nsimplification to the learning task, however as it also holds for the planar case\nit is not clear whether this is necessarily the best way forward."
8711,1,"""This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used."
8712,1," E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc."
8713,1,\n2. The generalization over TT makes sense.
8714,1, \nOverall the process by which GPNs are made tractable to train leverages many recent and not so recent techniques.
8715,1,"  Also, it is unclear to me why this is an interest question."
8716,1,"""This paper presents a neural architecture for converting natural language queries to SQL statements."
8717,1," The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context."
8718,1,\nThere is only one layer here so we don\u2019t have complex non-linear things going on?
8719,1,\n\nReferencing is okay.
8720,1,? What are do the shaded regions on plots indicate?
8721,1, But these are also the same set of experiments performed by Cai et al.
8722,1,  The generated textual samples look good and offer strong support for the model.
8723,1, It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights.
8724,1,\n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems?
8725,1," I can't however, think of other baselines other than \""ignore\"" so I guess that is acceptable."
8726,1,\n\nPros:\n1. Well written paper with clear presentation of the method. 
8727,1, \n\nThe paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain.
8728,1,  The proposed approach claims two advantages over a baseline maximum likelihood estimation-based approach.
8729,1,"\n\nWhile the contribution is explored in all its technical complexity, fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phase."
8730,1, This seems like a very minimal change in the overall architecture proposed.
8731,1,"\n\n1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a \""discrepancy\"" to me."
8732,1,\n[3] M.K. Titsias and N.D. Lawrence. Bayesian Gaussian process latent variable model. 
8733,1," \n\nIn equation 2, please check the measure of the mixture."
8734,1, \n\n*Quality* \nThe problem addressed is surely relevant in general terms.
8735,1, In that report \\alpha_l is a scalar instead of a vector.
8736,1,d.\n\n# Summary\nThis paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks.
8737,1,\n- It would be helpful to define t_k explicitly to alleviate determining whether it is the interval time between ordered events or the absolute time since t_0 (it's the latter).
8738,1, How large can this be expected to scale (a few thousand)?
8739,1, I would have liked to have seen results on ImageNet.
8740,1, I know very well the facial alignment literature and I do not understand this reference.
8741,1," So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree)."
8742,1," The main novelties of the paper are: (1) the use of Tumblr data,"
8743,1,\n\n+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem.
8744,1,"\n\nFinally, the effect of choosing GAN features vs a more \""naive\"" feature\nspace is not explored in detail."
8745,1," In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game."
8746,1,\n\n\n- Pros of this work\n\nThe paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN.
8747,1,"\n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.)."
8748,1,"\n\n- Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer?"
8749,1, The comparison of the dual critic to the true Wasserstein distance is very interesting.
8750,1,"?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces"
8751,1,\nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10
8752,1,\n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)
8753,1," However, a fully\ncooperative agent can be exploited by a defector."
8754,1,"Overall, it is unclear to me what the advantage of the algorithm is over pure supervised learning, and I don\u2019t think a compelling case has been made."
8755,1, The paper does an insufficient job describing why deep RL is the right way to formulate this problem.
8756,1," While I appreciate the technical contributions,"
8757,1, That these steps are general?
8758,1," In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE."
8759,1," They could have done a better job explaining the quality of their final results, though. "
8760,1, \n+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.
8761,1,"  I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa?"
8762,1," As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term."
8763,1,"\""\n-This sounds somewhat vague."
8764,1,"\n21. Appendix A to E are not necessary, since they are from the literature.[[CNT], [PNF-NEG], [CRT], [MIN]]\n22. sct 3.1, par 2: \""is approximately unitary.\"" -> \""is approximately unitary (cf Appendix F).\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n23. sct 4, par 1: \""and backward operations.\"" -> \""and backward operations (cf Appendix G and H).\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nPros\n------\n\n1. Nice Idea that allows to decouple the hidden size with the number of hidden-to-hidden parameters."
8765,1," \n\nThe paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid."
8766,1,"""This paper presents a model to encode and decode trees in distributed representations."
8767,1,"""This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix."
8768,1,\n\n2) The theoretical results justify the optimization procedures presented in\nsection 5.
8769,1,"\n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts."
8770,1,\n\n- Most of the experiments in the main paper are on toy tasks with small LSTMs.
8771,1,"  This manuscript formulates a convex program of optimal control without the separate system ID step, resulting in provably optimality guarantees and efficient algorithms (in terms of the sample complexity)."
8772,1,"\n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation)."
8773,1,"\n\n6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse?"
8774,1, \n\nThe major problem with this paper is its clarity.
8775,1, They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features.
8776,1,\n- Reads well
8777,1,"\n\nThe presentation of the core idea is solid,"
8778,1," Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure."
8779,1," Eq. (13) is introduced as a \""solution\"" to a non-existent problem, because the virtual observations are drawn from the same prior as the real ones, so it is not that we are \""coming up\"" with a convenient GP prior that turns out to produce a computationally tractable solution, we are just using the prior on the observations consistently.\n\nIn general, the authors seem to use \""approximately equal\"" and \""equal\"" interchangeably, which is incorrect."
8780,1, But I find the paper lacking a lot of details and to some extend confusing.
8781,1," For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside).\"
8782,1," \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified."
8783,1," They show that their analytical solution implements a form of BFS algorithm, characterize the probability of introducing \""bugs\"" in the algorithm as the weights move away from the optimal solution, and how this influences the error surface for different depths."
8784,1, I recommend the authors to perform either proper literature review or cite one or two papers on the time complexity and their weakness.
8785,1, \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings.
8786,1,"\n- The most influential deep learning paper here might be Seide, Li, Yu Interspeech 2011 on CD-DNN-HMMs, rather than overview articles"
8787,1, but it shows significant gains from it.
8788,1,")\n\nBecause the authors improved the manuscript, I upwardly revised my score to 'Ok but not good enough - rejection'."
8789,1,   Would it have fixed the Google gorilla problem?Why or why not?
8790,1,"\n\n* \""Furthermore, we exploit the existing network architecture as variational decoders rather than resort to variational decoders that are not part of the neural network architecture."
8791,1, The word similarity scores are also generally low: it's easy to achieve >0.76 on MEN using the plain PPMI matrix factorization on Wikipedia.
8792,1," I suppose it is L2 norm loss, but it must be clear in the paper."
8793,1, It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited.
8794,1,\n\nThe proposed algorithm is new and writing is clear.
8795,1,"""Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization."
8796,1,"\n\nA/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss."
8797,1,"\"" \n\nHighway networks were published half a year earlier than resnets, and reached many hundreds of layers before resnets. Please correct."
8798,1, I think that this paper should quantify the effect of an increase of $L$.
8799,1,The robot learns inverse and forward models through autonomous exploration.
8800,1,"\n\nResults show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. "
8801,1,\n\nSentence after eq. 15 seems to contain a grammatical error.
8802,1,\n- It is not clear what Table 2 is showing.
8803,1," There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help."""
8804,1,"\n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \""inference\"" several times without definition"
8805,1,"\n- It is unclear what the influence of the smoothing is, and how the smoothing parameter is estimated / set."
8806,1,"\n- The details in appendix B are interesting, and I think they should really be a part of the main paper.[[CNT], [CNT], [APC], [MAJ]] That being said, the results in Section B.5, as the authors mention, are somewhat preliminary,"
8807,1, Do you have experiments by fixing the number of layers and varying the hidden size?
8808,1,"  However, where the paper falls a bit short is in the discussion / outlook in terms of suggestions of how one can go about tackling these shortcomings.  """
8809,1," (So, the *interpretation* of the message is learned). I like this idea,"
8810,1,"""Neal (1994) showed that a one hidden layer Bayesian neural network, under certain conditions, converges to a Gaussian process as the number of hidden units approaches infinity."
8811,1," This is much worse than the baseline of 12% in LeCun et al. (1998), using simple linear classifier without any preprocessing."
8812,1, \n+ ablation study / analysis of influence of parameters
8813,1," \n\nGiven the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable. "
8814,1," I suggest the authors try this experiment as well and I hope the answer is no, as separated input embeddings for gate functions was verified to be useful in some \""old\"" works with syntactic features as gate values, like \""Semantic frame identification with distributed word representations\"" and \""Learning composition models for phrase embeddings\"" etc."
8815,1,\n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy.
8816,1,\nThese subgoals are linear functions of the expert\u2019s change in state (or change in state features).
8817,1,"""The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams."
8818,1,"  While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset."
8819,1,\u201d Is there any empirical evidence for this?
8820,1,"\n- In the table 1 caption, it is written \""same graph structure with training set\"" --> do you mean \""same graph structure than the training set\""?"""
8821,1, (It would be excellent if an outcome of this paper was that commercial MT providers answered it\u2019s call to provide more realistic noise by actually providing examples.)
8822,1, The MNIST example is compelling.
8823,1,"  Here are a few:\n- \""For L=16, batch size of 20, ...\"": not a complete sentence. "
8824,1,"\n\nIn Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp."
8825,1,"""Summary: This paper tackles the issue of combining TD learning methods with function approximation."
8826,1,"""The paper proposes an approach to learning a distribution over filters of a CNN."
8827,1," The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST."
8828,1,"\nQuickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E."
8829,1, It it thus hard to properly evaluate your method against other proposed methods.
8830,1, Also fig 6 misses captions.
8831,1, It is problemistic that if the baselines have bad parameters.
8832,1," Even for the example the authors give in section 2, it is\nunclear that this would be true."
8833,1, \n-   Equation (5)  should be - O(\\alpha^2)
8834,1,\n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n
8835,1,\n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability)
8836,1,"\n\nFor few-shot learning, the authors mentioned that the \\alpha's are adaptable parameters but did not mention how they are adapted."
8837,1," In particular, if we seek to minimize f(W) such that W belongs to asset that can be easily projected on, then projected gradient descent would apply traditional gradient descent on the current iterate, followed by a projection step onto this set."
8838,1," 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16."
8839,1, Why the same intuition of UIE can be applied to RNNs?
8840,1," \n\nNevertheless, while the idea behind the proposed approach is definitely interesting,"
8841,1," But, in the experimental section results are shown only for a  single value, alpha_new=0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested."
8842,1,"""This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity."
8843,1,\n\n* The references section is highly inadequate -- no venues of publication are given.
8844,1,"\n\nSignificance\nThe work could be potentially significant,"
8845,1,"\n\nOverall, I think the contributions of this paper are too marginal for acceptance in a top tier conference."
8846,1, It would really help if the paper gave some expository examples.
8847,1," \n- 36x36 patches have a plausible size within a 84x84 image, this is rather large, do semantic parts really cover 20% of the image?"
8848,1, Currently a generic bound is employed which is not very insightful in my opinion.
8849,1,"\n\nIn summary, I think the paper can be accepted for ICLR."
8850,1,"\n\nOverall the results seem interesting,"
8851,1,"\n\nFirstly, I suggest the authors rewrite the end of the introduction."
8852,1,"  It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available."
8853,1," The primary motivation comes from the use of a separate target network in DQN, which seems to be needed in Atari (though I am not aware of any clear result that demonstrates why, rather just from informal discussions)."
8854,1,\n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data)
8855,1, Why are the classification errors for DBN and MLP in the Tab 1 so high?
8856,1," I would expect to see a comparison of the vanilla prototypical nets against their method for each one of the different scenarios of the free parameters of the S matrix, something like a ratio of accuracies of the two methods in order to establish whether learning the Mahalanobis matrix brings an improvement over the prototypical nets with an equal number of output parameters.  \n\n"""
8857,1,  The paper does suggest learning a p(z|x) but does not provide implementation details nor experiment with this approach.
8858,1," Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future."
8859,1," Therefore, the authors derived the update formulate based on the analytical continuation technique."
8860,1,"\n\n\nMinor issues & typos\n- Section 3.1, psi_gh = psi_g psi_h."
8861,1, The ideas are illustrated through several well-worked micro-world experiments.
8862,1," All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems."
8863,1," As a consequence, it uses:\n-\tOne temporary memory storage (inspired by hippocampus) and a long term memory\n-\tA notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks."
8864,1," \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising."
8865,1, Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data.
8866,1, Complexity for solving the Babi tasks could be added later.
8867,1,    \n The final presence values though are sampled using Gumbell-softmax.
8868,1,"\nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart?"
8869,1,  The model is trained on a new dataset collected from Tumblr.
8870,1, Also large batches are used mainly during training where memory is generally not a huge issue.
8871,1,\n-How does the distribution of Q values look like during different phases of learning?
8872,1,"\n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}."
8873,1," theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used)."
8874,1,\n\n\nClarifications:\n- See the above mentioned issues with the exposition of the technique.
8875,1," I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot."
8876,1,\n\nQuality:\nThe quality is very good.
8877,1,"""This paper introduces an appealing application of deep learning: use a deep network to approximate the behavior of a complex physical system, and then design optimal devices (eg airfoil shapes) by optimizing this network with respect to its inputs."
8878,1," I would be interested to know what happens if the causal graph is not known, and even worse cannot be completely identified from data (so there is an equivalence class of possible graphs), or potentially is influenced by latent factors."
8879,1,\n\npros:\n\n- good use of intuition to guide algorithm choices
8880,1,  The networks it is demonstrated on are not particularly large (largeness usually being the motivation for pruning) and the need for making them smaller is not well motivated.
8881,1, The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems.
8882,1,  More discussion is needed about the role of edges in E.
8883,1,"\n\nAdditionally, I do not think the current gridworld task adds anything to the experiments, I would rather actually see this on a more interesting linear function approximation on some other simple task like Mountain Car than a neural network on gridworld."
8884,1," Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution). "
8885,1," There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture."
8886,1,\n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.
8887,1,"""The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick."
8888,1, This is not the fault of the present paper but an unfortunate tradition in the field.
8889,1,"""There are many language issues rendering the text hard to understand, e.g.,\n-- in the abstract: \""several convolution on graphs architectures\""\n-- in the definitions: \""Let data with N observation\"" (no verb, no plural, etc)."
8890,1," The idea is to jointly embed an image and a \""confidence measure\"" into a latent space, and to use these embeddings to define prototypes together with confidence estimates."
8891,1, The following section (the part starting from 5.3) presents the key to the success of the proposed measure.
8892,1, The paper does not aim at learning causal structure from data (as clearly stated by the authors).
8893,1,\n? p.7: What does \u201cfind the mid-point of the bin\u201d mean and should it not be 1018 instead of 1000 bins?
8894,1,"\n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new."
8895,1," The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards."
8896,1, Essentially the authors propose an approach to use a node embedding to achieve graph classification.
8897,1,"\n4) The paper does not give any background on stochastic differential equations, and why there should be an optimal noise scale 'g', which remains constant during the stochastic process, for converging to a minima with high evidene."
8898,1, There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one.
8899,1,"""The paper studies the local optima of certain types of deep networks."
8900,1, The legend of the figure is too tiny.
8901,1,  The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.
8902,1, The experimental results are not well explained and there is not enough analysis of the results.
8903,1,"  \nShin, J.H., Adaptation in spiking neurons based on the noise shaping neural coding hypothesis."
8904,1,  It avoids learning parameterized policies.
8905,1,"  While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same"
8906,1," Among other things, the example used in the text in section 3 seems bad to me.[[CNT], [EMP-NEG], [CRT], [MIN]] It isn't really the case that \""split off something\"" means \""divided something\"".  (\""Sally split off a sliver of wood\"" does not mean \""Sally divided a sliver of wood\""."
8907,1,"  Under such an approach, the unedited string receives probability one, which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive normalization.  "
8908,1," This seems to suggest that for small \\tau, Q' is not a good approximation of Q."
8909,1, One offshoot of it has its own society with conferences and a journal devoted to it (The International Artificial intelligence in Education Society: http://iaied.org/about/).
8910,1," As a result, Algorithm 1 does not use importance sampling."
8911,1,  This paper should be accepted.
8912,1,  A comparison is needed.
8913,1,It is not clear exactly when this will be useful. 
8914,1,Large successful evaluation section provided\n\t\u2022\t
8915,1,"""The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets."
8916,1,"\n- In general the topic is interesting, the solution presented is simple but needs more study"
8917,1, The dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable.
8918,1," \nDifferently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores."
8919,1," For instance, the swan part is not really stand alone."
8920,1,"  There are several places where I think the terminology does not quite reflect what the authors perhaps hoped to express, or was otherwise slightly clumsy e.g:"
8921,1,\n\nThe text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion.
8922,1," \n----> Figure 8 should be Figure 10\n(b) Page 4, \""yet is is unclear how many pixels are required...\""\n----> yet is is"""
8923,1,"\n\nBased on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization."
8924,1,"   Each object is described by a position, appearance feature and confidence of existence (presence)."
8925,1,"\n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value)."
8926,1, You still propose to learn the proposal parameters using SMC but with lower number of particles?
8927,1,\n\nQuality\n======\nThe approach seems sound
8928,1,. That this is the case should be made clear in the title and abstract
8929,1,"\n\nHowever, there appear to be a range of issues."
8930,1, \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean?
8931,1,"\n(ii) \""One benefit in using the GP is due to its Bayesian nature, so that predictions have uncertainty estimates (Equation (9))."
8932,1, Any smooth function can be approximated by such networks.
8933,1,\n\nSection 5.1:\nInference networks with FFG approximations can produce qualitatively embarrassing approximations.
8934,1,"""The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions."
8935,1,\n\nPros:\n- Task of reducing computation by skipping inputs is interesting\n
8936,1," For example, batching and convolution as mentioned by authors would be more significant. \n"""
8937,1," This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful."
8938,1,"The use of the proposed gamma distribution, as a simple alternative, overcomes this problem."
8939,1,"""This paper extends the framework of neural networks for finite-dimension to the case of infinite-dimension setting, called deep function machines."
8940,1, They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features.
8941,1,\n\nThe motivation of the work is not clear at all.
8942,1, and \n(2) Extract the penultimate layer output as features to train a conventional classifier such as SVM.
8943,1," Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously."
8944,1,"""1. This is a good application paper, can be quite interesting in a workshop related to Deep Learning applications to physical sciences and engineering."
8945,1, What are the conditions under which the method is likely to perform well?
8946,1, It provides some insights on the challenges and benefits of replay based memory consolidation.
8947,1," \n\nAt the beginning of section 2.1, I think the authors suggest the PATH function could be pre-trained independently by sampling a random state in the state space to be the initial state and a second random state to be the goal state and then using an RL algorithm to find a path."
8948,1," It would not work if the heatmap is multimodal, e.g. when there are multiple instances in the same image or when there is a lot of clutter.\n\nThere is a minor conceptual confusion on page 4, where it is written that \""Group-convolution requires integrability over a group and identification of the appropriate measure dg."
8949,1," I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \""wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples."
8950,1,"\n\n(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer."
8951,1," In addition to the observability, approximate Markov TFT (amTFT) methods are more processing-intense, since they fall back on a game's Q-function, as opposed to learned policies, making CCC a lightweight alternative."
8952,1,"""I very much appreciate the objectives of this paper:  learning compositional structures is critical for scaling and transfer."
8953,1,"""This paper concerns open-world classification."
8954,1,"""This paper introduces a model for learning robust discrete-space representations with autoencoders."
8955,1," I think that having\nless number of parameters is a good thing in this setting as the data is scarce,\nhowever I would like to see a more in-depth comparison with respect to the number\nof features produced by the model itself."
8956,1, If this shift exists (as for the\ncase of negative images) it is possible to refer to the extensive deep domain adaptation literature.
8957,1,\nThe encoder maps the input to a probabilistic latent space.
8958,1," It would be good to clarify this in the text."""
8959,1,"Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix)."""
8960,1,"  \n\n- I suggest to divide Section 3.1 in two subsections. The first one introducing Stein\u2019s identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title \u201cStein Control Variate\u201d."
8961,1," \n\nOther concerns:\n1.\tIt seems that one needs to train at least three embedding matrices: A, C, D which represent input embeddings, output embeddings, and interactive embeddings, respectively."
8962,1," For example, during the early sections you  keep referring to a loss function which will allow for learning the objects, but you never really give the form of this loss (which you should as soon as  you mentioning it) and the reader needs to search into the appendices to find out what is happening. "
8963,1," Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection."""
8964,1," I don't find (the too small) Figure 2 to be compelling evidence that \""our dropmax effectively prevents\noverfiting by converging to much lower test loss\""."
8965,1," Also on p.6 I'm not entirely clear on how the \""network reduction\"" is performed ---"
8966,1,"and when some are left undefined, creating a more abstract generation task."
8967,1," \nIn conclusion, I think the research direction is worth pursuing, but the proposed modification from VPN is not well-justifie"
8968,1,"\n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB."
8969,1,\n\nI also find it difficult to assess whether the proposed model is actually generating reasonable time series.
8970,1, The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region.
8971,1,  The related work include auto-encoders where the weights of symmetric layers are tied.
8972,1,\n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks.
8973,1,"""I am overall positive about the work but I would like to see some questions addressed."
8974,1,"  However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word."
8975,1,"The setup is analogous to generative adversarial networks, where the role of the discriminator is played by a structured prediction energy network (SPEN) and the generator is played by an inference network. "
8976,1,\n\nCons:\n1. Technical novelty is somewhat limited
8977,1, Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space.
8978,1, \n3. In the function approximation case the value function and q functions parameterized by \\theta are only approximations of the expected return.
8979,1,"  In fact, from the qualitative examples of Yelp, I do not think that the contents are well preserved."
8980,1,"""\n-----UPDATE------\n\nHaving read the responses from the authors, and the other reviews, I am happy with my rating and maintain that this paper should be accepted."
8981,1," With an appropriate fix, lemma 4 gives a cleaner set of assumptions than previous work in the same space (Nguyen + Hein \u201917), but yields essentially the same conclusions."
8982,1, Their method claims increased stability in contrast to the existing one.
8983,1,\n\nMinor comments:\n\u201cFor kennels with q(w) other than Gaussian\u2026 obtain very accurate results with little effort by using Gaussian approximation of q(w)\u201d.
8984,1,  I am not sure why this cannot be accommodated in the Table itself.
8985,1, \n\nI like the idea of using deep learning for physical equations.
8986,1, \n\nClarity: The paper is well-written. 
8987,1," They find many properties reminiscent of neurons in the mammalian entorhinal cortex (EC): grid cells, border cells, etc."
8988,1, \n\nThe presentation could also be improved with some language edits.
8989,1," On Page 8, P(B) should be a degree-4 polynomial of B.[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nThe paper does not contains any experimental results on real data."""
8990,1,  \n\nThe idea is nice.
8991,1, The presentation is clear 
8992,1,\n- explanations of algos / lack of 'algorithms' adds to confusion
8993,1," The result is not particularly different from previous ones,"
8994,1, Some examples beyond the contrived MNIST toy examples would be welcome.
8995,1, One way it could be improved is if it were compared with another system.
8996,1," \n\n- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks."
8997,1,"\n\nBeyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE. "
8998,1, Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.
8999,1," How would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil, without relying on any computational simulation at all?"
9000,1,. The key idea is to maintain a chart to take into account all possible spans.
9001,1," However, there are several issues."
9002,1,\n\nThe experiments have some issues.
9003,1,. The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger.
9004,1," Intelligent tutoring systems. Science, 228. 456-462."
9005,1, The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop.
9006,1, \n- It is necessary to put Table 5 in the main paper instead of Appendix.
9007,1,\n\t\u2022\tExcellently structured and presented paper\n\u00a0\n\t\u2022\t
9008,1,"\n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016."
9009,1, The authors should finish the experiments on comparison methods and fill the entries in Table 1.\n\n
9010,1,". However, publishing this  work is in my opinion premature for the following reasons:\n\n- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap"
9011,1,"In other words, the proposed technique is rather stable and can not be easily exploited."
9012,1," However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work."
9013,1,"\n\n(2) In Theorem 1.1, the notation is slightly unclear because B^T is only defined later."
9014,1,"\n\nSome minor issues:\n- The references are somewhat inconsistent in style: some have urls, others do not; some have missing authors, ending with \""et al\""."
9015,1, Do you have an idea why a low threshold hurts the performances?
9016,1," On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance."
9017,1,"  Eq. 5 clarifies this implicitly, but would be good to state outright. """
9018,1," Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed."
9019,1, The reasons for this choice are discussed and linked to theoretical properties of OT.
9020,1, The authors first introduce their suggested loss function and then go into details about what inspired its creation.
9021,1,"  More generally, beam search is normally an algorithm where at each search depth, the set of candidate paths is pruned according to some heuristic."
9022,1," Just because the transition function is shared and the model could learn to construct a tree, when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour. "
9023,1,  I find the authors\u2019 claim that FAME is performing superior global modeling interesting. 
9024,1,\n-            Sec 5.2: The authors have left out \u201cMishral et al\u201d from the comparison due to the model being significantly larger than others.
9025,1,\n- The paper is easy to follow and clearly describes the implementation details needed to reach the results. 
9026,1, This network is trained through supervised learning for the output and reinforcement learning for discrete operations.
9027,1," If the combination of supervised learning with RL is better, than this should be clearly stated."
9028,1," However, my main concern is the evaluation."
9029,1, 100 evaluation runs is very low.
9030,1, This seems quite suboptimal.
9031,1, While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation.
9032,1,", you will have to run node2vec several times to reduce the variance of your resulting discretized density maps."
9033,1,"""This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks."
9034,1, An empirical comparison could be helpful but not provided.
9035,1,\n\nPros:\n-Interesting problem and interesting direction.
9036,1," If it is from the bot response, then it is known which words are named entities therefore embedding can be constructed directly."
9037,1," \nThe only fully centralized baseline in the paper is GMEZO, however results stated are much lower than what is reported in the original paper (eg. 63% vs 79% for M15v16). "
9038,1," Given that this paper spends a lot of time\nmotivating the QA task as part of the training scenario, I was surprised not to\nsee it evaluated."
9039,1,"\n\nPost-rebuttal revision:\nAfter reading the authors' response to my review, I decided to leave the score as is."""
9040,1," Moreover, results on only two image data sets are not sufficient for convincing."""
9041,1,  \n- Why does NATAC perform much better than NATAC-k?
9042,1,  Region embeddings are then composed (summed) and fed through a standard model.
9043,1,"  The paper is well written, easy to follow, and have good experimental study."
9044,1," In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile."
9045,1," Importantly, this does not entail that using a better regularization a similar RNN model can indeed learn such a representation."
9046,1,"""Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2."
9047,1,\nWould it not be fine if the attention mechanism did a perfect job of selecting\nthe class ?
9048,1,"  However, it is not clear how one selects a $re^{i}{G}$ from among the various i values."
9049,1,\n\nExperimental results are not convincing.
9050,1," For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer."
9051,1," So I actually cannot tell which method is better, at least in the MNIST experiment."
9052,1," There also exists prior work on optimizing neural nets via GA (Leung, Frank Hung-Fat et al., IEEE Transactions on Neural networks 2003)."
9053,1,"   Consequently, it is difficult to see the influence of T."
9054,1," However, the authors do prove their work, which increases the novelty."
9055,1,"""This paper proposes MaskGAN, a GAN-based generative model of text based on\nthe idea of recovery from masked text."
9056,1," The paper\nsuggests always adding the I/O example that is least likely under this predictive distribution\n(i.e., the one that is most \""surprising\"")."
9057,1, So it is not clear if the proposed method can make a real difference on state of the art systems. 
9058,1,\n\nThe kernel matrix in Eq. 5 is not symmetric and the kernel function in Eq. 3 is not defined over a pair of inputs.
9059,1, This would require the combination of two Hessian matrices.
9060,1,\n\nThe main contribution of this paper is supposed to be the reconstruction mapping (6) and its effect in semi-supervised learning.
9061,1," If the paper were substantially cleaned-up, I would be willing to increase my rating. """
9062,1," \nMore recently, the noise-shaping hypothesis has been tested with physiological data:\nChklovskii, D. B., & Soudry, D. (2012)."
9063,1,"\n \nThere is a big gap between the last two paragraphs of section 3.[[CNT], [PNF-NEG], [CRT], [MIN]]\n \n4. Neuro as an agent[[CNT], [CNT], [CNT], [GEN]]\n \n\u201cWe add the following assumption for characteristics of the v_i\u201d -> assumptions for characterizing v_i[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n\u201cto maximize toward maximizing its own return\u201d -> to maximize its own return[[CNT], [CLA-NEG], [CRT], [MIN]]\n \nWe construct the framework of NaaA from the assumptions -> from these assumptions\n \n\u201cindicates that the unit gives additional value to the obtained data.[[CNT], [CLA-NEG], [CRT], [MIN]] \u2026\u201d I am not sure what this sentence means, given that \\rho_ijt is not clearly defined.[[CNT], [CLA-NEG], [CRT], [MIN]]\n \n5. Optimization\n \n\u201cNaaA assumes that all agents are not cooperative but selfish\u201d Why?"
9064,1," However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge."
9065,1, Why is using earth mover distance better than MMD based distance?
9066,1, The question is not \u201cwhat\u2019s it take to get to 0 variance\u201d but \u201chow quickly can we approach 0 variance\u201d.
9067,1,"""This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016)."
9068,1, It means that we multiplied by K the number of parameters in our model (K is the number of classes).
9069,1," It is also a little confusing that you begin this paragraph saying that you are doing a classification task, but then it seems like a regression task which may be postprocessed to give a classification."
9070,1, \n\n****************\nI had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA.
9071,1,"\n- Need to be more specific: \""use some channels to encode the id information\"". \n"""
9072,1, \n\n**Weakness**\n\nApproach:\n* A major limitation of the model seems to be that one needs access to both images and attribute vectors at inference time to compute representations which is a highly restrictive assumption (since inference networks are discriminative).
9073,1,". The network proposed here, FusionHet, fixes problem."
9074,1,\n\nPositives:\n\n- An interesting new idea that has potential to be useful in RL
9075,1, This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor.
9076,1," The paper mentions that it is prohibitively \nexpensive to obtain human-annotated set, but can it be possible to at least obtain a \nhandful of real tasks to evaluate the learnt model?"
9077,1,"\n\n- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing.[[CNT], [PNF-NEG], [CRT], [MIN]] RNNs are not restricted to NLP and I think there is no need to introduce an application at this point."
9078,1," \n\n1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation."
9079,1,"\n\nOverall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful."
9080,1,"\n\nI like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model."
9081,1, Just the FLD projections of the MCMCP chains are difficult to interpret.
9082,1, but I\u2019m not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse.
9083,1, But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method.
9084,1,The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others.
9085,1, Following from this connection authors propose a new loss function.
9086,1," \""we turn to the discussion of a second\""\n- etc.[[CNT], [CLA-NEG], [CRT], [MIN]]  \n\nQuality: High\nClarity: medium-low\nOriginality: high"
9087,1,"\n+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims."
9088,1, This reference should be included in the related work.
9089,1," Should that be z_t ?"""
9090,1, The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point).
9091,1,"  If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?"
9092,1," I found it interesting to see what it does with the mixed signals of the word \""but\"": on one hand, keeping it helps preserve the structure of the sentence, but on the other hand, keeping it makes it hard to flip the valence."
9093,1,"  What if we just selected the one past domain that was most similar to the new domain, by some measure?"
9094,1,\n- The proposed method improves the baseline by 5% on counting questions.
9095,1, The best experimental evidence for the authors\u2019 perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method.
9096,1," but this seems like an effective technique for people who want to build effective systems with whatever data they\u2019ve got. \n"""
9097,1,"   Moreover, the experimental evaluation is small, considering only two datasets."
9098,1," Especially the latter will be appreciated."""
9099,1, The paper is easy to follow although it is on an adequate technical level.
9100,1,\n\n=Originality=\nThis is an interesting problem that will be novel to most member of the ICLR community. 
9101,1,  \n\nSignificance\n- Trust-PCL achieves overall competitive with state-of-the-art external implementations.
9102,1," Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. "
9103,1,"\n\nSignificance\nThe relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area,"
9104,1,The time-aware agent shows improved performance in a time-limited gridworld and several control domains.
9105,1, What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?
9106,1," \""Deeply-supervised nets.\"" Artificial Intelligence and Statistics. 2015."
9107,1,"  In any case, this statement should be clarified."
9108,1," Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data)."
9109,1,"""The authors propose the use of a gamma prior as the distribution over \nthe latent representation space in GANs. "
9110,1,\n\n- Motivation in section 4.1 was a bit iffy.
9111,1,"\u00a0\u00bb\n\u00ab\u00a0 This approach is fast, statistically consistent, and reduces to simple\nlinear algebra operations."
9112,1,"""\nSUMMARY\n\nThis paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection."
9113,1," First, the use of reinforcement learning is quite straightforward."
9114,1," Second, it is not clear what \""parameters of the regularization\"" means."
9115,1," In the task, there are multiple taxi agents that a model must learn to control."
9116,1,\n\nComments:\n-- It\u2019s not clear to me how D is determined for each test.
9117,1, This is a common problem of architecture searching methods compared to handcrafted structures.
9118,1," They compare the evaluation methods based on five criteria termed disciminability, mode collapsing and mode dropping, sample efficiency,computation efficiency, and robustness to transformation."
9119,1,  \n- Why does the method require two images?
9120,1,"  Indeed, it is unlikely that this kind of data can be obtained and training on this type of data is just a kind of distillation of the optimal network making the weights of the network close to the right optimum."
9121,1,. This result is quite nice.
9122,1,I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods.
9123,1,\n\n- One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks.
9124,1," but should be\nfurther verified by multiple runs.\n"""
9125,1, I believe this statement is not correct.
9126,1,"\nA lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting."
9127,1," \n\n*** Update after author response ***\n\nThanks to the authors for their responses. My score is unchanged."""
9128,1, thus I would liked to see more thorough experiments here as well.
9129,1," \n\nNext, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics."
9130,1, \n\n# Detailed comments\n\n1. The distinction between parameters and hyperparameters (section 3) should be revised.
9131,1,\n1b. Why should we keep increasing the regularization constant beyond a limit?
9132,1, Can they elaborate more on how they with this?
9133,1, For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN.
9134,1," However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato)."
9135,1, so the only real contribution would be in the experiments.
9136,1,\n\nFigure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process.
9137,1," A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens."
9138,1, Why are the Concorde models faster than unigrams and bigrams?
9139,1,"\n\nNonetheless, I believe that the paper represents an interesting and worthy submission to the conference."
9140,1," Two proposals in the paper are:\n\n(1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training,"
9141,1, Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?
9142,1," According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why?"
9143,1, So solving the problem of natural noise is not so simple\u2026 it\u2019s a *real* problem.
9144,1, It seems that no reward shaping is used.
9145,1,  This analysis is missing a straw man.
9146,1,"  The GAN never seems to perform well -- the authors claim mode collapse, but I wonder if the GAN was simply hard to train in the first place and this is the key reason?"
9147,1,\n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model.
9148,1," The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters."
9149,1," In the game of Starcraft, only screen contains information about unit-types, but it\u2019s field of view is limited."
9150,1,"   At best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are."
9151,1," However, it is unclear that the rank of the logit matrix is the right quantity to consider."
9152,1," So, I am hesitant accepting the paper."
9153,1,"""This work exploits the causality principle to quantify how the weights of successive layers adapt to each other."
9154,1,"\nSpecifically, it would be nice if train, test and validation URLs would be operated chronologically. I.e. all train url precede the validation and test urls."
9155,1," This is assuming the local linear approximation given by the derivative is accurate enough one integer away; this may or may not be true, but it's not clear and there is little discussion of whether this is reasonable to assume."
9156,1," \n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies."
9157,1,"""The paper develops an efficient algorithm to solve the subproblem of the trust region method with an asymptotic linear convergence guarantee, and they demonstrate the performances of the trust region method incorporating their efficient solver in deep learning problems."
9158,1,"b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure."
9159,1,". It also needs a lot of more studies on related work."""
9160,1,\n\n2. I'm not sure how much to trust the Fourier-spectra.
9161,1,"  One would hope that, even if the search space is limited, the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same space."
9162,1,"\n\n-Major: When I look at figure 4D, I see that the proposed approach *also* only provides the master with the sum (or really mean) with of the individual messages...? So it is not quite clear to me what explains the difference.\n\n\n*In 4.4, it is not quite clear exactly how the figure of master and slave actions is created."
9163,1,"\n\nIssues with the paper:\n- Since the paper is focused on empirical results, having results only for ResNet50 on CIFAR is very limiting\n- Empirical results are based on simulations and not real workloads."
9164,1,\nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input;
9165,1," According to\nLemma 1, the proposed method is unbiased for the optimal weights in the large\ndata limit."
9166,1," Which of the MNIST models from Table 1 was used?\n"""
9167,1,\n\n(1) Related work. It is presented in a somewhat ahistoric fashion.
9168,1,\n\nMain questions:\n- Could you briefly comment on the training time in section 4.1? 
9169,1,  Answers to these questions can automatically determine suitable experiments to run as well. 
9170,1," \n\nThe extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable."
9171,1,"\n\nI vote accept."""
9172,1, The GAN training objective function utilizing 3 conditional classifier.
9173,1," For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents?"
9174,1,\n\nExperiments are performed on the usual reference benchmarks for the task and show\nsensible improvements with respect to the state-of-the-art.
9175,1," In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups."
9176,1, An ablation study of the impact of word embeddings on this model is required.
9177,1,"\n\nMajor comments\n-------------------------\nI was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature? "
9178,1,"""This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep, directed generative models with latent variables."
9179,1," The ERM for active learning has been investigated in the literature, such as \""Querying discriminative and representative samples for batch mode active learning\"" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper."
9180,1," \n\nAccording to that, the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough. """
9181,1,"\n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017."""
9182,1,  The idea is to explicitly model interactions between aspects and words expressing sentiment about them.
9183,1,"""Summary of paper:\n\nThis work proposes an extension to an existing method (Franceschi 2017) to optimize regularization hyperparameters."
9184,1," In particular, mini-ImageNet is a commonly-used benchmark for this task that this approach can be applied to for comparison with recent methods that do not use data augmentation."
9185,1," This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings."
9186,1, \n\nThe justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me:
9187,1, The transform allows for explicit rotations and swaps of the hidden cell dimensions.
9188,1,"  In NIPS 2013.\n[Ref3] Zhirong Yang, Jukka Corander and Erkki Oja. "
9189,1, How diverse are the sketches?
9190,1,  It would also be nice to observe failure cases of the model.
9191,1,"""Summary:\nThis paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention."
9192,1, I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input).
9193,1,"  I would like to see to the top k nearest neighbors of each of those words.\n"""
9194,1, Is this more of a simplifying assumption?
9195,1,\n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions.
9196,1," \n\nI don\u2019t immediately know of work that suggests bootstrapping if an episode is terminated early artificially during training but it seems a very reasonable and straightforward thing to do. \n\n"""
9197,1,\n\nUpdate:\nThank you for the rebuttal.
9198,1, A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.\n\n-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice.
9199,1,I welcome and are grateful for any theory in the area.
9200,1," \n\nAs for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables."
9201,1, We can then compute a posterior distribution over programs
9202,1," In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature."
9203,1,\nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components.
9204,1," In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective."
9205,1,"\u00a0\u00bb\n- p6: $C^{\\epsilon_0}_{A,B}$ is used (after Def. 2) before being defined. \n- p7: build->built\n\nSection II :\nA diagram explaining  the different mappings (h_A, h_B, h_AB, etc.) and their spaces (D_A, D_B, D_Z) would greatly help the understanding."
9206,1,   A concluding observation is that CNN models learn and generalize the structure content of images.
9207,1,"This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection."
9208,1,"\n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly"
9209,1,"""The paper proposes a model for prediction under uncertainty where the separate out deterministic component prediction and uncertain component prediction."
9210,1,\n\n\nAdditional comments: \n\n-\tThe experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised?
9211,1," Overall, the contribution is modest at best.v\n\n** DETAILED REVIEW **\n\nOn mistakes, it is wrong to say that an SVM is a parameterless classifier."
9212,1,"\n* The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2, which reduces the practical significance of the proposed approach. \n"""
9213,1," Given the kind\nof general trend the authors seem to want to show here, I feel that a more\ntheoretic measure of problem hardness would be more appropriate here."
9214,1,\n- The paper is well-written.
9215,1,\n6. There is no discussion of computational complexity and wall-clock time comparisons.
9216,1,\n\n4. How and which hyper-parameters were optimized?
9217,1," In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation)."
9218,1," Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \\sum_1^R u_ir u_jr u_kr u_nr."
9219,1, (Although [particle] MCMC is probably a better choice if one wants extremely low bias.)
9220,1,"\n\nI only have a few small questions/comments:\n* A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size. "
9221,1," \""  Taken literally, this would rule out a lot of modern machine learning, even obviously very good work."
9222,1,"\n\nAll of this is somewhat straightforward; a penalty is paid by representing numbers via fixed point arithmetic, which is used to deal with ReLU mostly."
9223,1,. I think the paper's comparisons are valid
9224,1,"\n\n5) In Table 1, the significance of the last two columns is unclear."
9225,1,"\nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain."
9226,1," The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states."
9227,1,"n\nComments:\n- Eq. (16): $j$ in the denominator should be $t_j$.\n"""
9228,1," \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results)."
9229,1," The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form. \n"""
9230,1," Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering."
9231,1,"\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen."
9232,1," A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent."
9233,1," That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level"
9234,1, It also have some visualization about how the model decay the weights.
9235,1, I would appreciate the authors to elaborate this a bit.
9236,1," The algorithms are shown to be consistent, and demonstrated to be more efficient than an existing semi-dual algorithm."
9237,1," \n\nI would have liked some analysis of the convergence results for TD learning with this constraint, or some better intuition in how this effects learning."
9238,1,"\n\n*Originality and significance*\n\nAs far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1."
9239,1," In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n"""
9240,1,\n\nResult (3) is more interesting as it is a new result.
9241,1,"""Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks."
9242,1," A larger stepsize is not always better, and smaller is not worse. "
9243,1,"\n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018."""
9244,1," \n\nImportance:\nWhile the majority of papers nowadays focuses on the representation part (i.e., how we get to \\Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm."
9245,1," They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering."
9246,1, or if the success of this paper is mostly due to clever processing of text features using DNNs.
9247,1, Were the baselines with concatenated features optimized independently?
9248,1,"  Or is it fine to have an incomplete example set?\n\n"""
9249,1,"\n\n\""2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence."
9250,1,"\""Vanilla\"" NN indeed may fail miserably ."
9251,1," The underlying technique that is used to operate on the irregular graph is spectral decomposition, which enables convolutions in the spectral domain."
9252,1,\nUsing so-called learning curves is a good way to answer this.
9253,1,"\nOverall, the proposed method seems to be very useful for the RWA."""
9254,1,  \n\nClarity \n- The main idea of the proposed method is clear.
9255,1,"\n\n\"", we cam simply\"" -> \"", we can simply\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""Figure 4 <newline> .\"" -> \""Figure 4."
9256,1," Maybe there is something deeper going on here, but it is not obvious to me."
9257,1,"\n\nThis also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,"
9258,1, I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network.
9259,1," Does it have some support?\n"""
9260,1," The FGSM method (Goodfellow et al, 2015) is basically 1 inference operation and 1 backward operation."
9261,1," Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this."
9262,1,. I had a very hard time understanding the set-up of the problem in Figure 2.
9263,1," The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced)."
9264,1," \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term."
9265,1, \n\n- What is the rationale for setting the gamma (concentration?) parameters to .01?
9266,1," In general I have a positive opinion about the paper, however, I\u2019d like to ask for some clarifications."
9267,1," Although the motivation is well grounded,"
9268,1,\n\nOverall I found the paper interesting but not ground-breaking.
9269,1," This seems to be a limitation of the methodology: unless I'm missing something, this situation cannot be addressed using locally open maps."
9270,1, The best SQuAD (devset) results are shown as several percent below the SOTA.
9271,1, \n\nThey obtain significant quantitative improvements over the previous 2 papers in this domain (video prediction on tabletop with robotic arm and objects) on both type of metrics - image assessment and motion accuracy.
9272,1," \n\nMinor -- \nCite Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation by Yoshua Bengio, Nicholas Leonard and Aaron Courville for straight-through estimator."""
9273,1,\n\n2. Clearly written\
9274,1," In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network."
9275,1,. Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction. 
9276,1," While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms),"
9277,1," For example, the algorithm is never fully described, though a handful variants are discussed."
9278,1," It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline. """
9279,1, This projected input is then used to produce the classification probabilities.
9280,1, \n\n3) I have some questions on the details.
9281,1,"\n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223\u2013242, April 2001,"
9282,1,\n\nThe paper is well written and provides some new insights on incorporating kernels in CNN.
9283,1,"However, the writing still needs to be polished."
9284,1, How do you deal with this case?
9285,1, The additional encode/decode mechanism seems to introduce unnecessary noise.
9286,1," Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \""general English\""."
9287,1," As discussed above, it is encouraged to elaborate other potential causes that led to performance differences."
9288,1," For instance, \""its variants\"" is confusing because there is only other variant to VProp."
9289,1, Can authors provide more intuition on these precise definitions?
9290,1, But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form.
9291,1,\n- Useful possible applications identified
9292,1,"\nAdditionally, the idea of factorized representation idea (describable component and indescribable component) has a long history."
9293,1, The architecture includes a differentiable warping scheme which allows back propagation of the error and is inspired by the fundamental solution of the PDE model. 
9294,1," \n\n[1] Wang & Manning, Fast Dropout Training."
9295,1, I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications).
9296,1,\ and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates.
9297,1, \nHere are a few comments that I have:\n\nFigure 2 is very confusing for me. 
9298,1, but would benefit from better figures and from tables to report the numerical results instead of inserting them into plain text.
9299,1,  We don't need the same view for all methods.
9300,1,. \n\nThe paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper)
9301,1,\nExperimental results show that this outperforms an epsilon-greedy baseline.
9302,1,"\n- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique?"
9303,1," The experimental results are encouraging,"
9304,1,"\n- In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0"
9305,1," \n\n- Generally, the quantitative impact of the adversarial loss never comes together. The only statistically significant improvement is on perceptual image realism."
9306,1,"\n\nYou spend too much time on common, well-known information, such as the LSTM equations."
9307,1,"  Journal of Machine Learning Research, 17(187): 1-25, 2016."""
9308,1,\n\nSec 4\nWhat are the characters embedded with? This is important to specify.
9309,1,"  The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index."
9310,1, With the same plot one could sell SGD as the superior algorithm.
9311,1," Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier."
9312,1," It seems that other choices are equally well justified, including the L_2 norm in parameter space, which then defeats the central argument of the paper."
9313,1,"""The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits."
9314,1,\n- good compression with little loss of accuracy on best strategy\
9315,1," however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines."
9316,1, since I think it much makes it clearer to explain
9317,1," Since the authors took the numbers from [Oh et al.] rather than replicating VPN, it is possible that the gap comes from implementation details (e.g., hyperparameter)."
9318,1, \n-- The ylabel in Figure 1 is \u201cTest Loss\u201d which I didn\u2019t see defined.
9319,1," What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken?"
9320,1,"  When the mean of input is zero, there is no angle bias in the first layer."
9321,1," The observations are interesting, despite being on the toyish side."
9322,1,\n\n\n\nStrengths:\n\nThe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms.
9323,1,"""In this paper, the authors define a simulated, multi-agent \u201ctaxi pickup\u201d task in a GridWorld environment."
9324,1, Can you apply this method to other multi-agent problems?
9325,1,  The authors should highlight in\nbold face the results of the best performing method.
9326,1, There are many ways to create rational incentives for neurons in a neural net.
9327,1, However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story.
9328,1,"   \n\nAlso, experimental results are very preliminary and not properly analyzed. "
9329,1, Adding a couple of equations on this would improve the readability of the paper.
9330,1,\n- This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.
9331,1, Some parts of the paper are hard to read.
9332,1, Thus the novelty of this aspect of the paper is overstated.
9333,1,"""This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input."
9334,1,"\nThe paper does not present clear benchmarks showing a) what is the fraction of CPU cycles spent in evaluating the activation function in any reasonably practical neural network,"
9335,1,\nCons\n-\tThe idea implementation is basic
9336,1, \n\n2. Approximating an integral is a well-studied topic. I do not find a good discussion on all the possible methods.
9337,1," \n\n1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly."
9338,1, It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks.
9339,1,\n\nIt is rather unclear why changing the learning rate affects the performance of the model and it is would have been interesting to discuss.
9340,1,"""The authors define a deep learning model composed of four components:  a student model, a teacher model, a loss function, and a data set."
9341,1,\n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.
9342,1,\n            (2) how was the masking done?
9343,1,"\n- There are techniques for incremental adaptation and a constrained MLLR (feature adaptation) approaches that are very eficient, if one wnats to get into this"
9344,1,"\n\nMajor comments:\n\n1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over, It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem?"
9345,1, I gave up on keeping track at\n  this point but there are many more.
9346,1,"""I hate to say that the current version of this paper is not ready, as it is poorly written."
9347,1,\n\nCons:\nEmpirical gains might not be very large.
9348,1,\n\n\nNegatives:\n1.) The task of exact correspondence identification seems contrived.
9349,1,  The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions.
9350,1,\n\n3) Also the results of experiments 2 and 3 in figure 2 are not surprising.
9351,1, \n\nI'm also not sur to see much differences with the previous work by Haarnoja et al and Schulman et al.
9352,1," The proposed framework, task graph solver (NTS), consists of many approximation steps and representations: CNN to capture environment states, task graph parameterization, logical operator approximation; the idea of reward-propagation policy helps pre-training."
9353,1," For example, \n- Section Method, equation (4)"
9354,1," However, is there any theoretical guarantee or empirical evidence\nto show the proposed method does not suffer from the drawback of high variance?"
9355,1," The novel content of the paper sums up to the proposed method, that is composed of building blocks of existing models, and fails to impress in experimental results."
9356,1,"\n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W."
9357,1, \n\nI thought the paper is very well written and provides a really nice exposition of the problem of training deep networks with hard thresholds.
9358,1,"""This paper studies the amortization gap in VAEs."
9359,1, \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?
9360,1, Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.
9361,1, Could the authors provide insight into why they did not use the ResNet structure from the  tcml paper in their L-MLMA scheme ?
9362,1,"  \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 \u2014> see Fig.1\npage 4just before equation 8: the the"""
9363,1," Experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method,"
9364,1," The experiments in section 5.3 are conducted in simple but previously unseen maps and cannot logically contradict results (Mirowski et al, 2016) achieved by training on static maps such as their \u201cI-maze\u201d."
9365,1,"  Finally, the authors notice that \""In the rotation-invariant case, where \u03a9 is a discrete set, heuristics are available\""."
9366,1,  It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4.
9367,1," Our rule of thumb is, when the training accuracy raises slowly, run SGD for 10 epochs (because it\u2019s already close to minimum)."
9368,1," This means that the authors are adapting\nQ-learning methods so that they look Bayesian, but in no way they are dealing\nwith a principled posterior distribution over Q-functions."
9369,1,  They systematically characterize the fragility of several widely-used feature-importance interpretation methods.
9370,1, Which architecture is used for building the mapping ?
9371,1,\n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?
9372,1,"So it is very difficult to determine if the model is appropriate for the dataset in the first place, and whether the gain from the non-adversarial setting is due to the adversarial setup or not."
9373,1,". They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined."
9374,1,". To establish such a behavior, the authors propose early stopping as well as other heuristics."
9375,1," I think this is a valuable engineering contribution,;"
9376,1,\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.
9377,1,"Overall, I think this paper would be a better fit in a recsys, applied ML or information retrieval journal."
9378,1,"\n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text."
9379,1,"  It could have more contribution if the authors model the interactions within the attention model itself, instead of a simple prediction layer, which is problem-dependent."
9380,1,\n\nComments:\n\n1. Using expectation to explain why DReLU works well is not sufficient and convincing.
9381,1,\n2. Useful for object counting problem.
9382,1, This aspect is not really discussed.
9383,1," In the experiments, it is strange that this method can also achieve comparable or better results."
9384,1, \n- It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2.
9385,1,    This just doesn't make sense to me.
9386,1,The paper is clearly written and presents the theory and experimental results nicely.
9387,1,"\n\nThe elimination module is interesting,"
9388,1, The paper should come up with a better term for that evaluation.
9389,1, What is driving those difference in results?
9390,1," If so, better move that part to the models section instead of mention it briefly in the experiments section."
9391,1,\n- medium resolution of the resulting prediction
9392,1,"Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835."""
9393,1,\n\nClarity\nI found the paper hard to read at times and it is often not very clear what the most important differences are between the proposed methods and earlier ones in the literature. 
9394,1, I understand that it is not an issue in the test set.
9395,1,  This would require a phi(s) function that is trained in a way that doesn\u2019t depend on the action a.
9396,1, What's similar vs dissimilar is trained with a binary classifier.
9397,1, Clearly not.
9398,1,"""my main concern is the relevance of this paper to ICLR."
9399,1,  The coding scheme then mimics the proporitional-integral-derivative idea from control-theory.
9400,1," Statistical tests are performed for many of the experimental results, which is solid."
9401,1,"  Indeed, the discovered network diagrams (Figures 4 and 5) fall in this space."
9402,1, The paper claims that this may help to prevent model collapsing.
9403,1, \n\nIf good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes.
9404,1," SeaRnn improves the results obtained by MLE training in three different problems, including a large-vocabulary machine translation."
9405,1, How are they to be interpreted?
9406,1,\n-\tThe generated examples are in some cases useful for interpretation and network understandin
9407,1,"""Summary: \n\nThe authors consider the use of attention for sensor, or channel, selection."
9408,1,"   In Equation 5 this is mysteriously replaced with v_t, which is the target aspect embedding and so may have been the intention for u all along?"
9409,1,"  In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition."
9410,1,"Clearly the authors were able to make it work, with good results."
9411,1," In the paper, the author argued \""we propose and evaluate the minimal changes...\"" but I think the these type of analysis also been covered by [1], Figure 5."
9412,1,"\n\nSome further points:\n\n- There are several hyperparameters set to the \""standard\"" or \""default\"" value, like Adam's beta parameter and the batch size/BPTT length."
9413,1,"""The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized."
9414,1,\nI am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist
9415,1,".\n(2) the proposed method is nicely designed to solve the specific real problem. For example, the edit distance is modified to be more consistent with the task."
9416,1,"""This paper proposed the new approach for feature upsampling called pixel deconvolution, which aims to resolve checkboard artifact of conventional deconvolution."
9417,1," Would policies generalize to later tasks better with larger, or smaller networks?"
9418,1," since the approach is interesting for out of sample data,"
9419,1," However, these issues are ignored here,  and it is is unclear why existing optimization/planning approaches are poorly suited to this problem, which is a fundamental assumption being made here. "
9420,1,"\n\nOverall, I thought the paper was clearly written and extremely easy to follow."
9421,1," I think authors should cite Sentibanks.[[CNT], [null], [DIS], [GEN]] Also, at some point authors should compare their proposal with previous work."
9422,1, They compare the method to EWC with the same architecture.
9423,1, but ultimately the experiments were not.
9424,1," but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15."
9425,1, This limits the method from discovering meaningful structures.
9426,1,"\n\nThe expression Trans\u2019( (s,s\u2019), a) = (Trans(s,a), s\u2019) was confusing."
9427,1,\n(2) The introduction and related work are well written.
9428,1," Even if the logits are zero, the class would have a\nnon-zero probability and would receive gradients."
9429,1, I personally find this exciting/refreshing and will be useful in the future of machine learning.
9430,1,  They are therefore unable to support the paper's claim on robust performance
9431,1,\n\nOriginality\nThe main contribution of the paper is similar to multiplicative gating.
9432,1," Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced."
9433,1," Even if we accept this premise (and why should we? They are of the same syntactic category after all), using the similarity scores to make this argument is not reasonable, as there is no absolute interpretation or calibration of the similarity scores that can be applied across models."
9434,1,\nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation.
9435,1,"""The problem of interest is to train deep neural network models with few labelled training samples."
9436,1, This section could be improved by demonstrating the approach on more datasets.
9437,1, Spherical interpolation as recommended by White (2016) may\n  improve qualitative results.
9438,1,"""Summary:\n\nThis paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations."
9439,1, This works address a conjecture proposed by Tian (2017).
9440,1,"  The incremental architectural changes, different dataset and training are responsible for most of the other improvements."""
9441,1, Is the generalization hurt?
9442,1,"  In particular, how does the variational posterior change as a result of the hierarchical prior? "
9443,1, The overall effect would just be to raise the\nmagnitude of logits across the entire softmax.
9444,1, \nWhat is not clear is the \\Delta_{ij} definition.
9445,1, It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails.
9446,1," You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear."""
9447,1, \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.
9448,1, The evaluation with the sequence of checkpoints was created by using every fifth image.
9449,1, Yet it is unclear how this informs us about the quality of the learned forward models f.
9450,1," Typically this problem is split into a (non-convex) system ID step followed by a derivation of an optimal controller, but there are few guarantees about this combined process."
9451,1, They also examine the effects of the temperature and step size of the perturbation.
9452,1, \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.
9453,1," U. Gutmann and J. Corander, \u201cBayesian optimization for likelihood-free inference of simulator-based statistical mod- els,\u201d Journal of Machine Learning Research, vol. 16, pp. 4256\u2013 4302, 2015. \n\nG. da Silva Ferreira and D. Gamerman, \u201cOptimal design in geostatistics under preferential sampling,\u201d Bayesian Analysis, vol. 10, no. 3, pp. 711\u2013735, 2015. \n\nL. Martino, J. Vicent, G. Camps-Valls, \""Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models\"", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017."
9454,1," however, the performance is still worse compared against baselines such as Lu et al, 2017 and Yao et al, 2016."
9455,1," The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. "
9456,1,"\n\n Minor details:\n\nThe problem scenario states that the model/weights is private, but later on it ceases to be so (weights are not encrypted)."
9457,1, if D(f||g) < gamma then the concept represented by f entails the concept represented by g.
9458,1,"\n\nThe answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities."
9459,1, Alternatively please show WGAN-GP and/or other method results in at least one or two experiments using the evaluation methods in the paper.
9460,1,"Previous methods, e.g. FGSM, may work on arbitrarily sized images."
9461,1, This important flaw in the evaluation prevents any fair comparison with the state of the art.
9462,1,\n\\pi^* is defined as a distribution over state-action space and then \\pi is defined as a function; 
9463,1,"\n\n\nMinor comments:\n - Abstract: saying that word2vec and GloVe treat prepositions as \""content words\"" seems slightly wrong; really they treat them just as \""words\"" since all words are treated the same \u2013 though one can argue that most words are content words and the method of modeling word meaning is generally much more appropriate for content words."
9464,1,Do the authors have any insight on this?
9465,1," It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input."
9466,1, I would be excited to see similar analysis of other toy problems involving graphs.
9467,1," In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT."
9468,1,\n\n- The \u201cno adversary\u201d/\u201cno adv\u201d condition in Table 1 and Figure 4 is misleading.
9469,1,"\nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked."
9470,1, I enjoyed learning about the authors\u2019 proposed approach to a practical learning method based on the information bottleneck.
9471,1, The prior might be Gaussian but the posterior is not?
9472,1,\n\nMy main criticism is with the experiments and results.
9473,1,"\n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \""deep reinforcement learning\""."
9474,1," If I\u2019m correct, that\u2019s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = \u2026."
9475,1,\n* Simple but effective design to achieve a better result in testing time with same total parameter budget.
9476,1, is there a generative assumption of the data underlying the theorem? and the assumption of all samples being norm 1 seems too strong and perhaps limits its application?
9477,1," Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips."
9478,1,"\nIf the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.\n\n[1] "
9479,1," but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST)."
9480,1," Given the amount of data 2*10**6 samples, this seems sufficient."
9481,1," I think there\u2019s an easier way to make this argument:\n\nGiven an unbiased estimator \\hat{Z} of Z, by Jensen\u2019s inequality E[log \\hat{Z}] \u2264 log Z, with equality iff the variance of \\hat{Z} = 0."
9482,1," For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work."
9483,1," The notion of a \""sibling\"" graph used in this paper is not clearly defined, but it seems to only be useful if the sibling graphs are likely under the distribution."
9484,1, This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks.
9485,1," What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation?"
9486,1, The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class.
9487,1, \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A).
9488,1," The authors say \""... and after tuning\nour algorithm to emged this dataset, ...\"", but this isn't enough."
9489,1, \nMixing the concept of unsupervised/semi-supervised learning is confusing.
9490,1, In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials.
9491,1,\n\nIt is wasn\u2019t clear to me that the properties described in Theorem 1 actually hold.
9492,1," but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \""lazy evaluation\"")."
9493,1," The paper counts up these word-pair co-occurrences in a tensor, then applies a tensor decomposition and low-rank approximation method to produce word and preposition representations."
9494,1, \n-- It is not easy to put together the conclusions in Section 6.1 and 6.2.
9495,1," My detailed comments are below.\n\n1. Multi-task learning can have various settings. For example, we may have multiple groups of tasks, where tasks are correlated within groups but tasks in different groups are not much correlated."
9496,1, Sufficient experiment results show that the proposed method has better convergence rate than [1].
9497,1, Do the authors have any intuition why?
9498,1, It shows that deep RNN is signficantly better than shallow one in this metric.
9499,1," Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.\n"""
9500,1,"Indeed, there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles, as well as work that plans using predictive models of vehicle behavior (e.g., see the work by Jon How's group)."
9501,1, I understood the reason not to compare with certain related work.
9502,1,"\n\n- Section 5.2, this was nice and contributed to my favorable opinion about the work.[[CNT], [CNT], [APC], [MAJ]] However, any kind of standard convergence theory for usual SGD requires the stepsize to change per iteration and decrease toward zero."
9503,1,"\n\n\nTypos:\n1. Optimization -> optimize (bottom of page 2)[[CNT], [CLA-NEG], [DFT], [MIN]]\n2. Should be a period after sentence starting \""Several algorithms\"" on page 2.[[CNT], [CLA-NEG], [DFT], [MIN]]\n3. In algorithm box on page 5, enable_projection is never used."
9504,1,"""This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies."
9505,1, Why is this a reasonable starting point to study the emergence of grid cells?
9506,1,\nb)\tWhat is the spatial independence assumption needed for such a generator?
9507,1,\n\nThe work of Konidaris et al [1] is a more appropriate reference for this work (rather than the Thomas reference provided).
9508,1, Is this test loss the cross entropy?
9509,1, This modifies the generator objective.
9510,1, The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception.
9511,1," Clearly, the distribution of information over the agents is crucial."
9512,1, \n* The practical suggestions on page 16 are a true highlight and could provide an outline for possible revisions.
9513,1," For example, some deer or truck pictures."
9514,1," Only 1024 examples are considered, which is\nby no means large."
9515,1,"  The paper claims that the exponential has been noted to be ad-hoc, please provide a reference for this."
9516,1,\nThe experiments are really lacking:\nNo ablation study
9517,1,"\n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017]."
9518,1, It uses demonstrations to learn in an off-policy manner as in these papers.
9519,1," It has a lemma which claims that the \""minimax and the maximin solutions provide the best worst-case defense and attack models, respectively\"", without proof, although that statement is supported experimentally."
9520,1, But it's not at all clear that the gradient of either the loss or the quantization error w.r.t. the number of bits will in general suggest increasing the number of bit (thus requiring the bit regularization term).
9521,1,3) $c_i$ in page 5 is not defined.
9522,1," What are \u2018edge devices\u2019, \u2018vanilla parameters\u2019?"
9523,1," In this setting, they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names. "
9524,1," \n\nThe overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher-dimensional and stochastic environments."
9525,1,"\n\n6. In the paper, the author mentioned another sparse-complementary baseline (sc-seq), which applies sparse kernels sequentially."
9526,1," This problem is often regularized by adding a \""gradient penalty\"", \\ie a  penalty of the form \""\\lambda E_{z~\\tau}}(||\\grad f (z)||-1)^2\"" where \\tau is the distribution of (tx+(1-x)y) where x is drawn according to the empirical measure and y is drawn according to the target measure."
9527,1," It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset."
9528,1," \n\nPath-wise training is not original enough or indeed different enough from drop-path to count as a major contribution.\"" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet."
9529,1," For instance, to a spectral clustering method for the semi supervised clustering, or\nsolving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006."
9530,1," The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks."
9531,1, Similar trend holds for other metrics like BLEU-4.
9532,1,\n* Are there particular reasons in using policy learning instead of other reinforcement learning approaches
9533,1, how did you decide on the words to mask? was this at random?
9534,1,"\n\nFigure 6: Looking at those 2D PCAs, I am not sure any of those method really abstracts the rendering away."
9535,1,\n\n* Typo: p captures the how -> p captures how
9536,1,"\n\nTo summarize, I believe that the paper addresses an important point,;"
9537,1,"\n- As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL."""
9538,1,   There is not much difference between rankLM and LMLM as well to draw a clear conclusion between the two.
9539,1," and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn."
9540,1, The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers.
9541,1," \n\nOn the experimental side, to draw the conclusion, \""weighted sum\"" is enough for LSTM."
9542,1,\n\n\nSpecific comments/questions follow:\n\n\nFigure 2 is too small.
9543,1,"\nThe paper is well-structured, and the proposed method is clearly described."
9544,1,\n\n- The text is confused in several points and the final overall conclusions are not fully clear.
9545,1," For the task of prediction under the shift in\ndesign, shift-invariant representation learning (Shalit 2017) is biased even in\nthe inifite data limit."
9546,1, Have you considered training jointly (across the tasks) as well?
9547,1,"\nIn addition, the authors collected the largest sketch dataset, I know of."
9548,1," the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn"
9549,1," Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO."
9550,1,\n\nThe authors should consider adding equation numbers.
9551,1,"\n- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5."
9552,1,The authors go on to compare their baseline-corrected (BC) method with several established methods for dimensionality reduction of spike train data.\n\n
9553,1,The paper does propose to use an image-only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper).
9554,1,"""This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words."
9555,1,"5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size? "
9556,1,"\n\""The Socher et al. (2011b) propose a basic form"
9557,1, Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it. 
9558,1,"\n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline."
9559,1, Here a few thousand\ndata points are used.
9560,1,"""Paper Summary:\nThis work proposes a new geometric CNN model to process spatially sparse data."
9561,1," B does not appear much more structured than a, to me it looks like a simple transformation of a. "
9562,1, I see no strong reason why this particular paper needed the extra space.
9563,1,"n\nAssuming that the authors can address these clarity issues, I would in principle be happy for the paper to appear. """
9564,1,  It will be informative to provide results with a single GP model.
9565,1,"   \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting."
9566,1,"\n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing \u201csum_reduced\u201d\n-\tISTA \u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative. "
9567,1,\n2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error.
9568,1," Moreover, I find the proposed algorithmic approach interesting."
9569,1,"  It is very similar to \""Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies\"" and is effectively an indirect form of tile coding (each could be seen as a fixed voronoi cell)."
9570,1, \n\nThis paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap.
9571,1,"\n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method."
9572,1," Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015."
9573,1, Perhaps this statement could be qualified.
9574,1, \n\n\n\n\nWhat follows are comments on specific parts of the paper:
9575,1,"  Gated Graph Neural Networks (GGNNs, trained by maximum likelihood objective) are then run for 8 iterations at test time."
9576,1,"\n* Lack of ablative / introspective experiments\n* Weak empirical results (small or toy datasets only)."""
9577,1,"\n\nTable 1: \u201c52th percentile vs actual 53 percentile shown\u201d. \n\n> Table 1: The more fuzzy, the higher the percentile rank of the threshold"
9578,1,\n\n\nMy main problem  was about the lack of novelty. 
9579,1,  This is possibly one of the best papers on graph generation using GANs currently in the literature.
9580,1," Please give references for the evaluation metrics used, for proper credit and so people can look up these tasks."
9581,1, Do you know why the trend is not consistent across datasets?
9582,1, This work to my knowledge is the first to use a DSL closer to a full language.
9583,1,"""This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages."
9584,1," We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.\n"""
9585,1," For example, Hazzan & Jakkola (2015) in \u201cSteps Toward Deep Kernel Methods from Infinite Neural Networks\u201d consider GP constructions with more than one hidden layer."
9586,1," The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free"
9587,1,"\n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \""chrisc\"") that has significantly higher EM/F1 scores than the proposed model."
9588,1, They apply the rotations before the final tanh activation of the LSTM and before applying the output gate.
9589,1,"  It simply says it is prohibitively expensive to compute for large models.  Can the \""Gaussian approximation\"" to the evidence (equation 10) be approximated efficiently for large neural networks?\n2) The paper does not prove that SGD converges to models of high evidence, or formally relate the noise scale 'g' to the quality of the converged model, or relate the evidence of the model to its generalization performance."
9590,1,"\n\n- Compared to many existing techniques, on 9 tasks"
9591,1," While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper."
9592,1,"\n\u2013 The paper\u2019s main contribution seems to be a neural network with a GA optimization for classification that can learn \u201cintelligent combinations of features\u201d, which can be easily classified by a simple 1NN classifier."
9593,1," However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method."
9594,1, Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training.
9595,1,".\n\nThe principal shortcoming of the paper is that there was essentially no effort to establish that the baseline systems that are being improved through reranking via these methods are decent baselines for such a use, or to really specify these systems in a way that would allow for replication of the results being presented in the paper."
9596,1," It needs to be better defined."""
9597,1,"\u201d: It\u2019s better to explain this part in more details, possibly with some equations. It is hard to understand the difference.\n\n"""
9598,1,"""This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function."
9599,1,"\n\nOverall, the paper proposes an interesting improvement to this area of synchronous training, however it is unable to validate the impact of this proposal."""
9600,1,\n* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs.
9601,1,"\n- when evaluated on hand designed small maps, the agent doesn't perform very well (figure 6)."
9602,1," However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported."
9603,1,"\n\nThe overall idea and approach being pursued here is a good one,"
9604,1," To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER."
9605,1,  This leads to the question of cheating in the learning process.
9606,1," Moreover, in Sec. 4.3, it is said that agents can see around them +10 spaces away; however, experiments are run in 7x7 and 10x10 grid worlds, meaning that the agents are able to observe the grid completely."
9607,1, If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper.
9608,1," \n- Another effect (possibly due to greedy decoding + choice of model), is that the numbers in Table 1 are rather low compared to the COCO leaderboard."
9609,1,"\n\n* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the \u201ccontent\u201d is an old idea that has appeared in the literature and should be cited accordingly."
9610,1,"\n2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5."
9611,1,"\n\nThe proofs are mainly limited in that they don't refer in any way to the class of approximating networks or the optimization algorithm, but rather only to the optimal solution."
9612,1,\n-            Provides valuable insight into the MAML objective and its relation to probabilistic models\n\n
9613,1," However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained."
9614,1,"""The main strength of this paper, I think, is the theoretical result in Theorem 1"
9615,1," In fact, after 10^4 iterations, training with and without skip connection seem to achieve the same error."
9616,1, \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP).
9617,1, Diagonal covariances are still very restrictive.
9618,1," This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable."""
9619,1," The argument in Section 8.6 is not convincing, ie, there is potentially a large approximation error in (41) and one cannot claim that Lemma 1 and Theorem 4 are still valid without the isolated component condition. "
9620,1," \n\nRegarding the experimental part, it can not make strong support for all the claims."
9621,1, The authors need to provide more justification for this motivation.
9622,1, Grammatical and spelling mistake are frequent.
9623,1,\n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD.
9624,1, Maybe you could comment on this.
9625,1,"  \n\nIn a similar vein, from the supplementary material, sentiment analysis on IMDB and action classification on UCF-101 are well-studied problems, but the authors do not compare with any previously published results on these tasks."
9626,1,\n\nThe paper is well written and easy to follow.
9627,1,  Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior?
9628,1,"\n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD."
9629,1," \n\nHowever, where possible experiments should compare directly with prior published results on these tasks; none of the experiments from the main paper or supplementary material report any numbers from any other published work."
9630,1,"\n\n\""and is hardly reusable\"" -> \""and are hardly reusable\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""Skill composition is the idea of constructing new skills with existing skills (\"" -> \""Skill composition is the idea of constructing \nnew skills out of existing skills (\"".\n\n\""to synthesis\"" -> \""to synthesize\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""set of skills are\"" -> \""set of skills is\"".\n\n\""automatons\"" -> \""automata\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""with low-level controllers can\"" -> \""with low-level controllers that can\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""the options policy \u03c0 o is followed until \u03b2(s) > threshold\"": I don't think that's how options were originally defined... beta is generally defined as a termination probability.[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""The translation from TLTL formula FSA to\"" -> \""The translation from TLTL formula to FSA\""?"
9631,1,"\n- Re. the formulation in Thm 2: is it clear that there is a unique global optimum (my intuition would say there could be several), thus: better write \""_a_ global minimum\""?"
9632,1,"""This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN."
9633,1,"\n\n- only ~2 models / datasets examined, so difficult to assess generalization"
9634,1,\n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS.
9635,1," I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do."
9636,1," Performance on training set by the best model is close to perfect (99.5%), so the model is really learning the task."
9637,1," In particular, the introduction/review of the work of Balestriero and Baraniuk 2017 not very useful to the readers."
9638,1, The experimental setups are explained in detail.
9639,1,"""This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution."
9640,1," As long as both of them are identically distributed, then no discrepancy exists."
9641,1," The method includes two independent components: an \u2018input defender\u2019 which tried to inspect the input, and a \u2018latent defender\u2019 trying to inspect a hidden representation."
9642,1,"  But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure."
9643,1, Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable.
9644,1," \n\nOffering a related work \""feature matrix\"" that helps readers keep track of how\nprevious efforts scale learning rates or minibatch sizes for specific\nexperiments could be valueable."
9645,1," Anecdotally, real-life distributions are far from uniform, so this should be a common issue."
9646,1, Is there some central limit theorem explanation?
9647,1, Could the authors comment on that?
9648,1, Is this on bAbi as well?
9649,1, Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic.
9650,1," The idea is quite straightforward, and the paper is relatively easy to follow."
9651,1," What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach."
9652,1,", but the abstract and introduction make very strong claims about outperforming \""state-of-the-art supervised approaches\"""
9653,1," \n\n* \""In this experiment, we compare PIBs with ....\"" -> I find this whole section hard to read, the description of how the models relate to each other is a little difficult to follow at first sight."
9654,1,"  The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper."
9655,1, Are these the results after all agents had converged?
9656,1, It is proposed to optimize the formulation using the method of Lagrange multipliers.
9657,1,. It will be useful to also provide time to a certain accuracy that all of them get to e.g. the validation error of 0.1609 (reached by the 3 important cases).
9658,1, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used).
9659,1, \n\nWriting:\n1. The GRU model for which the algorithm is proposed is not introduced until the appendix.
9660,1, Some examples are:\n- Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.
9661,1, This is not an accurate portrayal.
9662,1, and less content on problem definition and formulation.
9663,1, It's also unclear from which epoch the adversarial examples were generated from in figure 5.
9664,1,\n\nThe idea is important and this paper seems sound
9665,1, Networks are incrementally grown; child networks are initialized with learned parameters from their parents.
9666,1,\n2.\tThe paper is well written making it easy to follow.
9667,1,"""Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network."
9668,1,\n\n+ Significance: \n- I think the paper contribution is lighter vs. ICLR standard.
9669,1, But Propositions 1 and 2 seem to just say that the only way to get a perfectly tight SMC ELBO is to perfectly sample from the joint posterior.
9670,1,  Are they based on different alpha and lambda values?
9671,1," However, I am not convinced that the improve is resulted from a better algorithm."
9672,1," E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words."
9673,1,"\n\nSecond, there is no analysis of the representativeness of the found sets of\nconstraints."
9674,1,"  They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters."
9675,1," This is particularly important, since GTD provides convergent TD updates under nonlinear function approximation; the role for a heuristic constrained TD algorithm given convergent alternatives is not clear."
9676,1," The paper over-uses acronyms; sentences like \u201cIn this figure, VBP, VBP with FBA, and ITD using FBA for VBP\u2026\u201d are painful to read."
9677,1,"""This paper presented a Generative entity networks (GEN)."
9678,1,"""The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system."
9679,1,  However it is unclear if these gains in training speed are significant enough for people to flock to using this (more complicated) method of training.
9680,1,    All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).
9681,1, This should be at least discussed in the paper.
9682,1,\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks.
9683,1,\n\nOverall the math looks reasonable.
9684,1," Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms."
9685,1, \n\nThe result that existing architectures didn't do very well at these generalizations (especially b and c) *may* be important -- or it may not.
9686,1,The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.
9687,1," Thus, in the PPD the only (jointly) winning\nmove is not to play, but selfish agents are again tempted to defect and try to score points even though\nthis decreases total social reward."
9688,1," This learning procedure is heuristic, and there is no theoretical guarantee about the correctness (convergence) of this learning procedure."
9689,1,"  While the intuition is nice and interesting,"
9690,1," And for (2), it is a relatively standard approach in utilizing CNN features."
9691,1," (I would add an explanation of the spectral estimation in the Appendix, rather than just citing Rodu et al. 2013)."
9692,1,Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student?
9693,1, The paper also fails to provide any quantitative evaluation of the proposed method.
9694,1,"\n* Some key historical references are overlooked, like the SAMMANN."
9695,1, One could imagine that computing the posterior\napproximation in equation 6 has some additional cost.
9696,1," \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline."
9697,1," \n\n1). In the \""Meta-learner\"" section 4.1, the authors talk about word features (u{_w_{i,j,k}},u{_w_{i,j',k}})."
9698,1, The paper did not discuss these related work and did not compare the performances.
9699,1," Second, the phrase \""additive skip connections combining outputs of all layers\"" has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?)"
9700,1,\n\nComments and questions:\n\n1) It is unclear whether this paper is motivated by training SPENs or by training structured predictors. 
9701,1," It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low-rank algo."
9702,1,\n\nPros:\n- Interesting formulation.
9703,1," Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels. "
9704,1,\n\nWhen you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again.
9705,1, Most of my comments focuses on this.
9706,1," Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking."
9707,1, This corresponds to a mixture of Neural Statisicians.
9708,1,"  I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic."
9709,1, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context.
9710,1,"""Quality\nThe paper is well-written and clear, and includes relevant comparisons to previous work (NPI and recursive NPI)."
9711,1," \n \nFor the experiments, other baselines should be included, particularly just regular Q-learning."
9712,1,\n\nThe paper is clear to follow and the objective employed appears to be sound.
9713,1,\nRotation and scale from the polar origin result in translation of the log-polar representation.
9714,1, In that sense this paper could be useful to researchers wanting to better understand this field.
9715,1, Neal (1994) and Williams (1997) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions.
9716,1,"""The paper focuses on a very particular HMM structure which involves multiple, independent HMMs."
9717,1,"\n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication."
9718,1,Are the choices of decoder etc. similar?
9719,1," For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs."
9720,1," In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper."
9721,1," Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target."
9722,1, I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there).
9723,1,"To my knowledge, the explanation of universal perturbations in terms of positive curvature is novel."
9724,1,"\n\nOverall, the paper presents several incremental improvement over existing theories."
9725,1,"  This is obviously possible with any regularization technique, but I think it is more of an issue here since parts of the data are not even used in learning."
9726,1, The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet)
9727,1,"""The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article)."
9728,1," However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy."
9729,1,"\n3. Some typos\n- Section 2 - 1st para - last line: \""These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.\""."
9730,1, The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.
9731,1,  This could be seen as a game design issue.
9732,1," I quite like the formulation of the NIPS ratings: \""if this paper does not get accepted, I am considering boycotting the conference\""."
9733,1, All of these are well known methods.
9734,1, How is it compared  the L_1 regularization vs. the proposal?
9735,1," Although similar methods appeared in the tensor literature before, I don't see any theoretical ground for their correctness.\n\n  - Second, there is a significant difference between the symmetric CP tensor decomposition and the non-negative symmetric CP tensor decomposition."
9736,1,"\nPage 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated."
9737,1,"   Heck, in practie ridge regularization will also do something similar for many function classes. "
9738,1," The authors might want to consider shrink it down the recommended length. """
9739,1,"""This very well written paper covers the span between W-GAN and VAE."
9740,1," The authors argue they could not reproduce Osband\u2019s bootstrapped DQN, which is also TS-based, but you could at least have reported their scores."
9741,1,                    \u201c distributions has\u201d ->  \u201c distributions have\u201d.
9742,1," however in order to put the \""clear accept\"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).\n\n"""
9743,1,A penalty term is added that forces an agreement between the hidden states of the two decoders.
9744,1,"""This is a well-written paper with good comparisons to a number of earlier approaches."
9745,1, \n\nCan you clarify the contributions of the paper in comparison to the R3NN?
9746,1, The paper states that it builds upon the formulation in [Defferrard et al. 2016] as explained in section 3.
9747,1, It would be nice to report the training loss to see if this is an optimization or a generalization problem.
9748,1,"\n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014)."
9749,1,  but does not address some important issues.
9750,1," \n\nSignificance\n- The paper could compare against other relevant baselines that combine model-based and model-free RL methods, such as SVG (stochastic value gradient)."
9751,1,"\u3000Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims."
9752,1,"  Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n"""
9753,1,\n\nCons: \n- The results are not very appealing visually. 
9754,1,\n\nThe experiments supported the hypothesis that the RNNs are able to \n\n- generalize zero-shot to new commands.
9755,1,"\n\n    PATH(  s,  Tau( s, th^g ),  a ; th^p )\n\n    d / { d th^g }  PATH(  s,  Tau( s, th^g ),  a ; th^p )"
9756,1, \n\nMinor comments: \n\n- What is the meaning of the dashed lines and the solid lines respectively in Figure 1? 
9757,1,"\n\nIntroduction\n- please define HMM/GMM model (and other abbreviations that will be introduced later), it cannot be assumed that the reader is familiar with all of them (\""ASG\"" is used before it is defined, ...)"
9758,1,"\n- It is unclear why the Auto Encoder network is added, and what its function is."
9759,1, It is not explicitly stated.
9760,1,"\nHowever, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods"
9761,1,"  The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works."
9762,1,\n2. Lacks in sufficient machine learning related novelty required to be relevant in the main conference
9763,1," At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables."
9764,1,"  Second, \""VCs tent to occur for a specific class\"", that seems rather a bold statement from a 6 class, 3 VCs experiment, where the class sensitivity is in the order 40-77%."
9765,1,  I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct.
9766,1,".\n\nThe empirical results are only presented in table-of-numbers format (graphical comparisons would be easier to understand), and tables 5-8 are all zero, which doesn't make sense for these classification tasks."""
9767,1,"""\nSummary: the paper proposes an idea for multi-task learning where tasks have shared dependencies between subtasks as task graph."
9768,1,"""In the paper titled \""Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\"", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks."
9769,1," Reporting the training objective value makes little\nsense to me, unless the time taken to train on MNIST is taken into account in\nthe comparison."
9770,1," See, for example, uses of Langevin dynamics as a non-convex optimization method."
9771,1,"(On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)"
9772,1,"\n\nMinor:\n- t of $G_t$ in page 2 is not defined so far.\n- What is \""gr\"" in Section 2.2"
9773,1,  The explanation maps mirror for the considered methods the model's reaction to the input.
9774,1, Such comparison has been carried out both in theory and practice for simple low dimensional environments with linear (and RKHS) value function approximation showing how TD methods can have much better sample complexity and overall performance compared to pure MC methods.
9775,1, There is nothing that prevents the model from using multiple dimensions to capture related structure of the data.
9776,1,TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations. 
9777,1," The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title"
9778,1," Without a proper motivation, its difficult to appreciate the methods devised."
9779,1, It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment.
9780,1," \n\nSection 4.1 is the second most important section of the paper, where properties of VCs are discussed."
9781,1, but I also don\u2019t find the results strikingly surprising. 
9782,1," Maybe more justifications are needed."""
9783,1," Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss."
9784,1,. Would the approach work as well using a more standard encoder-decoder model with determinstic Z?
9785,1," As a result, the motivation does not seem clear."
9786,1," As far as I can glean, the topology of the neural network is constructed using the chart of a CKY parser."
9787,1,"\n\nIn addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training."
9788,1,"\n\nMinor:\n\nIn (5), what is the purpose of the -1 term in R_e?"
9789,1," \n\nClarity\n=====\nThe paper reads well,"
9790,1,\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors.
9791,1,  It could be removed to focus more extensively on the continuous case (right example).
9792,1,"?\n8. In the Monte-Carlo sampling, how many pairs are sampled? "
9793,1, but I have some concerns regarding this work.
9794,1,"\n\nThe paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al). "
9795,1, It seems that the validation error is still decreasing after 25 epochs?
9796,1," To clearly establish your contributions, the authors must do a better job of relating their work to [1] and [2]."
9797,1, \n 2) 3d voxel occupancy grids with a small resolution.
9798,1,"\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017)."
9799,1,\n\nTherefore I would now support accepting this paper.
9800,1," \nThe paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth."
9801,1," That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?)."
9802,1, The activation function keeps the activation roughly zero-centered.
9803,1," They may hide serious problems."""
9804,1,"\n\n\nNitpicks:\nthe gradient descent -> gradient descent or the gradient descent algorithm\nseeming -> seemingly\narbitrary flexible -> arbitrarily flexible\ncan name \""gradient descent that maximizes\"": gradient ascent.\nThe mini- max or the maximin solution is defined -> are defined\nis the follow -> is the follower\n"""
9805,1," If I understand it correctly, it\u2019s simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation."
9806,1,\n\nThe paper mentions \u201csignificant improvements\u201d in only two places: the introduction and the conclusion.
9807,1,\n\n== Novelty/Significance ==\nControllable image generation is an important task in representation learning and computer vision.
9808,1,"\n\n3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section."
9809,1, I enjoyed reading it and the various evaluations that it described that target both the use of prepositions inside phrasal verbs as well as in its role in indicating grammatical relationships between different elements in a sentence.
9810,1,  It seems to me that the model trained on meaningful data should have a larger margin.
9811,1,"\n\nYou claim that the number of layers \""increase exponentially\"" in FractalNet. This is misleading."
9812,1," Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to."
9813,1, The paper just told difference stories section by section.
9814,1,\n\nClarity. What is shared attention exactly?
9815,1," The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals."
9816,1," It is possible that this model does not do well, but there is an equally good model for i which also does well on j."
9817,1,"  Unsuprisingly, this leads to an improvement."
9818,1,"""SUMMARY: This work is about prototype networks for image classification."
9819,1," \nOverall, I like the idea, so I am leaning towards accepting the paper,"
9820,1,".\n\nThough the authors claimed that they used 3 techniques to accelerate synchronous SGD, only partial pulling is proposed by them (the other 2 are borrowed straightforwardly from existing papers). "
9821,1,"\n3.) Since the paper describes a computer-vision image synthesis system and not a new theoretical result, I believe reporting the actual run-time of the system will make the paper stronger."
9822,1, Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters.
9823,1, The paper seems rushed to me so authors should polish up the paper and fix typos.
9824,1, It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one.
9825,1, The use of learned representations needs more rigorous justification
9826,1,"\""\n-Please explain what is meant here by 'hand crafted information', my understanding is that the f^i in figure 1 of that paper are learned modules?"
9827,1,? How much translation
9828,1," The reference to \""Model counting\"" is\nincomplete."
9829,1,  \n\nSection 2.7 tries to relate the spike-based learning rule to the biologically observed STDP phenomenon.
9830,1, The proposed GAN generator consists of two components where one focuses on generating foreground while the other focuses on generating background.
9831,1, But this does not matter since you already use the labels in the softmax loss.
9832,1, The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).
9833,1,\n\nExperiments: appreciated the wall clock timings.
9834,1,  The current paper's authors talk about previous work not being well-controlled for number of parameters.
9835,1,  I think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the paper.
9836,1,"- although of course, since all results should be reported on cross-validated testing subsets anyhow,"
9837,1,"\n\n[1] Easy Questions First? A Case Study on Curriculum Learning for Question Answering. Sachan et-al.\n[2] Learning Word Vectors for Sentiment Analysis. Maas et-al."""
9838,1," In a response to a question I posed, you mentioned that we you meant was \""we use about 40% memory for the gradient computation and storage\""."
9839,1,"  \n\n(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale?"
9840,1,"  In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters.\"" ===>  Well all this needs to be included in the same paper. "
9841,1," The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations."
9842,1,"  It's nice to have it very clear, since \""gradient step\"" doesn't make it clear what the stepsize is, and if this is done in a \""Jacob-like\"" or \""Gauss-Seidel-like\"" fashion."
9843,1,  The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST.
9844,1,\n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.
9845,1, 1. Visual navigation where images of start and goal states are given as input.
9846,1,"  For example,  it is hard to obtain the conclusion \""more independence lead to better performance\"" from the experimental results."
9847,1, N-grams are by definition contiguous sequences... The authors may want to consider alternatives.
9848,1,"""This paper provides a new generalization bound for feed forward networks based on a PAC-Bayesian analysis."
9849,1, \n6. Extra models like Deep Networks with/without matrix factorization could be added.
9850,1,\n\nI disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA.
9851,1,"\nThe basic idea is to use datasets with known factors of variation, z, and measure how well in an information theoretical sense these are recovered by a representation trained on a dataset yielding a latent code c."
9852,1," This is important in order to evaluate the complexities involved in computing its Hessian.\n"""
9853,1,\n\n\nClarity\n=====\n\nThe paper is clear and well-written.
9854,1,"\n- It has a limited capacity so if the distribution of environments is too large, its performance will plateau (figure 5)."
9855,1,\n\nSmaller comments:\n\nYou say that you base the Hessian and gradient estimates on minibatched samples.
9856,1,"""The authors present a novel evolution scheme applied to neural network architecture search."
9857,1," In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3."""
9858,1, The generator is trained with a WGAN optimization.
9859,1," If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified."
9860,1,"\n\nIn a question I posed earlier, you claimed that this scaling makes no difference when batch normalization is used."
9861,1,. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it.
9862,1,\n3) Outperforms SkipThought in evals
9863,1," However, it would also be useful to do an ablation study of the \u201cfactorization\u201d of action values. "
9864,1,\n\nIt is very positive that the figures are very helpful for delivering the information.
9865,1,"""This paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network VGG19 due to Simonyan and Zisserman."
9866,1,"\n- The authors claim their method is \u201cvastly\u201d better at paraphrasing phrasal verbs than baselines, based on qualitative comparison."
9867,1, The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets \u2013 this is called \u201csuper-convergence\u201d.
9868,1, It would require very major editing to be fit for publication.
9869,1," Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing."
9870,1," \n\nAlso, the authors should provide more detailed description regarding the scheduling of the alpha and lambda values during training, and how sensitive it is to the final clustering performance."
9871,1, \n\nThe experimental results seem promising in the illustrative MNIST domain.
9872,1," b) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors,"
9873,1,\n\nI went over the math.
9874,1," For example, the authors spend quite a bit of space focusing on the rank-1 (CP) decomposition, which is well known, as opposed to focusing on the merits of their technical contributions."
9875,1,"\n\nPros:\n1. Well-written paper, with clear contributions."
9876,1,"\nThe focus on \""regret minimization perspectives\"" didn't really get me too excited"
9877,1,"""The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph."
9878,1,\n\nCons:\n- The proposed model is pretty hand-crafted.
9879,1," \n\nOverall, I think this dataset is very useful for RC."
9880,1,  So I would have liked a comparison to that simple method (using similar regression technique to generalize over states with similar features).
9881,1," It seems that the header \""data set\"" should be \""approach\""\nor something similar."
9882,1," This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets."
9883,1,  Also in the formulation D_S is not optimised.
9884,1," \n-- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016."
9885,1," For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration."
9886,1, It would be also useful to see a comparison of different CNNs and see how the observation holds more or less depending on the performance of the network.
9887,1,\n\nA more detailed review follows.
9888,1,\n\nIn Table 2 the difference between inception scores for DCGAN and this approach seems significant to ignore.
9889,1, The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster.
9890,1,\n\nCorrectness: The paper is technically correct.
9891,1, This would explain the massive gains in Yago.
9892,1,". Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested"
9893,1,\n   transformations chosen?
9894,1," e.g., \np.2:\nT: s \\to a \\to s', \\pi : s \\to a\nA denotes a set and its cardinal\nIn (1), shouldn't it be \\Phi(o)?"
9895,1,\n- Comparison with existing clustered MTL methods mentioned above are missing.
9896,1,  The authors use notation p_D() in eqn (12) without defining it.
9897,1, This would give a much more convincing account of the value of saliency in this context.
9898,1,  The M in MVProp in particular seems to be very useful in scaling up to the large grids.
9899,1, It would help the reader to be more explicit here.
9900,1,"""The paper introduces a generative model for graphs."
9901,1,\n- An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method.
9902,1, It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism.
9903,1," The experiments show that this approach can be used to successfully detect adversaries for several datasets, including MNIST, CIFAR10, and a small subset of ImageNet, and also investigates the robustness across different variations of attack.\"
9904,1,"  Thus, the authors insist on context modeling to obtain a relevant analysis of a word's meaning."
9905,1," The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour."
9906,1, I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients.
9907,1," \n\n\nIn detail:\n\n(1) This paper misses some obvious connections and references, such as \n* Krauth et. al (2017): \u201cExploring the capabilities and limitations of Gaussian process models\u201d for recursive kernels with GPs."
9908,1,  Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases? 
9909,1,\n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.
9910,1,"\n\nOverall, I feel that the submitted version is not ready for publication in the current form.\n"" "
9911,1, Just a single setting (with a single random sampling of seen/unseen classes) has been evaluated.
9912,1," For example, attention, which have been widely used and been shown to help capture structures in many tasks, are not included and compared in this paper."
9913,1," The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points."
9914,1, It's not compared against any previous published results.
9915,1, What is the perplexity of all the language models corresponding to  Tables 4 and 5?
9916,1,\n- The mathematical derivations have some errors
9917,1, The writing could be improved.
9918,1," Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution."
9919,1,"\n\nSignificance:\nThe method may prove to be useful to regularize recurrent networks, however I would like to see a comparison with ensemble methods."
9920,1,\n - there are some errors in figures that I think were all mentioned by previous commentators.
9921,1," The accuracy on a held-out test set is not guaranteed to be monotonically increasing, right? "
9922,1,"  There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences."
9923,1, How can the reader conclude any functioning from these images?
9924,1," \n\nTheorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014."
9925,1," I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup."
9926,1,"\n\nThough novel,"
9927,1,\n- The results of Table 5 need to be presented and interpreted in the light of this caveat of greedy decoding.
9928,1,  but it would be important to argue that those options are likely to be useful again under some task distribution.
9929,1, Why not try on multiple different word embeddings?
9930,1," The theoretical analysis shows that if the loss is strongly convex, then the algorithm returns a solution which is close to the optimal solution."
9931,1,"""This paper introduces a comparison between several approaches for evaluating GANs."
9932,1,\nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that.
9933,1," Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper."
9934,1,"""\nGENERAL IMPRESSION:\n\nOverall, the revised version of the paper is greatly improved."
9935,1," The work is original, but I would say incremental, and the relevant literature is cited."
9936,1,  It is almost trivial to run DDPG on Torcs using the openAI baselines package [https://github.com/openai/baselines].
9937,1," \nIndeed, if some games have been learnt by the proposed algorithm, the authors do not precise what modules have to be retrained to learn a new game."
9938,1," Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype."
9939,1," The model does not assume an explicit prior over the underlying dynamical systems, instead only uncertainty over observation noise is explicitly considered."
9940,1, But what exactly should the distribution be like to be learnable and how to quantify such \u201crelated\u201d or \u201csimilar\u201d relationship across tasks? 
9941,1, \n\nClarity:  The paper is clearly presented and easy to follow.
9942,1," SAP does not perform as well as adversarial training, but SAP could be used with a trained network."
9943,1," The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels."
9944,1," Nevertheless, the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of NNs."
9945,1," This experiment shows improvement over such baselines,"
9946,1," \n\nAt the same time, though, I see several important issues that need to be addressed if this paper is to be accepted."
9947,1,  That could be worthy of some (brief) discussion.
9948,1,"\n\nOverall this paper examines interesting structured and randomized low communication updates for distributed FL,"
9949,1,\n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.
9950,1,"\n\nTheorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization?"
9951,1," Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results."
9952,1,"""The paper discusses dropping out the pre-softmax logits in an adaptive manner."
9953,1, \n\nAdditional comments/questions:\n\n* The description of the Q-learning implementation is unclear.
9954,1,\n-- The plots in figure 1 and 2 have different colors to represent the same set of techniques.
9955,1, It would be great to see the authors address this issue in a serious manner.
9956,1," For every neuron out of two, authors propose to preserve the negative inputs."
9957,1," Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets)."
9958,1,\n\n\nSmall comments and questions
9959,1," In section 3.2 they explain that \""Our attack uses existing optimization attack techniques to...\"", but one should be able to understand the method without reading further references."
9960,1,\nAnother aspect that worries me about the system is how it can be extended to higher dimensional action spaces.
9961,1,    \n\nThe authors limit what the car is able to do \u2013 for example it is not allowed to take actions that would get it off the highway.
9962,1,"""\nI think the first intuition is interesting."
9963,1," \n\nAlso, it is not clear whether the GAN samples indeed are improved qualitatively (with the incorporation of the PIR objective score maximization objective) vs. when there is no PIR objective."
9964,1,"\n\nIn the SMT tasks, the baselines reported seem to be far away from results presented in the literature on the IWSLT task (see http://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_3.pdf)"
9965,1,"""--------------------\nReview updates:\nRating 6 -> 7\nConfidence 2 -> 4\n\nThe rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I\u2019ve increased my score.\n-----"
9966,1," One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes)."
9967,1,"\n\n(-) The evaluation, however, is quite limited."
9968,1," It is basically a straightforward generalization of the idea of punishing, which is common in \""folk theorems\"" from game theory, to give a particular equilibrium for cooperating in Markov games."
9969,1,"\n\n6. If there is a section on INFOGAIN exploration, why not mention it in the main text?"""
9970,1,  Moreover the paper is clearly written.
9971,1,\n- I am not an expert in this area.
9972,1,\n- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.
9973,1, I was wondering where this assumption is used?
9974,1," They also observe that the gap persists at test time, although it does not examine how it relates to approximation error."
9975,1," Compared with traditional learners such as LSTM, the model is better at capturing graph structures and provides a flexible solution for training with arbitrary graph data."
9976,1,"For example there is no such thing as a \""variational Bayes model\""."
9977,1,"   Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same."
9978,1,"""This paper proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function."
9979,1,"\n\nSome of these questions that need to be addressed IMHO:\n\n- A critical general question: if the addressed problem is the alignment between e.g. images and not image generation, why not formalizing the problem as a similarity search one (using e.g. EMD or any other transport metric)."
9980,1, The overall structure of the paper is appropriate.
9981,1," Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \""context unit\"" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \""context\"" vectors are exchanged)"
9982,1," The proposed embeddings just barely beat the baseline on product classification and sentiment classification, but significantly beat them on aspect extraction task."
9983,1,"""Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention."
9984,1," \n- How are the networks trained, with what objective, how validated, which training images?"
9985,1,  This quantity is \u2018the probability of an input being assigned the correct output.
9986,1," At most recent, we have seen some more explicit way for visual grounding like: (c). Bottom-up and top-down attention for image captioning and VQA (https://arxiv.org/abs/1707.07998). Anderson et al. arXiv 2017."
9987,1,"\ng)\tClearly, with a large T (number of RW steps), the RW is not modeling just a single community. Is there a way to choose T?"
9988,1,\ Can PixelNN run in real-time?
9989,1," At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU."
9990,1,"\nUsing the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1)."
9991,1, Nothing major really depends on that.
9992,1,"  \n\nThe shorthand notation in the paper is hard to follow in the first place btw, perhaps this could be elaborated/remedied in an appendix, there is also some rather colloquial writing in places:\""obscene wast of energy\"" (abstract), \""There's\"" \""aren't\"" (2.6, p5)."
9993,1,"\n\nSome typos:\n- Section 2.1:\nin the definition of G_t, the expectation is taken over p as well\nI_w and T_w should be a subset of S"
9994,1," Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks."
9995,1,  Why not use one matrix instead?
9996,1, \nModel evaluation is questionable.
9997,1,\n\n* Using dictionary embeddings a la Hill et al.
9998,1,"""This paper is an extension of the \u201cprototypical network\u201d which will be published in NIPS 2017."
9999,1,\n\nSignificance:\nThis is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumstances.
10000,1,"  Even if the LM is trained on the common-crawl corpus, it has  a very low OOV rate, and fine tuning on the tasks only lowers it b t 1%."
10001,1," \n- It would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which GANs are often used"
10002,1,  This is a standard result coming from the fact that the Fenchel dual problem to regularized maximum likelihood is the maximum entropy problem with a quadratic objective as (2).
10003,1," While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section."
10004,1,"\n\nGeneral Questions\n\nI am  wondering how come you didn't consider a geometry image representation of the meshes, and went for a slightly more general, and yet very confined alternative (the adjacency requirement, which in some sense is the same type of constraint as geometry images)."
10005,1," I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds."
10006,1,\u201d \u2014 Isn\u2019t this more than initialization?
10007,1," As this is an important component of the proof of the main result, the paper would benefit from an explanation of this step?\n"""
10008,1,"\n4) If I understand dependency reinforcement embedding correctly, a RNN is trained for every trace. If so, is this scalable?"
10009,1," \n\nIt is nice that the authors tried to extend the \""noise as target\"" to the clustering problem, and proposed the simple \""delete-and-copy\"" technique to group different data points into clusters."
10010,1,"""The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance."
10011,1,"""This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se)."
10012,1, The GAN is 1 forward operation.
10013,1," However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims."
10014,1,"\n\npage 3:\n- \""f that plays the role of an appraiser (or critic)...\"": this paragraph could be extended and possibly elements of the appendix could be added here.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Section 4: the way clipping is presented is totally unclear and vague.[[CNT], [PNF-NEG], [CRT], [MIN]] This should be improved.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Eq (5): as written the distribution of \\tilde{x}=tx+(1-t)y is meaningless: What is x and y in this context?"
10015,1,  Comparisons against these approaches would make the paper stronger.
10016,1,\n\nThe experiments compare with the recent relevant literature.
10017,1,"\n\nThe attention score is then a function of these concatenated vectors i.e. \\alpha_{ij} = \\exp(S(HoW_i^C, HoW_j^Q))"
10018,1,"\n\nWhile the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts),;"
10019,1,  but I don\u2019t know the area well enough to make specific suggestions 
10020,1,\n\n2. Does the Deep Metric network always return a '64-dim' vector?
10021,1,  At the same time I think the paper does not have a significant amount of machine learning novelty in it.
10022,1, This should probably also be referenced.
10023,1, \n\nThe function class Zonotope is a composition of two parts.
10024,1,"""This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks."
10025,1,"\n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising"
10026,1," Although, the authors use powerful PixelCNN priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive components."""
10027,1,  but it has neither been carefully expanded or investigated.
10028,1,  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.
10029,1,"Finally, while it proposes an interesting formulation of a well-studied problem"
10030,1,\n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.
10031,1,"""3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc."
10032,1, \n \nb) why the authors use the softsign instead the tanh:  $tahnh \\in C^2 $! Meanwhile the derivative id softsign is discontinuous.
10033,1, For example: What happens when the decoder is trained separately using more flexible inference (e.g. Hamiltonian MC) and the encoder is trained later?
10034,1,"\n\n\nCons:\n\nAlthough the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article."
10035,1, \n\nThe resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples.
10036,1,"""The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique."
10037,1,"""This paper proposes a variant of neural architecture search."
10038,1," Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet)."
10039,1,"n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017"""
10040,1," This brings us back to the fact that features encoding the actual dynamics, potentially on many consecutive states (e.g. feature expectations used in IRL or occupancy probability used in Ho and Ermon 2016), are mandatory."
10041,1,  but it is not really clear what the claimed contribution is.
10042,1,"\n -- \""we can lower bound the log-likelihood of each **dataset X** ...\"
10043,1," In fact, the number of layers is linear in the depth of the network."
10044,1,"\"" PLoS computational biology 11.8 (2015): e1004315."
10045,1,"\nFrom the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly."
10046,1,"""- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics."
10047,1, Can the VGG be instead trained from scratch in an end-to-end way in this model?
10048,1,"  They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \""Very well, run your algorithm 10x longer and see where you end up!\"" "
10049,1,\n-Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your 'master'.
10050,1," \n6. In some of your experiments, every training method converges to the same accuracy after enough training (Fig.2b), while in others, not quite (Fig. 2a and 2c)."
10051,1," why is the policy a sum of \""p^{cost}\"" and \""p^{reward}\""? "
10052,1, I recommend acceptance to ICLR18.
10053,1, What was the width of each layer? 
10054,1,"\u201d In this case, these are just different interventions on possibly the same object."
10055,1,"\n-- in the computational section: \""Training size is 9924 and testing is 6695. "
10056,1, Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps.
10057,1,    That made it hard to get excited about the results in a vacuum.
10058,1," Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals."
10059,1,"\n\nEvaluation: The link prediction task is too easy, as links are missing at random."
10060,1,"\n- sec3.2 \""and 1/2 v^T v the kinetic\"": \""energy\"" missing\n"
10061,1, Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data? 
10062,1," On a related note, the authors only seem to report results from a single random seed (ie. deterministic architectures are trained exactly once)."
10063,1," Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector,"
10064,1,"""This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder."
10065,1,\n\nIt would be great if the authors could include the code that implements the model.
10066,1, \n\nOverall the paper is very well written.
10067,1," In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities."
10068,1,  Or is this work focused on improving the performance of existing methods?
10069,1,"  To improve the understanding of the CCC-based operation, it would further be worthwhile to provide an additional graph that shows the action choices of CCC agents over time to clarify behavioural characteristics and convergence performance."
10070,1,"\n\nWeaknesses: \n1. The phrase of \""computational chicken-and-egg loop\"" in the title and also in the main body is misleading and not accurate."
10071,1,"""Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH."
10072,1, Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices.
10073,1, This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost.
10074,1," The \u201creverse validation\u201d method described in Ganin et al., Domain-adversarial training of neural networks, JMLR, 2016 might be helpful."
10075,1,"In general, when we project our update to be orthogonal to the maximal change of the next states value, how do we know it is a valid direction in which to update?"
10076,1," However, there are several weaknesses existed:\n\n1."
10077,1,\n 2. Manifold analysis of the intrinsic structure of DNN is a important direction for further study.
10078,1,\nWhat is the reconstruction error in case 1?
10079,1," The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL."
10080,1,". In the proposed protocol, the model \u2018sees\u2019 the generated data D_gen (which is fixed before training) multiple time across epochs. "
10081,1, Explanations are clear.
10082,1, with no other novel interpretations or insights into deep architectures.
10083,1,\n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).
10084,1,"""This paper proposes using a feedforward neural network (FFNN) to extract intermediate features which are input to a 1NN classifier."
10085,1," To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix."
10086,1,\n\n- The simplest most standard baseline of all (FSGM) is missing.
10087,1,"\n- Section 4.3: \""Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update\"" is ambiguous: what is exactly meant by \""iteration\"" (and sometimes step elsewhere)?"
10088,1,"\n\nThe second part makes the link with the self-normalization. This is\nnot really surprising. This was already explained in the same way in\npapers from Pihlaja,Gutmann and Hyvarinen published in 2010/12."
10089,1," They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data."
10090,1, The paper then argues that LSTM is redundant by keeping only input and forget gates to compute the weights.
10091,1,"\n2. Scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?"
10092,1," \nTo avoid the intractable marginalization over latent variables, the paper applies variational inference to approximate the posterior within the context of given training data."
10093,1," What is meant by \""number of layers connecting the stochastic latent variables\""?"
10094,1," Essentially, the authors just took the previously used objective function and used it with a different network."
10095,1, This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations.
10096,1,\n\nWeaknesses\n- Improvement in accuracy is small relative to previous work.
10097,1,"  For example, just after (18) it is said that \u201c\\hat{Q} could be obtained through bootstrapping by R + gamma V_Q\u201d."
10098,1,"""It is hard to interpret this work as the authors do not mention the original work by Gutmann and his colleague on the NCE in the required details."
10099,1, This choice is quite ad-hoc.
10100,1,"\n\n(3) to summarize points (1) and (2), block diagonal architectures are a nice alternative to pruned architectures, with similar accuracy, and more benefit to speed (mainly speed at run-time, or speed of a single iteration, not necessarily speed to train)"
10101,1, It is hard to judge the significance of this extension.
10102,1,". Moreover, the new metric is introduced with the following motivation \u201c[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution\u201d. "
10103,1," More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4)."
10104,1,"  \n- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition"
10105,1," The experiments are interesting,"
10106,1,"\n\n- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined."
10107,1," There are many things unclear. For example:\n\n-  it starts with talking about multiple tasks, and then immediately talks about a \""filter F\"", without defining what the kind of network is being addressed."
10108,1," Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.\n """
10109,1," The use of canonical coordinates is certainly a sensible choice (for the reason given above),"
10110,1," Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two)."
10111,1," This is easy to fix, but not correct as written."
10112,1,"\n\nOther than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible."
10113,1, It seems that most safety constraints can be expressed via masking.
10114,1," The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers."
10115,1,"\n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation."
10116,1, What type of\ntuning did you do to choose in particular the latent dimensionality and the\nlearning rate?
10117,1," Specifically, it poses just little constraints and presents no stochasticity (options result in stochastic outcomes)."
10118,1,\nIt is also surprising that the efficiency the L2-regularized version is not evaluated.
10119,1, b) and what is the percentage of cycles saved by employing the ISRLU.
10120,1," Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro."""
10121,1,"  To summarize, the following are the pros of the paper:\n\n  - clarity and good presentation;"
10122,1,"\n3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it."
10123,1, The paper leverages a novel method in determining the coefficient of relative entropy.
10124,1," These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain."
10125,1,"\""\n\n* I don't understand"
10126,1," (A) [Train] [deep feed-forward networks] [without skip-connections] and (B) [Train] [deep feed-forward networks without skip connections]. In (A), the `without skip-connections' modifies the `train' and suggests that training took place without skip connections."
10127,1, This paper is mostly about linear networks.
10128,1," For real color samples, it is harder to figure out the mixture."
10129,1," If not, please update the experiments to be consistent with the baselines."
10130,1,\n\nPositives:\n\n(1) The approach is straightforward to implement and trains networks in a reasonable amount of time.
10131,1, \nOn a related point: What would Figure 2 look like for the constand uncertainty setting?
10132,1," But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read."
10133,1,\n\n2.  If the authors\u2019 encoding scheme really works I feel that they could beef up their experimental results to demonstrate its unqualified advantage.
10134,1," \n\nThe paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''."
10135,1,"\n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems."
10136,1,"\u00a0\u00bb \n\nNormally, I should have stopped reviewing, but I decided to continue  since those parts only concerned the preliminaries part."
10137,1,"\nThey formulate it for a blackbox and a semi-blackbox setting (semi being, needed for training their own network, but not to generate new samples).\"
10138,1," This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case."
10139,1,"  Overall, the paper is interesting and promising;"
10140,1,"\n\nIn the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities."
10141,1,  Binary tree generated in pooling layers are kept for unpooling layers in decoder network.
10142,1, \n\nNotes on clarity:\nBefore Eq 1 it\u2019s hard to know what the antecedent of \u201cwhich\u201d is without reading ahead.
10143,1, This seems like a pretty intense process: solving some representative subset of all possible RL problems for a particular environment \u2026 Maybe one choses s and s\u2019 so they are not too far away from each other (the experimental section later confirms this distance is >= 7.
10144,1,"\n\n*Remarks on methodology*\n\nBy initializing a decoding by \u201cguessing\u201d a value, the decoder will focus on high-probability starting regions of the space of possible structures."
10145,1," It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \""structured noise\"", and (3) Samples from out-of-dataset classes."
10146,1,"\n\n* Again, from Algorithm 1, it is not clear which parts can be performed asynchronously."
10147,1,\n\nReview:\nThe paper is well written. 
10148,1," The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent)."
10149,1," I could imagine that at different\ntimes, different posterior samples of the weights will be used to compute the\ngradients."
10150,1, \n2.\tThere is another work that also considers the target-context interaction using interactive attention model.
10151,1, There are several typos in the proof of Theorem 2.
10152,1," There, a simple deterministic condition (the null space property) for successful recovery is proved."
10153,1,"""The paper addresses the problem of learning the form of the activation functions in neural networks."
10154,1,"\n\nThe central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them."
10155,1," \nAlso this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D."
10156,1, The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis.
10157,1,"""\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee."
10158,1," Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context."
10159,1, The experimental section is strong and it has evaluated across different datasets and various scenarios.
10160,1," For example, it is not described anywhere what loss is used for learning the model. "
10161,1," So by subtracting features from successive states, the method mainly encodes the action as it almost encodes the one step dynamics in one shot."
10162,1, \n\nClarity - The paper is not self-contained in terms of methodology.
10163,1,"""The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks."
10164,1," \n\n2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments."
10165,1,"\n\nSection 5.2:\nWhen trained on a small dataset, training amortization error becomes negligible."
10166,1," In the introduction section, the paper claims that \""the expert must set M to a value that is larger than the time horizon of the currently considered task\"" when mentioning the limitation of the previous work."
10167,1,  What's the exact challenge of training VAEs addressed by the convolution stochastic layer?
10168,1," However, it seems that hyper-parameters for RNN haven\u2019t been tuned enough."
10169,1," What the authors call \""intelligent mapping and combining system\"" for the proposed system is simply a fully connected neural network.[[CNT], [null], [DIS], [GEN]] Such systems have been largely investigated in the literature."
10170,1,"  Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations."
10171,1,"\n- In section 2, \""This [pre-training or co-training with maximum likelihood]\n  precludes there being a latent encoding of the sentence."
10172,1,"""The paper presents the word embedding technique which consists of: (a) construction of a positive (i.e. with truncated negative values) pointwise mutual information order-3 tensor for triples of words in a sentence"
10173,1,"\n\nIn general, I find many of the observations in this paper interesting."
10174,1, \nAuthors propose a number of baseline models as well as metrics to assess the quality of \u201cdefogging\u201d.
10175,1, and literature review is sufficient.
10176,1,"\n\n- The paper is reasonably clear,"
10177,1," For now let\u2019s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value."
10178,1, They claim that these augmentation types provide orthogonal benefits and can be combined to yield superior results.
10179,1,"  but in my opinion without a substantial contribution and working only in a  very limited setting when it is heavily relying on many unproven hacks and heuristics."""
10180,1, \n\nThe experimental results show the effectiveness of the approach.
10181,1, This makes me\nwonder if the comparison in table 2 is fair.
10182,1,\n\n4. The real data experiments shows some improvements in predictive accuracy with fast inference.
10183,1," This leads to a few questions:\n\n1. What was the performance of the \""regression policy\"", that was learned during the supervised pretraining phase?\"
10184,1,"\nConceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression."
10185,1,"If I understood correctly, the authors first train \\theta and \\phi and then fix \\theta to train \\phi_x and \\phi_y."
10186,1, \n\nReview:\n\nUsing an implicit step leads to a descent step in a direction which is different than the gradient step.
10187,1, \n- The dual formulation simplifies adversarial training
10188,1," Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015)."
10189,1, Maybe this is what helps?
10190,1," Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided."
10191,1,\n\nStrengths:\n1. The authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolution. 
10192,1,\n- It seems like the performance of AE-k is increasing with increase of dimensionality of latent space for Fashion-MNIST.
10193,1, I would be interested to know if they authors see ways to generalize to better classifiers.
10194,1,"\n\nI am not saying that these works are equivalent to what the authors are doing, or that there is no novelty, but the evaluations seem extremely unfair to only compare against matrix factorization techniques, when in fact many higher order extensions have been proposed and evaluated, and especially so on the tasks proposed (in particular the 3-way outlier detection)."
10195,1,"""The paper presents interesting algorithms for minimizing softmax with many classes."
10196,1," For each output token, they aggregate scores for all positions that the output token can be copied from."
10197,1, This suggests a fairly significant sensitivity to this hyperparameter if so.
10198,1," It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone."
10199,1,They also present results for a 'harmonic' combination which was not explained in the paper at all.
10200,1,"\n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive."
10201,1,"\n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA?"
10202,1, The paper is generally clearly written and represents a valuable contribution.
10203,1,"""This paper introduces a method for learning representations for prepositions."
10204,1,Certain gating in RNN is very useful but I don't know whether one can train any reasonable RNN with all multiplicative gates
10205,1, It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning.
10206,1, Traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixels.
10207,1, How to choose lambda?
10208,1,  This is more of a shortcoming than a fundamental issue.
10209,1,"\n\nIn section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity."
10210,1,\n\nQuestions\n--------------\n\n1. Section 3: You say that you can vary 'pf' and 'qf' to set the trade-off between computational budget and performances.
10211,1,"""The paper proposes a method for generating images from attributes."
10212,1," Could the output grammar be extended to support joins, for instance? "
10213,1,\n\nThe work seems to be a little bit incremental.
10214,1," In\nProceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015), ACM, pp. 1310\u20131321."
10215,1," what are the effects of regulariazations in this regard? """
10216,1," Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea."
10217,1,  But these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already available.
10218,1," One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n"""
10219,1," If this is a fact, there is no equation describing how this flow is computed."
10220,1," Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers)."
10221,1,"\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1."
10222,1," The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification."
10223,1," The test samples are obtained by picking consecutive frames of\nlength 10 uniformly at random across the sequences. \"" - To me this is very unclear."
10224,1," The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (\u201cconcepts\u201d and \u201ccontext\u201d are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper."
10225,1," However, it is not very well organized and many points are not defined."
10226,1," Here,  the KL is not symmetric but its directional aspect is not significant."
10227,1, It would be great to have more intuitions.
10228,1, This is nonstandard and confusing.
10229,1, \u201cCustomers\u201d randomly appear throughout the task and the taxi agents receive reward for moving to the same square as a customer.
10230,1, Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time.
10231,1," In this case, the projections of the data via the kernel are not necessarily in a RKHS."
10232,1," However, the authors only conjecture as to the potential of this method without describing clear ways of approaching this subject, which somewhat lessens the strength of their argument."
10233,1,\nThe paper ultimately is light on comprehensive evaluation of popular models on a variety of datasets and as such does not quite yield the insights it could.
10234,1,"\n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides."
10235,1,\n\nThe intuition behind comparing against a classifier trained on a noise-perturbed version of the data is not explained clearly. 
10236,1," A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1."
10237,1, This is the main take-away from this paper.
10238,1,"\n\nAnother drawback, perhaps resulted from the \""genetic algorithm\"" motivation is that the proposed method has not been well explained."
10239,1," However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case."
10240,1, The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes. 
10241,1, \n- Section 4: The acronym MLP is used but never defined.
10242,1, One would expect Q_MC to work well in Grid Map domain if the conjecture put forth by the authors was to hold universally.
10243,1,n\nThe bot performance significantly better than the fully trained agent.
10244,1,"\n- Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it"""
10245,1," \n\n4. The Experiment section is not well structured, at least for me, I cannot understand it well."
10246,1,\n_______________\nORIGINAL REVIEW:\n\nThis paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations.
10247,1," In the current version, the paper does not explain the HDDA formalism and learning algorithm, which is a main building block in the proposed system (as it provides the density score used for adversarial examples detection)."
10248,1, \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.
10249,1," To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc."
10250,1,CEGIS both selects fewer examples and has a shorter median\ntime for complete synthesis.
10251,1," While the experimental setup does not match (Mirowski et al, 2016), thereby invalidating claim [d], the experiments are thorough and do show that that architecture does not generalize to unseen mazes."
10252,1,"\n- There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network."
10253,1," I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs."
10254,1,  They then show that the result matrix satisfies a restricted isometry condition.
10255,1,"  \n\nI could not appreciate the results in Figure 2 since I was missing the definition of PIR, how it is drawn in the training setup."
10256,1, This needs to be spelled out and a reference needs to be added.
10257,1,"\n- \""The episode length (time budget) was randomly set for each episode in a range such that 60% \u2212 80% of subtasks are executed on average for both training and testing."
10258,1," The contributions here are: 1) experiments testing the DTP algorithm on more difficult datasets,"
10259,1,"\nTo deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix."
10260,1," Table 1 is good,"
10261,1, but also a time-honored concept. 
10262,1, Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches.
10263,1,"\n\nHowever, the paper shows only preliminary results in which the generator trained to maximize the PIR score (computed based on VGG features to simulate human aesthetics evaluation) indeed is able to do so."
10264,1,\n\n- Quality of figures.
10265,1," Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model."
10266,1,\nA large set of experiments are provided to support the claims of the paper.
10267,1,  It seems because of other factors instead of the stochastic convolutional layer.
10268,1,\n\ncons:\n(a) The proposed search algorithm in the semantic latent space could be computationally intensive.
10269,1,\\nI think the value in this paper comes from a practical/simple way to do policy randomization in deep RL.
10270,1,"  The faces experiment is\nsimilar to previous work done by Martin (2011) and Kontsevich\n(2004) but unlike that previous work does not investgiate whether\nclassification features have been identified that can be added to an\narbitrary image to change the attribute \""happy vs sad\"" or \""male vs female\""."
10271,1, Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization.
10272,1," For example, in Section 3.3, how each s_(i, j, k) is sampled from S?"
10273,1," The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients."
10274,1,"  It's unclear how to compute per-token accuracies for structured prediction tasks, such as speech recognition, parsing, and translation."
10275,1," It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space."
10276,1, Figure 2 & 3 helps the readers understand the core algorithm.
10277,1," On the complexity of teaching. Journal of Computer and Systems Sciences, 50(1), 20-31."
10278,1," It begins by observing that in time-limited domains, an agent unaware of the remaining time can experience state-aliasing."
10279,1," With respect to KL-divergence, a G-test can be used (see https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback.E2.80.93Leibler_divergence)."
10280,1, The authors have definitely found an interesting untapped source of interesting images.
10281,1,"""This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem."
10282,1, \n\nThe experiments are limited.
10283,1,\nI especially like the idea of \u201csoft unitary constraint\u201d which can be applied very efficiently in this factorized setup.
10284,1," While the experiments are not particularly impressive,"
10285,1, This is an example of a constraint which cannot be expressed by masking actions and in fact requires breaking the top speed limit for a bit in order to be safer in the longer term.
10286,1,  \n\nThe writing of the draft leaves much to be desired.
10287,1,"\n+++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository"
10288,1,"""This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space."
10289,1, \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated
10290,1,"\n- Experiments limited to small data sets, some obvious questions remain"""
10291,1," However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds,"
10292,1, The paper does not seem to include any method development.
10293,1,"\n4. However, computing and inverting the Fisher information matrix is computationally expensive."
10294,1,"\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: The main idea is clearly motivated and presented,"
10295,1," First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle."
10296,1," Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided,"
10297,1,"\n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point."
10298,1,\n\n3) Why the different initializations for the recurrent weights for the hexagonal vs other environments?
10299,1," During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold."
10300,1," Specifically, they could analyze for a set of query words, what the most similar words are in the embeddings obtained from different subsections of the data."
10301,1," but it is not clear if it is practical, because the false alarm rates for high detection are quite high."
10302,1,"""This paper proposes a learning method (PIB) based on the information bottleneck framework."
10303,1," I would say that it is \""similarly decoded to $\\mathbf{c}$\"", since it is \\mathbf{c} that gets decoded."
10304,1,"\n\nThere is no notion of class invariance, so the GAN can find the space of filters that transform layer inputs into other classes, which may not be desirable. Have you tried conditioning the GAN on class?"
10305,1,Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove.
10306,1,"\n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data."
10307,1,"  The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1.\n"""
10308,1,"\n\nThe list goes on...\n\nFor such a complex architecture, the authors must try to analyze separate modules as much as possible."
10309,1, Some (theoretical) analysis would be nice.
10310,1,"\n2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al."
10311,1,"\n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously."
10312,1," Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models)."
10313,1, \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets.
10314,1," Below, I give some examples (and suggest improvements), but there are many others.[[CNT], [null], [SUG], [MIN]] There is a risk that in its current state the paper will not generate much impact, and that would be a pity."
10315,1," The new derivation of the method yields a much simpler interpretation, although the relation to the natural gradient remains weak (see below)."
10316,1,"\n\n[Other comments]\n\n* \""Given this state of affairs, perhaps it is time for us to start practicing\n  what we preach and learn how to learn\""\n\nThis is in my opinion too casual for a scientific publication..."
10317,1," The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively. "
10318,1,"\nThe fact that 1-layer softmax and 2-layers MLP perform worse than VGG is not surprising, I do not see it as an \ninteresting contribution."
10319,1," The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding."
10320,1,"\n\n%%% After Author's Clarification %%%\nThis paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly."
10321,1, It is furthermore not clear whether training was carried out on multiple seeds or whether these are individual runs.
10322,1," \n\nFinally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited."
10323,1,"\n2) eq2, usually we talk about truncated n-step returns include the value of the last state to correct the return."
10324,1,"\n-\tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier."
10325,1,\n2) the formulation in equation 4 seems to be problematic
10326,1," The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive."
10327,1," A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear."
10328,1," If this allows the removing of the target network for instance, in those more difficult tasks, then this would be a huge deal."
10329,1," k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on."
10330,1, Learning the Speech Front-end With Raw Waveform CLDNNs.
10331,1, however this intuition is not fully tested.
10332,1," Also, quantitative figures would be useful to get the big picture."
10333,1,"\n\nOriginality\n\nThis is one of the first uses of GANs in the context of neuroimaging."""
10334,1,"  \n\nI think reading the paper, it should be much clearer how the embedding is computed for Atari, and how this choice was made."
10335,1,\n   c. Only MNIST and CIFAR10 dataset with Lenet32 are used in the experiment.
10336,1," However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis. "
10337,1," However, there is no experimental comparison with these methods."
10338,1," To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable."
10339,1,\n\nI found the paper unclear and lacking in detail in several key aspects:
10340,1,\n\nThe experiments in this paper does not provide enough evidence to tease apart the possible causes of this dramatic reduction on computational resources.
10341,1,"\""\nso part of my negative impression may be pure mis-understanding of what\nthe authors had to say."
10342,1,"\n-\tPag7 there are two \u201cso that\u201d in 3.1; capital letter \u201cIt used 32x10^12..\u201d; beside, here, why do not report the difference in computation w.r.t. not-spiking nets?"
10343,1,
10344,1," I would like to suggest to add an approach similar to Duvenaud et al., \""Convolutional networks on graphs for learning molecular fingerprints\"", NIPS 2015.\"
10345,1,"""\n1) Summary\nThis paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction."
10346,1,"  I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings."""
10347,1," There seems to be a gap between the first paragraph and the second paragraph.[[CNT], [PNF-NEG], [CRT], [MIN]]  The authors mentioned that \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system."
10348,1,\n\n* Unclear derivations:\nThe derivations of Appendix A.1 is unclear.
10349,1,\n\nThe paper closely follows Hein and Andriushchenko (2017).
10350,1," In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC."
10351,1,"   The authors state that \"". . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset."
10352,1,"  This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures."
10353,1,\n\nThe draft does need some improvements and here is my suggestions.
10354,1,"\n- Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule,"
10355,1,"  The paper, however, is not flawless."
10356,1," Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results,"
10357,1,\n- The paper is overall well-written and clear.
10358,1," However, Narang et. al., 2017 already explore this idea, although briefly."
10359,1,"\n --\""... to refer to the **class X_i** ...\""."
10360,1,"\n\nAnother concern I have is regarding the definition of the mutation operators in Section 3.1.[[CNT], [CNT], [CRT], [MIN]] While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it)."
10361,1,"\n\nIn contrast, the paper \""Coco-Q: Learning in Stochastic Games with Side Payments\"" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning."
10362,1,"\n\nReview:\n\nThe paper reads well,"
10363,1,". In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy. "
10364,1," The authors should analyze if this also holds true for ResNet and Inception, which are more widely used than VGG16."
10365,1,\n\nQuestions:\n- Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping.
10366,1,"""In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward."
10367,1,"""In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner."
10368,1," This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract)."
10369,1,"""\n\n- Equation 2 seems sloppy. \u201cj\u201d appears as a free index on the right side, but it doesn\u2019t appear on the left side."
10370,1," Are the results still valid for another measure of discrepancy based for instance on another measure, e.g. Wasserstein?"
10371,1,  The paper seems to claim that since overlapping architectures have higher expressivity that answers (a)
10372,1," The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2."
10373,1," The most difficult part is the lack of precision on the maths, it is hard to figure out what the authors contribution indeed are."
10374,1, Could you provide a large sample of the data at an anonymized link?
10375,1,. That is an advantage.
10376,1, The dimensions of the base embeddings are some kind of latent attributes and each individual dimension could be used by the model to capture a variety of attributes.
10377,1," Shouldn't the computational complexity then be at least O(n^2), which makes it actually much slower than, say, SQuAD models that do greedy decoding O(2n + nm)?"
10378,1,\n- How does bipolar activation compare to model train with BN on CIFAR10?
10379,1, Tuning the architecture of the single multi-layer NN adversary might be as good?
10380,1," \n\nOTHER COMMENTS:\n- p3: both images in Figure 1 are labeled Figure 1.a\n- p3: typo \""theis\"" --> \""this\"" \n\nAbe & Mamitsuksa (ICML-1998)"
10381,1," As such, I find it difficult or impossible to judge this paper fairly relative to other submissions."
10382,1,"   It's not totally clear to me that if totally naive humans (who had never seen spatial relationships before) were evaluated on exactly the training/testing scenarios described above, that they would generalize particularly well either."
10383,1,\n\n- The paper is mostly clearly organized and presented.
10384,1," Please tell me if I am missing something, but I am not sure of the contribution of the paper."
10385,1,"\n\nIt is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input."
10386,1,\n\nOverall the paper has very good motivation and significance.
10387,1,\n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities.
10388,1,"""The main contribution of the paper is a method that provides visual explanations of classification decisions."
10389,1," \n\nThe authors claim that \""Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance\""."
10390,1, The ideas are straightforward and make sense given the current trends in the field.
10391,1,"\n\nYou say that \""First, path-wise training procedure significantly reduces the memory requirements for convolutional layers, which constitutes the major memory cost for training CNNs."
10392,1,"\n\nI found the description of the training of the method to be rather superficial, and I don't think it could be replicated from the paper in its current level of detail."
10393,1," The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS."
10394,1,"\n\n6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration."""
10395,1,"\n\nTo make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients)."
10396,1," It is not clear if the same rules holds to other CNNs, and images of other categories. \n"""
10397,1, (why should they be different?)
10398,1,"\n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. "
10399,1,"""This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way."
10400,1,\n\n+ The paper is easy to follow.
10401,1,"\ One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs."
10402,1, Interesting parallels with human cortical and hippocampal learning and memory are discussed.
10403,1,"\n- In Sec. 4.4, why is it important that the samples are fresh?"
10404,1,"\n* The term \""AI-related task\"" sounds a bit too broad[[CNT], [PNF-NEU], [SUG], [MIN]]\n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection."
10405,1," To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells."
10406,1,\n\nI think the writing needs to be improved on the following points:\n- The abstract doesn't fit well the content of the paper.
10407,1,"\n\nPlanning lane-change maneuvers is an interesting, important problem for self-driving vehicles."
10408,1," This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss."
10409,1,\n2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method. 
10410,1," Although the method could mix well when applied to those particular experiments,"
10411,1," Hence, the motivation for the algorithm you present is invalid."
10412,1," Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units.."
10413,1,"""This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition."
10414,1," I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion."
10415,1,"""Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes."
10416,1, Initial applications to domain adaptation and generative modeling are also shown.
10417,1,"  In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why."
10418,1," It may be true that \""one plot showing synthetic ICU data would not provide enough information to evaluate its actual similarity to the real data\"" because it could not rule out that case that the model has captured the marginal distribution in each dimension but not joint structure."
10419,1," In particular, it classifies existing methods into four different categories, according to the representation of the interactions of users, items and attributes."
10420,1,"""In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR)."
10421,1,"\n\n[1'] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, D. Samaras. Neural Face Editing with Intrinsic Image Disentangling. CVPR 2017"
10422,1, Results look promising too.
10423,1," In other words, the authors do not compare the proposed method with other methods doing data augmentation."
10424,1, Will a ResNet(32) show similar performance?
10425,1,"\n4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y).[[CNT], [PNF-NEG], [DFT], [MIN]]\n5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10."
10426,1," From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods. "
10427,1," Especially, projecting the data points to a uniform sphere can badly blur the cluster boundaries."
10428,1, A more convincing argument for a slow server should be provided
10429,1,\n\nThe proposed model and method are reasonably original and novel.
10430,1,", and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples."
10431,1," \n\n3. In section 4.2, it said \""Although we can also define Hessian on ReLU function, it is not well supported on major platforms (Theano/PyTorch)."
10432,1," It can consider different classifiers and loss functions, and a sampling strategy for making the optimization problem scalable is proposed."
10433,1,Can the authors address the earlier comment about \u201ca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms\u201d
10434,1,"""This paper focuses on the problem of \""machine teaching\"", i.e., how to select a good strategy to select training data points to pass to a machine learning algorithm, for faster learning."
10435,1,  The idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly.
10436,1," Suggest to rephrase \""unreasonable\"" to something more positive. "
10437,1, I would suggest the authors also run the experiments on CIFAR-10.
10438,1,"\n\nPros\n\nImportant message about network limitations.[[CNT], [null], [DIS], [MIN]]\n\nCons\n\nStraightforward testing of network performance on specific visual relation tasks."
10439,1,   The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference.
10440,1," Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n"""
10441,1," Otherwise, the paper is well-structured and easy to follow.\"
10442,1,"\n\nThe paper is generally well written, easy to read and understand, and the results are compelling."
10443,1," The authors specifically examine random subsampling (which is the same as random masking, with different weights) and probabilistic quantization, where each element of a gradient update is randomly quantized to b bits."
10444,1, (actor-critic on machine translation)\nMiao & Blunsom (2016) Language as a Latent Variable: Discrete Generative Models for Sentence Compression.
10445,1," \"" -> \""...datasets\""\n-  First sentence of 6.2: \""we turn to the discuss a second\"" -> "
10446,1," It would be more instructive to see the results for a better, convolutional classifier."
10447,1, The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring.
10448,1," In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network."
10449,1," If that is so, should this paper also compare with STE?"
10450,1," The authors could have expressed their network using a clean recursion, following the parse chart, but opted not to, and, instead,  provided a round-about explanation in English."
10451,1," As there are not\nenough experimental details to judge, it's hard to figure out the problem,\nbut this ppaper is clearly not publishable at any of the quality machine\nlearning venues, for weakness in originality, quality of the writing,\nand poor experiments.\n"""
10452,1," Even in games where there is a cooperative solution that maximizes the total welfare, it is not clear why players would choose to do so."
10453,1," While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.\n\n"""
10454,1,  The proposed method jointly trains an RNN encoder with a GAN to produce latent representations which are designed to better encode similarity in the discrete input space.
10455,1," Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong."
10456,1, \n\nTheorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k\u2019=k..
10457,1," They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach."
10458,1, \n\nThere is a factually incorrect statement - depthwise separable convolutions were not introduced in Chollet 2016.
10459,1, The core idea is to perform a random projection of the data (which is supposed to decrease the impact from adversarial distortions). 
10460,1,\n- why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic? 
10461,1,"\n\n5. Figure 4: For the models in Figure 4, it would be useful to indicate the starting CER of the stage-1 model before stage-2 training to get a sense of how stage-2 training impacts performance."
10462,1,\n\nThe experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms.
10463,1,"  \n\n- The authors write \""diWdt generates slightly better results and we adopt it in our experiments."
10464,1, \n\nI also think that this method really ought to be evaluated on some other domain(s) in addition to binary image drawing.
10465,1,"Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation."
10466,1, I think it might be of interest to some audiences in ICLR.
10467,1, This is not convincing.
10468,1, although most results are shown on a medical dataset - which I feel is very hard for most readers to follow.
10469,1, I do not agree with statements like \u201cWe demonstrate that the proposed objective function generalizes ML and RL objective functions \u2026\u201d that authors have made in the abstract. There is not enough evidence in the paper to validate this statement.
10470,1,n\nThe paper is well written and easily to follow.
10471,1," In both circumstances, they demonstrate the efficacy of their technique and that it performs better than other reasonable baseline techniques: self-paced learning, no teaching, and a filter created by randomly reordering the data items filtered out from a teaching model."
10472,1," The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community."
10473,1,"\n\nThe approach is based on `profile coefficients\u2019 which are learned for every channel in a convolution layer, or for every column in the fully connected layer."
10474,1,"\n\nThey face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform."
10475,1, \n\n* Related Work\nThe authors do a good job describing and listing the papers most related to the current submission.
10476,1,"\n- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art."
10477,1," \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs,"
10478,1,\n\nI also have the following questions I hope the authors could help me with:\n\n1. I failed to understand how Eqn (5). Could you please clarify.
10479,1,"\nAs far as I understand, Equation (4) is not an embedding into an Hilbert space but\nis more a proximity space representation [1]."
10480,1," Do other deep neural networks, such as Resnet, Googlenet, can  have the same phenomenon?"
10481,1,\n4. Formulate complex analogue of Glorot weight normalization scheme
10482,1, What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?
10483,1,"""The paper is mostly a survey about clustering methods with neural networks."
10484,1," My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \""only\"" 6 billion words \u2013 2 orders of magnitude less data."
10485,1," They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting."
10486,1, How many times did you run node2vec on each graph? \n
10487,1, Explicitly mentioning this may help the reading grasp the formulation.
10488,1," I would have liked the experiments to be more thorough, with comparison to the state of the art models for the two datasets. \n"","
10489,1, It is well known that the residual connections are important in training deep CNNs and have shown remarkable performance on many tasks.
10490,1," In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer."
10491,1," For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them."
10492,1,\n\nThe introduction/experiments section of the paper is not well motivated.
10493,1," \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections."
10494,1," For example, there is this statement:\n\""We observe that our method (TR) converges to solutions with much better test error but\nworse training error when batch size is larger than 128."
10495,1,Authors need more experiments to show their approach\u2019s effectiveness.
10496,1," \n\n# Novelty and Significance\n- The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN. "
10497,1,  LL wins by a bit more in Table 2.
10498,1," However, the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernels."
10499,1,  \n\nPros.\n- the paper is clearly written.\
10500,1,"  \n\n-  Figure 2, synthetic data."
10501,1, It makes use of the CP or rank-1 tensor decomposition to define the meaning of rank for a tensor.
10502,1," \n\nThe paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers."
10503,1," As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network."
10504,1,"\n - there is a sentence that doesn't end at the top of p.3: \""... the original GAN paper showed that [ends here]."
10505,1,". However, this is unclear from the writing. "
10506,1," First, the model is *not* trained on the true distribution which is unknown."
10507,1,"  After all, the cluster membership is found based on the nearest target in the test stage."
10508,1," Even for an experimental paper, I would argue that novelty in the experimental methodology is an important fact."
10509,1," The method is novel, and the paper is generally well written."
10510,1,  In this case the authors look at finding interpretable teaching strategies.
10511,1, Did you consider larger dataset for evaluation?
10512,1,\n\nPros:\n- Addresses an issue of RWAs.
10513,1," I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples."""
10514,1, When does it not work?
10515,1, Optimal with respect to what task?
10516,1,"\n- Materials of the paper sometimes appear tedious to follow, mainly in sub-sections 3.4 and 3.5."
10517,1, there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture;
10518,1,\n\nThe Latent variable interpolation experiment could also use more explanations.
10519,1, The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label.
10520,1,"""The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it."
10521,1,\n\nMinor comments\n- Figure 1 is hard to read on paper. Please improve it.
10522,1, Hence this analysis does not add anything to the paper-  it states a general case which does not fit the current scenario and its relation to the work is not clear.
10523,1,\nIt should be noted that the limitations of the IB for deep learning are currently under heavy discussion on OpenReview.
10524,1, Does it correspond to the seconds per update step or the overall training time?.
10525,1, but this idea alone might not be signifcant enough as a contribution.
10526,1,"  Another misleading thing is the term self-organizing\nused throughout, which is roughly synonym to learning according to me, and\nnot something uniquely belonging to the SOM family of models, as used by\nthe authors."
10527,1, Results are reported on about 50 UCI datasets with different topologies.
10528,1,"\n* Another detail which I found missing is whether authors use just a screen, a mini-map or both."
10529,1,   \n\nSo how should authors handle this criticism?
10530,1," The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow."
10531,1," (1) What is an \""informal\"" theorem?"
10532,1," \n* At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points."
10533,1,"  I am wondering how D_S is trained, how the GT labels are obtained and whether it is trained jointly."
10534,1,"""The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified, "
10535,1,  \nFigure 2 which is the graphic representation of the model is hard to read.
10536,1,\n\nImportance: \nUnderstanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work.
10537,1," The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird."
10538,1,"\""\n\n-Minor: I think that the statement \""which only supports broadcast-ing communication of the sum of the signals\"" is not quite fair: surely they have used a 1-channel communication structure, but it would be easy to generalize that."
10539,1,The proposed technique is validated both empirically and theoretically.
10540,1, some parts are difficult to parse.
10541,1, What data augmentation was used for the CIFAR-10 dataset?
10542,1," \n\nOriginal Review\n=============\nSummary:\nThe contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections."
10543,1,"\n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix."
10544,1,"That being said, this particular use of deep learning in this context might be novel."
10545,1,\n\n\nMinor:\n- Probably figure 4 can be drawn better.
10546,1,"\n\n[5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015\n\n[6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016"
10547,1," I have listed this review as a good for publication due to the novelty of ideas presented,"
10548,1, This makes a lot of the questions asked in this paper extremely relevant to the field.
10549,1," This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery."
10550,1,"Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees."""
10551,1,"\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)"
10552,1,\n\nComments:\n1. One concern about this paper is that it doesn\u2019t fully answer the reasons why this normalization works better.
10553,1, \n\nPros: \n- The problem is relevant and also appears in similar form in domain adaptation and transfer learning.
10554,1, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models.
10555,1," Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning."
10556,1,"\n\u2013 The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling"
10557,1,\n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer.
10558,1,"\n\nThe analysis of the results are interesting, both from the quantitative & qualitative point of view."""
10559,1,"\n- The related works: Some recent papers in program synthesis are missing and should have been included in this paper such as:\nRobustFill: Neural Program Learning under Noisy I/O, ICML 2017"
10560,1,"\n\nIn summary, this is an interesting and well-written paper on a timely topic."
10561,1," Therefore, it is hard for me to understand what\u2019s the motivation to identify the core units in a one-vs-remaining manner."
10562,1,  \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic!
10563,1,. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case
10564,1," The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG."
10565,1," To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance."
10566,1," Specifically, the paper lacks of justification on why adjusting the learning rate based on the class labels are crucial to improve training FCNs, more specifically, how does it help resolving the exploding and vanishing gradient problems?"
10567,1," Under the setup of FL, it is assumed that training takes place over edge-device like compute nodes that have access to subsets of data (potentially of different size), and each node can potentially be of different computational power."
10568,1,This seems like a key claim to establish.
10569,1," Nevertheless, I think this work is important given its performance on the task."
10570,1,\n\nThe two parts in this paper are not new.
10571,1, With better ways of adjusting the noise level via step-size control (larger step sizes mean more noise) the loss of generalization associated with large mini-batch sizes can be brought down.
10572,1, \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].
10573,1," If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth."
10574,1,"\n\n- Theorem 1 is neat, well done!"
10575,1,\n\nThe authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best.
10576,1, Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to\nany of the known categories.
10577,1, \n\nOther issues are listed as follows:\n(1). Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3?
10578,1, \n\n(2) I am not convinced if the main results are novel.
10579,1,"\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets."
10580,1," For example, if I increase the number of importance samples, even if I'm overfitting in Fig 3(b), wouldn't the green line move towards the red simply because my estimator depends less on a poor q?"
10581,1, Is it the case that the defenses fall flat against samples generated by different architectures? 
10582,1, It should be supposed L is at least locally convex.
10583,1," \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks."
10584,1," Unfortunately, it's not just talk, they are also the point of\ncomparison in the experiments, i.e., there are no comparison with modern\ndeep learning methods."
10585,1,"\n- Only single runs are shown, considering the noise on those the results might not be reproducible."
10586,1,"\n3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose."""
10587,1," In a language model, the figure looks like it is attending to the word that is being generated, which is clearly not what you want to convey since language models don\u2019t condition on the word they are predicting."
10588,1,"  It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret). "
10589,1,\n- In appendix A why duplicate memory data instead of just using a smaller memory size?
10590,1," \n\nIt could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block."
10591,1, These aspects may be a sign of convergence issues.
10592,1, \n\n5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input.
10593,1,"  Also, it is not clear to me how the node and graph embeddings are initialized and how they evolve along the learning process."
10594,1, \n\nThe paper revisits mostly familiar ideas.
10595,1,"\n\n\n[1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst."
10596,1, The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices.
10597,1,\n\n* I'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simulator.
10598,1, how do you initialize h? fully at random ?\n\n5) seems the paper is following implementation by Kim et al. what happens if the discriminator is like in cycle GAN acting on pixels.
10599,1," Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments"
10600,1," The authors run 700 epochs and even 1400 epochs with path-wise training on CIFAR, while the baselines only have 160~400 epochs for training."
10601,1,  Why are two buffers necessary?
10602,1, A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion.
10603,1, This mitigates the impact of the manuscript.
10604,1, It appears to be the number of latent convolutional filters or channels generated by the state embedding network. 
10605,1," However, these assumptions are very constraining for multi-output classification and also interesting multi-output regressions."
10606,1," Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server."
10607,1,  The fact that there may be many policies which satisfy a particular reward function (or TLTL formula) is ignored.
10608,1," The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers."
10609,1,"""The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick."
10610,1, Please clarify this in the paper explicitly.
10611,1," Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib."
10612,1," The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training."
10613,1,"  So, it should be something else, but it does not become clear, what this f_p is."
10614,1,. Citations in [2] could be a good place to start.
10615,1,   One main issue is that the notation $phi$ is not clearly defined.
10616,1,\n- Lack of analysis of the learned memory behavior.
10617,1,\n-A new cross domain mapping is proposed\n-Large set of experiments\nCons\n-Some parts deserve more formalization/justification\n-Too many materials for a conference paper\n-The cost of the algorithm seems 
10618,1,\n\n4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST).
10619,1,\n\nSignificance: The idea of using factorization for RNNs is not particularly novel.
10620,1,"""This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window)."
10621,1," Additionally, it is evaluated over the MEN and Mturk datasets."
10622,1," Need more experiment on more dimensionality to prove that. \n6. In the appendix B results part, sometimes the word \u2019S_attack\u2019 is typed wrong. And the values in  \u201cdistorted/distorted\u201d columns in Table 5 do not match up with the ones in Figure 3(c)."""
10623,1," and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems. """
10624,1,\n2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches.
10625,1,\n\nThe Q&A task could be used to describe a simpler system with only a decoder accessing the DB table.
10626,1, 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input.
10627,1, and c) when to stop performing actions.
10628,1," When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework."
10629,1," it neither is written well nor it contains much of a novelty in terms of algorithms, methods or network architectures."
10630,1," Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation."
10631,1, This idea would have more impact if it generalized to arbitrary kernel dimensions.
10632,1," Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios."
10633,1," I find the definition of the \""state\"" in this case very interesting."
10634,1, The experiments back up the claim.
10635,1,"\n\nMinor comments:\n- Section 1, first paragraph, last sentence, \""that\"" -> \""than\""?[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Section 3.2, \""... using which...\"" formulation in two places in the firsth and second paragraph was a bit confusing[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Page 7, second line, just \""IS\""?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?"""
10636,1,\n to quickly sample examples with large inner products with the current parameter vector \\theta.
10637,1,"?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n"""
10638,1,"                                                            \n                                                                     \nOverall, the paper presents a novel idea, which is well motivated and clearly presented."
10639,1," The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves."
10640,1,\n2. The title of this paper is weird.
10641,1, You just introduce this concept without any explanation. 
10642,1,\n\nThe comparisons between various architectures are not very enlightening as they aren\u2019t done in a controlled way -- there are a large number of differences between any pair of models so it\u2019s hard to tell where the performance differences come from.
10643,1,  MLE-based methods penalize syntactically different but semantically equivalent programs.
10644,1," Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal."
10645,1, Classifying digits? Recreating digits?
10646,1," The paper proposes an actor-critic algorithm, called Normalized Actor-Critic (NAC), based on the entropy-regularized formulation of RL, which is defined by adding the entropy of the policy as an additional term in the reward function."
10647,1, \n- It would be interesting to see qualitative visual results on recognitions.
10648,1,"""The paper investigates the iterative estimation view on gated recurrent networks (GNN)."
10649,1,"  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value."
10650,1," From what I understood from the paper, the only technical contribution is the use of a so called 2-pass decomposition, which is simply an implementation of projected gradient descent on the set of rank-R tensors."
10651,1,"  Test accuracy is not improved, however."
10652,1,\n\nThe idea of applying the Fourier-based method to generalization is interesting.
10653,1,"\nIn some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy."
10654,1," The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule."
10655,1, The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming \u201cnot statistically-significantly different\u201d because no significance test is reported.
10656,1, Has this any connection with the final performance of the network?
10657,1," Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''."
10658,1, \n\n- This paper has a lot of orthogonal details.
10659,1," Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin."
10660,1," Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better!"
10661,1,"\n\n-The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. \\alpha_{ij}^l, \\alpha_{ij}^h, \\alpha_{ij}^u) it will learn to attend to \""different regions for different level\""."
10662,1, Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance.
10663,1,"""Summary:\nThe contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics."
10664,1," I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.\n"""
10665,1," The algorithm, tagged 'Consequentialist Conditional Cooperation', uses outcome-based accumulative rewards of different strategies established during prior training."
10666,1,\n\nThe extensions in section 5 don't seem to be very useful.
10667,1,"\nThen, I think that the claim of page 6 saying that Domain Adaptation can be performed \""nearly optimally\"" has then to be rephrased."
10668,1,  \n\n(2) The results are reported for different numbers of augmented instances.
10669,1, \n\n1. Returns are defined from an initial distribution that is stationary for the policy.
10670,1,"\n\nIn summary, the results and presented method are good, and eventually deserve publication."
10671,1," This is exemplified by the fact that the data set is hardly described at all and the 14 abnormalities/pathologies, the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint."
10672,1,(like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this. 
10673,1,  Resnet is powerful but previous methods did not use that.
10674,1,\n\nFinally the experimental results are okay
10675,1,\n\nNone of the figures is legible on a grayscale printout of the paper.
10676,1,"\nHowever, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks."
10677,1,"""The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation."
10678,1," That is, the task seems a completely valid one \u2013 one would like to be able to show that \""sparked off\"" is a synonym of \""provoked\"", but the actually results provided on this task seem quite uncompelling."
10679,1, There is a penalty to pay for choosing a small epsilon -- it seems to be visible from Figure 2.
10680,1,  These do not appear in the smoother PCA version.
10681,1," It\nwould be better to take the best of many runs or to somehow show error bars,\nto avoid the reader wondering whether gains are due to changes in algorithm or\nto poor exploration due to bad initialization."
10682,1,"\n[2] Zhang, Cheng, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt. \""Advances in Variational Inference.\"" arXiv preprint arXiv:1711.05597 (2017)."
10683,1,"\n\nMinor issues:\n- In the last equation on page 2, the right-hand side is missing a \""min max\""."""
10684,1," The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX' has a negative eigenvalue."
10685,1,\n(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words?
10686,1, This is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon.
10687,1, Why don't you present evaluation results on all tasks in the multitask setting?
10688,1,.\n\nThe results are not a strict improvement over existing works.
10689,1," \n\n[Original review]\nThe authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC."
10690,1,\n(vi) The argument why B ~ N is not clear to me.
10691,1," Instead, one could just say that one wants to learn a causal model and the proposed procedure is called CausalGAN? (This would also clarify the paper's contribution.)"
10692,1,"  In particular, the experiments are not rigorous enough to give serious evidence that PIBs improve generalization and training speed."
10693,1," \n\nThe paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW."
10694,1,"\n\nThe experimental part establishes a baseline using standard seq2seq models on the new dataset, by exploring large variations of model architectures and a large part of the hyper-parameter space."
10695,1,\n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.
10696,1," \n\nIn the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community."
10697,1,y.\n\nMy main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). 
10698,1,"\n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader."
10699,1, Results are preliminary.
10700,1," \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? "
10701,1,"It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum."
10702,1," In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels)."
10703,1,.\n\nMy comments:\n\nThe paper is well-written and I really enjoyed reading this paper.
10704,1,"n\n2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation."
10705,1,\n\n4. Why not comparing to Bootstrapped DQN since the proposed method is based on it?
10706,1, The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm.
10707,1,"\n\nIn a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term."
10708,1,"\n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data."
10709,1,The original one or the modified one?
10710,1," For more insights on this one could refer to the literature on decentralized POMDPs.\n\n\n\n\n"""
10711,1," Once again, a more in-depth analysis of this phase behavior would be very welcome."
10712,1,\nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.
10713,1,"""This paper proposes to use 3D conditional GAN models to generate\nfMRI scans."
10714,1,\n- The generalization ability of composition over primitive commands.
10715,1,\n2. The empirical performance is very poor.
10716,1, There are two major problems with this approach:
10717,1,"\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one."
10718,1,"""The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study."
10719,1," I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method."""
10720,1,"n\nI am looking forward to the test performance of this work on SQuAD."""
10721,1,"\n\nCons:\n\u2022\tHard to replicate experiments without the deep computational pockets of DeepMind.\n"""
10722,1,"n\nOriginality: I'm not familiar with LSTMs, it is hard for me to judge the originality here."
10723,1, For example The Bhatnagar 2009 reference should be Maei.
10724,1,\n\nAnother point relates to the fishing game.
10725,1, Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16.
10726,1, \n\n* Please clarify what are the hyper-parameters of your meta-training algorithm\n  and how you chose them.
10727,1," For one, the problem is solvable with breadth first search in O(L^2) time, for an L x L maze."
10728,1,"   The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist. "
10729,1," From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference."
10730,1, Does that mean that larger updates are always better?
10731,1," If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option."
10732,1,"\nThe experimental evaluation only considers two methods, comparing Info-GAN, a state-of-the-art method, with a very basic PCA."
10733,1, Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above.
10734,1,"Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons;"
10735,1, most of the conclusions are not entirely unexpected.
10736,1,".\nSince it is not known in advance what might be a good set of transformations,"
10737,1," Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN."
10738,1," From this analogy, one could expect the method to improves over MMD, but not necessarily significantly in comparison to an approach which would use more powerful discriminators."
10739,1, \n\nWeaknesses:\nThe paper is 8.5 pages long
10740,1," And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability."
10741,1," The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique."
10742,1," Thus, they are essentially learning the skip connections while using a human-selected model."
10743,1,\n\nPositive aspects:\nThe paper is well written and clear to understand.
10744,1,"I didn\u2019t see any mention to those in the definition for I(x,t). You only mention initial conditions."
10745,1," Since it works so well, maybe it could be promoted into the method section?"
10746,1, and (b) symmetric tensor CP factorization of this tensor.
10747,1, I am not sure why did the authors mentioned the work on over-parameterization though.
10748,1, I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further.
10749,1, The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned.
10750,1,\n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one.
10751,1,"\n\nTurning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO."
10752,1, It would enhance readability of the paper if the results were more self-contained.
10753,1,"""This paper presents some reviews on clustering methods with deep learning."
10754,1, Why no performance boost?
10755,1, It will be informative to see how the model holds in high-dimensional settings.
10756,1," Am I correct in understanding that this step depends only on Y, and that given Y,"
10757,1," 2. the \""approximation gap\"": the part of the slack due to using a restricted parametric form for the posterior approximation."
10758,1," Also, in the adding problem, it would be cleaner if you down-sampled a bit the curves (as they are super noisy)."
10759,1,"\n- Citations for \u00ab Deep Reinforcement Learning with Double Q-learning \u00bb and \u00ab Dueling Network Architectures for Deep Reinforcement Learning \u00bb could refer to their conference versions\n- \u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner \u00bb: please specify the exact formula"
10760,1,"  Then 2.5 pages are spent on channel pruning.[[CNT], [PNF-NEU], [DIS], [MIN]] \nI would have liked if the authors spent more time justifying why we should trust their method as a privacy preserving technique (described in detail below)."
10761,1," Sec. 3.4 \u201cGEN Image Encoder\u201d has some typo, it is not clear what the conditioning is within q(z) term."
10762,1," The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time."
10763,1," For those interested in word embeddings, this work suggests an alternative training technique, but it has some issues (described above).  """
10764,1,"\n\nIn section 5, the authors mention briefly some works from the RNN domain & the classical use case of memory networks."
10765,1,They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity.
10766,1,\n\nRELATED WORK:\n\nGood contrast to hierarchical learning: we don\u2019t have switching regimes here between high-level options
10767,1, The novelty is limited
10768,1,"\n\nIt would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network."
10769,1," For practical usage, especially in computer vision, GPU speedup is needed to show an impact"
10770,1,"""This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism."
10771,1, There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work. 
10772,1, Isn't there any existing benchmark where this could have an impact?
10773,1," In future revisions, this should be rectified."
10774,1,  This is the weakest claim of the paper.
10775,1, The approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward.
10776,1," Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change."
10777,1,"""This ms presents a new clustering method which combines deep autoencoder and a recent unsupervised representation learning approach (NAT; Bojanowski and Joujin 2017)."
10778,1," The very title \u201cDeep neural networks as Gaussian processes\u201d is misleading, since it\u2019s not really the deep neural networks that we know and love."
10779,1," The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach."
10780,1, We need a better reason than morphology to want to do source-side dependency parsing.
10781,1, any remedy for this problem?
10782,1,\nPage 5:\n-\tSection 2: the risk analysis is done in a standard Bayesian way and leads to a ratio of PDFs in equation 5.
10783,1, The reviewer is therefore surprised by the poor quality and lack of structure in the maps obtained from the DeepLift method.
10784,1," These topics seem sufficiently related to the proposed approach that the authors should include them in their related work section, and explain the similarities and differences."
10785,1, Figure 4 shows the only experimental results to \u201csupport\u201d the motivation.
10786,1,\n\n- I would have liked to see some context as how these results compare to an approach trained with aligned corpora. 
10787,1, Is there any reference for them?
10788,1," but I don't think the quality is high enough\nto warrant publication at this time.\n"""
10789,1,\n- the changes in performance of TSTR are large enough that I would have difficulty trusting any experiments using the synthetic data.
10790,1,\n\nPrevious method used at the distribution of softmax scores as the measure.
10791,1," Second, a set of 20 qualitative images does not (and cannot) validate any research idea."""
10792,1," Is it possible to transfer student-teacher training practices to other tasks?"""
10793,1,"\n\nEven though the title claims that the model disentangles the latent space on an entity-level, it is not mentioned in the paper."
10794,1, This iterative projection tends to perform better than iteratively optimizing f(W) and then applying the projection step only once at the very end of the optimization (assumedly the CP-ALS method that is used for comparison).
10795,1, Results are shown on MNIST and Fashion MNIST.
10796,1," However, the procedure for \""selecting arbitrary number of frames\"" to report performance seems really unnecessary...."
10797,1,"\n\n========================================================================\n\nThis paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance."
10798,1,2014\n\n+ Clarity: The paper is easy to read.
10799,1," This is pervasive throughout the paper, and is detrimental to the reader\u2019s understanding."
10800,1, \n\n\nComment: I kinda like the idea and welcome this line of research.
10801,1, This seems like an important assumption of the manuscript but the issue is brushed off and not discussed.
10802,1, Therefore this\napproach will be of limited use.\
10803,1,\n\n- The first experiment (dim red) is not clear to me.
10804,1,"\n\nTechnically, the paper is sound."
10805,1," The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).\n\n---\n\nQuality: The experiments compare the three proposed neural network architectures with two syntax-based architectures."
10806,1,"\n\nI appreciate that the authors developed extensions of the core method to more complex group structures,"
10807,1,  Some points have been clarified but other still raise issues.
10808,1," In the churce-vs. tower case, a  relatively weak MLP classifier was used."
10809,1, It is unclear how this approach manages tradeoffs between objectives that are specified as a conjunction of TLTL goals.
10810,1," \n\nAlso, I can't tell if I really fully believe the results of this paper."
10811,1,"With longer returns, n>>1, the role of the value function in the target is down-weighted by gamma^n---meaning its accuracy is of little relevance to the target."
10812,1,\nI think that results show that the approach is theoretically justified but optimality is not here yet.
10813,1, That is not apparent from the results.
10814,1, \n2. The results are depicted with a latent space of 20 dimensions.
10815,1,"\n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima;"
10816,1,"""The paper takes a closer look at the analysis of SGD as variational inference, first proposed by Duvenaud et al. 2016\nand Mandt et al. 2016."
10817,1,"\""\n - evaluation of the best method from the cifar10 experiments on the new dataset"
10818,1, The proposed method uses \n - a generator trained in a GAN setup\n - an autoencoder to obtain a latent space representation\n - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it). 
10819,1," Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'..."
10820,1, Would RINs readily learn to reset parts of the hidden state?
10821,1, Again exp3 provides little information about the experimental setup.
10822,1,  It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty.
10823,1,\n\nI think you may want to consider minimising ||a(x'|z) - a(x|z_k)|| instead to show that moving from x -> x' is the same as is invariant under the transformation z -> z_k  (and thus the corresponding movement in filter space).
10824,1," However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here."
10825,1,"\n\n- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks."
10826,1," This, in my opinion, is a biased and limited starting point, which ignores much of the literature in learning theory."
10827,1, 2) make minimal change in the middle so that the attack is not detectable.
10828,1,"""This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering \""sparring\"" follow-the-regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4)."
10829,1," For example, Snooper which uses a RandomForest  (https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-016-3281-2) and hence would be of interest as another machine learning framework."
10830,1,"\n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \""regret minimization\"" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives."
10831,1, Is there a typo?
10832,1,\n \n \n6 Experiment\nsetare -> set are
10833,1,\n? p.6: How does \u201cRatings Only\u201d work as DistMult gets no information of the specific entities?
10834,1," In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel."
10835,1," (or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483)."
10836,1, It seem to be promising when using transfer learning.
10837,1, How does performance change with different amounts of training data
10838,1," The paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations, or even what it means geometrically that positive curvature obtains at every data point."
10839,1,\npros:\n 1. The paper is well written and easy to follow.
10840,1, Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example.
10841,1," In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?)."
10842,1,"\n\nFigure 3: This can be inferred from the text (I think), but I had to remind myself that \u201cIW train\u201d and \u201cIW test\u201d refer only to the evaluation procedure, not the training procedure."
10843,1,"\n\nis a hierarchical -> are hierarchical\n\nyields -> yield\n\ntwice \""otherwise\"" after Eqn. 7\n\nare can be viewed\n\nthey occurs\n\ncan can readily expanded\n\ntransfer transform\n"""
10844,1,\n\n\n6 The model size of CrescendoNet is larger than residual networks with similar performance.
10845,1,I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.
10846,1, \n\nCons:\nThe idea seems to be simple and does not have significant originality.
10847,1,"  For example, by referencing a page that attempts to establish the state of the art on standard data sets (https://github.com/syhw/wer_are_we), we can find links to papers by Povey et al (http://www.danielpovey.com/files/2016_interspeech_mmi.pdf) and the Deep 2 paper in your citations, which themselves include baselines from other papers that cut the error rate in half versus even your best scoring systems, let alone your baselines."
10848,1,"  One simple one would be what if they only used the f and draw z samples for N(0,1)?"
10849,1," As far as I know, this kind of loss is commonly used in most existing methods. """
10850,1," I went on to read the relevant HDDA paper, but it is also not clear which of the model variants presented there is used in this paper."
10851,1,\n\nThe paper is written clearly
10852,1," Again, this makes the paper less strong because there is no baseline to compare."
10853,1,"""The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space)."
10854,1," To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays)"
10855,1, The work is rather incremental but is competently executed.
10856,1, The paper shows how the densities at each nodes are computed (and normalised).
10857,1," I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant."
10858,1," Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods."
10859,1," There is nothing wrong about this, but perhaps other more complex (non-linear) models to combine data could lead to more robust learning."
10860,1,"\n\nRegarding experiments, one straight-forward baseline is missing."
10861,1,"\n\nComment:\n\n1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap. "
10862,1,\nThis paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound).
10863,1, An open-source release would be ideal.
10864,1,Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent.
10865,1,\n\nWith my evaluation I do not want to be discouraging about the general approach.
10866,1," However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study."
10867,1, The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets.
10868,1, i.e |\\partial L / \\partial z \\partial w|<= B.
10869,1,"""One of the main problems with imitation learning in general is the expense of expert demonstration."
10870,1," In experiments, the proposed method synthesizes correct Karel programs (non-trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training."
10871,1,"\nConcerning the contribution of the model, one novelty is the conditional formulation of the discriminator."
10872,1,"\n3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM, "
10873,1," That is not to say that an empirical exploration of the practical implications is not of value, but that the paper would be much stronger if it was better positioned in the literature that exists."
10874,1," \n\nThe paper does not really introduce new methods, and as such, this paper should be seen more as an application paper."
10875,1,"\n- The authors should have compared their approach to the \""base\"" approach of Natarajan et al."
10876,1,"\n\n\nMinor Comments:\nSection 3.1, First Line. \u201df(ul(g(x),y))\u201d appears to be a mistake."
10877,1,"\n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty."
10878,1," After reading through both, my review score remains unchanged."
10879,1,"  I agree with your pair of sentences in the conclusion: \""Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning."
10880,1,  It is possibly not a fair claim to say that human drivers were \u201cless safe\u201d but rather that it was difficult to play the game or control the car with the safety module on.
10881,1," \n\n\nADDITIONAL THOUGHTS\n\nWhile the authors compare to an unassisted baseline, they don\u2019t compare to methods that use an action model\nwhich is not a fatal flaw but would have been nice."
10882,1,\n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2.
10883,1,"""This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \""style\"" as used in this paper)."
10884,1," \n- \""Observation 1\"": real and generated data points are not introduced at this stage... data points are not even introduced neither!"
10885,1, Hence inferring a structured latent space is a challenge.
10886,1, So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset).
10887,1,  They motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation.
10888,1,"\n- Unfortunately, no guideline or solution is offered in the paper."
10889,1,"""The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix."
10890,1," \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method."
10891,1,"  From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016)."
10892,1,"  \n4.\tThere are too many typos in the paper, e.g., \\alpha is replaced by a, etc."
10893,1," In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?"
10894,1," The aim is to learn embeddings from supervised structured data, such as WordNet."
10895,1," In any case, be consistent, most of the time these are used but then its stated \""for all Theta\"" (whithout superindex)\n * Proposition: what is L? is L = L_d? \n\n\nOther\n\n  * Assumption 5: decay -> delay?\n\n"""
10896,1, The introduction and related work part are clear with strong motivations to me.
10897,1,\n\nThe authors conclude that in this experimental setting:\n- AT seems to defend models against shared dx's.
10898,1, as well as cleaning process provided\n\t\u2022\tEvery figure and plot is well explained and interpreted\n\t\u2022\t
10899,1," On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation."
10900,1," The border pixels are probably sufficient to learn the\nprogram perfectly, and in fact this may be exactly what the neural net is\nlearning."
10901,1,"\n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance."
10902,1," except for the WSJ speech task, which is almost meaningless."
10903,1," The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960."
10904,1, This has been used in multi-class classification before.
10905,1," The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying."
10906,1,"\n\nIn the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set?"
10907,1," Examples include \""QUASAR: Datasets for question answering by search and reading\"", \""SearchQA: A new q&a dataset augmented with context from a search engine\"", and \""Reinforced Ranker-Reader for Open-Domain Question Answering\""."
10908,1, but I think the presentation could be clarified.
10909,1,\n- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood.
10910,1," \n- For the objective function defined in the paper, it may be hard to balance the \""structure loss\"" and \""content loss\"" in different problems, and moreover, the loss function may not be even useful in real tasks (e.g, in MT), which often have their own objectives (as discussed in this paper)."
10911,1," Publishing the code, as the authors mentioned, would certainly help with that."
10912,1,"""Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation."
10913,1," All the statements are very clear, the structure is transparent and easy to follow."
10914,1," However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are."
10915,1,This was very unclear to me.
10916,1,\n- The work is less about benefiting from demonstration data and more about using off-policy data.
10917,1," Without additional experiments on other datasets, it is hard for the reader to draw any meaningful conclusion about the proposed method in general."
10918,1," However, it's still more convinced if the paper method is demonstrated in more domains."
10919,1,\n\n- The baselines are just minor variants of the proposed method.
10920,1," Right now, there is no theoretical justification for the approach, nor even a (in my opinion) convincing movitation/Intuition behind the approach."
10921,1, \n\nA better model would be the addition of a distractor sentence as this preserves the information in the original passage.
10922,1,"\n\nOverall:\n- As is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of AT."
10923,1," What is the effect, in terms of convergence, in modifying the gradient in this way?"
10924,1, except for TCML which has been exluded using a not so convincing argument.
10925,1, These two methods seem to be closely related and should be thoroughly compared.
10926,1,  \n\nThe authors perform numerous empirical experiments on several types of problems.
10927,1," Does the \""knowledge of the current user utterance\"" include the word itself?"
10928,1,\n\n- Technical point: it's not clear to me that the training procedure as described is consistent\nwith the desired objective in sec 3.3.
10929,1,"\n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \""reading\"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%)."
10930,1," For example, Fig 4 shows -except for CIFAR10- not a clear relation between class index and proposed relative improvement, it is also unclear if there is just a difficult class (eg at index 150), or that the experiment has been repeated several times."
10931,1," \n\nIn the words of one of the paper's own examples: \""It has a great atmosphere, with wonderful service.\"" :)\nStill, I wouldn't mind knowing a little more about what happened in the kitchen...\n\n"""
10932,1,\n\nCons:\n\n- How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ?
10933,1," If the decision boundary are curved, then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curved."
10934,1,"""The paper adopts the concept of saliency to explain how the deep model makes decisions with adversarial perturbations. "
10935,1, What are these mixing conditions?
10936,1,"\n\n6. Although the results on the WSJ set are interesting,"
10937,1,"""The paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm, which is mixed with maximum likelihood objective."
10938,1,\n\nWhy is hard attention (sec 3.3) necessary?
10939,1," This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. "
10940,1," In their experimental results, the phase transition is not observed anymore with their protocol."
10941,1,"\n\nOn reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case. "
10942,1," \n\nOverall, I feel that while this line of research is worthwhile, at this stage the work is not yet ready for publication."""
10943,1," To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack."
10944,1," First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance."
10945,1,"""An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning."
10946,1, Why is this necessary?
10947,1,"""The authors disclosed their identity and violated the terms of double blind reviews.\nPage 2 \""In our previous work (Aly & Dugan, 2017)\n\nAlso the page 1 is full of typos and hard to read."""
10948,1,"""This paper proposed an end-to-end network that generates computer tokens from a single GUI screenshot as input. "
10949,1, \n\nAll the conclusions are based on one or two datasets.
10950,1, It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3).
10951,1,"   They show that while recent approaches (e.g. relational networks) can generalize reasonably well on some tasks, these results do not generalize as well to held-out-object scenarios as might have been assumed."
10952,1,\n- we propose ... as propose -> unclear: what do you propose?
10953,1,"More to a point. I think the search tree perspective is interesting,"
10954,1,"This could, for example, be done by optimizing a joint objective which modifies the input to both maximize error in the classifier and in the detector simultaneously, while remaining close to the original input. "
10955,1,"\n\n------------(Original review below) -----------------------\n\nThe authors present an enhancement to the attention mechanism called \""multi-level fusion\"" that they then incorporate into a reading comprehension system."
10956,1,\n-\tWhat are the hyperparameters values use for the LSTM\
10957,1," \n\npage 16:\n- proof of Proposition 2 : key idea here is using the positive and negative part of (f-g). This could simplify the proof."""
10958,1,"\n\n\n- Minor errors:\n\nEq. (4), for consistency, should use the identity matrix for the covariance matrix definition."
10959,1,"""The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment."
10960,1, Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label).
10961,1,Hence I strongly recommend accepting this paper.
10962,1,  Are there any\n  surprising long-range dependencies? 
10963,1,"The idea is straightforward,"
10964,1," I would imagine the benefit of using ImageNet is just to bring a random, high-dimensional embedding."
10965,1,\nA Learned Representation For Artistic Style
10966,1,"\nWe see that the authors build models that become more and more complex, but their motivation in combining attention and IT/CI is not clear: they learn the relation between context and target twice without any factorization."
10967,1,\nThe training procedure follows an alternative minimization in EM style.
10968,1,Operator learning has been already studied in FDA. See for e.g. the problem of functional regression with functional responses.
10969,1,"  Also, you claim that s_0 is the empty string, but isn't it more correct to model this symbol as the beginning of string symbol?"
10970,1," The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation."
10971,1, More baseline besides DC-SBM could better illustrate the power of GAN in learning longer random walk trajectories.
10972,1, \n\nThe idea of the proposed method is related to the classic Dyna methods from Sutton.
10973,1," It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function)."
10974,1, The paper provides several advances:\n- \\epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training.
10975,1, But I feel this sounds like a new angle and not the one that is adopted by the authors in this paper.
10976,1,"""The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter."
10977,1," Although by looking some features maps, this rule might be hold as shown in Fig.5."
10978,1," The authors should consider a test where the robot remains stationary with a fixed goal, but obstacles are move around it to  see how it affects the selected action distributions."
10979,1," If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations."
10980,1, \n\nMinor Commments:\n\n- Tables 1 and 2 would be easier to unpack if the authors were to list the names of the variables (i.e. azimuth instead of z_0) or at least list what each variable is in the caption.
10981,1, The authors are well aware of this. They still manage to provide added value.
10982,1,"To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters)."
10983,1,  The proposed method is theoretically analyzed and experimentally tested.
10984,1," Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \""eps\"": this is not guaranteed to give the right result."
10985,1, there is a very saturated market of papers proposing various architectures for CIFAR-10 and related datasets.
10986,1,   The blue points seem to consistently become more dense from (a) to (c).
10987,1, The experimental results are somewhat limited but the overall framework looks appealing.
10988,1,I do not see the utility of being able to generate a diversity of adversarial images.
10989,1," They mention some subtle details that must be taken into account, such as scaling the plot axes by the filter magnitudes, in order to obtain correctly scaled plots."
10990,1, In a number of places there are clear signs of sloppiness (e.g. undefined citations). 
10991,1," While the idea would be interesting in general,"
10992,1,"""This is an emergency review, as the replacement of an overdue review.[[CNT], [CNT], [CNT], [GEN]] \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit."
10993,1,"""The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks."
10994,1,"\n- In the section \""Open Problems in RAML\"", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q."
10995,1," If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior."""
10996,1, This is especially critical if a deep neural network is used since overfitting is a real issue.
10997,1,"\n \nMinor \n1. In Table 1, where is sigma defined?"
10998,1," At some point I thought I did understand it, but then the next equation didnt make sense anymore..."
10999,1,\n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains.
11000,1, The other methods use only the first epochs of considered deep neural network to guess about its curve shape for epoch T.
11001,1," For completeness, and to come to a stronger conclusion on how much uncertainty really affects the proposed method, this experiment needs a case in which 50% of the pixels are given. "
11002,1," In particular, the authors tend to take shortcuts for some of their statements."
11003,1,"\n\n  - First of all, I do not see why using small random subsets of the original tensor would give a desirable factorization."
11004,1," Further, the speech data sets, Fisher and Wall St. Journal, have what would seem to be very high word error rates versus what should be possible with standard open-source speech recognizers such as Kaldi. "
11005,1,\n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.
11006,1,"""The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs."
11007,1, \n\nEquation (4) shows that the proposed approach would weight Q different GC\nlayers.
11008,1, It seems the right resolution will be to show that after the overlap is set to a certain small value
11009,1," Naturally, the literature review deals with data augmentation technique, which supports my point of view."
11010,1, Does AESMC give a better generative model?).
11011,1, I would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plots.
11012,1,"\nIn Sec. 5.2, only two learning rate scales are tested (0.1 and 0.5)."
11013,1," The motivation for this control is to evaluate the impact of the adversarial loss, which is presented as the key conceptual contribution of the paper."
11014,1, The concrete mappings used to create the NE keys and attention keys are missing.
11015,1,  Label leaking is not a mysterious phenomenon.
11016,1,"  If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task?"
11017,1," Eg a global model is to be trained by local updates that occur on mobile phones, and communication cost is high due to slow up-link."
11018,1,  It is not clear how the observations made from the experiments generalize beyond these specific learning tasks or how one may take advantage of them in practice.
11019,1,"""After reading the rebuttal:\n\nThe authors addressed some of my theoretical questions."
11020,1,"""This work proposes an approach to meta-learning in which temporal convolutions and attention are used to synthesize labeled examples (for few-shot classification) or action-reward pairs (for reinforcement learning) in order to take the appropriate action."
11021,1," In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels)."
11022,1," The Seq2seq architecture incorporates both intra-temporal and intra-decoder attention, and a pointer copying mechanism."
11023,1,\n\nCons:\n\n-]The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein\u2019s identity etc.
11024,1," But here, essentially no kernel learning is happening."
11025,1,\n\n- I don't understand how speedup is being computed in Figure 4.
11026,1," Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN."""
11027,1,"\n\nYou use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs?"
11028,1,\n\n\nThe work of representing emotions had been an field in psychology for over a hundred years and it is still continuing.
11029,1," However, the validation procedure is not clear in the article."
11030,1," \n\nIn related work, I would cite co-training approaches."
11031,1,"\n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \""in which there is further sharing of masks between gates\"" and the one with \""independent noise for the gates,\"" as described in the footnote."
11032,1,"\n- I think this paper might miss the point of the \""bigger\"" problem of efficient exploration in RL... or even how to get \""deep\"" exploration with deep RL."
11033,1,"\n\nCaption of Fig 1: \""subject/object\"" are syntactic functions, not semantic roles."
11034,1, The task is performed on tabletop videos of a robotic arm manipulator interacting with various small objects.
11035,1,\n\nQuality:\n\nI found the quality to be low in some aspects.
11036,1,\n\n3. Clarification on the assumption (3).\nWhere is this assumption coming from? I can see that this makes the analysis go through but is this a reasonable assumption?  Does most of system satisfy this constraint? Is there any?
11037,1,"""The paper studies methods for verifying neural nets through their piecewise\nlinear structure."
11038,1," The main idea is to combine the approaches in (Han et al., 2015) and (Ullrich et al., 2017) to get a loss value constrained k-means encoding method for network compression."
11039,1,"n\n9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)\n\n10."
11040,1," For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight."
11041,1,\n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances.
11042,1,It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting. 
11043,1, Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation.
11044,1,"\n\n- A very small dataset (3,300 subjects and 3,353 images) is used in the whole investigation."
11045,1," \n\nI find the topic of universal perturbations interesting, because it potentially tells us something structural (class-independent) about the decision boundaries constructed by artificial neural networks. "
11046,1,"\n\nSecond, as the author mentioned, it\u2019s hard to compare the experimental results in this paper wit those in [1] because different hardware devices and software versions were used."
11047,1," However, the novelty is hurt by the lack of clarity with respect to the model design."
11048,1," To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes."
11049,1, The fact that the language generated is non-trivial (Java-like) is a substantial plus\n5) Good discussion of limitations\n\n
11050,1,\n\nI have two specific concerns:\n* Did you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)?
11051,1,Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time.
11052,1,\n\nWAE is a bit oversold.
11053,1,\n- It's not clear to my why PSNR is a useful way to measure privacy loss.
11054,1, Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive.
11055,1, The authors should emphasize the first half of the method section are from existing works and should go into a separate background section.
11056,1, In fact this proof shows the situation is much better than that.
11057,1," In addition (and more importantly), the margins appear to have been reduced relative to the standard latex template."
11058,1, \n\n3. Adding n^2 linear layers for image classification essentially makes the model much larger.
11059,1,"In particular,  the P@5 and P@20 numbers are only slightly better"
11060,1, \n\nThe paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model.
11061,1,  It is more convincing to show the superiority of the proposed method than existing ones on the same ground.
11062,1,\n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.
11063,1, The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration.
11064,1,  Experiments with multiple checkpoints are clear required to complete the picture regarding the empirical performance of this method
11065,1,"\n- In experiments, although the authors say \""lots of datasets are used\"", only two datasets are used, which is not enough to examine the performance of outlier detection methods."
11066,1,"\n\n- It is unclear what does the following sentence means: \""ConvNets are more pruned to deep networks than RNNs\""?"
11067,1, I would suggest keeping a  consistent color scheme
11068,1," However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)"
11069,1, They substitute the simple add operation with a selection operation for each input in the residual module.
11070,1,\nThe proposed approach does as well or better than competing approaches.
11071,1," A rich model is required in order to minimize the discrepancy between the distributions of the domains, while a  not too complex model is necessary to avoid mappings that are not \u00ab\u00a0meaningful\u00a0\u00bb.\n To this aim, the authors  introduce a new notion of function complexity which can be seen as a proxy of Kolmogorov complexity."
11072,1,"  Experiments with simpler energy functions seem to be absent, though the experiments are unclear (see below)."
11073,1,"\n\nThe results obtained from the analytical forms of the critical points are interesting,"
11074,1,"  From my own experience, even if I spend several hours copy-pasting from project gutenberg, it is not enough for even good matrix factorization embeddings, much less tensor embeddings."
11075,1," Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i"
11076,1,"\""\nWhat does this mean?"
11077,1,\n\nThe way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2.
11078,1," \n\nGenerally, a clear introduction of the problem is also missing."
11079,1," Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer."
11080,1, \n\nThe notion of local openness seems very well adapted to deriving these type of results in a clean manner.
11081,1," The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability. "
11082,1, Why is Theorem 1 not a function of L?
11083,1, Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks.
11084,1,\n - an experimental evaluation of the methods on the cifar 10 dataset
11085,1," \n\nSecondly, many existing works on multiple passage reading comprehension (or open-domain QA as often named in the papers) found that dealing with sentence-level passages could result in better (or on par) results compared with working on the whole documents."
11086,1,"""Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy."
11087,1," The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels."
11088,1,"""Summary:\nThis paper investigated the problem of attribute-conditioned image generation using generative adversarial networks."
11089,1, The authors point to recent GAN literature that provides some first results with high resolution GANs but I do not see quantitative evidence in the high resolution setting for this paper.
11090,1,"\n\nCons:\n- The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward,;"
11091,1," \nFor one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability."
11092,1,"  The cavalier presentation of specific details regarding your baseline systems (which is critical for any sort of replicability) and the uniformly weak performance of these systems relative to widely reported results, leads me to discount the probability that your methods would actually result in improvements on truly solid baselines."
11093,1,"""Pros:\n1. A new DNA structure GAN is utilized to manipulate/disentangle attributes."
11094,1," Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it."
11095,1, Is it just choosing the most common class?
11096,1,"\"" \n\nNot true. A main problem with the 1985 paper is that it does not cite the inventors of backpropagation."
11097,1," First, the paper could do a better job at motivating the problem being addressed"
11098,1, Do all objectives happen to yield their best performance under the same LR?
11099,1,". While they do show good performance,"
11100,1,They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation.
11101,1,"""The paper presents a provable algorithm for controlling an unknown linear dynamical system (LDS)."
11102,1,"""[After author feedback]\nI think the approach is interesting and warrants publication."
11103,1,\n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community.
11104,1, The claim seems like a subjective understanding of conscious perception and unconscious perception of affective stimuli is totally disregarded.
11105,1," The paper assumes the tasks are \u201crelated\u201d or \u201csimilar\u201d and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution."
11106,1," For instance, there are many related papers on:\n\n-taxi fleet management (e.g., work by Pradeep Varakantham)\n \n-coordination in multi-robot systems for spatially distributed tasks (e.g., Gerkey and much work since)\n\n-scaling up multiagent reinforcement learning and multiagent MDPs (Guestrin et al 2002, Kok & Vlassis 2006, etc.)"
11107,1," The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion)."
11108,1,"\n\n- Experiments are only presented in one domain, and it has some peculiarities relative to \nmore standard program synthesis tasks (e.g., it's tractable to enumerate all possible inputs)."
11109,1, That method could also be considered as a possible approach to compare against here.
11110,1, Could we not also apply this to negative examples?
11111,1, and for that reason I don't think it has a place as a conference paper at ICLR.
11112,1,\n \n\u201cAnother one is credit assignment.
11113,1,\n\nThe matrix completion approach is based on robust PCA.
11114,1, This is a very active area of research and the paper needs to justify their approach.
11115,1, See Dynamic Programming and Optimal Control and references too it.
11116,1," \n\n3) In the current state, it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty."
11117,1," Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)?"
11118,1,"and I do not feel like this paper actually performs any \""test\"" (in a statistical sense)."
11119,1,"\n\nIt should also be noted that the PANAS (Positive and Negative Affect Scale) scale and the PANAS-X (the \u201cX\u201d is for eXtended) scale are questionnaires used to elicit from participants feelings of positive and negative affect, they are not collections of \""core\"" emotion words, but rather words that are colloquially attached to either positive or negative sentiment."
11120,1,"\n\nSignificance \n\nThis work demonstrates failures of relational networks on relational tasks, which is an important message."
11121,1," Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent."
11122,1,"\n- I miss a clearer results comparison with previous methods, like Vondrick et al. 2015."
11123,1,\n\nCons/Questions\n- They mention not quantizing the first and last layer of every network.
11124,1," The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation."
11125,1," The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \""expected policy gradient\"" by Ciosek & Whiteson."
11126,1,"  \n\nClarity: The paper makes many mistakes, and is difficult to read. [N] is elsewhere denoted as \\mathbb{N}."
11127,1," please discuss it.\n\n- The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup.\n"""
11128,1," So if this works, it could be an impactful achievement.."
11129,1,"  Two problems are considered:  \""any-time\"" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images."
11130,1,"\n+During CIFAR-10 experiments when r=1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels?"
11131,1,"\n\nFrom my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges."
11132,1," That matrix is then mapped back to system id (A,B,C,D)."
11133,1," \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc?"
11134,1,"Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it\u2019s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is."
11135,1,"\n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0."
11136,1, but the original contributions are adequately identified and they are interesting on their own.
11137,1, These issues make the paper hard to read.
11138,1," To this end, the authors combined a text reconstruction loss, an adversarial decoding loss, a cyclic consistency loss and a style discrepancy loss."
11139,1," The feature of providing auxiliary visual information (also conclusion) is much more convincing (but also a feature of Hartono et al, 2015)."
11140,1,"\n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum."
11141,1, It will be fair/interesting to see the result for CPU time where small batch maybe favored more.
11142,1,  The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.
11143,1," \n\nFor Charades, all reported results appear significantly lower than the baseline methods reported in [5] and [6] with no explanation."
11144,1, An experimental comparison is needed.
11145,1,   It seems that baseline is additionally constrained vs the ego car due to the speed limit and the collision avoidance criteria and is not a fair comparison.
11146,1, I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly.
11147,1," but instead uses a single synthetic data point in dimension 5, and k=1."
11148,1,"\n\nOverall, the paper seems to have both a novel contribution and strong technical merit."
11149,1," In general, this manuscript is well written."
11150,1,\n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice
11151,1," \n- The manuscript is full of grammatical errors, and the following are some of them:\n\""encoder only only need to\""\n\""For for tree reconstruction task\"""
11152,1, \n\n1) The total budget / number of training samples is fixed.
11153,1,"\n\nIt combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L."
11154,1," On the other hand, since many of the edges are shared the \""siblings\"" may be nearly isomorphic to the input graph, which is not useful from a graph modeling perspective."
11155,1,Apologies if I am missing something obvious.
11156,1," Subgoals are then used to learn different Q-value functions, one per subgoal."
11157,1, Does this theorem imply that the global optima can be reached from a random initialization?
11158,1,   \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.
11159,1, It is bit confusing because the job of that network is to obfuscate the passage.
11160,1,    The computation of f-y_r\\phi(s_r) makes it hard to understand.
11161,1,Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems.
11162,1, \nCons:\ni) The architecture of the 2 layer MLP used to learn weights for a particular depth channel is not provided.
11163,1,"\n\n(4) Moreover, neural networks are mostly interesting because they learn the representation."
11164,1,"  On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited."
11165,1," \n\n2. The approach:\n(1) Based on the discussion in the related work section and the approach section, it seems the proposed approach proposes to augment each instance in the visual feature space by adding more features, as shown by [x_i; x_i^A] in 2.3."
11166,1," Thus, the authors need to refine BM's result for comparison."""
11167,1,\n\nMinor comments\n=============\n1. The authors should use \u2018significantly\u2019 only if a statistical hypothesis was performed.
11168,1," However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions."
11169,1," There are many such methods, see for example \n\nJiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?"
11170,1, The results seem like they may be very important.
11171,1,"\n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment."
11172,1," Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand."
11173,1," More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label."
11174,1,"\n\nIndeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network."
11175,1," It is trained with supervised learning, by minimising the cross-entropy error between labels and the softmax output.\n\nThe paper's claim of combining unsupervised (self-organising) with supervised training is misleading and confusing."
11176,1,\n\n3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to.
11177,1,\n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks.
11178,1,Cons:\n\n1. Experimental results are neither enough nor convincing.
11179,1, Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse.
11180,1,. More evidence in the paper for why it would work on harder problems would be great.
11181,1,"""Summary: \nBased on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network."
11182,1, The paper is well written and provides excellent insights. 
11183,1,  Max pooling on graph is done by using Graclus multilevel clustering algorithm.
11184,1," For prediction purposes, this restriction might be OK, but for control purposes, many interesting plants does not satisfy this assumption, even simple RL circuit."
11185,1,"""The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which \nuses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions:\n- the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems"
11186,1,"""In this paper, the authors proposed a method to transfer the text style to a specific target style."
11187,1," Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3."
11188,1,"\n\nThis appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods)."
11189,1," So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills?"
11190,1,"  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value."
11191,1,"\n\n- The training procedure of the proposed method remains unclear in this paper. \n\n\n"""
11192,1,"\n\nSecond, it is not clear whether the label used as input in eq. (4) is a model choice or a model requirement."
11193,1,"""The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer."
11194,1, Showing YF to perform very well on several deep learning tasks without (or with very little) tuning.
11195,1,"\n\n- Similarly, for Algo 2, add references."
11196,1,\n\nResults on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match).
11197,1,\n1. Why use CNN for the style representation layer?
11198,1, The memory consists of a 2D array and includes trainable read/write mechanisms.
11199,1,"    On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems."
11200,1," Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have."
11201,1,"\n\n* The section 3.2 explains standard things (policy gradient), but the details are a bit unclear.[[CNT], [SUB-NEG], [CRT], [MIN]] In particular, I do not see how the Gaussian/softmax layers are integrated; they do not seem to appear in figure 4?"
11202,1, I think this would also shed some light as to why this approach alleviates the problem of mode collapse.
11203,1,"""This paper revisits the idea of exponentially weighted lambda-returns at the heart of TD algorithms."
11204,1,. Examples are given where appropriate in a clear and coherent manner\n\t\u2022\t
11205,1," In addition the formulation seems to contrast ML with hierarchical Bayesian modeling, which does not make sense/ is wrong and confusing."
11206,1,\n- Are the image features fixed or learned?
11207,1,"""This paper proposes an interesting variational posterior approximation for the weights of an RNN."
11208,1,"  Also, a lot of types make the descriptions more difficult to follow, e.g., \""may not helpful or even harmful\"", '\""Figure 3\"", \""we show this Section 6\"", \""large size a vocabulary\"", etc."
11209,1, No training details are provided.
11210,1," In particular, in section 3, several sentences are taken as they are from the Downey et al.\u2019s paper."
11211,1, but are minor contributions.
11212,1," This problem is basically active learning for\nprogramming by example, but the considerations are slightly different than in standard active\nlearning."
11213,1, \n\nHave you considered the widely-used NCE method to handle the large vocabulary?
11214,1," In summary, I think the paper needs very careful editing for grammar and language and, more importantly, it needs solid experiments before it\u2019s ready for publication."
11215,1, Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map.
11216,1, Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results.
11217,1,"The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that)."
11218,1," Further, L_TD is strangely defined: why a squared norm, for a scalar value? "
11219,1, \n\nIt is not easy to follow the main idea of the paper.
11220,1,"n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.\n"""
11221,1," I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper."
11222,1,"   \n\n    4. In Equation 6, on the RHS in the expansion, the \\alpha_1 W c_i should be \\alpha_1 W c_1."
11223,1,"\n\n[3] Huang, Gao, et al. \""Deep networks with stochastic depth.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n"""
11224,1,  There are a few reasons for this. 1) I was not convinced that deep Q-learning was necessary to solve this problem.
11225,1," I think which algorithm is faster depends on values of L, T and M."
11226,1,"  Without this, it is actually really hard to talk about general mechanism of learning adaptive Fourier features for kernel algorithms (which is how the authors present their contribution); instead we have a method heavily customized and well-tailored to the (not particularly exciting) SVM scenario (with optimization performed by the standard annealing method;"
11227,1," Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed."
11228,1,"\n\nThose claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful."
11229,1,\n\nSo overall given limited novelty 
11230,1,\n\n(9) Lemma 3.2: Is \\hat{D} defined in the paper?
11231,1," First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD."
11232,1, What is the big picture behind this claim?
11233,1," (In fact, because of the presentation of the paper, I was hesitating whether I should suggest acceptance.)"
11234,1,"\n\n  - Minor comment. In the shifted PMI section, the authors mention the parameter alpha and set specific values of this parameter based on experiments."
11235,1,"""The paper proposes training ``inference networks,'' which are neural network structured predictors. "
11236,1," Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters."
11237,1," The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017."
11238,1, Numerical experiments comparing the two implementation or at least a discussion is necessary.
11239,1,"\nFurther, I don't think that clarifying that this method is *not* universal/general really hurts the paper... you could just add a section in 4.2 pointing out that the \""chain\"" example wouldn't work if you needed to do different actions at each timestep (this algorithm does *not* perform \""deep exploration\"")."
11240,1," \n\nFor the theory, there are a few steps that need clarification and further clarification on novelty."
11241,1,  \n\nSome more minor comments: \n\n-\tPlease enlarge Fig. 4.
11242,1, A Monte Carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead prediction.
11243,1," Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model."
11244,1, Do we need to give the name of the column the Attention-Column-Query attention should focus on?
11245,1,\n\nMy rating of this paper would remain the same.
11246,1,\n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images.
11247,1,. Can the authors provide better examples here?
11248,1," The paper provides theory to show that their proximal variant (even with the CG approximations) can lead to convergent algorithms (and since practical algorithms are not necessarily globally convergent, most of the theory shows that the proximal variant has similar guarantees to a standard gradient step)."
11249,1,"Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization)."
11250,1,"\n6. I do understand the notation used in equation (8) on page 4. Are <, > meant to denote less than/greater than or something else?"
11251,1, This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017).
11252,1," Also, it was interesting how that was found to be high on AAE due to mode-collapse."
11253,1,"""This paper discusses the application of word prediction for software keyboards."
11254,1," In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices;"
11255,1,"""This paper presents an experimental study on the behavior of the units of neural networks."
11256,1,"\n\n- How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions."
11257,1,\n\nWhy is the paper focused on these specific contributions? 
11258,1,\n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.
11259,1,\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments.
11260,1," To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks."
11261,1, Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014).
11262,1,?\n- Do you think your approach would benefit from having a few parallel training points?
11263,1, There is a gap that how this corollary implies generalization.
11264,1," For example, while the configuration the paper's baseline models are not given, the baseline accuracy of MNIST classification using MLP is 16.2%."
11265,1," 2015.\n\n[2] Lee, Chen-Yu, et al."
11266,1," Again, since the main contribution is to solve a specific problem, it would be worthy to compare with a more extensive benchmark, including state of the art algorithms used for this problem (e.g., heuristics and metaheuristics)."
11267,1," While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied."
11268,1,\n\nOn the experiments side the authors could have also experimented with miniImageNet and not only omniglot as is the standard practice in one shot learning papers.
11269,1,"\nHowever, there is no reason to automatically expect that this will decrease the importance of shape."
11270,1,"\n\nQuestions:\n- \""Inventing plausible fine details while preserving identity\"" -- since identity is created and there is no ground truth, where does the line between \""fine detail\"" and \""new identity\"" lie?"
11271,1," Regardless, the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but I am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method."""
11272,1, The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter.
11273,1,\n\nThe Prototypical Networks results in Tables 1 and 2 do not appear to match the performance reported in Snell et al. (2017).
11274,1," Why is the proposed approach better than the\npreviously published ones, and when is that there is an advantage in using it?"
11275,1,"   A trained \""narrator\"" could learn to actually change the correct answer."""
11276,1," \n\nThe authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem."
11277,1,"\n\nThis is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above)."
11278,1," However, from the\nequations in the paper it seems that the logit is set to zero."
11279,1, but has a couple shortcomings and some fatal (but reparable) flaws:.
11280,1,"\n\nThe heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets."
11281,1, What is the coordinate system used\nby the authors in this case?
11282,1,"\n\nIn my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision."""
11283,1,"\nThis additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments)."
11284,1,  \n\n\nThe paper presents an interesting idea and is fairly well written.
11285,1, It is very hard not to see the content of the introduction as addressing a linguistics/computational linguistics audience rather than the mainstream of the ICLR audience (you get this impression rather strongly from the start of each of the first 3 paragraphs of the introduction...).
11286,1," In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals."
11287,1," They conduct a complete experimentation, testing different popular RNN architectures, as well as parameter and hyperparameters values."
11288,1, you need to explain QA data sets and how the questions are solved.
11289,1,   The paper also doesn't illustrate any novel results on tasks for which it would be reasonable to assume that complex-valued inputs would be particularly important.  
11290,1," Also, it is not clear whether models for large graphs can be learned."
11291,1,. Is there any limit on how many examples each worker has to label?
11292,1,"\n\n6) Top of p.5: the sentence \""Since we need tilde{beta} to satisfy (...)\"" is currently awkwardly stated."
11293,1," Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections."
11294,1," Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement."
11295,1,"\nGiven this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix."
11296,1," The algorithm is closely related to boosting and MKL, while there is no such comparison."
11297,1,\n- The learning of such soft combination is done jointly while learning the tasks and is not set\n  manually cf. setting permutations of a fixed number of layer per task.
11298,1,"\"".\n\n\"", disagreement emerge\"" -> \"", disagreements emerge\""?[[CNT], [CLA-NEU], [QSN], [MIN]]\n\nThe paper needs to include SOME definition of robustness, even if it just informal."
11299,1,"\n\nOverall, there are a few problems with the paper."
11300,1,"As a result, I am wondering whether this separated question embedding really brings new information, or the similar improvement can be achieved by increasing the size of LSTM layers."
11301,1,"  For the second dataset, some points are not clear too: why the labels and the pictures seem not to match (in appendix E); why there are more training iterations with spikes w.r.t. the not-spiking case."
11302,1," By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements."
11303,1,"\nNevertheless, the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN.\n\n"""
11304,1,"  Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity)."
11305,1,". However, I also believe the authors managed to clear essentially of the criticism in they reply."
11306,1,\nThe idea of re-using a pretrained agent has both pros and cons.
11307,1,\n\n4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc).
11308,1," For one thing, there is the\nirksome and repeated use of \""discrete structure\"" when discrete *sequences* are\nconsidered almost exclusively (with the exception of discretized MNIST digits)."
11309,1,"""The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers."
11310,1,"\n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way"
11311,1," but in practice, this seems to be more of the case than say large deviation/adversary exists."
11312,1,"  For example PANAS-X includes words like:\u201cstrong\u201d ,\u201cactive\u201d, \u201chealthy\u201d, \u201csleepy\u201d which are not considered emotion words by psychology."
11313,1,"  This observation is neat in my opinion, and does suggest a different use of the Jacobian in deep learning."
11314,1, \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score.
11315,1," I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior?"
11316,1," The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution."
11317,1," I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated?"
11318,1,. \n\n5) Some of the choices made in the Experimental setup seem questionable to me:
11319,1,"   This seems like a missed opportunity to capitalize on structure to bias predictions (neutral sentiment is closer to positive than is negative, after all, but the model does not know this as currently specified)."
11320,1, \n\nComments:\n\n1. Why force the Lag. multipliers to 1 at the end of training? 
11321,1," It trained 20 epochs for the studied case of CIFAR, so 1-3 minutes per epoch on the CPU can be implemented with zero overhead while the network is training on the GPU."
11322,1,\n\n************************\nThe authors provide an algorithm-agnostic active learning algorithm for multi-class classification.
11323,1,"\n\nSome other comments:\n- In the last paragraph of section 3 and first paragraph section 3.1, the authors mentioned that \""...CNN to perform unsupervised feature learning...\"""
11324,1,"   \n\nTo estimate the MI between a hidden layer and the relevance variable, a multilayer generalisation of the variational bound from Alemi et al. 2016."
11325,1,  Many are/were Gaussian.\n*
11326,1," It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today."
11327,1,"\n\nIf MI is invariant to monotone transformations and information curves are determined by MIs, why \u201ctransformations basically makes information curve arbitrary\u201d?"
11328,1," I suggest instead to say that \""|tilde{beta}- beta| <= 1/d (gamma/2B)^(1/d) is a sufficient condition to have the needed condition |tilde{beta}-beta| <= 1/d beta over this range, thus we can use a cover of size dm^(1/2d)."
11329,1,\n\nSo the learning dynamics for VAN and ResNet are very different because of this scaling. 
11330,1, though I didn't find the related experiments particularly convincing.
11331,1,"""\nThis paper revisits an interesting and important trick to automatically adapt the stepsize."
11332,1,\n7. Why is average accuracy the right thing?
11333,1," \n\nOnce we learn the similarity function, the rest of the approach is straightforward, without any particular technical ingenuity."
11334,1,"\n\n[1] Laine S, Aila T. Temporal Ensembling for Semi-Supervised Learning[J]. arXiv preprint arXiv:1610.02242, ICLR 2016."
11335,1, As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss.
11336,1,\n\nPros\n- Greatly improves the data efficiency of recursive NPI.
11337,1, \n\nThe paper is overall quite interesting and the study is pretty thorough: no major cons come to mind.
11338,1,"Therefore, it should not be accepted in its current form."""
11339,1,"\n\nHowever, the experimental part is less satisfying."
11340,1," Appendix A seems obvious but it cannot prove the validity of the assumption made in problem (2).[[CNT], [null], [CRT], [MAJ]] Based on previous works such as \u201cMulti-task Sparse Structure Learning with Gaussian Copula Models\u201d and \u201cLearning Sparse Task Relations in Multi-Task Learning\u201d, when the number of tasks is large, the task relation exhibits the sparse structure."
11341,1, The authors suggest that training a GAN provides a speed benefit with respect to other attack techniques.
11342,1," \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting."
11343,1," By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers."
11344,1, If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary.
11345,1,\n\n(3) The main results of this paper seem technical sound.
11346,1,"\n\nClarity:\n* Eqn. 5, LHS can be written more clearly as \\hat{a}_k."
11347,1,"\n\nSecond, all experiments were conducted on the MNIST dataset."
11348,1," This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time."
11349,1," For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below)."
11350,1,\n\nThe SMC algorithm used is a sequential-importance-sampling (SIS) method.
11351,1,\n\nMy main main concern is extrapolation out of the training set which is particularly important here. I don't find enough evidence in 4.2 for that point.
11352,1," The motivation, notation, and method are clear."
11353,1," So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem."
11354,1," In particular, tie in the stepsizes tau and tau_theta here."
11355,1,"\n[ ============================== END OF REVISION =====================================================]\n\nThis paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). "
11356,1,": Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references)"
11357,1, What it actually investigates is the performance of existing image classifiers and object detectors.
11358,1, \n\nOverall:\nRejection.
11359,1," After generating both parts in parallel, they are combined using alpha blending to compose the final image."
11360,1,\n3.\tThe experiments and analysis presented in the paper are insightful.
11361,1," \nHow is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model?"
11362,1," One issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks, which is useful for learning from multiple tasks."
11363,1,\n* Distance preservation appears more and more like a dated DR paradigm.
11364,1," At each iteration, SGD samples a subset of training samples and labels."
11365,1," which could be fairly large, thus the method is computationally much more expensive than random sampling."
11366,1,"\n\nFrom a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small \\lambda] -- what is the rationale of this? "
11367,1," The classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode. "
11368,1, \n\t\nStrengths:\n\n1.\tThe data collection process is interesting.
11369,1," Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows\"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product)."
11370,1,"  I think that this work could be quite useful to the field, but that a number of frustrating weaknesses prevent me from recommending it without qualifications."
11371,1," However, I am not sure if the confusion is solely mine, or shared with other readers."
11372,1, The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights. 
11373,1, \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline?
11374,1," However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. "
11375,1,"   Then the input image could be rectified to a canonical orientation and scale, without needing equivariance."
11376,1,"\n\nThe paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading."
11377,1,\n\n(2) I feel that the model design is the main reason for the good overall RC performance.
11378,1,"""The paper \u2018Deep learning for Physical Process: incorporating prior physical knowledge\u2019 proposes\nto question the use of data-intensive strategies such as deep learning in solving physical \ninverse problems that are traditionally solved through assimilation strategies."
11379,1,\nA few comments:\n1. How do the authors propose to deal with multimodal true latent factors?
11380,1,\n\nThere is a more fundamental question for which I was not able to find an explicit answer in the manuscript.
11381,1, \n2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.).
11382,1," \n\nThe experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017)."
11383,1, \n\nMy impression is that the present line of work will not be relevant for deep learning and will not beat state-of-the-art results because of the lack of a structured prior.
11384,1,"Further, how many tau\u2019s one should evaluate?"
11385,1,"\n- 8.1. contains copy-paste from the main text.[[CNT], [null], [DIS], [MIN]]\n- \""proposition from Goodfellow\"" -> please be more precise\n- What is Fig 8 used for?"
11386,1,"However, this reduction grows to infinity the larger the update is."
11387,1," The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments."
11388,1,"\n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context."
11389,1,"""Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS)."
11390,1,". For instance, averaging the node embeddings is something that has shown promising results in previous work"
11391,1, I would suggest just including the visualizations of the optimization trajectories which are done correctly and focus on deriving interesting/useful conclusions from them.
11392,1,"\n\n(2) Ablations. The proposed method has multiple ingredients, and some of these could be beneficial in isolation: for example a population of size 1 with an interleaved distillation phase where only the high-reward trajectories are preserved could be a good algorithm on its own."
11393,1," For example, it is claimed that the proposed decomposition provides \""enhanced representation ability\"", but this is not justified rigorously either via more comprehensive experimentation or via a theoretical justification."
11394,1," Right now, it is quite confusing to follow, and I fail to see the motivation for some of the analysis."
11395,1,"... I also didn't feel the experimental evaluations drove a clear message except \""ARM did better than all other methods on these experiments\"""
11396,1,"""This paper proposes to use RGANs and RCGANS to generate synthetic sequences of actual data."
11397,1,The possibility of computing a faster approximation of ISRLU is also mentioned
11398,1, Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data.
11399,1,"  To estimate the ID, the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network, in which the intrinsic dimension is determined (in a more or less standard way) by the rank at which two consecutive singular values has a ratio exceeding some threshold."
11400,1," \n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible."
11401,1," Page 5, the term $\\omega$ intervening in the definition of the policy $\\pi$ is not defined."
11402,1,\n\t2.1) The number of methods in comparison (InfoGAN and PCA) is limited.
11403,1," I suggest shortening Section 2, and it should be possible to combine Sections 3 and 4 into a page at most."
11404,1,\n \nThere are possible future directions to be developed.
11405,1, Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ? 
11406,1," Together with the summary of other methods and introduction, it composes the first 8 pages of the paper."
11407,1,"""The idea of using MCMCP with GANs is well-motivated and well-presented"
11408,1,"""The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN."
11409,1," Besides, the idea of using adaptive learning rates are not completely new, and somewhat closely related to second order optimization methods."
11410,1,\n\n- In the rope knot-tying task no slam-based or other classical baselines are mentioned.
11411,1,\n(2) how is the level of uncertainty related to performance?
11412,1,"""Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks."
11413,1," Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation."
11414,1, Experimental results show that the simplified versions work as well as the full LSTM.
11415,1,"  The first sentence on page 6 gives this intuition; this should come much earlier.[[CNT], [PNF-NEG], [CRT], [MIN]] \n\nPage 5: \u201ca feed-forward auto-encoder with N input neurons\u2026\u201d Previously, N was defined as the size of the input domain."
11416,1,"\n- there are multiple distinct metrics that could be used on the x-axis of plots, namely: wallclock time, sample complexity, number of updates."
11417,1," If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module."
11418,1, although I find the contributed results slightly overstated.
11419,1,This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results.
11420,1,"""The authors tackle the problem of estimating risk in a survival analysis setting with competing risks."
11421,1,\n\nThe experimental results shown in this paper are clearly compelling in exposing the weaknesses of current seq2seq RNN models.
11422,1,"\n\n* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017."
11423,1," Examples include , \""Attention-Based Convolutional Neural Network for Machine Comprehension\"", \""A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data\"", and \""Coarse-to-fine question answering for long documents\"""
11424,1,\n\nPROS:\n+ nice tensor factorization model for learning word embeddings specific to discrete covariates.
11425,1," But I failed to see a clear, new contribution of using this causal regularization, compared to some of the previous methods to achieve invariance (e.g., relative to translation or rotation)."
11426,1," Basically, it means that B(P_X) should be a small constant."
11427,1,\n\nQuality\n\nThis paper does not set out to produce a novel network architecture.
11428,1," However, I find this result to be out of the context with the main theme of the paper."
11429,1, \n\nThe paper presents strong quantitative results and qualitative examples.
11430,1," \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest."
11431,1,\n\nIt would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict
11432,1,\n* One thing I\u2019m unclear on is how convergence was assessed\u2026 my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn\u2019t this also depend on the depth in some way?
11433,1,\n\nThe method is evaluated in one experiment with many different settings.
11434,1, The manuscript itself makes it difficult to assess (more on this later).
11435,1,"\n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing."
11436,1," \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?"
11437,1, While a result has been stated for single-hidden ReLU networks.
11438,1,"\n\nOn a personal note, I've difficulties with part of the writing. "
11439,1," Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them."""
11440,1," on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network."
11441,1,"\n\nIt is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features."
11442,1, This would cut down the text a bit to make space for more experiments.
11443,1, It would be clearer if they stated explicitly that the alignment is between covariate-specific embeddings.
11444,1, The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary. 
11445,1, The number of clusters found by the algorithm is not particularly accurate and the NMI values obtained by the proposed approach does not show any clear advantage over baseline methods that do not use PCN.
11446,1," To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments."
11447,1,\n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.
11448,1,\n\nOther remarks. \n\n- In section 2.2 and 4 there is some confusion between iteration indices and samples indices \u201ci\u201d.
11449,1, This fact weakens the scope of the online hyper-parameter optimization approach.
11450,1," I\u2019m especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance."
11451,1,"\nCurrently, it looks \n\nBased on that, I can\u2019t recommend acceptance of the paper."
11452,1," In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet)."
11453,1,? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily.
11454,1,"\n\nIn the same section, the expression for \\alpha_{ij} seems to assume that \\delta_{ijk} = \\dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions)."
11455,1,"\n\nCons:\n* it seems that there are quite some typos in the paper, for example:\n    1. Section 1, in the second contribution, there are two \""then\""s."
11456,1,  How does such model choice affect the final performance?
11457,1, Why is TreeQN's approach better than VPN's approach?
11458,1, \n\n3. The result of the optimal mini-batch size depends on the training data size.
11459,1, Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference.
11460,1,"""This paper tackles the object counting problem in visual question answering."
11461,1,\n-\tPag6 k_{beta} is not defined in the main text;
11462,1,"\n\nThe paper is written well, good to understand, and technically sound."
11463,1," \n* Section 3. A summary of all the hyperparameters should be given.[[CNT], [SUB-NEU], [DIS], [MIN]] \n* Section 4.1. The number of steps is not given.[[CNT], [SUB-NEU], [DIS], [MIN]] Do you present the same graph multiple times."
11464,1, There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable.
11465,1,"""The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network."
11466,1,"""This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models."
11467,1,\n6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs?
11468,1, The techniques introduced are a small permutation of previous results.
11469,1,\n\nThe idea is exciting LDS by wave filter inputs and record the output and directly estimate the operator that maps the input to the output instead of estimating the hidden states.
11470,1," The proposed approach is simple and has an appealing compositional feature,"
11471,1,"\u201d These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3."
11472,1, The idea is to jointly train
11473,1, There is reasonable novelty in the proposed method compared to the existing literature.
11474,1," I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet."
11475,1, It is very probably more costful.
11476,1,"\n\nThis model is a relatively simple extension of the Neural Statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation."
11477,1,"\n\n* Paper Presentation\nIn the reviewer\u2019s opinion, the primary limitation of the paper is how it is written and organized."
11478,1, It would be helpful to discuss the conditions where we can benefit from the proposed method.
11479,1," Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch."
11480,1, This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.
11481,1," It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked)."""
11482,1," \n\n4) Sec 3.5 Matching objects in successive frames using the Hungarian \nalgorithm is also well known, e.g. it is in the matlab function\nassignDetectionsToTracks ."
11483,1, The paper is interesting but lacks strong empirical results.
11484,1,  \n- Figure 3 is plotted for just one random starting state.
11485,1," However, the weak convergence results are good."
11486,1, This might also help the readers better understand where the polytopes in Figure 1 come from.
11487,1,"\n\nMy opinion on this paper remains, and I think the contribution of this paper to machine learning is not very clearer at the current stage."
11488,1," However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands."
11489,1,Is it possible to use all the binary attributes in the dataset.
11490,1, As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction.
11491,1,"nThe paper is well-written, relevant and interesting"
11492,1,\n\nYou claim that FractalNet shows no ensemble behavior.
11493,1,\n\n\n\n\nDetails:\n* references are wrongly formatted throughout.
11494,1,"\nd.\tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample?"
11495,1, \n\nThe authors propose a simple method of mixing the global model with user specific data.
11496,1,"\n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics."
11497,1," The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training."
11498,1, The constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next state\u2019s value.
11499,1," \n- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments"
11500,1," They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. "
11501,1," \n\nSo while the performance of the overall system is impressive,"
11502,1,  The performance of this approach is measured on several UCI datasets and compared with baselines.
11503,1," It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees."
11504,1, Now what do we do with this list?
11505,1," Those results show that as structural information is removed, the GGNN's performance diminishes, as expected."
11506,1," For example, how well do humans classify these sketches?"
11507,1,"  Also, it should be said whether the node embeddings and graph embeddings for the output graph can be useful."
11508,1, but the following section about momentum operator is hard to follow.
11509,1, \n\nThe authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments.
11510,1,"\n2) I believe the control experiments are encouraging,"
11511,1,\n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ())
11512,1, How about real online learning with streaming data where the total number of data points are unknown?.
11513,1,  This leads to the question: How much CCC perform compared to random policy selection?
11514,1," Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer?"
11515,1,\n[4] Convex multitask learning with flexible task clusters (ICML)
11516,1," \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC?"
11517,1, The paper is clearly motivated and authoritative in its conclusions
11518,1,"  Real vehicles also decouple wayfinding with local vehicle control, similar to the strategy employed here. "
11519,1,"  Random walks sometimes are overused in the graph literature, but they seem justified in this work."
11520,1, A mixture of experts layer further improves performance.
11521,1," Moreover, claim [d] about repeatability is also invalidated by the fact that\u00a0the experiments described in the paper use different observations (no velocity inputs), different action space, different reward structure, with no empirical evidence to support these changes."
11522,1," There are a few limitations, e.g. the difficulty of evaluating the rationales, and the fact that the resolution is fixed to 128x128 (which means discarding many pixels collected via ionizing radiation), but these are readily acknowledged by the authors in the conclusion."
11523,1,"  Assumption 1 alludes to this but doesn't specify what is \""small\""?"
11524,1," \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n"""
11525,1," While this is certainly believable/expected, such a result would depend on the external rewards R_{it}^{ex}, the observation costs \\sigma_{jit}, and the network topology."
11526,1,"\n\n5. For each weight w, we add K learning rates u_w^j."
11527,1,"\n- Although the method is atypical compared to standard HRL approaches, the same pitfalls may apply, especially that of \u2018option collapse\u2019: given a fixed Path function, the Goal function need only figure out which goal state outputs almost always lead to the same output action in the original action space, irrespective of the current state input phi(s), and hence bypass the Path function altogether; then, the role of phi(s) could be taken by tau(s), and we would end up with the original RL problem but in an arguably noisier (and continuous) action space. "
11528,1," \n\nAll in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset."""
11529,1," \n\n\n- I believe that I understand the authors' intention of the caption of Fig. 1, but \""samples outside the dataset\"" is a misleading formulation."
11530,1,\n(v) I am confused by the B_opt \\propto \\eps statement.
11531,1,". This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. """
11532,1," However, the work has some problems in rigor and justification and the conclusions are overstated in my view."
11533,1,"Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is"
11534,1," However, still we need to calculate F."
11535,1,"\n\n4) It would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance, besides the case study in Figure 3"
11536,1, An adversarially trained model learns on two different distributions.
11537,1," VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution."
11538,1,"\n\nOverall, I really enjoy reading this paper and recommend for acceptance!"
11539,1,\n+ better performance
11540,1,\n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models.
11541,1," However, the clarity of the paper could be improved."
11542,1," Therefore it is not very convincing whether the conclusion applies to other types of data, e.g. train/test on RGB&Gray image pairs, which are more commonly seen."
11543,1,"\n\nDetailed answers:\n\n1) Indeed, I was not aware that the paper only focuses on one dimensional functions."
11544,1,"\n \nAll that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable."
11545,1,"\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017)."
11546,1,"\n\nOriginality: The works seems to be relatively original combination of ideas from Bayesian evidence, to deep neural network research. "
11547,1,  The paper performs a modest evaluation of such\nsimple models and shows surprisingly good r-squared values.
11548,1,"""In this paper, a model is built for reading comprehension with multiple choices."
11549,1,\n- can you also compare the training times?
11550,1,"\n\nIn a summary, the main idea of adding rotation to LSTM cells is not properly justified in the paper, and the results presented are quite weak for publication in ICLR 2018."
11551,1," \n\n# Detailed comments\n* Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable."
11552,1, \n\nQuestions/Comments:\n\n- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method. 
11553,1,"  Doesn't the good\n  performance without TS on, e.g., ResNets in Table 2 imply that the\n  Deep ResNets subfigure in Figure 3 should start out at 80+?"
11554,1,"\n-            Sec 5.1: It took some effort to understand exactly what was going on in the example and particular figure 5.1; e.g., in the model definition in the body text there is no mention of the NN mentioned/used in figure 5, the blue points are not defined in the caption, the terminology e.g.  \u201cpre-update density\u201d is new at this point."
11555,1,"  \n\nThe second part offers an exciting result:  If we learn policy pi_1 to satisfy objective phi_1 and policy pi_2 to satisfy objective phi_2, then it will be possible to switch between pi_1 and pi_2 in a way that satisfies phi_1 ^ phi_2."
11556,1,\n- What is the reader supposed to take away from Table 1?
11557,1,"\n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case."
11558,1," In pixel by pixel MNIST, some of the legends might have some typos (FC uRNN), and you should use \""N\"" instead of \""n\"" to be consistent with the notation of the paper."
11559,1," So if the proposals are missing a single object, this cannot really be counted."
11560,1,\n\nIn Figure 4 some of the samples are quite non-physical.
11561,1,\n\nI do have some serious questions/concerns about this method:
11562,1,"  Instead, using only one set of test parameters, the authors compare their algorithm to a \u201cgreedy baseline\u201d policy that is specified a \u201calways try to change lanes to the right until the lane is correct\u201d then it tries to go as fast as possible while obeying the speed limit and not colliding with any car in front."
11563,1,". If these are better than random and the perturbations are more successful it would be a much more compelling story. """
11564,1,\n2. The last equation in pp. 3 defines the decision function f by an inner product.
11565,1, Is there any reference demonstrating that rodent locomotory behavior in a 2D arena is random?
11566,1,\n\n3) Some parts of the paper are hard to read. Sections 3 and 4 are not easy to understand.
11567,1,\n\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization.
11568,1," IMO, it is a very nice, clean, and useful approach of combining causality and the expressive power of neural networks."
11569,1, \n\nThe goal here is to make the target sentence generation non auto regressive.
11570,1," In some sense, however, Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task. """
11571,1, The authors derive relevant sample complexity results for their approach.
11572,1,"\nThe curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only."
11573,1," I think the following one is also relevant:\n-- Law et al. Deep spectral clustering learning. ICML 2015.\n\n"""
11574,1, It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term.
11575,1,"""This paper presents a nearest-neighbor based continuous control policy."
11576,1," In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16)."""
11577,1," If the duality parameter is large enough, the functions become convex given that the initial losses are smooth."
11578,1,"The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629\n). "
11579,1, The authors demonstrate\nthe method on a corpus of books by various authors and on a corpus of subreddits.
11580,1, \n+ strong experimental results
11581,1, Each class is represented by a class prototype which is given by the average of the projections of the class instances.
11582,1," This leads to a natural, testable notion of generalisation."
11583,1,"\n   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc."
11584,1,"   I would recommend this work for a workshop presentation at this stage.\n"""
11585,1," Most, if not all, of the determiners are missing."
11586,1,\n\nCons:\n-- Evaluation metrics for the morphological agreement task are unsatisfactory
11587,1,"\n\nA second part of the paper looks at whether explanations are global or local.[[CNT], [CNT], [CNT], [MIN]] The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case."
11588,1,\n\nPros:\n(1) the paper is very well organized and easy to read
11589,1,   This papers experimentation sections sets a positive example by exploring a comparatively large space of standard model architectures on the problem it proposes.
11590,1, The experimental results seem to show that such a learned teacher strategy makes machine learning algorithms learn faster.
11591,1," \n\nGenerally speaking, I like the overall idea, which, as far as I know, is a novel approach for dealing with discrete inputs."
11592,1, The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset).
11593,1,\n\nTable 5: I had a hard time understanding Table 5 and the corresponding discussion.
11594,1, \n\n3 The authors conducted experiments on three benchmark datasets and show promising performance of CrescendoNet .
11595,1, however it is simply a classification problem
11596,1," The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot products\u2019. While sentences like those can be deciphered, they aren\u2019t that appealing."
11597,1, The manuscript is hard to read due to unclear descriptions and grammatical errors.
11598,1," Since F(Yj) is not a constant, I think that d is inversely proportional to ||G(Yj)||/||F(Yj)||, however, in the interpretation the dependence on ||F(Yj)|| is ignored."
11599,1, \n\nCons:\n- Parts of the paper are unclear and some details are missing.
11600,1,"\n[2] Beal, M.J. Variational Algorithms for Approximate Bayesian Inference."
11601,1, Little to no detail about these features is included.
11602,1," However, as pointed out by the authors, there is a lot of similar prior work in software testing."
11603,1,\u201d. What does \u201csmoother convex\u201d mean?
11604,1, In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.
11605,1,  \n- Competitive with state-of-the-art external implementations
11606,1,"\n\nThe paper acknowledges that complex networks are not new, and that the findings of previous authors is that complex networks perform less well than real-valued alternatives."
11607,1,"\n\nSecondly, the experiments could have more thorough/stronger baselines."
11608,1, Evaluation is unclear and incomplete.
11609,1," If the writing was improved, I think the paper may have even more impact."
11610,1,"  This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result."
11611,1," The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x."
11612,1,\n\nI agree with most of the criticism raised by other reviewers
11613,1,"""Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights."
11614,1," In the current statement, the result seems independent to $L$ when $L \\geq 2$."
11615,1,"""This paper proposes replacing fully connected layers with block-diagonal fully connected layers and proposes two methods for doing so. "
11616,1, \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.
11617,1, although a more direct comparison would be helpful (it is frustrating to flip back and forth between Tables 2 and 3 in an attempt to tease out relative performance) and and it is not clear how the settings (\u03b5 \u000f\u000f\u000f= 0.5 and \u03b4 \u2264 9.8 \u00d7 10\u22123) were selected or whether they provide a useful level of privacy.
11618,1, In order to evaluate a generative model one should test on the generated data only (tau=1) I believe.
11619,1,"\n- Typo in eq. 3: the - in the max should be a comma[[CNT], [CLA-NEG], [CRT], [MIN]]\n- There is a good amount of typos and grammar errors,"
11620,1," Generated CIFAR images seem similar than the originals, although CIFAR images are very low resolution, so judging this is hard."
11621,1,"  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition."
11622,1, This is an unrealistic scenario.
11623,1,"While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community."
11624,1,"   The paper uses a hard decision on the top-k objects (there can be at most k objects) in the final object list, based on the soft object presence values (I have not understood if these top k are sampled based on the noisy presence values or are thresholded, if the authors could kindly clarify)."
11625,1,"Several examples are used to show that even visually similar adversarial examples can have very different saliency maps, which motivates using these saliency maps to detect adversaries."
11626,1, \n\n[1] A Greedy Approach to Adapting the Trace Parameter for Temporal Difference Learning .
11627,1,\n\nThe provided phase analysis and its relation to the depth of the network is also very interesting.
11628,1,"\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They\u2019re just different."""
11629,1," While\nresults are pretty good,"
11630,1, \n\nI don't see experiments comparing having random projections and not.
11631,1," Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD."
11632,1,"Also, finding nearest neighbors takes time on large graphs."
11633,1, Could the authors comment on how sensitive the method is to this parameter?
11634,1," Also, the attack model should formally introduced."
11635,1,\n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure.
11636,1,\n\nPros:\n--The paper is very well written and easy to follow.
11637,1, No simple baseline like the Unstructured [1] + simple concatenation of an image vector is provided.
11638,1, They are the best in terms of precision.
11639,1," The authors could, therefore, provide more detail in relating the contribution to these papers and other relevant past work and existing algorithms."
11640,1,"""The topic discussed in this paper is interesting."
11641,1, (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this).
11642,1,"\n  \u2022 [p6] \""Dosovitskiy & Koltun (2017) have not tested DFP on Atari games."
11643,1," The main contribution of the paper is an online update of the output kernel, which measures similarity between pairs of tasks."
11644,1,\n\nMinor notes:\n - The paper was very well written/edited.
11645,1,\n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer.
11646,1,  The experiments further verify the performance gain compared against the baseline.
11647,1,\n\nNegatives:\n- The novelty of the paper is limited.
11648,1, \n\nOriginality\nThe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy.
11649,1," The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed)."
11650,1,"\n\nMy impression about the paper is that even though it touches a very interesting problem,"
11651,1,\n\nI guess that networks including convolutional layers are covered by\ntheir analysis.
11652,1,"\n\nIs the \""Karel DSL\"" in your experiments the full Karel language, or a subset designed for the paper?"
11653,1,"  As a result of comments 1,2,3, even though I do believe in the merit of the intuition pursued and the techniques proposed, I am not convinced about the main claim of the paper."
11654,1," After estimating this operator, it is straightforward to use this to generate the estimated optimal control input."
11655,1,\n\nPro:\n- Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking.
11656,1,"(again, here further experiments are necessary, e.g., varying the dose of adversarial points). """
11657,1,\n- In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze?
11658,1,We use the method\nof Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a\npoint they receive a reward of 1 and the other player receives \u22122.
11659,1," The datasets are relatively smaller scale, and datasets such as MNIST and CIFAR are known to overfit."
11660,1, Labelled data and unlabelled data are therefore lie in the same dimensional space.
11661,1,\nMy staying power was exhausted around equation 31.
11662,1," The non-conservative part leads to the fact that the dynamics of SGD\tmay show oscillations, and these oscillations may even prevent the algorithm from converging to the 'right' local optima."
11663,1,  This could be done by visually showing the partition constructed or seeing how the model learned to merge solutions..
11664,1, It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others.
11665,1," The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training."
11666,1," It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \""Dot Product Proportionality Property."
11667,1,\n\nI hope all of this makes it clear to the authors that it is inappropriate to claim that artificial intelligence has \u201clargely overlooked\u201d or \u201clargely neglected\u201d.
11668,1,"\n- This is visible on universal perturbations, which become less effective as more AT is applied."
11669,1,"\n\n- Eq (9), this is done component-wise, i.e., Hadamard product, right?"
11670,1,"\n- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper."
11671,1, though I make some suggestions for the camera ready version below to improve clarity.
11672,1,"It definitely does not seem able to learn higher level concepts like certain scenes being more likely to be close to each other than others (e.g. it is likely to find an oven in the same room as a kitchen sink but not in a toilet).[[CNT], [CNT], [DIS], [GEN]] It is not clear whether this is achievable by the current system even with more training data."
11673,1, It is a multi-view extension of variational autoencoder (VAE) for disentangled representation.
11674,1, The authors assume that the global model will depict general english.
11675,1, The hard concrete distribution is a small but nice contribution on its own.
11676,1, but the accusation of misrepresentation below is a serious one and I would like to know the author's response.
11677,1," \nMost data that are available for learning are in discrete forms and hopefully,\nthey have been digitalized according to Shannon theory."
11678,1," If so, would it be possible to plot the complete curves?"
11679,1," However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood."
11680,1,\n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate
11681,1," However, the results seem a bit limited because it does not apply to neural-network function approximator. "
11682,1,"\n\nTypos:\n- The paper needs a cleanup pass for grammar, typos, and remnants like \""Figure blah shows our \nneural network architecture\"" on page 5."
11683,1,"  \n\nWithout such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers."
11684,1, The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task
11685,1, Can it be made crisper?
11686,1,. The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice. 
11687,1, We can also see from Table 1 that if we replace KL by its symmetrical variant we get similar results.
11688,1,\n\nThis is a very interesting area and exciting work.
11689,1, \n(3) The computation of w in Algorithm 2 is problematic.
11690,1," As long as they yield collisions at the same rate, these two generative models are \u2018equally diverse\u2019. Isn\u2019t coverage of equal importance?"
11691,1,\n\nThe execution of the proposed ideas in the experiments was a bit disappointing to me.
11692,1,\n2. Compare more clearly setups where you fix the number of parameters.
11693,1,"\n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks?"
11694,1," The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach?"
11695,1,"\nare clearly of great interest, especially when compared to state-of-the-art assimilation strategies\nsuch as the one of B\u00e9r\u00e9ziat."
11696,1," I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together."
11697,1,  They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions.
11698,1, \n\nPros:\n1. This paper tackles an important research question. 
11699,1, but I feel that it is not well-suited / sufficient for ICLR audience.
11700,1,. A discussion of this relationship seems really important.\
11701,1,  The characteristics and theoretical analysis of the proposed method are discussed and explained.
11702,1,"n\nOverall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side. "
11703,1,"\n\nI'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC."""
11704,1, How do you estimate the diffusion parameter D?
11705,1,"I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed."
11706,1, It may also be useful to develop further insights into current models (although the authors do not go that route).
11707,1," In fact, a VAE would be nicely suited when proposing to work with low-dimensional latent spaces."
11708,1, I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information).
11709,1,\n\nI would be less concerned about the above points if I found the experiments compelling.
11710,1,"  The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning?"
11711,1,- sec4: the acronym L2HMC is not expanded anywhere in the paper\n
11712,1," However, no comparison is clearly made."
11713,1,"\n\nOther questions:\n- is \""n-gram\"" really the most appropriate term to use for the symbolic representation?"
11714,1,"  So some regions are more important than others, and the top half may be more important than an equally spaced global view."
11715,1," \n\n         =  {d / d {s\u2019 }  PATH } ( s,  Tau( s, th^g ),  a )    d / {d th^g}  Tau( s, th^g)"
11716,1,\nThe paper is also light on discussion of related work other than Makhzani et al
11717,1," 2) proposing a minor modification of the DTP algorithm at the output layer,"
11718,1," \n\nOverall, the results didn't warrant the complexity of the method."
11719,1, Could you explain how classes are predicted given a test problem?
11720,1," However, for some key steps in the proof, they refer to other references."
11721,1," However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout"
11722,1,"  \n\nWhile this might be lost because of the clarity problems described above, the model itself is also never really motivated."
11723,1," Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks."
11724,1,"""The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise."
11725,1, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.
11726,1, and I reckon the paper was written in haste.
11727,1,"\n\nThe main weakness of the experimental section resides in the lack of comparison with classical approach in sentiment analysis: none of the state-of-the-art approaches are implemented here (RNN, basic models on W2V aggregations, ...)."
11728,1," However, the experiments 2show that CrescendoNet is worse than ResNet."
11729,1,"Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components."
11730,1,\n- weak experiments for an application paper
11731,1,"""Summary:\n\nThis paper proposes an adversarial learning framework for machine comprehension task."
11732,1, Why did the choices that were made in the paper yield this success?
11733,1,"""The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy)."
11734,1," And in order to achieve three important targets which are auto completion, auto error correction and real time, the authors first adopt the character-level RNN-based modeling which can be easily combined with error correction, and then carefully optimize the inference part to make it real time."
11735,1,  Suggest adding a forward reference to (5).
11736,1, Results show that intra-attention improves performance for only one of the datasets.
11737,1,  A less significant issue is that the English is often disfluent.
11738,1,  it is not clear at all whether for other downstream kernel applications this approach for optimizing the alignment would provide good quality models) that uses lots of task specific hacks and heuristics to efficiently optimize the alignment.
11739,1,"""Summary:\nThis paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction."
11740,1," In addition, it would be useful to see how the method put forward in the paper compares with other (reward-shaping) techniques within MARL (especially in the perfect information case in the pong players' dilemma (PPD) experiment) such as those already mentioned."
11741,1, Even storing a 50k x 50k matrix requires about 20GB of RAM.
11742,1,  These scaling rules are confirmed experimentally (DNN trained on MNIST).
11743,1, The missing experiment concerns me in the sense that the network could just be memorizing previously seen objects.
11744,1,"\n\nPros:\nSimple, interesting idea\nWorks well on toy problems, and able to prevent divergence in Baird's counter-example"
11745,1," However, this contribution is far from correctly addressing the problem."
11746,1,\n\n+ new and large dataset
11747,1,". What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output?"
11748,1,"\n\nThe authors train a neural network to solve the defogging task, define several evaluation metrics, and argue that the neural network beats several naive baseline models."
11749,1," For comparison the existing methods ICA, Tucker2 should also be evaluated for the baseline corrected data, to see if it is the constrained representation or the preprocessing influencing the performance."
11750,1, The paper is well-written and easy to follow.
11751,1,"""Summary: The paper introduces \""Phase Conductor\"", which consists of two phases, context-question attention phase and context-context (self) attention phase."
11752,1, The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures!
11753,1," On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work!"
11754,1,"\n\nI found the paper quite interesting, but meanwhile I have the following comments and questions:"
11755,1,"""On the positive side the paper is well written and the problem is interesting."
11756,1, and the empirical work is concerning.
11757,1, The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights.
11758,1," Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours."
11759,1,"   \n\nWhy is it important to maximise I(X_l, Y) for every layer? "
11760,1, This statement seems completely unjustified.
11761,1,   From (a) to (b) the red points move closer to the center while in (c) they move further away (why?).
11762,1, The proposed model is not very innovative but works fine for the DQA task.
11763,1, I found many aspects of the exposition difficult to follow.
11764,1,"\""\nThis claim is highly biased by who is giving the \""intuitive sense\""."
11765,1,\n\nI agree that the relation to previous work is not adequately outlined.
11766,1,"""The paper is incomplete and nowhere near finished, it should have been withdrawn"
11767,1," Perhaps the title should be \""Rotation Operation in Long Short-Term Memory\""?"
11768,1,"\n\nThe experiments are well-presented, and appear to convincingly show a benefit."
11769,1," Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS."
11770,1,"  Such dependence for CCNNs appears to be a weakness in comparison."""
11771,1,"\n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state."
11772,1, They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters).
11773,1,"\n\nOverall, I think this is an interesting paper,"
11774,1,"\n\nI suggest using the same axis limits for all subplots in Figure 3."""
11775,1," While the first three tasks are smaller proof of concept, the last task could have been\n  more convincing if near state-of-the-art methods were used."
11776,1," De et al (AISTATS 2017) have a method for gradually increasing batch\nsizes, as do Friedlander and Schmidt (2012)."
11777,1,\nResults: The tables are difficult to read and should be clarified:
11778,1," \n3.\tIt is better to provide results in terms of accuracy for both datasets, as previous methods usually use accuracy for comparison."
11779,1, Is the proposed layer really useful once a powerful model is used?
11780,1,\n\nI am not following the assertion in 5.2.3 that MAML adaption curves can be seen as an upper bound on the performance of gradient-based methods.
11781,1,"By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT."
11782,1, It then derives the idea of eigen options from the successor representation as a mechanism for option discovery.
11783,1," \n\nFrom my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea."
11784,1,"\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used."
11785,1, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive).
11786,1,"\n\nRegarding the experimental results, is there any insight on why the dense networks are falling short?"
11787,1, though I have a few questions below that prevent me for now from rating the paper higher.
11788,1,"\n\nAlso, many experimental details are missing from the draft:"
11789,1,.\n\nThis is a really interesting perspective on probing invariances and should be explored more.
11790,1,"\n\nQuestions/other:\n- What is meant by \""implicit\"" models?"
11791,1," However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed."
11792,1, The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.
11793,1, Could the authors justify why minimizing w.r.t. W_1 is still a quasi-convex optimization problem?
11794,1,\n\nOriginality:\n\nThe proposed approach in which the weights in the last layer of the neural\nnetwork are the only Bayesian ones is not new.
11795,1, Is it possible to combine them together?
11796,1, It will be interesting to compare with some existing second-order optimization algorithms for deep learning.
11797,1," This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense."
11798,1,  the paper does not do a great job at properly comparing with baseline and make sure the results are solid. 
11799,1,"""In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. "
11800,1," The proposed approach closely mirrors that of Nakayama and Nishida, 2017 in that image-aligned text is encouraged to map to similarly to the grounded image."
11801,1, the contribution is not significant as the paper misses an important explanation for the phenomenon.
11802,1, \n\nComments:\n\n-I feel overall the contribution is not very novel.
11803,1,"""The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft."
11804,1,"\n* Finally, not being very familiar with multigrid methods from the numerical methods literature \u2014 I would have liked to hear about whether there are deeper connections to these methods.\n\n\n"""
11805,1," CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection."
11806,1,\n\nOne of the motivations behing the model is to force label representations to be in a semantic space (where two labels with similar meanings would be nearby).
11807,1, Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time.
11808,1, I found the presentation of this paper almost inaccessible to me.
11809,1," Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me."
11810,1,\nThe agent then samples from this posterior for an approximate Thompson sampling.
11811,1," \nWhat are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)?"
11812,1,This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal.
11813,1, \n- page 6: \u201con the last full precision network\u201d: should probably be \u201con the last full precision layer\u201d\n
11814,1,"\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?"
11815,1,"\n\nMinor points:\n- naively, the selection algorithm might not scale well with the population size (exhaustively comparing all pairs), maybe discuss that?"
11816,1,"\  It first shows that a simple weakly regularized (linear) logistic regression model over 200 dimensional data can perfectly memorize a random training set with 200 points, while also generalizing well when the class labels are not random (eg, when a simple linear model explains the class labels); this provides a much simpler example of a model generalizing well in spite of high capacity, relative to the experiments presented by Zhang et al (2017). "
11817,1,\n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each
11818,1," Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task)."
11819,1," Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn't leave the manifold of reasonable designs."
11820,1,"\n\nFor semi-supervised and active-learning results, please include error bars for the miniImagenet results."
11821,1," To the best of my knowledge, Mirowski et al. never made such a bold claim (despite the title of their paper)."
11822,1, Just test it and report.
11823,1,\n\nIn equation (10) is z_l=z_l^k or z_l^(k+1/2) (I assume the former).
11824,1, More iterations should be taken and the log-scale style figure is suggested.
11825,1,   My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work
11826,1," For example, the paper will be more interesting if inception scores were shown for various challenging datasets."
11827,1," Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1."
11828,1, In light of this (and of the paper being quite a bit over the page limit)- is this material (4.2->4.4) mostly not better suited for the appendix?
11829,1," So, it is hard to understand the core idea of this paper."
11830,1,"\nBut if the other reviewers argue that the paper should be accepted I will change my score.\n\n"""
11831,1,"\n\n2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space)."
11832,1, Or did they invent the margin loss?
11833,1," This theory seems to be interesting and might have further potential in applications."""
11834,1, There is no discussion on the failure cases.
11835,1,\n\nIssues:\n\n- The clutter in SIM2MNIST is so small that predicting the polar origin is essentially trivially solved by a low-pass filter.
11836,1, The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.\n\nPros: \n1.
11837,1,\n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).
11838,1," For reasons outlined above, I am not convinced that this approach in its current form would work very well on more complicated problems."
11839,1,\n\nThe paper suffers from a lack of focus
11840,1,"\n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context."
11841,1, (3) It is stated again later as Theorem 2.
11842,1," It seems to just subtract a constant 1 from the regularization term."""
11843,1,"  If I understand correctly, f_p is a representation of *all images* of a specific layer *k* at/around pixel \""p\"", (According to last line of page 3)."
11844,1, Therefore it is my opinion that reinforcement learning approaches to SLAM lack a concrete goal in what they are trying to show.
11845,1,"\nIn the third paragraph, starting with \""Therefore we consider a natural extension of this model, ...\"" it is unclear which model the authors are referring to. (RandWalk or their tensor factorization?)."
11846,1," \n\n- Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs.[[CNT], [null], [DIS], [MIN]]  I am confused why these are different numbers.[[CNT], [null], [DIS], [GEN]]  Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was?"
11847,1,"\n\nWhile the idea may be novel and interesting,"
11848,1,"\n\nIf you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton\u2019s method on convex problems, by Yurii Nesterov."
11849,1," \n\n3. Relatively minor: The writing of this paper is readable,"
11850,1," More importantly, the proposed activation functions reduce the errors only a bit (<0.5%)."
11851,1," In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric."
11852,1, Is it due to a good initial network structure?
11853,1,  This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results.
11854,1," This is a general comment over this kind of approach, but I think it should be addressed. \n"""
11855,1," \n\nTo understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines."
11856,1," \n\n2. the second question is related.[[CNT], [null], [DIS], [GEN]]  it is unclear how the optimal distribution would look like with the latent variable gan."
11857,1,\n\nCons: I found the paper difficult to read.
11858,1," It would be nice to know (1) how the various models perform\nat QA in both ZS1 and ZS2 settings, and (2) what the actual performance is NAV\nalone (even if the results are terrible)."
11859,1,"""The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface"
11860,1,\n--There were very minor typos and some unclear connotations. 
11861,1,"Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?"
11862,1, None of these results beat state-of-the-art deep NNs.
11863,1,"  https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions.\n\nOne of the most popular theories of emotion is the theory that there exist \u201cbasic\u201d emotions: Anger, Disgust, Fear, Happiness (enjoyment), Sadness and Surprise (Paul Ekman, cited by the authors)."
11864,1," In fact, in certain cases such as unsupervised word analogy, SGNS is clearly and vastly superior to other techniques (Stratos et al., 2015)."
11865,1,"""This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree."
11866,1,\n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs. 
11867,1," If TS does not select certain actions, the Q-function would not be updated for these actions."
11868,1, The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference.
11869,1," \n\nIn summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small."
11870,1,"\n\nHowever, several important concepts in the paper are not well explained or motivated.[[CNT], [null], [CRT], [MIN]] For example, it is a bit misleading to use the word \""covariance\"" throughout the paper, when the best model only employs a scalar estimate of the variance."
11871,1,"\n\nClarifications:\n1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper?"
11872,1, I think the approach is good and fruitful.
11873,1,   The performance of the question answering system in the presence of this adversarial narrator is of unclear significance and the empirical results in the paper are very difficult to interpret.
11874,1," \nThere are missing links and references in the paper and un-explained notations, and non-informative captions."
11875,1,\nI would only suggest to expand the experimental section with further (real) examples to strengthen the claim.
11876,1, One should note that the birthday theorem assumes uniform sampling. 
11877,1," So, in order to have options that lead to more direct, \""purposeful\"" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space."
11878,1,"  \n* The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs, but it\u2019s not clear if they do this for the (vanilla) ResNet as well."
11879,1,\n\n3. The authors should clarify if the method is specifically designed for GANs and VAEs
11880,1, although I do have some questions/concerns about the results in Table 1 (see below).
11881,1," If this is base-2 logarithms I would expect a value close to 1. """
11882,1,"""*Summary*\n\nThe paper proposes using batch normalisation at test time to get the predictive uncertainty."
11883,1,\n\nThe architecture search (Table 3 and Figure 4) seems to quite arbitrary.
11884,1, The general methodology also makes sense to me.
11885,1,"\n\nFig 1 shows \u201cthe histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers."
11886,1,"\n\nExperiments on known datasets are interesting, but none of the results are competitive with current state-of-the-art results (SOTA), despite what is said in Appending D."
11887,1,.\u201d Are these zero partial derivatives of the post-relu or pre-relu?
11888,1,". Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released."
11889,1, The correction requires only two extra multiplications per model parameter and update step.
11890,1,"""This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample ."
11891,1,"""This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets."
11892,1, How important is the AE in the loss?
11893,1," The authors quote the paper from Ian Goodfellow: \u201cFor GANs, there is no theoretical prediction as to\nwhether simultaneous gradient descent should converge or not."
11894,1, It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the \u201ctask distribution\u201d is too large to be meaningful.
11895,1,"\n\nORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them."
11896,1," I also like the \u201cShared Attention\u201d - it makes a lot of sense to say that if the \u201csemantic\u201d attention mechanism has picked a particular word, one should also attend to that word\u2019s head; it is not something I would have thought of on my own."
11897,1,"\n\n\n[Collins et al. JMLR 2008] Michael Collins, Amir Globerson, Terry Koo , Xavier Carreras, Peter L. Bartlett, Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks, , JMLR 2008.\n\n [Dziugaite et al. UAI 2015] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015"""
11898,1,\n\nThe submission has following PROS:\n\n+ The proposed visual Turing test provides a novel solution to evaluate the generation quality.
11899,1," \nRather than the \""baseline\"" epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy."
11900,1,Con:\nThe relationship to stochastic gradient markov chain monte carlo needs to be explained better.
11901,1,  Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.
11902,1,"\n\nIn summary, the idealized model gives a good demonstration of the problem itself."
11903,1, Some acknowledgement and discussion of this would be useful.
11904,1,\n* Hazzan & Jakkola (2015): \u201cSteps Toward Deep Kernel Methods from Infinite Neural Networks\u201d for GPs corresponding to NNs with more than one hidden layer.
11905,1,"  For example, for Conv4, how many channels at each layer?"
11906,1," \n\nSTRENGTHS: The paper in general is well written and easy to ready. I appreciate the idea of the Turing test and qualitative results presented are quite impressive. Also, the use of  diverse state-of-the-art generative models is also a strong point."
11907,1," However, the DrQA has less usage of LSTMs, and in order to cover a large reception field, the dilated CNN version of DrQA has a 2-4 times speedup, but still works much worse."
11908,1,  This is very interesting work. 
11909,1," Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training."
11910,1," \n\n- This is a minor point and did not have any impact on the evaluation but VAE --> VHE, reparameterization trick --> resampling trick."
11911,1," If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet."
11912,1,Different observability modes\n\t\u2022\tEvaluation against most compatible methods from other sources \n\t\u2022\t
11913,1," If we are evaluating the video prediction task for having real or fake looking videos, the turkers need to observe the full video and judge based on that."
11914,1,"""This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem."
11915,1,\n\n2. The authors measured the actual speedup on a single CPU (Intel Core i5)..
11916,1, \n\nI feel that the text is often redundant and that it could be simplified a lot.
11917,1," They also propose to use a soft unitary constraint on those small matrices (which is equivalent to a soft unitary constraint on the Kronecker product of those matrices), that is fast to compute."
11918,1,\n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1)
11919,1,  The grid tensors are matricized and the ranks of the resultant matrices are compared. 
11920,1,\n\nI have two main concerns with the presentation.
11921,1," First of all, errors learned from the noisy data sources are constrained to exist within a word."
11922,1," However, to evaluate the effectiveness of the method, it would be interesting to also report results with a kernel-based classifier, so as to see how it compares to the state of the art."
11923,1, It would be great to see the equivalent of Figure 7 with correctly rescaled $A$.
11924,1," It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,"
11925,1,\n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited.
11926,1,  (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?
11927,1,\n\nIn the description of algorithm 2: the pseudo-code does not specify that the implicit step is done with regularization coefficient tau_theta
11928,1," To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks."""
11929,1,\n\nCons:\u000b  1. The paper is poorly written and the proposed methods are not well justified.
11930,1,"  .\n\nThough experiments may not show show state-of-the-art performance, I think that they still serve to demonstrate the utility of the Skip-RNN architecture when compared side-by-side with a similarly tuned non-skipping baseline."
11931,1,"  If s_0 is start of string, the s_0:m is of length m+1 not length m."
11932,1,"  One way to overcome this potential problem is to add projection matrices between layers that will do some mixing, but this will blow the number of parameters."
11933,1," \n\nWhile the CIFAR-100 results look promising, the ImageNet-1k results are less impressive."
11934,1,  There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf.
11935,1, It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case
11936,1, The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory.
11937,1,"\n\nThis is a relatively new area to tackle, so while the experiments section could be strengthened, I think the ideas present in the paper are important and worth publishing."
11938,1, I would recommend refraining from using these terms here.
11939,1,"  This is not correct, as also reported in\nequation 2. "
11940,1, The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples.
11941,1," The student model learns according to a standard stochastic gradient descent technique (Adam for MLP and CNN, Momentum-SGD for RNN), appropriate to the data set (and loss function), but only uses the data items of the minibatch chosen by teacher model."
11942,1,\n\nI like the experiment presented in figure 5 in particular.
11943,1," On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment."
11944,1," Also, initialize \\alpha to zero."
11945,1, I find the findings not conclusive based on these.
11946,1," Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data."
11947,1,"  Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task."
11948,1, but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.
11949,1,\n\nThe paper is well-written overall.
11950,1,"\nThe second motivation, w.r.t. IB seems interesting"
11951,1, The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow.
11952,1," It could provide more insights to practical problems if more experiments were done: e.g. can this technique help domain adaptation?\n\n"""
11953,1, This paper proposes a method to uncover this structure from the filters of a trained ConvNet
11954,1,\n\nThe size of a kernel matrix depends on the sample size.
11955,1,\n\n-The experimental evaluation on the running time is limited to one particular problem.
11956,1, This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.
11957,1," Perhaps more important, measuring performance using the same policy used to build the options (the random policy) seems somewhat unsatisfactory to me."
11958,1,"\n\nThis is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers."
11959,1, The idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learning.
11960,1,\n\nThe model is applied to artificially generated data and to high-frequency equity data showing promising results.
11961,1," As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum. "
11962,1,\n\nOriginality: The generation of adversarial examples in black-box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too
11963,1,"\n\nFurther comments, questions and suggestions:\n- It might be useful to add more details of your actual approach in the Abstract, not just what it achieves"
11964,1,"\n\nWith these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved,;"
11965,1, Is it meaningful to perform ADD or SUBSTRACT on the leaned code?
11966,1,\n- It would be valuable to see experiments on bigger datasets than only MNIST and CIFAR-10.
11967,1,"\n\n[1] M. Defferrard, X. Bresson, and P. Vandergheynst."
11968,1, \n\n# Clarity\nThe paper is overall clear and easy-to-follow except for the following.
11969,1,"\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified."
11970,1," \n\n- Equation 8 is confusing in that it defines a Monte Carlo estimate of the expected reward, rather than an estimator of the gradient of the expected reward (which is what REINFORCE is)."
11971,1,"\n\nThe R_AC, penalizes correlation between responses of different nodes."
11972,1, The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game.
11973,1, It showed that the proposed method could mix between the modes in the posterior.
11974,1," You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem."
11975,1,"  They demonstrate the quality of the sequences on sine waves, MNIST, and ICU telemetry data."
11976,1,  This is encouraging.
11977,1," This learning is done without any supervision, with a loss that tries to predict actions which result in the state achieved through self exploration (forward consistency loss)."
11978,1," \n(-) The gains are reasonable,"
11979,1,"\n\nThe initialization of the CNN model is not clearly introduced, which however, may affect the performance significantly."
11980,1, The experiments show that the proposed algorithm allows a synthesizer to do a better job of reliably finding a solution in a short amount of time (though the effect is somewhat small).
11981,1,"  Or are the keys for already found entities retrieved directly, by value?"
11982,1, Please mention the numbers with unit normalization to give a better picture.
11983,1,  It would be better to lead with some high-level intuition about what the network is supposed to do before diving into the details of how it\u2019s set up.
11984,1," In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to \u2018negative interference\u2019 between tasks [1, 2]."
11985,1," Figures 2 and 3 do not add anything to the story, since 2 is just a plot of gamma pdfs and 3 shows the difference between the constant KL and the normal case that is linear in d. "
11986,1, there are few key shortfalls (see below).
11987,1, \n\nThere are several things to like about the paper:
11988,1," As it stands, it's not even clear if larger \nvalues are better or worse."
11989,1,". As a result, the paper carries very little scientific value."""
11990,1,".\n\n-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss"
11991,1," Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution)."
11992,1,"\n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n"""
11993,1," At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$."
11994,1,"""The paper presents an interesting framework for bAbI QA."
11995,1,"""Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping."
11996,1,"\n\nPros:\nThe authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer."
11997,1," In addition, what is done at test-time if the sequence length differs from the sequence length at training time?"
11998,1,\n\n1. The authors tested out this new activation function on RNNs.
11999,1, However I do have some concerns about the way the paper is currently written and I find some claims misleading.
12000,1," However, I would expect there to be an optimum."
12001,1,"""Summary:\nThe paper proposes a new dataset for reading comprehension, called DuoRC."
12002,1,"\n\n4. In section 3.2, it may be clearer to explicitly point out the use of the \""3-sigma\"" rule for Gaussian distributions here."
12003,1," Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here)."
12004,1, Later the presentation switches to a more general case.
12005,1," \n(viii) \""P denotes the number of model parameters\"" (I guess it should be a small p? hard to decipher)"
12006,1, \n- The overall objective function is missing.
12007,1,  Authors captured a new facial dataset for their evaluation and reported better results than PCA.
12008,1," This is different from the sum of the original losses, which seems to be the one used to train the \u201cequal weight\u201d baseline."
12009,1,"I also do not think it is necessary to over-sell, given the solid work in the paper."
12010,1,"The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option."
12011,1," I would like to see more details on how the evaluation is done here: presumably, the lower I set the threshold, the higher my score?"
12012,1, The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset.
12013,1,"""Summary: \nThe paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an SVM model."
12014,1,\n\nThe analysis is elegant.
12015,1,\n\nCons:\nI find the novelty of the paper limited: \nThe authors extend the work by (Onken et al. 2016) to subtract baseline (a rather marginal innovation) of this approach. 
12016,1, There is no discussion on the failure cases.
12017,1,\n\nHowever the paper in its current form has a number of problems:\n\n- The authors state that a major shortcoming of previous (efficient) unitary RNN methods is the lack of ability to span the entire space of unitary matrices.
12018,1,"\n  Although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data, this is not always correct"
12019,1," The combination of the two parts seems a bit incremental and does not bring much novelty."""
12020,1,\niii) Superior performance with fewer number of parameters compared to other methods.
12021,1,", but since it is left as future work, the paper should make it clear throughout."
12022,1," though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n"""
12023,1," It might be the case that the considered scenarios indeed happen in computer vision related problems, but I am not an expert in that regard."
12024,1, The authors show the convergence of the objective but not of the iterates sequence.
12025,1,"\nI think estimating the operator from the input to the output is interesting, instead of constructing (A, B, C, D) matrices, but this idea and all the techniques are from Hazan et. el., 2017."
12026,1, \n\n- Can this approach be applied to semi-supervised learning?
12027,1," \n? p.6: Do you have one model for all the relations or does every relation has its own LSTM, CNN, feed-forward network?"
12028,1,\n\n6th line of 5.1 theta_l is initialised uniformly in an interval -> could you explain why and/or provide a reference motivating this ?
12029,1,"\n- The correspondence with a semantic feature space is too pretentious, specially since no experiment in this direction is shown."
12030,1," Thus I found most of section 3 to be a reformulation of known results, including Theorem 5 and its proof.\n\nSame goes for section 5; the symmetric- anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years, and I did not find any new insight into those manipulations of the Fokker-Planck equation from this paper."
12031,1, This result is obtained almost directly by writing the bound on Lipschitz-continuous function\n | f(y)-f(x) | < L || y-x ||\nwhere x = x_0 and y = x_0 + \\delta.
12032,1," I have certain concerns regarding the motivation, model, and evaluation methodology followed:"
12033,1,"\n\nMinor comments:\n- P.1, L.5 in the third paragraph: architexture -> architecture[[CNT], [CLA-NEG], [CRT], [MIN]]\n- What does \""Cor\"" of CorGAN mean?[[CNT], [null], [QSN], [MIN]]\n\nAFTER REVISION\nThank you to the authors for their response and revision."
12034,1, 3) comparison to simply ensembling with random initialisations.
12035,1, A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.\n\n
12036,1, The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X (an introduction to C# offered on edx) and problems on the Microsoft CodeHunt platform.
12037,1, It's nearest neighbours in the embedding space (i.e. semantically similar words) will also have high values in coordinate 99.
12038,1,\n\nThe reconstructions show poor detail relative to the originals. 
12039,1,"\n\n- Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given."
12040,1,\n\nThis paper reads well and the results appear sound.
12041,1,\n \nThe motivation of the work is not very clear.
12042,1,"  Avoiding the estimation of a dynamics model was stated as a given, but perhaps more could be put into motivating this goal."
12043,1," the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty."""
12044,1, Numerical experiments show that such sparse networks can have similar performance to fully connected ones.
12045,1, I think it would be possible for the synthetic sequence to also generate an outcome measure (i.e. death) based on the first 4 hours of stay.
12046,1," Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO."
12047,1,"""In this paper the authors consider learning directly Fourier representations of shift/translation invariant kernels for machine learning applications."
12048,1, I would prefer less material but better explained.
12049,1, Further ablations such as the effect of the trigram repetition constraint will also help to analyse the contribution of different modelling choices to the performance.
12050,1, It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods\n3.
12051,1, \n\nAuthors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM.
12052,1," \n(iii) In which way is a Gaussian prior uncorrelated, if there is just a scalar random variable? "
12053,1,"Conversely, traditional ensembling involves separate training of the different instances and this enables the learning of an arbitrary number of individual nets."
12054,1," \n- f_1 is not defined sot the paragraph \""the latter equation...\"" showing that almost surely x \\leq y is unclear to me, so is the result then."
12055,1,The motivation behind it is that \nin GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions.
12056,1,"  If this does not happen, what is being shared across tasks?"
12057,1,"""This is a nice paper."
12058,1,"\n\nAdditionally, since it seems the main benefit of using a tensor-based method is that you can use 3rd order cooccurance information, multisense embedding methods should also be evaluated."
12059,1," \n\nMinor comments:\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint."
12060,1," Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model."
12061,1, It is probably unlikely that the discretisation method can be generalised to high-dimensional setting?
12062,1, but none of the results really push the SOTA.
12063,1, Having an algorithmic environment would make the description easier. 
12064,1," Parameter noise on the policy won't necessarily get you efficient exploration... and in some cases it can even be *worse* than epsilon-greedy... if you just read this paper you might think that this was a truly general \""statistically efficient\"" method for exploration (in the style of UCRL or even E^3/Rmax etc)."
12065,1, . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?
12066,1," \n\nPlease find my detailed comments/questions/suggestions below:\n\n1) IMO, the paper could have been written much better.[[CNT], [CLA-NEG], [SUG,CRT], [MIN]] At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}."
12067,1," \n\nThere are a few flaws/weaknesses in the paper though, making it somewhat lose."
12068,1," I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there\u2019s no need to mention information theory here."
12069,1, The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1.
12070,1,\n\nOne point I found confusing is how exactly the Combinatorial Concretization step works.
12071,1,"\n\nClarity:\nThe paper is well-written, the main ideas well-clarified."
12072,1,\n\nI\u2019m not fully convinced by the interpretation of Eq. 5: \u201c\u2026 d is inversely proportional to the norm of the residual modules G(Yj)\u201d.
12073,1," \n\n== Other comments\n\n1) Note that d(A, B'_theta) is *equal* to min_alpha max_w  (...)  above equation (2) (it is not just an upper bound)."
12074,1," \n\n3. In the first paragraph of Section 3.3: \""[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation."
12075,1," Why cannot you use the typical evaluation procedure for collaborative filtering,"
12076,1,\nThe dataset considered is noise-free and considers one class only.
12077,1,\n\nThere have existed several works which also provide surveys of attribute-aware collaborative filtering
12078,1,\n\nPros:\n1. A simple approach to encourage better representations learned from unlabeled examples.
12079,1,"\""  It is presented as a collection of observations with no synthesis or context for why they are important. "
12080,1, The part about the actor-critic learning seems to lack many elements (whole architecture training?
12081,1,"\n\nBelow are some more detailed comments, In Introduction, it would be nice if the authors made it clear that \""Their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand."
12082,1,"  \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size."
12083,1,"\n\nCons\n - text is a bit over-wordy, and flow/meaning sometimes get lost."
12084,1,\n\nThe idea of empirically studying the manifold / topological / group structure in the space of filters is interesting.
12085,1," However, I found their empirical evaluation and experimental observations to be very interesting."
12086,1,"\n\nMinor things:\n\nPlease rewrite the sentence \""When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L."
12087,1," This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition. "
12088,1," Hence, the learning framework of the\nauthors can be casted more as a learning with similarity function than learning\ninto a RKHS [2]."
12089,1, Could you comment why this would not happen?
12090,1," How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning?"
12091,1,"The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\"
12092,1,  Some work is cited in the wrong context.
12093,1, the ancillary evidence provided from the paraphrasing of prepositional phrase seems highly uncompelling to me.
12094,1," Hence, I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper, for example. "
12095,1," 2) Comparison with VQA counting work  (Chattopadhyay et al., 2017)."
12096,1,"  SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum."
12097,1,  These results are also lower than the state of the art performance.
12098,1, This could be discussed and some experiments on very HD data should be reported.
12099,1," In the second line, shouldn't the first term be \\tilde{\\mathcal{L}}(W_0 + \\Delta W) ?"
12100,1, The authors state that all units playing NOOP is an equilibrium.
12101,1,\n\n3. The paper wants to find a good trade-off on speed and accuracy.
12102,1," If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely."
12103,1," The expression for L_pred given in eq. (20) (exact) and eq. (41) (approximate) do not match and yet both are connected with an equality (or proportionality), which they shouldn't."
12104,1,"\n- Page 3: \""more computationally and\"" -> \""more computationally efficient and[[CNT], [CLA-NEG], [CRT], [MIN]]\""\n- Page 3: \""for performing final\"" -> \""for predicting final[[CNT], [CLA-NEG], [CRT], [MIN]]\""\n\n\nPoints in favor of the paper:\n- Simple method"
12105,1, \nThe experimental results are convincing enough to show that it outperforms other active learning algorithms.
12106,1,".ther approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., \""Representation Learning on Graphs: Methods and Applications\"", William L. Hamilton, Rex Ying, Jure Leskovec, 2017."
12107,1, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem.
12108,1, An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set.
12109,1, \n\non the positive side:
12110,1,\n\nSummary of review:\n\nThis is an incremental change of an existing method.
12111,1,   Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.
12112,1,"\n\n- (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g. However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be \ng(x) = min_{k \\neq c} f_c(x) - f_k(x)."
12113,1, The setup of the experiments on TIMIT is extremely unclear.
12114,1,"\n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases."
12115,1," On top of that, I feel the execution of the paper leaves much to be desired. \n"""
12116,1,"""The paper presents a reinforcement learning-based approach for program synthesis."
12117,1,"\n\nThis paper is below the threshold because there are issues with the : 1) motivation, 2) the technical details, and (3) the empirical results."
12118,1," If the goal is to generate transductively (with many similar edges), then it would be better to compare more extensively to alternative node embedding and matrix factorization approaches, and assess the utility of the various modeling choices (e.g., LSTM, in/out embedding)."
12119,1,\n\nWhy is the Related Work section at the end?
12120,1,"\n\n[as I am not primarly a neural net researcher, I had always thought pruning was done to decrease over-fitting, not to increase computation speed, so this was a surprise to me; also note that the sparse matrix format can increase runtime if implemented as a sparse object, as demonstrated in this paper, but one could always pretend it is sparse, so you never ought to be slower with a sparse matrix]"
12121,1,\n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm. 
12122,1,"\n\n  - Further, the proposed joint approach, where the second and third order information are combined requires further analysis."
12123,1, (2016).\nIt took me until section 3.5 to figure it out.
12124,1,\n\nThe authors wrote a clear paper with great references and clear descriptions.
12125,1,"\n\nIn section 2.2, how \\mu_i and \\sigma_i are computed?"
12126,1,  I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.
12127,1, They evaluate their model on 6 small scale RNN experiments.
12128,1,\n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning.
12129,1," These two strategies are thus \""equivalent\""."
12130,1," In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream."
12131,1,\n-\tEq(10)(11)(12) and some lines have a typo (a \\cdot) just before some of the ws;
12132,1,"\n\nWhile it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions."
12133,1," As for the related work, it seems the authors have missed some very relevant pieces of work in learning these Fourier features through gradient descent [1, 2]."
12134,1,\n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER.
12135,1, \n\nI think the paper could also discuss a bit more in detail the results provided.
12136,1,"\n\n# Novelty and Significance\n- The problem considered in this paper is new,"
12137,1,\n\n1. Why don\u2019t you report your model performance without data augmentation in Table 1?
12138,1,\n\nFurther comments:\n- The proposed method to incorporate numerical data seems quite ad hoc.
12139,1,"\n\n5. For each weight w, we add K learning rates u_w^j."
12140,1,  I am not an expert in this area.
12141,1,\n\nQuality:\nThe authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.
12142,1,\n\n-------------------------\nI read the response and the new experimental results regarding WGAN.
12143,1," Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.\no\tSNLI data should be described: content, size, the task it is used for"
12144,1,"""Summary: This paper introduces a model that combines the rotation matrices with the LSTMs."
12145,1," On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does)"
12146,1," While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference. "
12147,1,"\n\n- Overall, the only contribution of the paper seems to be the modification to Ba et al. is the Eq. (8)."
12148,1," If the text is from the user, a named entity recognizer is used."
12149,1,\n\n-Clarity/Quality\nThe paper is well written and pleasant to read.
12150,1," The fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models, adds to the quality of the experiment."
12151,1, Novelties are clearly identified by the authors.
12152,1," I understand that it is a metric to compare two images that is based on the mean-squared error so a very private image should have a low PSNR while a not private image should have a high PSNR, but I have no intuition about how small the PSNR should be to afford a useful amount of privacy. For instances, in nearly all of the images of Figures 21 and 22 I think it would be quite easy to guess the original images."
12153,1," Secondly, sentence accuracy (w.r.t. the sentences from which the normalized sentences are derived) is not indicative of morphological agreement: even \""wrong\"" sentences in the output could be perfectly valid in terms of agreement."
12154,1,. There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.
12155,1," It is deduced that HCGD generalizes better, however, this is the case only at the very end of training, while SGD with momentum and ADAM work far better initially."
12156,1, It would be interesting to understand how the different metrics relate
12157,1,"Detailed descriptions of all instantiations even parameters and comparison methods\n\t\u2022\tSystem specified\n\t\u2022\tValidation method specified\n\t\u2022\tData and repository,"
12158,1,"  In particular, the authors propose a new framework to \""sharpen\"" the posterior."
12159,1,"\n- Even through the submodular objective is only approximately solvable, it still translates into a convergence result."
12160,1,"\n3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from?"
12161,1,\n- I would get rid of Theorem 1 and explain it in words for the following reasons.
12162,1,\n\n- Generating high resolution images with GANs even on faces for which\nthere is almost infinite data is still a challenge.
12163,1, It uses back propagation (gradient descent) to improve the design. 
12164,1,  The analysis answers:\n\n1) When empirical gradients are close to true gradients\
12165,1," \n\nOverall, while the paper includes a wide range of experimental evaluation, they are aimed too broadly (and the results are too weak) to support any specific claim of the work."
12166,1, How different it will be compared to a perturbation in an input space? 
12167,1," however, due to its simplicity, I do not think that this paper is relevant for the ICLR conference."
12168,1," \n\nThe paper is well written, clear, organized and easy to follow."
12169,1,"\n\nTo this reviewer\u2019s understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work."
12170,1," However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible."
12171,1," Unfortunately, the proof of this result is poorly written:\n- equation (20) takes a long time to parse --- more effort should be put into making this clear"
12172,1," A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences."
12173,1,"\n\nThe main contribution seems to be the use of the \""graph shift\"" operator from\nSandryhaila and Moura (2013), which closely resembles the one from\nShuman et al. (2013)."
12174,1," see :\n\""Think Globally, Embed Locally\u2014Locally Linear Meta-embedding of Words\"", Bollegala et al., 2017\nhttps://arxiv.org/pdf/1709.06671.pdf\n\nThat paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission."
12175,1,"""This paper is an application paper on detecting when a face is disguised,"
12176,1, What do you mean does not scale to nonlinear function approximation?
12177,1,"  If so, is this done jointly with the training of the auto-encoder?"
12178,1, These issues may need a significant amount of effort to fix as I will elaborate more below.
12179,1,\n- no baselines or comparison
12180,1,"\n\nI recommend acceptance, despite some reservations."
12181,1,"\n\nThirdly, the experiments are only on the MNIST and EMNIST; still not quite sure any real-world problems/datasets can be used to validate this approach."
12182,1,\nFigure 1 is unreadable.
12183,1,\n\nCons:\n(-) The proposed method is a simple extension of ResNeXt
12184,1," In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful."
12185,1,  Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.
12186,1,\n\n2. The experiments are unconvincing.
12187,1, The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types.
12188,1,"\n\n\nErrors:\n- page 2 last para: \""gives an concrete\"" -> \""gives a concrete\""\n- page 2 last para: \""matching\"" -> \""matched\""\nFigure 1: I think \""passage embedding h\"" and \""question embedding v\"" boxes should be switched.\n- page 7 3.3 first para: \""evidence fully\"" -> \""evidence to be fully\""."
12189,1,\n\nSpecific comments and questions:\n\n-  Figure 1 is not clear.
12190,1, Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference.
12191,1,"   Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters."
12192,1,"\n\nOn the top of page 3: \""M is the matrix size\""."
12193,1, Do you use tweaks in the generation process.
12194,1," Using the generated images, paper reports improvement\nin classification accuracy on various tasks."
12195,1,"  But it is hard to determine whether the analysis is correct."""
12196,1," The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model."
12197,1, What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated?
12198,1,\n\nThe paper is very well-written and easy to follow.
12199,1, The authors have already provided several pages worth of additional comments on the website on further related work.
12200,1,"""The authors train an RNN to perform deduced reckoning (ded reckoning) for spatial navigation, and then study the responses of the model neurons in the RNN."
12201,1, Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance.
12202,1, Again I would ask the authors to make these plots for FSGM.
12203,1,Those clarification issues are important to address.
12204,1,"\n\nMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me."
12205,1," Here are two articles, one that provides a long and thorough discussion that is a definitive start to the literature, and another that is most relevant to the current paper, on applying pedagogical teaching to inverse reinforcement learning (a talk at NIPS 2016)."
12206,1, Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients.
12207,1,;\n - Assimilation methods are usually independent of the type of data at hand.
12208,1," The relevance of perceptual image realism to the intended task (control) is not substantiated, as discussed earlier."
12209,1," Gradient steps that optimize the encoder,\ndecoder and generator are interleaved."
12210,1,"\n- Regarding the optimization algorithm involving \\alpha^* at the end of section 3: It looks to me like a hacky, unprincipled method with no guarantees that just happened to work in the particular example you studied."
12211,1,  Still I give it a try.
12212,1," In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning."""
12213,1,\n* Section 7.1. It is not clear why degradation does not happen. 
12214,1, The proposed method works well on CIFAR and MNIST datasets.
12215,1," It is more like an application of these methods, and has limited contribution and novelty."
12216,1,"The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it."
12217,1,"\nFor example, having the gradient of both Q and V, as in (9), has been stated by [Haarnoja et al., 2017] (very similar formulation is developed in Appendix B of https://arxiv.org/abs/1702.08165)."
12218,1,"  It seems that this\napproach only works due to the peculiarities of the formulation of the only task that is considered,\nin which the program maps a pixel location in 32x32 images to a binary value."
12219,1, I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network.
12220,1," However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem."
12221,1," Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\"
12222,1, I would have also liked to see a more direct and systemic validation of the claims made in the paper.
12223,1,"\n\nOverall, the paper describes a computer visions system based on synthesizing images, and not necessarily a new theoretical framework to compete with GANs."
12224,1," The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning."
12225,1,"""The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized."
12226,1,"\n\n[D]: Kiros, Ryan, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. \u201cUnifying Visual-Semantic Embeddings with Multimodal Neural Language Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1411.2539.\n"""
12227,1,"""This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension."
12228,1,"  However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span. "
12229,1," From the second paragraph alone:\n\""impede their wider applications in new data domain\"" -> domains\n\""extreme collapse and heavily oscillation\"" -> heavy oscillation\n\""modes of real data distribution\"" -> modes of the real data distribution\n\""while D fails to exploit the failure to provide better training signal to G\"" -> should be \""this failure\"" to refer to the previously-described generator mode collapse, or rewrite entirely\n\""even when they are their Jensen-Shannon divergence\"" -> even when their Jensen-Shannon divergence\n I'm sympathetic to the authors who are presumably non-native English speakers;"
12230,1," \n\nSome minor comments:\nWhen applied to the rejected examples, wouldn't the ground truth # of clusters no longer be 4 or 10 because there are some known-class examples mixed in?"
12231,1,Is this data low rank? 
12232,1,"""Quality\nThe theoretical results presented in the paper appear to be correct."
12233,1, The second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or state.
12234,1,  Figure 5a seems to suggest that it would not appear (peaks appear to all have the same test accuracy).
12235,1," It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions."
12236,1," The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word)."
12237,1, It lacks a few references and important technical aspects are not discussed.
12238,1,"""This paper proposes a model for adding background knowledge to natural language understanding tasks."
12239,1,"""This paper try to analyze the intrinsic structure of VGG19 and give a new insight of deep neural networks."
12240,1," In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better."
12241,1, It is not easy to see what the assumption means.
12242,1," \n\nOn permuted MNIST, Table 2 could include results from [1-4]."
12243,1,"\n\n-Algorithm 1 is presented without any discussion about complexity, rate of convergence."
12244,1, Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space? 
12245,1,\n\nI am also concerned about the hyper-parameter tuning for the baselines.
12246,1,"""Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax."
12247,1," \n\n(3) Reproducibility. There are a lot of details missing; the setup is quite complex, but only partially described."
12248,1," Second, the model is built upon GAR until the elimination module, then according to Table 1 it seems to indicate that the elimination module does not help significantly (0.4% improvement)."
12249,1,  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.
12250,1," In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?"
12251,1," The authors provide yet another instantiation of such an approach, but this time with an LSTM."
12252,1," \n\nOn unconditional generation, your hypothesis on uncertainty is interesting and could be tested."
12253,1," For example, we\u2019re just told some values that hyperparameters were fixed at for both tasks - how were these chosen (including for the baselines)?"
12254,1," Their best results, however, do not outperform the best results reported on the leader board."
12255,1,"  \n3) Batch normalization is popular, especially for the convolutional neural networks."
12256,1," It's hard to get a clear overview of the previous research: datasets, methods and contextualization of the proposed approach in relation with previous work."
12257,1,. It seems that the methods outperforms existing methods for learning graph representations
12258,1, Indeed in the experimental section only those members are tested.
12259,1,"\n\n[1] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. A semantic matching energy function for learning with multi- relational data. Machine Learning 1\u201327."
12260,1," If the focus of the  paper is on  obtaining good entailment results, maybe an NLP conference can be a more suitable venue.\n"""
12261,1," \n\nFor instance, looking at the experiments, we can see that on MNIST-50k (the one with most data, and therefore, the one that best informs about the \""true\"" underlying NN structure) the inferred depth is 1 for the GP and 2 for the NN, i.e., not deep."
12262,1, but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds.
12263,1, The paper is completely inadequate in respect to related work; it re-invents known techniques like non-maximum suppression and matching for tracking; fails to learn convincing objects according to visual inspection; and fails to compare with earlier methods for these tasks.
12264,1, This may just be a matter of taste.\
12265,1," On the negative side, there seems to be relatively limited novelty: we can think of MSA as one particular communication (i.e, star) configuration one could use is a multiagent system."
12266,1," I strongly disagree with this statement, not only because the technique deals exactly with augmenting data, but also because it can be used in combination to any learning method (including non-deep learning methodologies)."
12267,1,\n(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation...
12268,1," For instance recently Wu et all in ICCV2017, bu also many other papers."
12269,1,"\n\n[1] White. Unifying task specification in reinforcement learning. Martha White. International Conference on Machine Learning (ICML), 2017."
12270,1,"\n\n(3) In Section 1.2 (Tracking a known system): \""given\"" instead of \""give\""[[CNT], [CLA-NEG], [CRT], [MAJ]]\n\n(4) In Section 1.2 (Optimal control): \""symmetric\"" instead of \""symmetrics\""[[CNT], [CLA-NEG], [CRT], [MAJ]]\n\n(5) In Section 1.2 (Optimal control): the paper says \""rather than solving a recursive system of equations, we provide a formulation of control as a one-shot convex program\"". Is this meant as a contrast to the work of Dean et al. (2017)?"
12271,1,\n\nSensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination.
12272,1,"\n- Sec 4.6: the authors refer to the \""order net\"" beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline."
12273,1, \n\nThere is also a remark that G(A) tends to be modular when lambda is small which is useful.
12274,1,"""In this paper, the authors propose a recurrent GAN architecture that generates continuous domain sequences."
12275,1," The authors by contrast, perform exact Bayesian inference, but\nonly on the last layer of their neural network."
12276,1," \n\nThe paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop."
12277,1," \n\nTypos and notation:\npage 7 last paragraph: \""We train the all\"" -> We train all\nnotation page 5: i find \\nabla_{\\theta_i} confusing when \\theta_i is a scalar, i would propose \\frac{\\partial}{\\partial \\theta_i}\npage 2: \""But this would come at the expense of long-term optimization process\"": at this point of the paper it is not clear how or why this should happen."
12278,1,"\n\nOverall, I think the current version of the paper is not ready for ICLR conference."
12279,1,"\n\nFirst, CBT: NE and CN numbers are too low."
12280,1," The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs."""
12281,1," In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules."
12282,1, In addition the notation of the modules in the figure is almost completely disjoint so that it is initially unclear which terms are used interchangeably.
12283,1," It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images"
12284,1, One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M)
12285,1,"\n\nA particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation."
12286,1,"  So are do the \""windows\"" correspond to spatial windows, and if so, how?"
12287,1, \n\nThe paper claims that operating on pre-structured data only (without using text) is an advantage.
12288,1,I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)?
12289,1,\n\n4. Is trade-off between 1 to 2 bits really important? 
12290,1,"It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail."
12291,1," but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset."
12292,1, How can the method make sure that  simple adding generated images with each component will lead to a meaningful image in the end?
12293,1," \n\nThis paper is concerned with both security and machine learning, but there is no clear contributions to either field."
12294,1,"\n\nThe connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. "
12295,1,"""In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train."
12296,1,. Right now the authors explain separation rank first and then discuss tensors / matricization).
12297,1,"  The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM."
12298,1, How does different initializations affect results?
12299,1,"\n\nOverall, the paper presents an interesting approach"
12300,1, The authors should explain more possibly.
12301,1," This choice seems to be appropriate, since standard methods are used."
12302,1,"\n\nOverall the paper is written quite well, and the authors do a good job of explaining their thesis."
12303,1," Second, I was very surprised that the inference network outperformed Viterbi (89.7 vs. 89.1 for the same CRF energy)."
12304,1, The reasoning here is that the image feature space may not be semantically organized so that we are not guaranteed that a small perturbation of an image vector will yield image vectors that correspond to semantically similar images (belonging to the same class).
12305,1,"  In general, it treats all object proposals as nodes on the graph."
12306,1," In other words, what is (and is there) a gap between an ensemble of a forward and backward rnn and the forward-rnn only, but trained with the state-matching penalty?"
12307,1,"For example I'm looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top-left but adding the attributes of choice."
12308,1,"\n\nThese results are interesting,"
12309,1," Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea."
12310,1,\n\nComments:\n- Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity?
12311,1,"  Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs."
12312,1," The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping)."
12313,1, Most of the training takes place without the skip connection.
12314,1,"\n\nI think the quality of the paper should be further improved by addressing these problems and currently it should be rejected."""
12315,1,\n\nPositive aspects:\n+ Emphasis in model interpretability and its connection to psychological findings in emotions
12316,1, The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.
12317,1," There are no clear insight, \nno theorems, and an empirical evaluation on an ill-defined problem in \ntime-series forecasting."
12318,1," It says \""By varying the number of layers and the number of nodes...\"", but the nodes and layer are not a part of the equation."
12319,1,\n\nQuality\nThe authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in Danihelka et al. (2016).
12320,1," \n\nRe: \u201cone creates additional samples by modifying\u2026\u201d be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf\u2019s, he called it \u201cvirtual examples\u201d and I\u2019m pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied)."
12321,1," However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit"
12322,1, A moderate improvement compared to other approaches is observed for all data sets.
12323,1,"\n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space)."
12324,1,"  Also, what does \""step size\"" mean in the TR method?"
12325,1," Unless I misunderstand the definition of CRPS and PLL, that width should matter, no?"
12326,1," In figure 1, pooling (strided convolutions) are not depicted between network stages."
12327,1,"\n\n## Relation to literature on \""randomized value functions\""\nIt's really wrong to present BDQN as is if it's the first attempt at large-scale approximations to Thompson sampling (and then slip in a citation to RLSVI as a BDQN-like algorithm)."
12328,1,"\n\nThey also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm."
12329,1,\n\np3. What does 'normalized' mean?
12330,1,"\n\nQ: The spatial grouping that is happening in the compositional stage, is it solely due to the multi-scale hypercolumns?"
12331,1," (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero."
12332,1, The loss function of Figure 2.
12333,1," \n\nIn page 1 ```existence proofs' without explicit constructions'' This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions. \n\n"""
12334,1,"\n\n3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones."
12335,1, Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)?
12336,1,"""This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible \nstrategies."
12337,1,"\n\nIn summary, the application domain of the theoretical results seems a bit restricted."
12338,1, There are lots more examples of unclear statements in this paper -- it should be heavily improved.
12339,1, Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines.
12340,1,"\n\n- Theorems 2.1, 2.2 and the observation (2) are nice!"
12341,1," Additionally, this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth."
12342,1, It seems to imply that VAN has skip connections.
12343,1," The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA."
12344,1, the form of equation (3) looks like an MMI criterion to me?
12345,1, Note that here I am assuming full technical correctness of the paper (and still cannot recommend acceptance).
12346,1," In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn\u2019t clear to me that this is the right baseline."
12347,1, I'm not an expert of the network inversion and not sure whether the related work of this part is sufficient or not.
12348,1,"It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results."
12349,1,"\n(ix) Usually, one should think of the Laplace approximation and the resulting Bayes factors more in terms of a \""volume\"" of parameters  close to the MAP estimate, which is what the matrix determinant expresses, more than any specific direction of \""curvature\""."
12350,1," The results seem to be worse than existing networks, e.g., DenseNet (Note that SVHN is no longer a good benchmark dataset for evaluating state-of-the-art CNNs)."
12351,1, The baselines are not particularly strong either.
12352,1,"It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted."
12353,1,\n\n1) k goes to infinity\n2) alpha goes to 1\n3) g(w*) goes to 0
12354,1,"""This paper presents an iterative approach to sparsify a network already during training."
12355,1, \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip.
12356,1,"  But the same efficient search is possible in many of the classic \""discriminatively-trained\"" KB completion models also."
12357,1," However, the authors claim that their method is the first to combine multi-frame video prediction with an adversarial loss, which is not true."
12358,1," There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\"
12359,1, The key part of the proof is showing a lower bound on the rank for networks with overlap
12360,1,"\n\n3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (\""it succeeds on 28% of the samples on MNIST;73% on CIFAR-10\"")."
12361,1,\n  That's surprising and the kind of thing that a scientific\n  investigation here should try to uncover; it's a shame to just put\n  up a table of numbers and not offer any analysis of why this works.
12362,1,"\n\n[1] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. \""Sparse deep belief net model for visual area V2.\"" Advances in neural information processing systems. 2008."
12363,1,"\n\n\nOriginality\n=========\n\nThis idea is not new: if we search for \""Lipschitz constant estimation\"" in google scholar, we get for example\nWood, G. R., and B. P. Zhang. \""Estimation of the Lipschitz constant of a function.\"" (1996)\nwhich presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull)."
12364,1, Some theoretical guarantees for the efficiency of reservoir sampling are provided.
12365,1,". In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance]"
12366,1,"\nFor example:\n\""the test example is correctly classified if and only if its ground truth matches C.\""(P5)"
12367,1, So it raises too concerns: First is it enough?
12368,1,"\""\n\nThe first paragraph is confusing since jumps from total correlation to correlation without making clear the differences."
12369,1,"""This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor."
12370,1,\n\n\n- No experiments
12371,1, \n\nTable 2:\nGMEZO win rates are low compared to the original publication.
12372,1,".\n\nIt appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. The authors should explore this behavior in greater detail."""
12373,1,"\nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply."
12374,1," There is no proof that it is always better, and I can't\nsee how there could be."
12375,1," However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$)."
12376,1,"\n- The toy example seems to be an example where Lagrangian relaxation fails, not where it succeeds."
12377,1,"""UPDATE: Following the author's response I've increased my score from 5 to 6."
12378,1," Indeed, SNN is a good choice for adding (Bayesian) context to a task."
12379,1, There are then of course two main questions to address (i) which states should be stored and how 
12380,1, The paper is not clearly written and there are several areas need to be improved.
12381,1,\n\nPROS: \nThe problem faced by the authors is interesting.
12382,1,"\n\nDepending on how the authors respond to the reviews, I would consider upgrading the score of my review."""
12383,1," It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. """
12384,1, Are long-range correlations irrelevant to the text style?
12385,1," The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. "
12386,1,"   Regardless, more discussion regarding what the Ax_i embeddings are meant to capture (in contrast to the Cx_i vectors) would be appreciated."
12387,1,.\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks.
12388,1,"\nThe proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people."
12389,1,  What is the conclusion from Figure 4(b)?
12390,1," But in this case, it is not."
12391,1,\n\nThe main issue of the paper is the lack of novelty.
12392,1,"  Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization."
12393,1,\n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. 
12394,1," Also, the authors should definitely show the grounding attention results of words and visual signal jointly, i.e., showing them together in one figure instead of separately in Figure 9 and Figure 10.\n"""
12395,1, The paper develops two models for the decision boundary:\n\n(a) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspace.
12396,1," Unlike deep neural networks, since RFF is such a simple model, I think providing precise theoretical understanding is crucial."
12397,1," \n\nThe method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. "
12398,1,"\nFirst paragraph of the experiments section: \u201dso that so that\u201d -> so that\n"""
12399,1, It\u2019s not immediately clear what the semantics of this posterior are then.
12400,1," \n\nPresumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej.\n\nMapping the learner and expert directions into subgoal space using Wv, the heuristic reward is\n\n   Rh = B <   Wv (S-S\u2019),  \n                    Wv SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 ) >"
12401,1," But the results  in Table 2 show that the TSTR results are quite a lot worse than real data in most cases, and it's not obvious that the small set of tasks evaluated are representative of all tasks people might care about."
12402,1, \nWhat many independent seeds where used for training?
12403,1, A few downsides are commented on below.\n\n
12404,1," References to classic weighted sampling are[[CNT], [CNT], [CNT], [CNT]] \n\n  The application is limited to certain loss functions for which we can compute LSH structures."
12405,1,How about cases when computing Z is intractable?
12406,1,"To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution)."
12407,1," As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable."
12408,1," However, there are several issues both in the approach and the \ncurrent preliminary evaluation, which unfortunately leads me to a reject score,"
12409,1," In large scale situations, working with the kernel matrix can be computational expensive."
12410,1," \n\nPros:\nThe paper is well written, the analysis interesting and the application of the Tucker2 framework sound."
12411,1,"\n\n- Below prop 4, discussing why not to make step (b) proximal, this was a bit vague to me.[[CNT], [CNT], [CRT], [MIN]] It would be nice to expand this[[CNT], [null], [DIS], [MIN]].\n\n- Page 6 near the top, to apply the operator, in the fully-connected case, this is just a matrix multiply, right?"
12412,1,"\n\nAnother minor comment: The legends in the figures are tiny, and really hard to read."
12413,1,\n\nThe paper does not write mathematically rigorous.
12414,1,  The idea is simple and well explained.
12415,1, The analysis is very thorough and the methods described may find use in analyzing other tasks.
12416,1,\n\nPros:\n- Generating programs with neural networks is an exciting direction
12417,1, \n\nIt will be better if the authors could provide more details in the methodology or framework section.
12418,1, Experiments on both convolutional and recurrent networks are used for evaluation.
12419,1,\n\nOriginality\n\nThere exist alternative deep predictive coding models such as https://arxiv.org/abs/1605.08104.
12420,1,"  From a couple of quick searches it seems like there are a few physics / chemistry applications where a symmetric A makes sense, but the authors don't do a good enough job setting up the context here to make the results compelling."
12421,1," The only explanation is that this method mimics the genetic algorithm. However, this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability."
12422,1,\n\n** original review **\n\n\nThe paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.
12423,1,"  It is quite common for everyday people to use emotion words this way e.g. using #love to express strong approval rather than an actual feeling of love.[[CNT], [EMP-NEU], [DIS], [MIN]]   \n\nIn their analysis the authors claim:\n\u201cThe 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale (Watson & Clark, 1999)\u201d."
12424,1," If yes, please update 6.1.1 to make this distinction more clear."
12425,1,"\n-- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015.\n-- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016."
12426,1,"\n\nFor analysis, the authors use t-tests directly on KL-divergence and accuracy scores; however, this is inappropriate (see Jaeger, 2008; Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models."
12427,1," It is important to understand, why there is such a big drop in performance in one-shot learning comparing to zero-shot learning?"
12428,1,\n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?
12429,1," Especially the second half of page 5 is at times very hard to understand as it is so dense.[[CNT], [PNF-NEG], [CRT], [MAJ]] \n- The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_\\Phi, C^\\mathcal{F}_{n,\\delta} and D^{\\Phi,\\mathcal{H}}_\\delta."
12430,1, though they do not harm the readability of the paper
12431,1, This happens a lot in Sec. 5.2.
12432,1, Using beam search on tree outputs is a bit of a minor contribution.
12433,1,"""The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization."
12434,1," The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed."
12435,1,"""This paper presents a method for matrix factorization using DNNs."
12436,1,"\n- The empirical evaluation is done on intuitively related, superficially unrelated, and a real world\n  task."
12437,1," Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully-connected neural networks conditioned on the last recurrent hidden state."
12438,1,"\n\nFirstly, in federated learning, each client independently computes an update to the current model based on its local data, and then communicates this update to a central server where the client-side updates are aggregated to compute a new global model."
12439,1,"\n\nHowever, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al."
12440,1,\n\nThe idea of employing ensemble of classifiers is smart and effective.
12441,1,"   Either human training data showing very effective generalization (if one could somehow make \""novel\"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN."
12442,1,". However, if this is the case, a simple solution would be to move the server to a different node."
12443,1,\n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme?
12444,1," \n\nThe paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016)."
12445,1,"  The linear models do very well, which means it should be\n  possible to look at the magnitude of the weights."
12446,1,"  Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)\n\n"
12447,1,"\n\n== Technical Quality == \nFirst, it is not clear how was the proposed annihilating operation used in the experiments (there is no explanation in the experimental section)."
12448,1,"\n\nWith respect to the rest of the paper, the level of novelty and impact is \""ok,"
12449,1," This was\nalready shown for negative sampling in the paper of Melamud et al. in\nEMNLP 2017. Therefore, nothing new here, the difference is slight."
12450,1,"  My problem with that paper is that even though at first glance learning adaptive feature maps seems to be an attractive approach, authors' contribution is actually very little."
12451,1," The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy."
12452,1," \n\nFor chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful."
12453,1," Not surprisingly, as this framework is able to learn from way more data (e.g. in Atari), it outperforms the baselines, and Figure 4 clearly shows the more actors we have the better performance we will have. "
12454,1," Then,  PACT is combined with quantizing the activations."
12455,1," \n\nThe datasets used by the authors are balanced, so they artificially transform them into long-tailed,"
12456,1, The description of the architecture is confusing with design choices never clearly explained.
12457,1,"\n \nI was able to work out the intuitions behind the heuristic rewards, but I still don\u2019t clearly get \nwhat the Q-value factorization is providing:"
12458,1,"\n\n--------------\nWeaknesses:\n--------------\n\n- Perhaps I'm missing something, but shouldn't the Single EN-DE/DE-EN results in Table 2 match the not pretrained EN-DE/DE-EN Multi30k Task 1 results? I understand that this is perhaps on a different data split into M1/2 but why is there such a drastic difference?"
12459,1,"n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width). "
12460,1,\n\n- How reliable are the interpretations?
12461,1," More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights."
12462,1," In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem."
12463,1," The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7)."
12464,1," Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input?"
12465,1,"""This paper presents a method to search neural network architectures at the same time of training."
12466,1, Could the authors also indicate the CER of the model used for initialization in addition to the final CER after stage-2 training in Figure 5.
12467,1,\n\nAnd the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth)
12468,1, The main goal is to quantify mode collapse in state-of-the-art generative models.
12469,1, L is defined for 3rd order tensors only;  how is the extended to n > 3?
12470,1,The practical way in which the adversarial examples are generated is not known to the player.
12471,1,"\n6 .In the evaluation, why are just 12 tasks used in the Amazon dataset?"
12472,1," In particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper."
12473,1," A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}."
12474,1, but it is a good idea to speed up the RC models.
12475,1,. It is interesting to add the memory cost per channel into the optimization process. 
12476,1, Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled.
12477,1,". To improve the paper, it would be helpful to evaluate the method under various settings."
12478,1,\n\nResults for linear networks are not an improvement over existing works.
12479,1," \n\npage 10:\n- Figure 6: this example should be more carefully described in terms of distribution, f*, etc."
12480,1,"\n\""To enjoy the best of both worlds, we also introduce a \u201chybrid\u201d method in the Figure 3, that is, first run TR method for several epochs to get coarse solution and then run SGD for a while until fully converge."
12481,1," It also should be shown on more than a single dataset and for a single network, at the moment this is more of a workshop level paper in terms of breadth and depth of results."""
12482,1, \n\nThe authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding.
12483,1, The main conclusion is intuitive.
12484,1,"\n\n-p5, beginning of Section 6.1:  \""to regularize the model produce\"" --> \""to regularize the model to produce\"" ?[[CNT], [CLA-NEU], [QSN], [MIN]]\n-p6, end of first par. \""is quite high for the ARAE than in the case\"" --> quite a bit higher than?[[CNT], [CLA-NEU], [QSN], [MIN]] etc...\n-p7, near the bottom \""shown in figure 6\"". --> table, not figure...\n-p8  \""ability mimic\"" -->\""ability to mimic"
12485,1," Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop."
12486,1,\n\nChapter 5 takes a stochastic differential equation as a starting point.
12487,1," \n\nAs far as the method is described, I believe it would be impossible to\nreproduce their results because of the complexity of the hyper-parameter tuning\nperformed by the authors."
12488,1,\n- Quantitative improvement with respect to the state the art.
12489,1, The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix.
12490,1, \n\n\nFigure 4:\nB) What does it mean to feed two vectors into a Tanh?
12491,1, \n- I\u2019m a little skeptical about how often this method would really be useful in practice.
12492,1,"2) It would be easier for discussion if the authors could assign numbers to every equation."""
12493,1," These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such."
12494,1, Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough.
12495,1, I could not understand how such analysis contributes advance of representation learning.
12496,1," \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n"""
12497,1," \"" What else does it depend on?"
12498,1, but does not appear to be explored in any depth.
12499,1, Using second order methods is not an end in itself.
12500,1,"\n\nAfter observing an experience ( S,A,R,S\u2019 ) we use Bellman Error as a loss function to optimize Qp for parameter p."
12501,1,"\n\nThe paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time."
12502,1, \n\nMy first concern is about the architecture description.
12503,1," Is it the \""Mesh Upsampling\"" operation defined at the end of page 4?"
12504,1,"  The implicit steps have the advantage that the choice of step-size is replaced by a choice of a proximity coefficient, which the advantage that while too large step-size can increase the objective, any value of the proximity coefficient yields a proximal mapping guaranteed to decrease the objective."
12505,1, Though the proof of Lemma 2 only appears to be using the 1-Lipschitzness property of phi as well as phi(0) =0. (Unless they can generalize further; I also suggest that they explicitly state in the (interesting) Lemma 2 that it is for the ReLU activations (like they did in Theorem 1)).
12506,1," For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment).\"
12507,1,"  The robustness term rho is essential, but it is not defined."
12508,1, The discussion here can be useful for other researchers.
12509,1,". If the aim is to have realistic samples, a visual turing test is probably the best metric."
12510,1,"  I would like the authors to consider the following questions - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few)."
12511,1,\n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section).
12512,1," \n\n[1] D.D. Johnson, Learning Graphical State Transitions, ICLR 2017\n[2] R. Stewart, M. Andriluka, and A. Y. Ng, End-to-End People Detection in Crowded Scenes, CVPR 2016"
12513,1," The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task."
12514,1, \n\nExample 2 from intro when comparing with other results on page 2:\nThe authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity).
12515,1, The proposed data augmentation is a general one and it can be used to improve the performance of other models as well.
12516,1,"""This paper introduces a method to learn a policy on visually different but otherwise identical games."
12517,1,\n\n+ novel loss
12518,1," Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice; multi-step updates, regularized against MC returns, stochastic mirror descent."
12519,1,"""Paper proposes a weak synchronization approach to synchronous SGD with the goal of improving even with slow parameter servers."
12520,1, I don\u2019t know whether the low-rank structure does exist in the cross-task transfer performance or not.
12521,1,"\n\n[1] Wang, Weiran, Honglak Lee, and Karen Livescu. \""Deep variational canonical correlation analysis.\"" arXiv preprint arXiv:1610.03454 (2016)."
12522,1,"\n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks."
12523,1,"""This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences."
12524,1," Currently, the improvement is only marginal,"
12525,1," I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer."
12526,1, But it is somewhat unfocused and does not seem make a clear contribution to any of these.
12527,1, The framework is evaluated on a relevant multi-task problem.
12528,1," \n\n\""satisfies task the specification)\"" -> \""satisfies the task specification)\"".[[CNT], [CLA-NEG], [CRT], [MIN]] \n\nFigure 4: Tasks 6 and 7 should be defined in the text someplace."
12529,1,  \n\nMajor\n- the medical use case is not motivating.
12530,1," \n\n6) Besides IHDP, did the authors run experiments on other real-world datasets, \nsuch as Jobs, Twins, etc?"""
12531,1,"\n\nThe experimental results presented in this paper are quite good,"
12532,1," \nHowever, the experiments are too weak to demonstrate the effectiveness of using discrete representations."
12533,1, Does the proposed approach still show gains over Attend Infer Repeat?
12534,1,"\n\n4. The results are reported mostly concerning the training iterations, not the CPU time such as in figure 3."
12535,1,n\nThey evaluate the methods on several metrics.
12536,1,\n\nPros\n- \u201cMonolithic\u201d policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable.
12537,1,"  \n\nFirst, please systematically compare your methods with existing methods on the widely adopted benchmarks including MNIST with 20, 100 labels and SVHN with 500, 1000 labels and CIFAR10 with 4000 labels."
12538,1, Which forward model is trained from which model-free agent?
12539,1, The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1).
12540,1,\n\nI have the following concerns:\n\n1) The paper is not self-contained.
12541,1, It can be trained with source sentences having various styles and it can produce sentences in a different style without changing the content much.
12542,1,"\n- In Eq 4, |C_i || y_j| seems a strange notation for union."
12543,1, \n\nChapter 2 provides a sort of a mini-tutorial to (Bayesian) model selection based on standard Bayes factors.
12544,1,"""This paper analyzes the loss function and properties of CNNs with one \""wide\"" layer, i.e., a layer with number of neurons greater than the train sample size."
12545,1,\n\nAdditional comments:\n\n- Figure 2b: this visualization is confusing.
12546,1,"""The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks."
12547,1," In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE."
12548,1,"\n\u20282- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense."
12549,1,.\n\nThe paper is generally clear and well written.
12550,1," but the presentation is too fuzzy to get it evaluated."""
12551,1,How many runs for evaluation?
12552,1," Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers."
12553,1,"\n\nOverall, I think it is a nice demonstration that non-recurrent models can work so well,"
12554,1, the paper is well-written and the results are strong;
12555,1,"""In this paper, the authors present a computational framework for the active vision problem."
12556,1,\n\nThe analysis of the phases in the hyperparameter space is interesting and insightful.
12557,1, One could do a method that completely distort the image and therefore will be classified with as a class.
12558,1, The proposed approach aims to overcome these drawbacks.
12559,1, The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data.
12560,1, whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures.
12561,1," Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do."
12562,1, Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results.
12563,1," It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail."
12564,1, \n\n**EVALUATION AFTER AUTHORS' REBUTTAL**\nThe reviewer has read the responses provided by the authors during the rebuttal period.
12565,1," Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units."
12566,1,"\nThere are T different models, one for each prefix y_{1:t} of length t."
12567,1,"\n3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space."
12568,1,"The best parameters found by Harmonica improve over the hand-tuned results for their \""base architecture\"" (ResNets).\"
12569,1,. The revised version does address my concerns.
12570,1," \n\u2022\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time."
12571,1,"""The authors provide a novel, interesting, and simple algorithm capable of training with limited memory."
12572,1, It\u2019s better to train some semi-supervised model to make the settings more comparable.
12573,1," The authors of this paper simply write \""Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty."
12574,1, How does your formalism take this into account?
12575,1," In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational."
12576,1,"\n- Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT."
12577,1,"\n(ii) While it is commonly done, it would be nice to get some insights on why a Gaussian approx. is a good assumption."
12578,1,"  In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach."
12579,1," Also, on a related note, in Figure 2, does each point correspond to a specific choice of these regularization parameters?"
12580,1,  Soft unitary constraints also have been introduced in earlier work (citations are also in the paper).
12581,1,"\n\n## Minor\n- I challenge the claim that thousands and millions of time steps are a common issue in \u201crobotics, remote sensing, control systems, speech recognition, medicine and finance\u201d, as claimed in the first paragraph of the introduction."
12582,1,There is a literature which follows on from the F&J paper.
12583,1,"\n\n1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong."
12584,1," The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms."
12585,1," \nUsing the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation."
12586,1,\n\nClarity:\nThe paper is lucidly written and very understandable.
12587,1, This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization.
12588,1," (2) the proposed CNN architecture, combining images and text (using word embedding."
12589,1," \n\n*Originality + significance*\n\nTo my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs."
12590,1," The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively."
12591,1, \n\nThis paper is well-written and easy to follow.
12592,1,"  As described in Section 4, the possibly evolutionary changes are limited to deepening the network, widening the network, and adding a skip connection."
12593,1, How you use IBM model for supervision.
12594,1," \n\n3. Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained?\n\nThanks."""
12595,1," On the downside, all these experiments concern predictive (discriminative) problems."
12596,1," To validate such theoretical result, a non-deep-learning model should be adopted."
12597,1, \n\nSummary: \nA new stochastic method based on trust region (TR) is proposed.
12598,1," First, it is difficult to see the different of the image generated using deconv and PixelDCL."
12599,1, Maybe finding better examples where the benefits of the proposed regularization are stressed could help.
12600,1," Broadly, the approach is to have a generator produce the \""full\"" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples."
12601,1, \nIn some domains this can be a much better approach and this is supported by experimentation.
12602,1," Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule."
12603,1,"\n- The standard units that most ASR systems use can be called \""senones\"", and they are context dependent sub-phonetic units (see http://ssli.ee.washington.edu/~mhwang/), not phonetic states.[[CNT], [null], [DIS], [MIN]] Also the units that generate the alignment and the units that are trained on an alignment can be different (I can use a system with 10000 states to write alignments for a system with 3000 states) - this needs to be corrected."
12604,1," Although DReLU\u2019s expectation is smaller than expectation of ReLU, but it doesn\u2019t explain why DReLU is better than very leaky ReLU, ELU etc."
12605,1,\n\nThe plots in Figure 2 include performance in terms of episodes.
12606,1,and (iii) the significant risks associated with training with a physical vehicle;
12607,1," Unfortunately, this seems to be a simple idea that doesn't work as well as the simpler idea (ResNets) that inspired it."
12608,1," However, the experiments to support the idea do not seem to match the motivation of the paper."
12609,1,\n- The authors say they use gameplay from no later than 11 minutes in the game to avoid the difficulties of increasing variance.
12610,1,"  Furthermore, how could FAME advance the previous state-of-the-art?"
12611,1,"\n\n2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects."
12612,1,"  If the emphasis of this conclusion is on  the number of images needed, then it will be good to show more analysis on Figure 6: e.g. Why does the accuracy curve drops to zero before going up?"
12613,1,"n\nI did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one?"
12614,1,"\n- Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further."
12615,1, The results are moderately convincing in favor of the proposed approach.
12616,1, \n\n- Figure 1 is helpful to clarify the main idea of a VHE.
12617,1,"""This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI.\"
12618,1, The use of a straight-through estimator allows the model to be trained with standard backpropagation.
12619,1,\n\nThe experimental numbers look skeptical.
12620,1, \n- The results are not fully-convincing.
12621,1,"\n\nShafto, P., Goodman, N. D., & Griffiths, T. L. (2014). A rational account of pedagogical reasoning: Teaching by, and learning from, examples."
12622,1," \n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised."""
12623,1,"""The authors present a testing framework for deep RL methods in which difficulty can be controlled along a number of dimensions, including: reward delay, reward sparsity, episode length with terminating rewards, binary vs real rewards and perceptual complexity."
12624,1," On the other hand, although the experiments are well designed and illustrative,"
12625,1,  The next utterance is then generated based on this dialog act.
12626,1,"\n\nAs a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack."
12627,1,"\n\n\nConclusion:\n\nSince RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem."
12628,1," As a non-expert in the field, I'd expect that ordering between pathologic patterns matters more."""
12629,1," The practical aspects are also interesting, because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains; these ideas could clearly be developed in future work."
12630,1,\n- The template seems off
12631,1, Better discussing the strong theoretical assumption should be incorporated.
12632,1, \n\nMy only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017).
12633,1,"""In this paper, the authors present an adaptation of space-by-time non-negative matrix factorization (SbT-NMF) that can rigorously account for the pre-stimulus baseline activity. "
12634,1,"""The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate"
12635,1, \n(3) TD methods can outperform pure MC methods when the return is mostly dominated by the reward in the terminal state.
12636,1,"\n\n- Another claim that is made is that complex numbers are key, and again the argument is the need to span the entire space of unitary matrices, but the same comment still hold - that is not the space this work is really dealing with, and no experimental evidence is provided that using complex numbers was really needed."
12637,1,"""The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal."
12638,1,\n\n- Is there a particular reason why the central points appears in both complementary kernels (+ and x)?
12639,1, Just because such a baseline wasn't previously proposed in literature (in the narrow scope of this problem) doesn't mean it's not an obvious baseline to try.
12640,1,"\n\nAs a minor remark, please make figures readable also in BW."
12641,1, The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.
12642,1," For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on."
12643,1,  The authors use only two datasets comprising only a few thousand data points (and hence the test sets comprise 500-1000 instances).
12644,1,\n\nClarity: Below average.
12645,1, \n\nReview - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. 
12646,1," For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy."
12647,1,\n\nPros:\n- very easy to follow idea and model
12648,1,"\n- It is unclear wether OCN requires/uses unseen class examples during training.\n- Last paragraph of 3.1 \""The 1-vs-rest ... rejected\"", I don't see why you need 1vsRest classifiers for this, a multi-class (softmax) output can also be thresholded to reject an test image from the known classes and to assign it to the unknown class."
12649,1," Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   """
12650,1," Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned."
12651,1,"4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper. "
12652,1,"  These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example)."
12653,1, The proposed method can jointly learn latent features and the cluster assignments.
12654,1,\n\nI think finding analogies that are not exact matches is much more compelling.
12655,1,\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016.
12656,1,"  Second, the relationship to existing work needs to be explained better.\n\n"
12657,1,"\n\nRegarding the former, I could not find any claim that the methods in (Mirowski et al, 2017) learn mapping and path planning in unseen environments, that could support claim [a]."
12658,1, They only compare it to a method named CORAL and to Typical\nVariation Normalization (TVN).
12659,1," The most useful part of the paper is the empirical evidence to\nbackup this claim, which I can't easily find in previous literature."
12660,1,"\u201c This is not true; rather, TD minimizes the mean-squared project Bellman error."
12661,1," As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above."
12662,1, The time and sample complexities of this approach are polynomial in all relevant parameters.
12663,1,"\n-Use of ACOL and GAR is interesting, also the idea to make \""labeled\"" data from unlabelled ones by using data augmentation."
12664,1,"""The paper is motivated with building robots that learn in an open-ended way, which is really interesting."
12665,1,\n\n(Cons)\n1. The authors overclaim to be state of the art.
12666,1," Yet, it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble."
12667,1, Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.
12668,1,"\n\nSome issues:\n- submission is supposed to be double blind but authors reveal their identity at the start of section 2.1.[[CNT], [null], [CRT], [MAJ]]\n- implementation details all over the place (section 3. is called \""Implementation\"", but at that point no concrete idea has been proposed, so it seems too early for talking about tensorflow and keras)."
12669,1, Here the covariates are morphological tags such as part-of-speech tags of the words.
12670,1, More experiments with different number of layers and different architecture like ResNet should be tried to show better results.
12671,1,"""This paper proposes an idea to do faster RNN inference via skip RNN state updates."
12672,1," The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks."
12673,1," Also, if this test is diagnostic, why use X-rays for diagnosis in the first place?"
12674,1, So I assume there are multiple steps between s and s\u2019?).
12675,1,"Overall, even though the architecture is not very novel,;"
12676,1, \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though.
12677,1,\n\n\nMinor:\n\nThe work is first introduced as multi-layer but only the single hidden layer case is actually discussed.
12678,1,\n\n- The presentation can be improved.
12679,1,"\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?"
12680,1, it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. 
12681,1,"\n\nThat said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets"
12682,1,"\n- There are many small errors in syntax;  it would be best to have this paper carefully proofread."""
12683,1," \n\nFor instance, the use fertility supervision during training could be better motivated and explained."
12684,1, It would be much better evaluated thought a mechanical turk test.
12685,1, This is not right.
12686,1,\n\nThe network architecture seems to be arbitrary and unusual.
12687,1, There seem to be inconsistencies\n  in the notation.
12688,1, This seems like an important baseline to report for the image caption ranking task.
12689,1," From the context, I guess the authors mean \""empirical training distribution\""?"
12690,1," In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains."
12691,1,\n\nThe proposed approach leverages recent work that gives a novel parametrization of control problems in the LDS setting.
12692,1,"\n \n7) I haven\u2019t had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done!"
12693,1, You optimize the coefficients of a polynomial. Did you try anything else?
12694,1," \"" It is my understanding that Pytorch support higher order derivative both for ReLu and Max-pooling."
12695,1, \n- The experiments show the better behavior of the method compared to adversarial training for domain adaptation
12696,1,". Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal."
12697,1, Each HMM emits an unobserved output with an explicit duration period
12698,1," It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right?"
12699,1,"""In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. "
12700,1," The technical proofs of the paper are in appendices, making the main text very smooth."
12701,1,"\n\nAs the main idea and the proposed model is simple and intuitive, the evaluation is quite important for this paper to be convincing."
12702,1,\n\nA deep architechture is proposed to solve the problem: see fig 1.
12703,1,\nCan the model recover richer structure that was imposed during data generation?
12704,1,"""The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant."
12705,1,  Results are also written in a confusing way as stating each critical point is a saddle or a global minima.
12706,1,"  However, the contribution is relatively minor here."
12707,1,". Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet."
12708,1,. Therefore the main advance is in terms of learning speed to obtain this similar performance.
12709,1, Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA?
12710,1," Second, the sentences are simple and non-realistic.[[CNT], [CNT], [CRT], [MIN]] Third, it is not used widely in the literature, therefore no benchmarks exist on this data."
12711,1, If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC?
12712,1,"\n(5). In section 4.2.2, the authors write \""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\""."
12713,1,"\n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques."
12714,1,\n\nCons:\n- I am not sure what is novel in the proposed model.
12715,1," \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels."
12716,1, The results are convincing to me.
12717,1, Joining SRM with MetaQNN is interesting as the method is a computation hog that can benefit from such refinement.
12718,1," Therefore, the motivation of the paper may make more sense if the proposed method is applied to a different NLP task."
12719,1,\n\nI really enjoyed reading this paper.
12720,1," \n- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the \""leave-one-out\"" version of the estimation."
12721,1, \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf .
12722,1,"  And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance."
12723,1," I am not aware of any use-cases, but if there are some, the authors should describe the rationales at length in their paper."
12724,1,"\n* In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer?"
12725,1,"  \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search."
12726,1, They discuss in detail some examples where tighter variational bounds in state-space models lead to worse parameter estimates (though in a quite different context and with a quite different analysis).
12727,1, Are higher or lower\n  values better? Maybe highlight the best scores for each column.
12728,1," The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. "
12729,1, For these reasons I \nbelieve the paper has sufficient merits to be published at ICLR. 
12730,1," More generally, we want options built under a given policy to reduce the diffusion time of other policies (preferably ones that lead to good control performance)."
12731,1," There are quite a few grammatical errors and typos which, unfortunately, can make the text difficult to comprehend in parts."
12732,1, but without more insights it's difficult to judge how generally useful they are.
12733,1,"""This paper studies a new architecture DualAC."
12734,1,\n\nThis is an interesting work. 
12735,1," We don\u2019t need to see SOTA performance to get some useful insights in comparing for example parametric vs non-parametric, but 40% more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices."
12736,1," In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training."
12737,1," This causes a wrong temporal credit assignment, because TreeQN can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions."
12738,1,\n6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments?
12739,1," \n\nWeaknesses:\nThe paper gives the impression to be rushed, i.e. there are citations missing (page 3 and 6), the encoder model illustration is not as clear as it could be."
12740,1,\n\nPro:\n1. Challenging and relevant problem solved better than other approaches.
12741,1," See, for example: https://arxiv.org/pdf/1711.00489.pdf.]\n3. Hybrid method is even better."
12742,1," Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. "
12743,1,"  Further, typical program synthesis approaches don't explicitly learn to produce correct syntax."
12744,1," Thus, it is not possible to elicit from the paper to which extent they are novel or how they are related..."
12745,1,\n\nCLARITY: The paper is very well written and is easy to follow.
12746,1, \n\nIt makes sense that the more flexible model proposed by this paper performs better than previous models.
12747,1,\n\nThe paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE.
12748,1," Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans."
12749,1,\n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p.
12750,1, The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters.
12751,1, The experimental results are also disappointing.
12752,1," The model does not seem to generalize to completely new action and commands (like Jump),"
12753,1,\n\n-----------\n\nThis paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling.
12754,1,\nMakes a strong case that random noise injection inside conditional GANs does not produce enough diversity
12755,1,"""Strengths:\n\n-\tThere is an interesting analysis on how CNN\u2019s perform better Spatial-Relation problems in contrast to Same-Different problems, and how Spatial-Relation problems are less sensitive to hyper parameters."
12756,1,\nThe experiments do not really convey how well this all will work in practice.
12757,1,"\n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \""is given in Figure 1\"" -> \""is given in Algorithm 1\""\n\n"
12758,1,"""\nThe authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL)."
12759,1,n2) When empirical isolated saddle points are close to true isolated saddle points
12760,1," There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way."
12761,1, Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function.
12762,1,"\n\nIn the experiments the authors compare with classical backpropagation, but they do not compare with \nthe explicit step of Carreira-Perpinan and Wang?"
12763,1,\n+ The idea is very interesting. 
12764,1, Given this and the other reviews I have bumped up my score from a 5 to a 6.
12765,1,\n\nMy comments / feedback: \n\nThe paper is well written and the problem addressed by the paper is an important one.
12766,1,"  That would make sense, given that then the dimensions of the vector f_p is a scalar (activation value) per image for that image, in layer k, around pixel p."
12767,1,"  While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step."
12768,1," I'm concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g. for optimization reasons)."
12769,1, \n\nThe difference between STB and the proposed method is not clear without reading the reference.
12770,1,"\n- We are told that the encoder's output is l2-normalized but the generator's\n  is not, instead output units of the generator are squashed with the tanh\n  activation."
12771,1," I think the paper is clearly written, and has some interesting insights."""
12772,1,"""The paper is clear and well written."
12773,1,"  \n\nThen a policy network is trained with deep Q learning whose architecture takes into account the objects in the scene, in an order agnostic way, and pairwise features are captured between pairs of objects, using similar layers as visual interaction nets."
12774,1,"\n\nOverall, the paper is sloppily put together, so it's a little difficult to assess the completeness of the ideas."
12775,1," From the methodology point of view, such extensions are relatively straightforward."
12776,1,"""This article tackles the extraction of sentiments at a fine-grained level."
12777,1, This will make the comparison much more clear.
12778,1,.\n\nI think this paper should be accepted as it is interesting and novel
12779,1,\ The plots suggest that the optimization has stopped earlier for some models.
12780,1, I am looking forward to a clarification in the rebuttal period.
12781,1,"\n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models."
12782,1," What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs."
12783,1,\n4. The experimental results are not convincing.
12784,1," In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation."
12785,1," It is presented well (modulo the above problems), and it makes some strong points."
12786,1," By \""circumscribed in two image domains\""?"
12787,1,"\n- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future."
12788,1," It is difficult to compare the goal image and the video otherwise. ""  "
12789,1," Once they\nhave been calculated in Algorithm 1, how are they used?"
12790,1," Whereas it is true that psychological studies rely on self - filled questionnaires, comparing a questionnaire (produced by expert psychologist) to the tags provided by users in a social network is to ambitious. (in some parts the authors make explicit this is an approximation, this should be stressed in every part of the paper)\n"""
12791,1,"""This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]."
12792,1, Examples of missing details are: how are the high-reward trajectories filtered?
12793,1,"""The authors introduce a sequential/recurrent model for generation of small graphs."
12794,1,"""The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \""standard\"" form and then into their correct morphological form,"
12795,1,"\n-p9 near the bottom \""The model learns a improved\"" --> \""The model learns an improved\"""
12796,1," It is not in depth to assume \""any known LSH scheme\"" in Alg. 2."
12797,1,"""=======\nUpdate:\n\nThe new version addresses some of my concerns."
12798,1, This generated image is then sent to the discriminator.
12799,1," While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc... "
12800,1," Fair enough, but \""gradient computation and storage\"" is not mentioned in the paper."
12801,1," \n\nQuality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs. \n"""
12802,1,"This is because they used the wrong statistical tests for analyzing the studies and it is unclear whether their results would stand with proper tests (I hope they will! \u2013 it seems clear that random samples will be harder to learn from eventually, but I also hoped there was a stronger baseline.)."
12803,1, \n* Formatting of figure 8 needs to be fixed.
12804,1,"\n\nTo begin with, what is the loss function of which (9) and (10) are its gradients?"
12805,1," Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting."
12806,1,"""This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU)."
12807,1," \n\nTo sum it up, it is a solid submission,"
12808,1,\n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence.
12809,1,  They show improved performance on a number of multi-task learning problems.
12810,1," \n\nOverall, I think for a more sincere evaluation, the authors need to better pick tasks that clearly exploit 3-way information and compare against other methods proposed to do the same."
12811,1,"Lillicrap et al. (2015) benchmarked against 27 tasks, Houtfout et al. (2016) compared in the paper also used Walker2D and Swimmer (not used in this paper) as did [2], OpenAI Gym contains many more control environments than the 4 solved here and significant research is pursing complex manipulation and grasping tasks (e.g. [3]). This suggests the author's claim has already been widely heeded and this work will be of limited interest."
12812,1,"\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description."
12813,1," For cheap emulators and fully deterministic games (Atari) this assumption holds, but in general restoring expensive, stochastic environments to some state is hard (e.g. robot arms playing ping-pong, ball at given x, y, z above the table, with given velocity vector)."
12814,1,"""The majority of the paper is focused on the observation that (1) making policies that condition on the time step is important in finite horizon problems, and a much smaller component on that (2) if episodes are terminated early during learning (say to restart and promote exploration) that the values should be bootstrapped to reflect that there will be additional rewards received in the true infinite-horizon setting.\n\n1 is true and is well known."
12815,1,"The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention"
12816,1, Results are promising.
12817,1,"  \n- It seems there are only two styles in Yelp dataset, and it's not clear how many styles are there in the \""On Chat\"" dataset."
12818,1, Table 1 has two sub-tables: left and right. The sub-tables have the AP column in different places.
12819,1, See for example the first paragraph of the Related Work.
12820,1, The given score will improve if the authors are able to address the stated issues.
12821,1," It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and \""confirmation biases\"" in the model."
12822,1,"""This paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained CNN on images within a same category."
12823,1,  Is there a particular reason for this?
12824,1, It appears that introducing the skip connection was next to useless and the model failed to recognize the usefulness of the skip connection early in training.
12825,1,"  Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy,;"
12826,1,"   Hence, it is not an explanation for not using ReLu and Max-pooling. Please clarify"
12827,1,\n\nThe paper is well-written and easy to follow.
12828,1," Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW."
12829,1,". Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models."
12830,1,but I find lack of description of a key topic.
12831,1,"\n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector?"
12832,1,\n\nTwo small concerns:\n1. In Section 3.3. I am not fully sure why the proposed predictor model is able to win over LSTM.
12833,1, Without that context it's hard to tell how broadly useful these results are.
12834,1," I don\u2019t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data."
12835,1,"\n5.\tIn the third phase of data collection (Paraphrase RC), was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2, or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3?"
12836,1," Some sentences need rewriting (e.g. in 4.1 \""which is as well used by Hendrycks...\"", in 5.2 \""performance becomes unchanged\"")."
12837,1,"""The authors propose a new algorithm for exploration in Deep RL."
12838,1, Is it an MDP or poMDP? 
12839,1,\n\nThis paper is quite original and clearly written.
12840,1,".\n\nThe results a are convincing, even if they are not state of the art in all the trials."
12841,1," However, there is nothing enforcing this and the gains just probably come from having more parameters/complexity."""
12842,1," However, I have the following concerns about the quality and the significance:\n- The proposed formulation in Equation (2) is questionable."
12843,1, The experimental results supports the claim.
12844,1," I can see the benefit of trainable approach here,"
12845,1, In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference.
12846,1, \nIt could be also interesting to (geometrically) interpret the coupling proposed.
12847,1," Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning."
12848,1,   The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) and therefore by just placing the inducing points in a dense grid gives a fast and accurate representation/compression of all GPs in terms of the inducing function values (denoted by U in the paper).
12849,1,"\n\n- Poor experimental validation\n\nWhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised manner, "
12850,1,"n\n2. Strong human prior, network morphism IV is more general than skip connection, for example, a two column structure belongs to type IV."
12851,1, How important is # of layers and residual connections?
12852,1, As a\nconsequence results on decoding learning task using low resolution\nimages can end up worse than with the actual data (as pointed out).
12853,1,"""General comment\n==============\nLow-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction performance."
12854,1," A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way."
12855,1,\n- Using p() as an approximate distribution in Section 3 is confusing notation
12856,1, A fully connected layer is used to fuse the multimodal information.
12857,1,\n\nSuggested References:\nBahdanau et al. (2016) An Actor-critic Algorithm for Sequence Prediction.
12858,1, Have you estimated the probability for positive vs. negative gradient values for  K=10?
12859,1," For example, the authors defer many technical details."
12860,1, I do find it hard to trust papers which do not compare with results from other papers.
12861,1,"""The authors propose a strategy for compressing RNN acoustic models in order to deploy them for embedded applications."
12862,1,\n\nThe first idea of the paper is to include time-remaining in the state.
12863,1, It is very unlikely that these\ndefaults are optimal across the different benchmarks.
12864,1, There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage.
12865,1, The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.
12866,1," All examples in the dataset are considered \""normal\"" to start with."
12867,1,"\n\n\nPros\n\nThe paper addresses an important application of deep networks, comparing the performance of a variety of different types of model architectures."
12868,1,"\n\nPros:\nWell written, thorough treatment of the approaches."
12869,1," For example \""horses\"" has highest value in embedding dimension 99."
12870,1,"\n\nMy most important piece of feedback is that I think it would be useful to include a few examples of the eICU time series data, both real and synthetic."
12871,1, It is a novel setup to consider reservoir sampling for episodic memory.
12872,1," I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic."
12873,1,"""Thanks for an interesting paper."
12874,1, Is 0.22 in similarity high or low?
12875,1, Shapeworlds dataset seems to be an interesting proof-of-concept dataset
12876,1," Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation."
12877,1,"\n\n6. Page 2, first paragraph of related work, the sentence \u201cOur method also closely related to ...\u201d is incomplete."
12878,1, I think this is a nice contribution that does yield to some interesting insights.
12879,1,    Thus the authors claim that one has to be careful about using feature importance maps.
12880,1,"   Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results,"
12881,1,"""In the paper, the authors discuss several GAN evaluation metrics."
12882,1,\n* elegant and simple solution
12883,1," Even though the authors find the orthogonal vectors, they\u2019re gated summed together very soon."
12884,1,"\n\nPaper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method."
12885,1,   Can this method be applied to lattices?
12886,1," Providing at least one more baseline (if not more considering the other things cited by them). \n\n"""
12887,1,"\n2. Only one task / No real-world task, such as Excel Flashfill.[[CNT], [SUB-NEG], [DFT], [MIN]]\n\n[1]: \""Neural Program Meta-Induction\"", Jacob Devlin, Rudy Bunel, Rishabh Singh, Matthew Hausknecht, Pushmeet Kohli"""
12888,1,\n\nCONS:\n- problem set-up not novel and existing approach not cited (experimental comparison needed)
12889,1,This seems to be a good direction.
12890,1, I do not recommend the acceptance of this draft.
12891,1,\n\n[Cons]\n- The proposed method is not much novel.
12892,1,"\n\nHave the authors considered training the net with small random perturbations added to the samples, to compare the \""vanilla\"" model to the more robust one, which has seen noisy samples, and compared explanations?"
12893,1,"\n\nFinally, nowhere in the paper do you mention which nonlinearities you used or if you used any at all."
12894,1,"\n\nThere are some important issues that need clarification:\n\n* \""Sukhbaatar et al. (2016) proposed the \u201cCommNet\u201d, where broadcasting communication channel among all agents were set up to share a global information which is the summation of all individual agents. [...] however the summed global signal is hand crafted information and does not facilitate an independently reasoning master agent."
12895,1,\n\nRemaining remarks\n- Just a very simple / non-standard ConvNet architecture is trained.
12896,1," For example, Theorem 1 is more like a list of desiderata and it already contains a forward reference to page 7."
12897,1,"\n\nForming the predictive distribution explicitly is intractable, so the paper suggests training a\nneural net to map from a subset of inputs to the predictive distribution over outputs."
12898,1," \n\nTaking into account this major weaknesses I cannot accept this paper and I do not think it is worth discussing results and applications in Sections 3,4,5 before authors detail, explain and clarify how exactly they have obtained these results. """
12899,1," \n\nFor deep nonlinear networks, the results require the \u201cpyramidal\u201d assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points."
12900,1,\n2. What is the intuition in adding target cluster entropy in Eq. 3?
12901,1," It seems that the authors need to revise both their bibliography and their logical reasoning: one cannot invalidate a broad set of algorithms for a broad goal, simply by taking a specific example and showing that it does not fit a particular interpretation of navigation *in previously unseen environments*.\n"""
12902,1, but not in the current form.
12903,1,"  Is the method giving us a picture of this data set?"""
12904,1,  \n\nThe plots could also use a bit of help.
12905,1,"\nIn section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers."
12906,1, The conclusion is that reading background knowledge from concept net boost performance using some architecture.
12907,1," \n\nIn conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. """
12908,1, The authors should try to give their opinion about the design obtained.
12909,1,\nThe theoretical analysis is satisfactory.
12910,1,"""The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets."
12911,1," The authors downplay the similarity of their paper to that of Le and Zuidema (2015), which  I did not appreciate."
12912,1," The central thesis is that\ninstead of the \""conventional wisdom\"" to fix the batch size during training and\ndecay the learning rate, it is equally effective (in terms of training/test\nerror reached) to gradually increase batch size during training while fixing\nthe learning rate."
12913,1,"\n\nComments:\n- The theoretical result (thm. 1) studies the case of full optimization, which is different than the proposed algorithm (running a fixed number of weight updates)."
12914,1," Here it appears from the description of Algorithm 1 that the transitions in the demonstration data have the same semantic as the interaction data, i.e., (s,a,r,s\u2019)."
12915,1," As a result, I suggest the authors add more ablation tests regarding (1) replacing the outer-fusion with simple concatenation (it should work for two attention layers); (2) removing the inner-fusion layer and only use the final layer's output, and using residual connections (like many NLP papers did) instead of the more complicated GRU stuff."
12916,1, Are you using it also on the vanilla RNN or the LSTM?
12917,1,\nb) Discussion of the experimental results is not sufficient
12918,1, The experiments are not reliable.
12919,1, RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017).
12920,1, The fact that the paper does not come with substantial theoretical contributions/justification still stands out.
12921,1,"\n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger."
12922,1,\nOur main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims.
12923,1," The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency."
12924,1,"\n\n\n\n\n\n[1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf\n[2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf\n[3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification. """
12925,1,  which has been accepted for publication in NIPS.
12926,1," \n\nAdditional comments:\n\nWhy is the variational approximation called \""sharpened\""?\n\nAt test time, normal VI just uses the fixed q(\\theta) after training."
12927,1,\n\nThis is a thorough exploration of a mostly under-studied problem.
12928,1,"""This paper presents methods to reduce the variance of policy gradient using an action dependent baseline."
12929,1, and 3) testing the DTP algorithm on locally-connected architectures.
12930,1,\n- 1D CNNs would be TDNNs
12931,1," \n\nNegative points:\nAlthough proposed idea is interesting,"
12932,1,"\n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs."
12933,1," Until that has been shown, the impact of this paper seems somewhat limited to me."
12934,1, Every category \nis defined by a specific combination of color and shape that is well recognized at test time.
12935,1," Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black."
12936,1, This technique generalizes existing work under full relaxation of assumptions.
12937,1," Marking my review as \""educated guess\"" since i didn't have time for a detailed review]"
12938,1," To force the probability near the endpoints you have to use alpha, beta < 1 which results into a \u201cbowl\u201d shaped Beta distribution."
12939,1, This seems too simple to really be the right way to add background knowledge.
12940,1," For readers less familiar with amortized variational inference in deep nets, the benefit would be larger."
12941,1," In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?"
12942,1, or did you spend a lot of time tuning it?).
12943,1, The model is inspired by current knowledge about long term memory consolidation mechanisms in humans.
12944,1,\n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?
12945,1,"\n\n\n* The paper mentions that to formalize the method as a policy gradient one, importance sampling should be used (the paragraph after (12)), but the performance of such a formulation is bad, as depicted in Figure 2."
12946,1,"n\nThe experiments are pretty robust, and they show that their method is better than the proposed baselines"
12947,1," However, it is not clear why the method is tested only on a single data set: MNIST."
12948,1, A solution using a pairwise convnet followed by hierarchical clustering is proposed.
12949,1,  The proposed approach is very interesting.
12950,1, This conclusion seems to be intuitive and expected.
12951,1,"\n* In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion."
12952,1, Face topologies are so regular that they can even be represented with a height map like geometry encoding in the image plane (See [1'] below).
12953,1, \n\nPros and positive remarks: \n--I liked the idea behind this paper.
12954,1, I do not understand why the compared methods are not consistently used in both experiments.
12955,1, 3) Generalization bound based on stability.
12956,1," But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing."
12957,1," While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper."
12958,1,\n\nThe method is evaluated on 6 Atari games (How were the games selected? Do they have exploration challenges?) against a single baseline (DDQN).
12959,1,"  For instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not?"
12960,1,\n\nSignificance: The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail.
12961,1,\n\u2013 Steady state assumption: How can this be relaxed to further generalize to non-static scenes?
12962,1, The proposed method doesn\u2019t seem always outperforming the baselines.
12963,1," By the way still, SRM is interesting method if it can be trained once and then be used for different datasets without retraining. "
12964,1," That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n"""
12965,1,\n\nOriginality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings. 
12966,1, This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements.
12967,1, Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.
12968,1," For neural networks, Nguyen and Hein (2017) assume the link function is differentiable."
12969,1,"\n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving"
12970,1,"Unfortunately, there is no investigation of this kind of attack in the paper. "
12971,1,\n\nTable 1: \u201capproach\u201d -> \u201capproaches\u201d
12972,1," In effect, this paper extends the existing literature suggesting end-to-end branching. "
12973,1," Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning."
12974,1," \n\nOne question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?"""
12975,1,\n\nThere is another theoretical point that could be clearer.
12976,1, See this pretty relevant paper which attempts the same task and also claims to have the target state generalization ability.
12977,1,.\nPreliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU
12978,1,"   Especially,  the dimension of $phi$ in Eq.(7) is unknown."
12979,1,"  \n\nDespite some shortcomings in the result section, I believe this is good work and worth communicating as is."""
12980,1,\n\nI would like to see the model ablation w.r.t. repetition avoidance trick by muting the second trigram at test time.
12981,1," In the toy example, you add (1 - \\alpha)wx whereas in the VANs you add (1 - \\alpha)x."
12982,1," It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results."
12983,1, This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting.
12984,1,\n- Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive?
12985,1, At best the generator seems to be working but I fear it is overfitting.
12986,1,"\u201d  This is a difficult comparison since the sensing strategies employed by real vehicles \u2013 LIDAR, computer vision, recorded, labeled real maps are vastly different from the slot car model proposed by the authors."
12987,1," Similarly, the representation analyzed in Figure 7 is promising,but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper."
12988,1,.\nThe approach seems costly anyway and I wonder if the authors could think of a smoother version of the algorithm to make it more efficient.\
12989,1," However, from a graph generation perspective, the problem formulation and evaluation do not sufficiently demonstrate the utility of proposed method."
12990,1," Therefore, the type of vanishing gradient that is observed when tanh saturates, which you combat in the toy model, is not actually combated at all in the VAN model."
12991,1, \n\nThis is an important problem and the paper attempts to tackle it in a computationally efficient way.
12992,1,"\n\nCons:\n* weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm"
12993,1,\n2. Can you introduce the Risk -R in the paper before using it in Theorem 4.1\n3
12994,1,\n\nStrengths\n - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.
12995,1,\n\nDetailed comments:\n1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper
12996,1,". Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework."
12997,1,"\n\nFurther in this direction, Section 4.1 claims \u201cZero partial derivatives are ignored to make the signal more clear"
12998,1," A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly."
12999,1, \n\nOne concern about the paper (but this is an unfortunate common feature of most GAN papers) is that it ignores the vast knowledge on saddle point optimization coming from the optimization community.
13000,1,"  But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick."
13001,1,"""[====================================REVISION ======================================================]\nOk so the paper underwent major remodel, which significantly improved the clarity."
13002,1,\n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.
13003,1,"  First, it does not\ndemonstrate increased scalability. "
13004,1,"\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization)."
13005,1,"\n\nFor generative modeling: you do have guarantees that, *if* your optimization and function parameterization can reach the global optimum, you will obtain the best map relative to the cost function."
13006,1,\n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)
13007,1,\n* The experimental results are promising on multiple datasets.
13008,1,"""This paper proposes a model of \""structured alignments\"" between sentences as a means of comparing two sentences by matching their latent structures."
13009,1,.\n\nPros\n------\n- Interesting idea
13010,1,\n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)?
13011,1,  \n\n* Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process.
13012,1,"  This approach extends the method presented on Arxiv on Sigma delta quantized networks (Peter O\u2019Connor and Max Welling. Sigma delta quantized networks. arXiv preprint arXiv:1611.02024, 2016b.)."
13013,1, \n- Linear interpolation in latent space may not be the best choice here\n  seeing as e.g. for a Gaussian code the region near the origin has rather low\n  probability.
13014,1,"""In this paper a new neural network architecture for semi-supervised graph classification is proposed."
13015,1,"\n- How do those compare with CE and L_{5, 1} with the proposed method?"
13016,1,\n\n\nGeneral recommendation: Clear rejection for now.
13017,1,"\nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?"
13018,1," In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than \u201crandom options\u201d built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals."
13019,1,\n- it may also be interesting to discuss the role of the language model to see which factors influence system performance
13020,1,"\nI thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition."
13021,1," If it is purely a scaling factor, how is the scale quantified? "
13022,1,"""Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words."
13023,1, The framework is represented by a feed-forward deep architecture analogous to a residual network.
13024,1," I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings."
13025,1,"\n - Section method, paragraph under equation (2) L(z(\\alpha),x,y)<=L(w,x,y) is NOT necessary."
13026,1," If the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the Stanford Question answering dataset or TriviaQA."
13027,1,\n\nMinor:\n- What is the difference between LSTM and left-branching LSTM?
13028,1,"  The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images."
13029,1," After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs."
13030,1," Only one relatively small dataset (cifar10) is used Moreover, the slow server problem is only simulated by artificially adding delays to the server."""
13031,1," Some additional discussion of the results would be appreciated (for example, explaining why the proposed method achieves similar performance to the LSTM/OPSRL baselines)."
13032,1," as such, I'd recommend the paper for acceptance."
13033,1, \n\nOther minor points:\n- In the definition of f_nodes: What is p(y)?
13034,1," it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words"
13035,1,"""Strengths:\n* Very simple approach, amounting to coupled training of \""e\"" identical copies  of a chosen net architecture, whose predictions are fused during training. "
13036,1,  \n\n- Is it possible to compute eigenoptions online??
13037,1, It is not clear whether the supposed improvement is related to the conditional formulation.
13038,1," Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not."
13039,1, While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK.
13040,1,"The argument of better generalization is based on very limited experiments and without any explanation, so I find that a weak argument (and it just seems weird that inexact CG gives better generalization)."
13041,1,"\n\nThe later point would normally make me attribute a score of \""6: Marginally above acceptance threshold\"" by current DL community standards,"
13042,1," The authors attribute this to the reduction of accumulated rounding error in training process, which is somewhat against the community\u2019s consensus, i.e., float precision is not that important so we can use float32 or even float16 to train/do inference for neural networks."
13043,1,"""This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space."
13044,1, \n\nI found the paper unnecessarily longer than the suggested 8 pages.
13045,1,  In practical terms this is a linear combination of these graph\nconvolutional layers.
13046,1,". \n\n-- In Figure 1, the \u201cG1\u201d on the right should be \u201cG2\u201d;\n-- Section 2.2.1, \u201cX_f\u201d should be \u201cx_f\u201d;\n-- the motivation of having \u201cz_v\u201d should be introduced earlier;\n-- Section 2.2.4, please use either \u201calpha\u201d or \u201c\\alpha\u201d but not both;\n-- Section 3.3, the dataset information is incorrect: \u201c20599 images\u201d should be \u201c202599 images\u201d;\n\nMissing reference:\n-"
13047,1," Also, it is not meaningful to measure the quality of a visualisation via the MDS fit."
13048,1,"""Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two."
13049,1, \n\nThe authors propose to use cosine distance between representations \nof what they call the \u201csubgoals\u201d of learner and expert.
13050,1,\n\n- This paper is poor in the reproducibility category.
13051,1," To this, they add channels based on low base quality, low mapping quality."
13052,1,"\n\nAuthors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method."
13053,1,  The authors should release the dataset to prompt the research in this area.
13054,1," On this graph, one can perform interventions and get a different distribution of labels from the original causal graph (e.g. a distribution of labels in which women have the same probability as men of having moustaches)."
13055,1, The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing.
13056,1," Using the visual concept embedding, two simple methods are used for few-shot learning: a nearest neighbor method and a probabilistic model with Bernoulli distributions."
13057,1," For example, the authors compare their model against rather weak baselines."
13058,1,"  After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation."
13059,1, The notation also make it easy to understand the differences between different models.
13060,1,\n\nLearning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago.
13061,1, See detailed comments below for problems just in the introduction. 
13062,1,\n\nThe graphs are WAY TOO SMALL to read.
13063,1," Figure 1c is pretty unrealistic to obtain for a real vehicle, especially for the four cars near the top where the topmost vehicles would be occluded at least partially from the vantage point of the ego-car. """
13064,1, The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games.
13065,1,\n\nOverall I think the idea proposed in the paper is beneficial.
13066,1,\n\n- Affect in language seems to me to be a very contextual phenomenon.
13067,1,"\n- Low performance on Charades dataset, no comparison to prior work\n"
13068,1,"CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise"
13069,1,"\n\nIn Section 3.1, \n>This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1."
13070,1,"Results on MNIST, CIFAR-10 and ImageNet are very competitive"
13071,1,". Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). "
13072,1,"  Further, some of the paper's pseudocode includes unexplained steps like \""invert by domain index\"" and \""scanco-occurrence\""."
13073,1, The performance improvement is expected and validated by experiments.
13074,1,"  Furthermore, only feedforward and locally connected networks (CNN) are considered since their architecture is considered more biologically plausible than convolutional neural networks."
13075,1," A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points. "
13076,1," (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it\u2019d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold."
13077,1," Since it is entirely general, I would rather expect a test on a dozen different data sets."
13078,1," The position {x,y} is a binned representation of discrete or continuous coordinates."
13079,1, How do the authors\nimplement the other graph-related approaches in this problem featuring\ntime series?)
13080,1,\n(b) Lacking details of the model setups and training strategies.
13081,1, The improved understanding of SPENs and potential for further work justify accepting this paper.
13082,1," Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks."
13083,1, I am wondering if the authors can clarify this point.
13084,1," After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all."
13085,1,"\n\n[2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011)."
13086,1, This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1).
13087,1,"""The paper presents a novel adversarial training setup, based on distance based loss of the feature embedding."
13088,1,"\n\n\n1) The related work section is outrageous, containing no references before 2016."
13089,1,"? Unlikely, because GPUs are involved."
13090,1,"\n\n4. The experimental part is ok to me, but not very impressive."
13091,1,"  If there is a clear separation from a different view, show that one instead."
13092,1,  It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original.
13093,1," This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers."
13094,1,"""This paper investigates multiagent reinforcement learning  making used of a \""master slave\"" architecture (MSA)."
13095,1," The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data."
13096,1," \nComparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q."
13097,1,\n\nThis work is well-written and cites previous work appropriately.
13098,1," Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models."
13099,1," Additionally, in order to avoid vanishing/exploding gradients in standard RNNs, a soft unitary constraint is used."
13100,1," given they are quite interesting.\n\n\n"""
13101,1," \"" It had more difficulty optimizing for the three-color result\"" why?"
13102,1," This proposed paper rejects some of the claims that were made in Mirowski et al. 2016, mainly the capacity of the deep RL agent to learn to navigate in such environments."
13103,1, The intuition of the proposed approach is clearly explained and it seems very reasonable to me.
13104,1, It\u2019s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions.
13105,1," However, there are a couple additional experiments that would be quite nice:\n\u2022\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X)."
13106,1," \n7. No large matrices need to be formed or inverted, however more passes needed per outer step."
13107,1, The experiment shows good privacy and utility.
13108,1,"  The first property (correctness) is a more essential property of this quantity, rather than the second (appropriateness as an example selection measure)."
13109,1,  I don't think the experiments show that ensemble behavior leads to high performance. 
13110,1," Second, the proposed method seems not significantly different from the architecture search method in [1][2] \u2013 their major difference seems to be the use of \u201cremove\u201d instead of \u201cadd\u201d when manipulating the parameters."
13111,1," Though limited to binary classification, the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk."
13112,1, Why have you not reported results for these kinds of tasks?
13113,1,"\n\ni.e. the authors form a vector \""HoW\"" (called history of the word), that is defined as a concatenation of several vectors:\n\nHoW_i = [g_i, c_i, h_i^l, h_i^h]\n\nwhere g_i = glove embeddings, c_i = COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word."
13114,1, It would be a good workshop paper.
13115,1," Instead it starts from gradient terms, each of which seems to be from different papers, and then simplifies them."
13116,1,"  In general, it is not ready for publication."
13117,1," Also Section 3 could be improved and simplified.[[CNT], [EMP-NEU], [SUG], [MIN]] It would be also good to add some more related work. "
13118,1," In light of this, the fact that SFNN is given extra epochs in Figure 4 does not mean much."
13119,1,\n\nNice work creating an implementation of fast GGNNs with large diverse graphs.
13120,1," \n\nPresumably, once one had found a path ( (s, a0), (s1, a1), (s2, a2), \u2026, (sn-1,an-1),  s\u2019 ) one could then train the PATH policy on the triple (s, s\u2019, a0) ?"
13121,1, \n \n- What is the value of the learned syntax in section 5.2?
13122,1,"   The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \""fidelity\"" of the weak label when training the student at the final step."
13123,1," \n\nOverall, this is an interesting paper."
13124,1," Likely what you meant is the q function, at state s_t?"
13125,1," Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions."
13126,1," \n\n*Additional comments*\n\nPage 2: Monte Carlo Droput --> Dropout\nPage 3 related work: (Adams, 2015) should be (Hernandez-Lobato and Adams, 2015)"""
13127,1, How would you train such baselines?
13128,1,  How to generate the neighborhood in Neigh(\\hat{u}_i) on page 5?
13129,1," They give results on the quality of the approximation using these operator networks, and show how to build neural network layers that are able to take into account topological information from data."
13130,1,"""The main contribution of this paper are:\n(a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion,"
13131,1," The term \""siamese kernel\"" is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs."
13132,1,"  This subset is chosen such that each command sequence corresponds to exactly one target action sequence, making it possible to apply standard seq2seq methods."
13133,1," I only have a few comments regarding some aspects of the paper that could perhaps be improved, such as the way eigenoptions are evaluated."
13134,1,\n\ncons\n- please provide the value of the diffusion coefficient for the sake of reproducibility
13135,1," In addition, it would be useful to investigate how this particular problem is different than a binary classification problem using CNNs."
13136,1," Furthermore, in the matrix completion scenario, you have O(log^2n) entries per row on average, which means with high probability few rows should have a constant number of entries."
13137,1, Examples of these related work include [1] and [2] (at the end of the review).
13138,1, Do you have any insights are speculations regarding this?
13139,1,"""This paper wants to probe the non-linear invariances learnt by CNNs. This is attempted by selecting a particular layer, and modelling the space of filters that result in activations that are indistinguishable from activations generated by the real filters (using a GAN). "
13140,1," More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. \n\n2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)? "
13141,1,"\nAuthors evaluate their RIN on the adding, sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models."
13142,1,"""This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning)."
13143,1, How about t_j in equation (5)?
13144,1,\n\nThe authors present multiple qualitative and quantitative evaluations.
13145,1," What are the instructions given to the workers?[[CNT], [CNT], [QSN], [MIN]]\n- In section 4.2. the authors state \""We also simultaneously learn a corresponding inference network, .... granular human biases captured\""."
13146,1,  The model is shown to improve on the published results but not as-of-submission leaderboard numbers.
13147,1,"\n4. Defining the loss w.r.t. the state, and taking the derivative of the state w.r.t. to theta is a bit odd."
13148,1," While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results."
13149,1,"\nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images."
13150,1, The paper focuses on the theoretical results and does not present experiments (the polynomials are also not elaborated further).
13151,1," First, iIt is unclear why coverage should be >=0.8 and firerate ~ 1, according to the motivation firerate should equal to coverage: that is each pixel f_p is assigned to a single VC centroid."
13152,1,"""The paper addresses the problem of learning mappings between different domains without any supervision."
13153,1,"""This paper produces word embedding tensors where the third order gives covariate information, via venue or author."
13154,1," The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published)."
13155,1,"\n\nSmall Nits: \n\nSection 4: \""In order to evaluate the efficacy of our experiment\"": I think you mean \""approach\""."
13156,1, They show that their method more efficiently suffers on permuted MNIST from less degradation.
13157,1,"\n- I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances)."
13158,1, Same goes for using a running average of phi(s) and the correct tau(s) in final policies.
13159,1,  A variety of experiments are conducted that demonstrate the efficacy of the proposed methodology.
13160,1,\n\nThe results presented by this paper shows improvement over the baseline.
13161,1,"""The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \""style memory\"", which would presumably capture non-class information."
13162,1,\n\nMy only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832).
13163,1," Smith and Le (2017)\npresent a differential equation model for the scale of gradients in SGD,\nfinding a linear scaling rule proportional to eps N/B, where eps = learning\nrate, N = training set size, and B = batch size."
13164,1," What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images."
13165,1,"  Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is \""we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates,\"" but I see literally no section that hints at anything like this (no mention of \""dynamic unfolding\"" or \""latent space trajectory embedding\"" ever occurs later in the paper)."
13166,1,"   My feeling is that this work is a bit preliminary at present,"
13167,1,\n\n3) Equation (3) -- put the missing 2 subscript for the l2 norm of |f_(w+u)(x) - f_w(x)|_2 on the LHS (for clarity).
13168,1,. It can be fine to not be SOTA as long as it is acknowledged and discussed appropriately.
13169,1, I believe that the confidence estimates are considered to be part of the critic and the w vector to be part of the theta_c parameters.
13170,1," Is this just a restatement of the previous paragraph, which concluded convergence will be slow if \\eta is too small?"
13171,1," The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information."
13172,1,"\n\n1. Actually I find the entire notion of an \""ideal\"" prior under the GAN setting a bit strange."
13173,1,"""The authors propose a method for reducing the computational burden when performing inference in deep neural networks."
13174,1,\n - The proposed model can be easily applied to any VQA model using soft attention.
13175,1,\n\n6. How does the narrator choose which word to obfuscate?
13176,1, The mask contains information about the presence/absence of objects in different pixel locations and the feature map contains information about object appearance.
13177,1, They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.
13178,1,  Evaluations are performed on ImageNet and CIFAR-100.
13179,1," Therefore, i found many of the discussion to be questionable."
13180,1, The Algorithm is poorly described and crucial parts of the algorithm are very confusing.
13181,1,"\n\n- Typos.\n(a) Page 7, \""Figure 8 shows the seeds and example images for 10 rounds...\"""
13182,1," My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds, why bother with deep RL here? """
13183,1,  The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target (random) speed within these limits.
13184,1," One of the main limitations of the model, as stated by the authors, is its number of parameters."
13185,1,"\n\n+ Significance:\nWhile the results are interesting,"
13186,1, The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it?
13187,1," One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy."
13188,1,\n\nAnother major concern is that the technical contributions of the proposed model is quite limited.
13189,1," For example, find the nearest german neighbour of the word \""dog\"" in the common representation space. The authors instead compare with very simple baselines."
13190,1, In that paper we have Q_pi instead of \\hat{Q} though.
13191,1," Highly peaked -> confidence, spread out -> out of distribution."
13192,1, The method is tested on four simple UCI datasets.
13193,1," However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC."
13194,1,  Small batch sizes\nsometimes make it easier for many machines to be working simultaneously.
13195,1, The paper is rigorous and ideas are clearly stated.
13196,1,"  The paper is also well written, a bit dense in places, but overall well organized and easy to follow."
13197,1, \n\n\n## Clarity\n\nOverall the paper reads reasonably well.
13198,1, This can be computationally demanding.
13199,1,"""The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them."
13200,1,"\n  While this does not significantly dilute the message, it would have made it much more convincing\n  if results were given with stronger networks."
13201,1," I would hope that at the very least a broad range of generative models and some recognition models are used to evaluate here, especially a variational autoencoder, beta-VAE and so on."
13202,1," but not very clearly written in some sections,\nfor instance I would better explain what is the main contribution and devote\nsome more text to the motivation."
13203,1,\n- Figure 1 seems inconclusive and it is a bit difficult to ascertain the claim that is made.
13204,1," I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions."
13205,1," How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language?"
13206,1,  One way to analyze whether this happens is to predict the identity of the task from the hidden vectors.
13207,1,"\nDuring testing, the robot is presented with a sequence of goals in a related but different task."
13208,1,\n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).
13209,1,"  Indeed, ResNet and DenseNet designs are both guided by extremely simple principles: stack a series of convolutional layers, pool occasionally, and use some form of skip-connection throughout."
13210,1,\n3. the actual sources of variation are interpretable and explicit measurable quantities here.
13211,1," Alternately, I would just say something like \""to produce a code vector, which lies in the same space as \\mathbf{c}\"", since the decoding of the generated code vector does not seem to be particularly relevant right here."
13212,1,"\""  Do the authors mean that the negative log-likelihood will be improved in this case?"
13213,1," RBMs are not state-of-the-art in topic modeling, therefore it\u2019s difficult to assess whether this is helpful."
13214,1,"\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach."
13215,1,"""the paper presents a way to encode discrete distributions which is a challenging problem."
13216,1, Are the test molecules somehow generated in a directed or undirected fashion?
13217,1,"\n  3. The results shown in Fig. 7 are surprising -- in general, it does not seem like a regular VAE would do so poorly."
13218,1, Here the assumptions of locality and stationarity underlying CNNs are sensible and I don't think the first paragraph in Section 3 justifying the use of the CNN on the maze environment is necessary.
13219,1,  The final threshold for 'successful' speedups feels somewhat arbitrary -- why 16ms in particular? 
13220,1," Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript."
13221,1, The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers.
13222,1, Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.
13223,1,\n \nThe introduction of GAN is very abrupt.
13224,1," Overall, I really liked this paper."
13225,1," Although the overall presentation is clean,"
13226,1," The\npaper isn't presented in exactly these terms, but the idea is to consider a uniform distribution\nover programs and a zero-one likelihood for input-output examples (so observations of I/O examples\njust eliminate inconsistent programs)."
13227,1," The approach of using BN after non-linearity is termed \""standardization layer\"" (https://arxiv.org/pdf/1301.4083.pdf)."
13228,1, Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments.
13229,1," If desired, the results of timed comparisons can also be reported, but reporting just a timed comparison with an artificial limit of 1 second may mislead some readers into thinking that we are farther along than we actually are."
13230,1,"\n- covariate specific analogies presented confusingly and similar but simpler analysis might be possible by looking at variance in neighbours v_b and v_d without involving v_a and v_c (i.e. don't talk about analogies but about similarities)"""
13231,1," For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?"
13232,1," \n\n(ii) the authors do not seem to address one of the main criticisms they make about previous work and in particular \""[a lack of evidence] of such specific 2D connectivity patterns\"". "
13233,1, The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.
13234,1," \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones."
13235,1," The technique sounds, the presentation is clear and I have not seen similar paper elsewhere"
13236,1," I can't fully assess that the paper has major contributions.  """
13237,1,\n+ It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses.
13238,1, If these are arXiv give the proper ref.
13239,1," Also, the combination of Kronecker product and soft unitary constraint is really interesting."
13240,1," So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G, or simply due to the fact that it considers historical D models (which could be motivated by sth other than the online learning theory)."
13241,1,\n \nMore description should be provided to explain the reward visualization on the right side of figure 2. What reward?
13242,1," However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel."
13243,1, As such it's not even clear if this is proper for a conference.
13244,1," What would be a good measure for an \""effective latent representation\"" that substantiates the claims made?"
13245,1, \n- Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6).
13246,1," While this is not explicitly mentioned in the FractalNet paper, it clearly would not break the design principle of FractalNet which is to train a path of multiple layers by ensembling it with a path of fewer layers. "
13247,1,  It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset.
13248,1, Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined.
13249,1," A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 \u201cResults\u201d p. 3 is not really results but part of the methods."
13250,1, but lack of novelty.
13251,1,  This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space.
13252,1,"\n\n> Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data"
13253,1," Here I list some of my questions:\n+About the MBEM algorithm, it\u2019s better to make clear the difference between MBEM and a standard EM. Will it always converge? What\u2019s its objective?"
13254,1,"\nMoreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...\n\n"""
13255,1,"\n\nUnfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1."
13256,1," Since w_j^t can be defined iteratively and recursively (as a dynamic program), it\u2019s probably worth writing both out, for expository clarity."
13257,1,\n6. SGD also needs to update at least d times for all d latent tensors.
13258,1,".\n\nOverall, I believe the paper is interesting but not ready for publication."
13259,1,"\nThe authors have tried to use mathematical formulations to motivate their choice, but they lack rigorous definitions/developments to make their point convincing."
13260,1,\n\nThe proposed algorithm is clearly described.
13261,1," How do features of networks pre-trained on ImageNet, and then fine-tuned for the medical domain, compare to features learned from medical images from scratch?"
13262,1, \n1) Use the unifying notation to discuss strengths and weaknesses of current approaches (ideally with insights about possible future approaches).\
13263,1, So the provided proof is new?
13264,1," \n\nSection 5.3:\n- What is the \""a : b :: c : d\"" notation?\n"""
13265,1," More specifically, there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2."
13266,1," Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information."
13267,1,"  Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible. "
13268,1,"Currently, the authors mention in the conclusion that, as is known to often be the case with GANS, that the results were indeed sensitive."
13269,1,"""This  paper is  on ab important topic : unsupervised learning on unaligned data."
13270,1,"\n\nThese strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained \non some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting\nmultiple times a large dataset), then they are used on a final target task with again few labeled data and large \nunlabeled samples but beloning to a different set of categories."
13271,1," Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is."
13272,1," If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result."""
13273,1," When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds)."
13274,1," In the latter, the notion of \""Probabilistic Lipschitzness\"", which is a relaxation of the \""cluster assumption\"" seems very related to the actual work.\n\nReference:\nBen-David and Urner."
13275,1,\n2. The authors should explain why choosing VGG19 for analysis.
13276,1," \n\nIn equation 1, it wasn\u2019t obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation?"
13277,1," Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability"
13278,1, If it is L_1 norm on the output coefficients the comparison is misleading.
13279,1," For example, Table 1 shows that the baseline method uses less filters than the proposed method that selects the number of filters through an innovated heuristic measure."
13280,1," Inspired by recent works on large-batch studies, the paper suggests to adapt the learning rate as a function of the batch size."
13281,1,"""This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples."
13282,1, \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight\u2019s learning rate is somewhat novel.
13283,1," There is nothing wrong with the fundamental idea itself,"
13284,1,". For example, why JMVAE performs much better than the proposed model when all attributes are given."
13285,1," The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e."
13286,1," The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > \\sqrt{N}."
13287,1," The proposed technique has some limitations,"
13288,1,"\n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison? "
13289,1,  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator.
13290,1,\n\n8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al 
13291,1, \n\nMy main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture.
13292,1,"""The paper is generally well-written and the intuition is very clear. "
13293,1, Questions which I'd like to seen answered: how good is the OCN representation when used for clustering compared to the PCN representation?
13294,1, There is nothing deep in this architecture.
13295,1," But I\u2019m worried about the empirical evaluation, and the omission of crucial algorithmic details."
13296,1, The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions.
13297,1,"""This paper studies active learning for convolutional neural networks"
13298,1," Does it happen on this dataset only or it is the case for many datasets? """
13299,1,\nUntil then it is not possible to draw any conclusion from this work.
13300,1,"\n\nSection 2.2 requires some polishing as I found hard to follow the main story\nthe authors wanted to tell.[[CNT], [CLA-NEU,PNF-NEU], [SUG], [MIN]] The definition of the weight of a path seems\ndisconnected from the main text, ins't A^k kind of a a diffusion operator or\nrandom walk?"
13301,1,CrescendoNets do not extend beyond this design principle.
13302,1," The arguments for skipping this experiments are respectful,"
13303,1," Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%."
13304,1," For example, if we are minimizing the changes our update will make to the value of the next state, what would happen if the next state is equivalent to the current state (or equivalent in our feature space)? "
13305,1,\n-\tThe experimental results how the idea holds some promise
13306,1," \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?"
13307,1, \n\nThe plots in Fig 2 with the marginals on CKY charts are not very enlightening.
13308,1,"Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta."""
13309,1,"\n\nI find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below."
13310,1,  \nThe suggested method follow an iterative process in which the student and teacher are interchangeably used.
13311,1," Partial explanations are provided, again using results in compressive sensing theory."
13312,1," It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017)."
13313,1," This paper is 12+1 pages long, plus a 5 page supplement."
13314,1,"\n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks?"
13315,1," The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks?"
13316,1, If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results.
13317,1,? Should the hyperparameters be tuned separately for each generative model being evaluated?
13318,1," When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning."
13319,1," Shouldn\u2019t be something like \u201cuniqueness\u201d, that is how unique is an image in a batch of images be a better indicator?"
13320,1,"\n- in the introduction (page two) the authors refer to SST prediction as a 'relatively complex physical modeling problem', whereas in the conclusion (page ten) it is referred to as 'a problem of intermediate complexity'. This seems to be inconsistent."""
13321,1, Why is there no multi-channel baseline for the GRID results?
13322,1, \n\n- Empirical results do not clearly show the advantage of the proposed method over state-of-the-arts.
13323,1,"\n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?"
13324,1,\n\nComments:\n1. I recommend the authors to tone down their claims.
13325,1, \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar.
13326,1,"\n\nThe multiplicative relation analysis is interesting,"
13327,1," Also, predictive coding has been commonly used in neuroscience:\nSrinivasan MV, Laughlin SB, Dubs A (1982) Predictive coding: a fresh view of inhibition in the retina."
13328,1," The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks."
13329,1,\nGood baselines.
13330,1,\n\n-- A number of things were unclear to me with respect to the details of the training process: the feature extractor (VGG) is pre-trained.
13331,1,"  Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good."
13332,1," The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model."
13333,1,"   However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different."
13334,1," It is mentioned that results are expected to be lower than those produced by methods with a multi-layer classifier as the discriminator (e.g. Shen et al., Wasserstein distance guided representation learning for domain adaptation, Ganin et al., Domain-adversarial training of neural networks?)."
13335,1,"""This paper focuses on the zero-shot learning compositional capabilities of modern sequence-to-sequence RNNs."
13336,1, \n\nOn the whole this seems like a promising paper.
13337,1," However, to me it seems that even in 15 game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it."
13338,1," 6:\nin the definition of L_{SR}(s, s'), why \\psi takes \\phi(s) as argument?\n\n- in conclusion:\nthat that"""
13339,1," Also, I guess we are assuming the obj is strongly convex?"
13340,1," Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures?"
13341,1, \n\nThe paper is unclear at some places and writing gets confusing.
13342,1,\n11. Section 5.3 could be improved by providing a curve (compression vs. error) instead of just providing a table of sampled operating points.
13343,1, The paper is interesting and easy to follow.
13344,1, The approach is evaluated on:\n 1) several variants of MNIST.
13345,1," The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size."
13346,1,"\nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals."
13347,1," \n\nThere are some other issues with the presentation of the method, but these don't affect the merit of the method:"
13348,1,\n\nComments:\n\nThe model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).
13349,1, The interesting part here is that the linear projection is a function of the support points.
13350,1,"\n\nThe authors claim that they did not want to compete with state of the art DA, but the approach of Perrot et al., 2016 seems to a have a similar objective and could be used as a baseline."
13351,1, The paper also introduces an attention method over chart cells.
13352,1, The new results are quite informative and addressed some of the concerns raised by me and other reviewers.
13353,1,\n\nComments and questions:\n\n1) How computationally expensive is FTP?
13354,1,"\n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs, "
13355,1, A weakness of the paper is that it does not attempt to solve a real-world problem.
13356,1, Experiments demonstrate that their method is promising compared to the competitor \u201cNormProp\u201d which explicitly normalizes the weights of neural networks.
13357,1,"""This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item."
13358,1," This also seems to subsume the first condition, s\\geq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9."
13359,1,"\n\nWeaknesses:\n\n- There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work."
13360,1,"\nIf this is the case, all the analysis and results obtained are almost meaningless."
13361,1,"\n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from."
13362,1," As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data)."
13363,1," (It is obviously good to cite results from prior work, but then it would be more clear if the results are invoked as is without modifications.)"
13364,1,"""Summary:\nThe manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model."
13365,1,"\"" I would encourage the authors to revisit Le and Zuidema (2015), especially section 3.2, and consider the technical innovations over the existing work."
13366,1,  Glad to see that the code will be released.
13367,1,"\n3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(\u03b4x)| <= \u03b4^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation."
13368,1,"\n  2. Results on tiny datasets\n"""
13369,1, Are there other alternatives in the literature?
13370,1," \n\nThe second contribution is partially valid,"
13371,1, They learn the dropout distribution by variational inference with concrete relaxation.
13372,1, but it's somewhat lacking in detailed model or experiment descriptions.
13373,1, The experiments do not however demonstrate why the algorithm is\nperforming better.
13374,1,"  \n\nA second limitation of the work is the reliance on a \""true\"" set of disentangled factors.[[CNT], [null], [DIS], [MIN]] We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors."
13375,1, \n\nOne major question.
13376,1, 1D interpolations and 2D contour plots can be described in a few sentences each.
13377,1,". However, what is unique to RNNs is their ability to model long term dependencies across time."
13378,1, This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this.
13379,1,"\n\nWhile basically the approach seems plausible, the issue is that the result is\nnot compared to ordinary LSTM-based baselines."
13380,1," Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?"
13381,1," Of course, the moving-out is biased but the replacing is unbiased."
13382,1, Is it a snapshot of the Jul 14 one?
13383,1, The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible.
13384,1," Please do note that the authors' names are misspelled: Vuli\\'c not Vulic, Mrk\\v{s}i\\'c instead of Mrksic."
13385,1," Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly."
13386,1," As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1."
13387,1,\ni)\tThe RW model is actually similar to an HMM.
13388,1,"\n\nUltimately, I will have to suggest rejection, unless the authors considerably beef up their manuscript with more experiments, more details, and improve the grammar considerably."
13389,1, The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop.
13390,1, \nFor example the authors state in various parts that DSC does not work on\nnon-Euclidean data.
13391,1," . \n\n2) - Have you tried resnets generators and discriminators  at various depths , with padding so that the spatial resolution is preserved?"
13392,1, But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better).
13393,1," \""A simple way to initialize recurrent networks of rectified linear units.\"" arXiv preprint arXiv:1504.00941 (2015)."""
13394,1,"\n\nIn particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness."
13395,1, This point is not very clear and we do not know how many source instances are used to compute the mapping - the mapping is incomplete on this point
13396,1,";\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory"
13397,1, \n\nCons:\nNot clearly written.
13398,1, I am quite convinced that any somewhat correctly setup vanilla deep RL algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box.
13399,1," The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly."
13400,1,"""The paper represents an empirical validation of the well-known idea (it was published several times before) \nto increase the batch size over time."
13401,1, Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion)
13402,1,"\n\nThis paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach."
13403,1," To do this, they introduce a new dataset that facilitates the analysis of a Seq2Seq learning case."
13404,1," Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation."
13405,1, I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017).
13406,1, The experiments clearly show the viability of the approach and give interesting insights.
13407,1,\n\nThe paper is missing some of the original references to a discriminative LM (DLM) as well as  references to the use of a NN LM directly in decoding (presented in ICASSP and Interspeech conferences over the last 5 years).
13408,1," Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning."
13409,1, Are there any\n  comparison with these previous efforts?
13410,1, Some suggestions / criticisms are given below.\n\n1) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex.
13411,1," Then it is stated: \""the number of hidden neurons, as well as the structures for the deep neural networks\nwere empirically tried, and the results of the best settings were registered for comparison\"" (1st paragraph Sec. 3)."
13412,1," Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N."
13413,1,"\n\nFor example, it was not investigated whether the method identifies\nclassification features that generalize."
13414,1,"  The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general-purpose method for program synthesis example selection."
13415,1, The paper is easy to read and its experiments show the effectiveness of the method.
13416,1,"""This paper learns to construct masks and feature representations from an input image, in order to represent objects."
13417,1,"  However, it is unclear to me why it is necessary to have the optimal matching here and why the simple nearest target would not work."
13418,1," In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset."
13419,1," The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle."
13420,1," It can rather be seen as an interesting empirical study, with \""negative result\"
13421,1, \n-  Open source codes.
13422,1, \n\nThis paper takes this idea and applies it to deep neural networks.
13423,1, Specially a formal representation of the method should be included.
13424,1, It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice.
13425,1,"""This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j)."
13426,1,"\n\n- About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap."
13427,1,"\n\nThe section on the variance of the weights is rather unclear mathematically, starting with the abstract and even continuing into the paper."
13428,1," The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately."
13429,1,"Conclusion is short and a few directions for future research would have been useful."""
13430,1,"\n\n# Typos\n\n* sec1 second parag: did you really mean \""in the architecture or loss function\""? unclear.[[CNT], [CLA-NEG], [QSN], [MAJ]]\n* sec2: over a family\n* \""common structure, so that\"" (not such that)\n* orthgonal\n* sec2.1 suggestion: clarify that \\theta and \\phi are in the same space\n* sec2.2 suggestion: task-specific parameter $\\phi_j$ is distinct from ... parameters $\\phi_{j'}, j' \\neq j}"
13431,1,"\n\nMaybe I missed or overlooked some detail, but I didn't spot exactly what the classification task was."
13432,1," Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks."
13433,1,"\n6) Sec 2 para 7: \""L_target is a target objective which can be a negative class probability or in the case of heart failure, predicted BNP level\"" -- for predicted BNP level, are you treating this as a probability and using cross entropy here, or \nmean squared error?."
13434,1, These two proposed neural network models seem performing well empirically.
13435,1,"\nFor a paper interesting in large scale aspects, the experimental evaluation is rather weak."
13436,1,"\n* In Section 5.2.1, L_iw is used to characterize encoder overfitting where the argument is that L_ais is not a function of the encoder, but L_iw is, and so the difference between the two summarizes how much the inference network has overfit."
13437,1,  This is true even for random perturbations.
13438,1, but focusing on only a single game (Doom) is a weakness that needs to be addressed because one cannot tell if the choices were tailored to make the method work well for this game.
13439,1,"It combines the advanced attention mechanism, pointer networks and REINFORCE learning signal to train a sequence-to-sequence model for text summarization."
13440,1, An iterative algorithm is developed for model optimization.
13441,1," However, there are some questions (as mentioned in the Weaknesses section) which need to be clarified before I can recommend acceptance for the paper."
13442,1," In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization."
13443,1," This involves producing some additional data points, either by adding noise to the projected semantic vector, or by choosing a number of that vector's nearest neighbours."
13444,1,"""\n\n- 3.2: \""In general, if we wish to learn a model for X in which each latent variable ci affects some arbitrary subset Xi of the data (**where the Xi may overlap**), ...\"": Which is just like learning a Z for a labeled X but learning it in an unsupervised manner, i.e. the normal VAE, isn't it?"
13445,1,  I would have appreciated more intuition behind these approaches.
13446,1,"\n\nUnfortunately, I fail to see a significantly valuable contribution from this work."
13447,1, The experiment lacks the C&W's attack.
13448,1," This means that the upsampling operation is not really upsampling a \""true\"" graph/mesh."
13449,1,n   what effect the monte-carlo approximation of the objective has on things.
13450,1,  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.
13451,1,"\n\nSignificance: Because of all the things outlined above, the significance is below the bar for this round."
13452,1,\n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\
13453,1,"""This paper proposes using long term memory to solve combinatorial optimization problems with binary variables."
13454,1, Earlier work on tree kernels (in terms of defining tree distances) may be related to this work.
13455,1,  Is 8 good for architectures A through E?\n3.
13456,1,  The authors then provide several evaluations of complex-valued networks on some standard ML benchmark tasks.
13457,1, This paper improves on the upper bound given by [2] and the lower bound given by [1].
13458,1,"""The article deals with regularization/penalization in the fitting of GANs, when based on a L_1 Wasserstein metric."
13459,1,"\nOverall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs."
13460,1, \n\nSec 6\n Nothing too insightful is said about the RNN Model.
13461,1," \nThe idea presented in this paper is that instead of performing data augmentation in the image space, it may be useful to perform data augmentation in a latent space whose features are more discriminative for classification."
13462,1,\n6) the experiments are not convincing.
13463,1, Could you comment on this problem?
13464,1,"\n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance."
13465,1, and (ii) insufficient attack evaluations - the defender performance is evaluated against weak attacks or attacks with improper parameters.
13466,1," Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication"""
13467,1, How is L_iw affected by the number of samples used in the estimator?
13468,1,"n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). "
13469,1," \n - Page 2: \u201cthe neural network computes the probability for other examples not in the subset\u201d[[CNT], [null], [QSN], [MIN]] \n - Page 3: \u201cthe probability of all the examples conditioned on"
13470,1," In any case, this figure should be corrected to reflect this."
13471,1, Only a simple example will make this paper more convincing.
13472,1,"""Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions."
13473,1,\nThe evidence is presented on very small input data. 
13474,1,"\n\n[1] Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, Ziyu Wang. Deep Fried Convnets. ICCV 2015."
13475,1," Also, DQN does not optimize the Bellman residual, it\u2019s a TD update. \n"""
13476,1," It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task\no\t \u2018inverter\u2019 here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces."
13477,1,\n\n4) The experimental results are a bit less satisfactory:
13478,1,  How should this be tuned so developers would want to use the tool?
13479,1,\n\n* Introduction. I am not convinced by the discussion on graph grammars in the second paragraph.
13480,1,"\n- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)\n- Don't start sentences with And, or But\n\n"""
13481,1," I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is."
13482,1," Instead, the second model could have been proposed directly, with the appropriate citation from the literature, since it isn't new."
13483,1," To solve this task, the paper also presents a new\nmodel architecture that effectively computes a low-rank attention over both\npositions and feature indices in the input image."
13484,1,\n- How the budget in terms of Mul-Adds is actually estimated?\
13485,1, Here it would be interesting to see how CCC fares (in all combinations with cooperators and defectors).
13486,1, It then argues that speeding up the activation function may be important since the convolution operations in CNNs are becoming heavily optimized and may form a lesser fraction of the overall computation.
13487,1,\nWhy did you just apply the rotations only on d_{t}.
13488,1, A corresponding argument for perceptual realism is missing.
13489,1," it looks like finer scales are progressively dropped in successive blocks,"
13490,1, Maybe they could be relegated to the appendix?
13491,1,\n\n=========================\nUpdate after author rebuttal:\n=========================\nI have read the author's response and have looked at the changes to the manuscript.
13492,1,"   Moreover, what support vectors should be removed by optimizing Eq. (7)?"
13493,1, They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.
13494,1, These methods are time consuming in prediction time.
13495,1," Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader."
13496,1,\n\nThe robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem.
13497,1, It also shows that the phenomenon that sharp minima lead to worse result can be explained by Bayesian evidence.
13498,1,\n\nCons:\n- Doesn't introduce and novel methods of its own\n- Could do with additional experiments (as mentioned above)\n- Minor grammatical error
13499,1, The pictorial explanation for how the CNN can mimic BFS is interesting
13500,1,Especially the stochastic gradient method in [2] is strongly related to the existing approach.
13501,1, I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods.
13502,1,"\n\nCons:\n- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there."
13503,1," From the machine learning perspective, the proposed \""attacking\"" method is standard without any technical novelty."
13504,1," It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels."
13505,1,"\n- Results are very promising, with 5x speed-ups and same or better accuracy that previous models."
13506,1, There is no\nclear trend in many of them and a lot of noise. 
13507,1," And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully."
13508,1," For 1) they examine a low-rank version of distributed SGD where instead of communicating full-rank model updates, the updates are factored into two low rank components, and only one of them is optimized at each iteration, while the other can be randomly sampled."
13509,1,"\n- If one needs to run an MLP for each edge in a graph, for each channel and for each layer, the computation complexity seems quite high for the proposed network."
13510,1,The idea is character based.
13511,1,"""The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version."
13512,1," But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective."
13513,1,"  Specifically, they propose to perform the augmentation on the semantic space representation, obtained from the encoder of this autoencoder."
13514,1, Was that meant to be z?
13515,1, It is mentioned in a footnote that variational autoencoders were tested but that they failed.
13516,1,"First of all, \""neural networks are good at generalizing to examples outside their train set\"". This depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample space."
13517,1, It makes it difficult to verify the derivations.
13518,1,\n+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of\nsemantic relatedness between the source and the target sets\n\nFew notes and questions
13519,1,"\n2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection."
13520,1, This will limit the applicability of the approach in most applications where fully connected networks are currently used.
13521,1,\n--There were a lot of unanswered questions:\n (1) how does sharpening lead to lower variance?
13522,1, The weights thus change according to the imposed neighborhood relationship and depending on the class labels.
13523,1,\n\nQuality and significance:\nThe proposed methodology is simple and straightforward.
13524,1,"\n\n\nPart 3 is the most interesting part of the paper,"
13525,1,"It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate."
13526,1," Thus, these non-core units for a particular class could be core units for separating another class from the remaining ones."
13527,1,"  \n\nThe authors show how spike-based learning can be implemented with spiking neurons using such coding, and demonstrate the results on an MLP with one hidden layer applied to the temporal MNIST dataset, and to the Youtube-BB dataset."
13528,1,"This could be interesting for low power case, even if the \""effective compute\"" is larger than the baseline.\n"""
13529,1,"""Compared to previous studies, this paper mainly claims that the information from larger neighborhoods (more directions or larger distances) will better characterize the relationship between adversarial examples and the DNN model."
13530,1," The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data."
13531,1, I think that\nwithout an experiment the small section on non-conjugacy should be removed.
13532,1, The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets.
13533,1,"""After reading the authors's rebuttal I increased my score from a 7 to a 6."
13534,1," One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3."
13535,1," [In general, I also recommend against using figure captions to describe the setup."
13536,1, A discussion of the links is necessary and will clearly bring more theoretical ground to the method.
13537,1,"\nAs far as I am aware, this is a novel approach to the problem."
13538,1," Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples."
13539,1,"\n\nThe paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies."
13540,1, The authors claim that their use in the latent space makes it more practival
13541,1, I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts.
13542,1," First, the pivot objective and bidirectional loss are exactly the same thing."
13543,1, \n- Some details of the experiments/methods are confusing.
13544,1," This seems to contradict the later discussion, where they suggest that probably current architectures cannot handle such visual relationships."
13545,1,"\n\nUsing MFSC, or more commonly known as log mel filter bank outputs, has been pretty common since [8]."
13546,1, though the introduction could have motivated the problem a little better (i.e. why would we want to do this).
13547,1,"   I think the right citation is \u201cIncorporating invariances in support vector learning machines\n\u201c Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 \u201cTraining invariant support vector machines.\u201d ""."
13548,1,\n\n2. The analysis of all facets of the proposal is missing.
13549,1," For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better."
13550,1," The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms."
13551,1, The draft is well written with convincing experiments.
13552,1,and only the 6th is really good.
13553,1," Learn to generate graphs is a key task in drug discovery, relational learning, and knowledge discovery."
13554,1, A domain name data set with two million instances is used for the experiments.
13555,1," There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments."
13556,1," In the worst case, the generator experiences mode collapse and performs badly."
13557,1,Why not compare a classifier trained on only (unperturbed) real data to a classifier trained on both real and synthetic data?
13558,1,"""The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs."
13559,1," The authors use a Resnet-50 which is a\n  smaller and lesser performing model, they do mention that benefits are expected to be \n  complimentary to say larger model, but in general it becomes harder to improve strong models."
13560,1," I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit."
13561,1,"""This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric."
13562,1, Consider expanding on that in section 4.5.
13563,1," Furthermore, simulation and real data examples to explore the properties and utility of the method are required. "
13564,1, The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.
13565,1," Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return."
13566,1,"\n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated."
13567,1,"\n\n[8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013"
13568,1,"\n\nThis skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn't converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don't see why they should be using the same batch size)."
13569,1,:\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve?
13570,1, The experiments are small scale and it is unclear how much the method improves random grid search.
13571,1,"  \"" I'm not sure I follow though, because the alpha terms do indeed depend on the word $t$, as per equation (1), which includes v_t, a vector representation of the target aspect."
13572,1,"""The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional\nnetworks to graph-structured data."
13573,1, An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown.
13574,1,  I'm also interested that clear examples of the information bottleneck principle in practice (e.g. CCA) are rarely mentioned.
13575,1," So, what is the advantage of the proposed \u201csc\u201d method compared to the \u201csc-seq\u201d method?"
13576,1,"""This paper proposes a new machine comprehension model, which integrates several contributions like different embeddings for gate function and passage representation function, self-attention layers and highway network based fusion layers."
13577,1," You need to mention explicitely somewhere that (w,\\theta) are optimized jointly."
13578,1, 3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity.
13579,1, The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information.
13580,1,"\n\n\nFor the JCP-S model, the loss function is unclear to me."
13581,1,\n7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution?
13582,1," There are vague references to the policy being difficult to define, but that motivates the importance of learning in general, not deep RL."
13583,1,"\n\nThe paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general:"
13584,1,\n- The algorithm that performs well is not the one that was actually derived.
13585,1," The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once)."
13586,1," although more of a discussion of the relationship to the technique of Genevay et al. would be useful: how does your approach compare to the full-dual, continuous case of that paper if you simply replace their ball of RKHS functions with your class of deep networks?"
13587,1,". In the construction presented here, the network\u2019s size is essentially in the layer of size m.."
13588,1, \n* Section 3.2. The functions f_m and g_m for defining graph embedding are left undefined.
13589,1,"\n\nTo summarize, I don't think there is enough interesting novelty in this paper."
13590,1,"\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions."
13591,1, but the paper does not provide many details on the underlying approach.
13592,1," but I found it quite hard to follow, especially Section 4, which I thought was quite unstructured."
13593,1, The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus.
13594,1,  Such kernels are very related to neural networks (for instance PNG kernels with linear rectifier nonlinearities correspond to random layers in NNs with ReLU) and in the NN context are much more interesting that radial basis function or in general shift-invariant kernels.
13595,1,"\n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax."
13596,1,  I think that the problem of accelerating this approach is a critical point that this publication is missing.
13597,1,"\n* The work on multimodal embeddings like \""Multimodal Distributional Semantics\"" by Bruni et al. or \""Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception.\"" by Kiela et al. could be discussed/cited."
13598,1,\n\nNotes to authors:\n\nI'm not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying.
13599,1,"\n\nIn the discussion of Table 1, it would be helpful to spell out the differences between the different Bary proj algorithms, since I would've expected EMD, Sinkhorn and Alg. 1 with R_e to all perform similarly."
13600,1,".\n\n--\nAfter rebuttal\n--\nAuthors have answered to many of my comments, I think this is an interesting paper, I increase my score.\n"""
13601,1," Given N inputs and output, this method allows one to specify a linear transformation with O(log(N)) parameters, and perform a forward and backward pass in O(Nlog(N)) time."
13602,1," Overall, this paper contributes a useful new dataset that can be quite useful for reading comprehension."
13603,1," the paper suffers from many problems in clarity, motivation, and technical presentation."
13604,1," One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. "
13605,1,"\n\nIt would be useful to describe exactly the extent to which supervision is used - the method only needs positive and negative links, and does not require any additional order information (i.e., WordNet strictly contains more information than what is being used)."
13606,1,\n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*.
13607,1, The first step of building a supervised initial model looks straight forward.
13608,1," The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm."
13609,1,  Substantial data set of 29m lines of code.
13610,1,  Can your work be compared experimentally with any of the constructive methods from the related work section?
13611,1," As shown in in Fig.8, the estimated dimensions and original dimensions are very different."
13612,1," This is a critical issue, because the experiments show that there is no gain in terms of performance to learn a shared embedding manifold (see DA-DRL versus baseline in figure 5)."
13613,1,\nShows a number of qualitative and quantitative results
13614,1,"  I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2"
13615,1,"\n\nA second significant improvement would be to add an in-depth  running example in section 3, so that the authors could illustrate why the BR strategy makes sense (Algorithm 2)."
13616,1," However, as the authors mentioned, there are more recent works which give better performance than this one."
13617,1, Authors should clarify how they use the inference network and what the two arrows from this inference network represent.
13618,1," Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention"
13619,1," \nThe paper is very easy to follows, and the results are explained in a very simple way."
13620,1,\n\nPositives:\n- The three properties of visual concepts described in the paper are interesting.
13621,1, The authors mention Loshchilov & Hutter in next section but do not compare it to their work.
13622,1," Then, the model used this intermediate representation to guide the navigation of agent in the 2D map and visual question answering as well."
13623,1," But the accuracy of both the models is significantly lower than that of their base model (BiDAF) on SQuAD, demonstrating the difficulty of the DuoRC dataset."
13624,1, \n\n(4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3
13625,1,  I'd like to see the idea works on larger scale problems -- as that is where the computation/speed matters.
13626,1,"\nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved."
13627,1," However, the main issue is that the meta-learning mechanism is a bit ad-hoc and empirical - therefore not sure how seamless and user-friendly it will be in general, it seems it needs empirical studies for every new application - this basically involves generation of a pareto front and then choose pareto-optimal points based on the user's requirements, but it is unclear how a privy net construction based on some data set considered from the internet has the ability to transfer and help in maintaining privacy in another type of data set, e.g., social media pictures"""
13628,1, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge.
13629,1,"\nSignificance - Apart from the issue of intractable inference which is arguably a large limitation of this work,"
13630,1,"\n5. General: How does your method compares with other factorization approaches, such as in \""Factorization Tricks for LSTM Networks\""?"
13631,1,  It is hard to believe that meaningful results are achieved using such a small dataset with random initialization.
13632,1, More explanation needed here. 
13633,1," Though it makes sense that your regularization might lead to a better estimator, you don't seem to have shown so either in theory or empirically."
13634,1,\n\n- Here's another good paper to cite for the end of 2.2.1:\n  https://arxiv.org/pdf/1707.00683.pdf.
13635,1," Similar experimental setups have been used in previous papers to report that a proposed method can achieve good results, but there is no doubt that this does not make a rigorous comparison without employing expensive hyper-parameter searches."
13636,1,"How do different agents perceive their time, is it synchronized or not?"
13637,1," In any case, these methods are useful to document, but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the paper."
13638,1, This paper suggests to factorize the recurrent weight matrix as a Kronecker product of matrices.
13639,1," To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images."
13640,1," And on a related note, how were the number of sampled"
13641,1," As a result, a gradient descent algorithm converges to the unique solution."
13642,1, The resulting DCN+ model achieved significant improvement over DCN.
13643,1, A major merit of DNN is that it can automatically extract useful features.
13644,1,"  \n\nThere are also some questions that to me, remain unaddressed namely:\n\ni.] the model of the experiments, particularly a description of the structure of the pong players' dilemma in terms of the elements of the partially observed Markov game described in definition 1."
13645,1,";\n- Gradient penalties work in all settings, but why is not completely clear;"
13646,1,"  The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students."
13647,1," - except for MultiCopy where it trains faster,"
13648,1,\n\nThis is an intriguing idea
13649,1,"""This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum."
13650,1,"\n\nIn section 4, in the algorithmic steps"
13651,1,"  They compare this\nbound with a similar recent bound proved by Bartlett, et al."
13652,1,"""This paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word (maybe head), the preposition, and right word (maybe dependent). "
13653,1,  I wonder if this brings redundant parameters that do not guarantee convergence.
13654,1,"\n\nIn Figure 4, the trajectories generated by the different eigenoptions are barely visible."
13655,1," \n\nTo improve readability, the authors should propose a diagram of the network, summarizing all notations"
13656,1," I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise."
13657,1, Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers.
13658,1," On other hand, using a unimodal function to model each class is an over-simplification that ignores intra-class complexity."
13659,1, It is unfair to compare to the baseline models with much fewer parameters.
13660,1,\nThis is a significant result in language modeling and a milestone in deep learning reproducibility research.
13661,1," Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated."
13662,1,Pipeline: -Data are augmented with domain-specific transformations
13663,1, This work proposes to do sequential prediction of adjacent pixel features (via intermediate feature maps) resulting in more spatially smooth outputs for deconvolution layer.
13664,1, The test not only distinguishes real from synthesized faces but also evaluates the observer ability by determining whether the observer is a human.
13665,1, The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.
13666,1, This is of course always possible in the MDP formalism.
13667,1," \nAdding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane."
13668,1, it is just hard to get excited by it.
13669,1,\nThe presentation of this algo is a bit short and could deserve more space (in the supplementary)
13670,1," Experiments show that the system achieves a better performance than different subparts of the system (through an ablation study), state of the art and common open source systems."
13671,1,"""The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator."
13672,1,"""This paper is an extension of the recent language style transfer method without parallel training pairs (Shen et al. 2017)."
13673,1, Is this because of some stopping condition or because of gradient explosion?
13674,1, I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations.
13675,1,"  Given its reduction of processing-intensive and need for larger number of iterations, how much worse is the random choice (no processing, independent of iterations)?"
13676,1," They first take co-occurrence counts counts of pairs of words in a local window of each preposition, and then factorize the matrix to find low dimensional word representations."
13677,1, Several experiments demonstrate the quality of the prediction and the uncertainty over dropout. 
13678,1,"\n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting."
13679,1," \nThe proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, \u2026)."
13680,1, JSMA and deep fool are not considered strong attacks now (see Carlini's bypassing 10 detection methods paper).
13681,1," Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image."
13682,1,"\n\n- Depth versus width: Another measure that is missing is also the number of feature maps how wide is the network , how does this interplays with the depth?"
13683,1," Examples include\n\n@article{wang-adelson-94,\n  author        = \""Wang,  J. Y. A. and Adelson, E. H.\"",\n  title         = {{Representing Moving Images with Layers}},\n  journal       = {{IEEE Transactions on Image Processing}},\n  year          = \""1994\"",\n  volume        = \""3(5)\"",\n  pages         = {625-638}\n}\nsee http://persci.mit.edu/pub_pdfs/wang_tr279.pdf\n\nand\n\n@article{frey-jojic-03,\n   author    = {Frey, B. J. and Jojic, N.},\n   title     = {{Transformation Invariant Clustering Using the EM Algorithm}},\n   journal   = {IEEE Trans Pattern Analysis and Machine Intelligence},\n   year      = {2003},\n   volume    = {25(1)},\n   pages     = {1-17}\n}\nwhere mask and appearances for each object of interest are learned. "
13684,1,\n\nThe comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform.
13685,1,\n\nI believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results.
13686,1, I think these experiments should be elaborated on.
13687,1,"\n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop."
13688,1, \n\n2) The finding that regularizing the RNN makes it more closely match the neural code is also foreshadowed somewhat by the 2015 Nature Neuro paper by Susillo et al.
13689,1,\n\nQuality\nThe paper makes relevant comparisons and is overall well-motivated.
13690,1, Can bugs and suboptimal configurations be ruled out during the experiments?
13691,1," \n\nThus I think that the paper should be published."""
13692,1," In Kangaroo, the domain where your method does best, the l2 error is worse."
13693,1,"\n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment."
13694,1,"""Paper Summary:\n\nThis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network."
13695,1,"""\nThis paper explores the use of simple models for predicting the final\nvalidation performance of a neural network, from intermediate values\nduring training."
13696,1,\n\nSpecific comments: I would remove the significance daggers from table 2 as the standard deviations are already reported and the null hypothesis for which significance is measured seems unclear.
13697,1, The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles.
13698,1," Unfortunately, this was not done. I suspect that in this case, the results would be very similar. \n\n"""
13699,1, However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks.
13700,1,\nand form a predictive distribution over the output for all the remaining possible inputs.
13701,1,"  Regarding the latter methods: what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish, and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be made."
13702,1,---------------\n\nI want to love this paper.
13703,1," the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that."
13704,1,"\n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach."
13705,1, Was the same procedure done for the experiments in the paper?
13706,1,"\n\nIn the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing."
13707,1,  Was it due to the convolutional net structure (you could test this)?
13708,1, This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.
13709,1,". For example, \n\n1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators."
13710,1, The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs.
13711,1," Arguably, the goal of RL algorithms is to learn to exploit their environment as quickly as possible in order to attain the highest reward."
13712,1," More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate."
13713,1, Are these part of the encoder or decoder?
13714,1, Motivation given in form of relevant applications and mention that it is relatively unstudied\n\t\u2022\t
13715,1,"3) when exact matches exist, simpler methods may be sufficient, such as matching edges."
13716,1,"""Pros:\nThe paper is easy to read."
13717,1, As I understand from the paper it is just compiling this already available data.
13718,1," \n\nIt can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent."
13719,1," \n\n-- While the authors have convinced me that data augmentation indeed significantly improves the performance in the domains considered (based on the results in Table 1 and Figure 5a),"
13720,1," r -> real? g -> generating? G -> generating? Sometimes, no subscripts are used (e.g., Fig 4 or figures in Sec. 8.13)"
13721,1,"\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured."
13722,1,\n\n** PAPER SUMMARY **\n\nThe author proposes to combine siamase networks with an SVM for pair classification.
13723,1, Where the paper falls short is motivating the problem setting. 
13724,1,"""Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results."
13725,1, Were they annotated by experts in the medical domain or random users?
13726,1, They have been extended to 2-d later on (Spatio-temporal TDNNs)
13727,1,\n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable.
13728,1, Methods and results are clearly described.
13729,1," By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level."
13730,1," I\u2019m not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals."
13731,1, A better generative model for cycles and trees could help.
13732,1," For example, the phrase \u201ccontinuous input sequence\u201d does not make sense; maybe you mean \u201cinput sequence of real valued quantities\u201d.\n\n"
13733,1,"""The paper describes a neural end-to-end architecture to solve multiple tasks at once."
13734,1,"""The paper presents a model titled the \""unsupervised tree-LSTM,\"" in which the authors mash up a dynamic-programming chart and a recurrent neural network."
13735,1,"""This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program."
13736,1, They put the this specific work in the right context of imitation learning and IRL.
13737,1," The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013)."
13738,1,"The main algorithm has not been properly developed; there is too much focus on the convergence aspects of the inner iterations, for which there are many good algorithms already in the optimization literature."
13739,1," Please see the major comment below.\n\nOne major comment:\n- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side."
13740,1," Therefore, I am not convinced that the paper is ready for publication at ICLR'18."
13741,1,\n\nI feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper.
13742,1, \n\nThe idea proposed is fairly straight-forward.
13743,1," To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet."
13744,1," This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf"
13745,1, They found it improved the accuracy.
13746,1,  paper has a number of critical problems. 
13747,1,"""Summary: This paper explores how to handle two practical issues in reinforcement learning."
13748,1,  The punchline of the paper is that the authors are able to achieve similar performance as \u201cfull ResNet training\u201d but with significantly reduced training time.
13749,1," However, the paper only did a very simple investigation on related works."
13750,1," They compare this optimization method with two baselines in MNIST and CIFAR, and provide an analysis of the decision boundaries by their adversarial examples, the baselines and non-altered examples."
13751,1," The main point is that the intervention distributions are correct (this fact seems to be there, but is \""hidden\"" in the CIGN notation in the corollary)."
13752,1,"  But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons."
13753,1,  Is it because the method can be understood as some form of block-coordinate Newton with momentum?
13754,1,"\n\n* You have a complicated method for constraining the parameters to be in [-0.5,0.5]."
13755,1, It would have been helpful if the author(s) could have made a formal statement. 
13756,1,\nThere is no obvious technical mistake  and the paper is written reasonably well.
13757,1,"""This paper proposes a model for learning to generate data conditional on attributes."
13758,1,\n\nMy main concern with this paper is the motivation: what are the practical scenarios in which one would want to used proposed method?
13759,1,"\n\nI am very disappointed in the authors' choice of evaluation, namely bAbI - a toy, synthetic task long abandoned by the NLP community because of its lack of practicality."
13760,1," \n\nWhile the results in this paper look good,"
13761,1,Is this a problem in practice (it seems to happen on your curves)?
13762,1,"  The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice."
13763,1," The authors state this question in the future direction, but it would make the paper more complete to consider it here.\n"""
13764,1,"\n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?"
13765,1," The authors suggested one possibility, namely that the server and some workers are located on the same machine and the workers take most of the computational resource"
13766,1," I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \\phi, a variational approximation over both \\theta and \\phi is needed, and a q that couples \\theta, \\phi and and the gradient of the log likelihood term wrt \\phi is chosen."
13767,1, It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge.
13768,1," \nThey also evaluate the algorithms on the quality of the final policies for their approach, DQN, \nand  a supervised learning from demonstration approach ( LfD ) that requires expert actions."
13769,1," In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications."
13770,1, I would suppose that flatness tends to increase the variability captured by leading eigenvectors ?
13771,1,"""Quality\n\nThe authors introduce a deep network for predictive coding."
13772,1,"\n\nIn detail:\n- please fix several typos throughout the manuscript, and have a native speaker (and preferably an ASR expert) proofread the paper"
13773,1,\n\nOne interesting experiment but missing in section 4.4 would be how the image features change after fine-tuning for the captioning task.
13774,1,\n- Figure 2 is also not clear.
13775,1,"""This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long)."
13776,1," It was hard to read and understand their value, just because mostly the text was structured as one lemma after the other."
13777,1,"\n- I suggest the authors to do some more literature review on tree generation\n"""
13778,1," Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task."
13779,1,What is the factor of augmentation
13780,1,\n- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering
13781,1," However, it is really nice to be able to decouple the hidden size and the number of recurrent parameters in a simple way."
13782,1,\n\nI have no major criticisms.
13783,1,"\nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016."
13784,1,\nA thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation.
13785,1," The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors."
13786,1,"\n- Not clear that the idea will be useful in more complex domains with unbounded inputs.\n"""
13787,1, I  would like to see more careful architecture search and ablation studies.
13788,1,  \n5. How many layers is the DenseNet-BC used in this paper?
13789,1,\n2. The notation D for a dataset in Section 3.3 is confusing with D in system D.
13790,1, I was aware that the features mostly refers to the inputs to softmax.
13791,1," This is misleading, as you need to store the weights of all convolutional layers to compute the forward pass and the majority of the weights of all convolutional layers to compute the backward pass, no matter how many weights you intend to update."
13792,1,. Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ? 
13793,1, More details are needed to understand this.
13794,1, The architecture consists of a \u2018slow\u2019 network which provides weight updates for the \u2018fast\u2019 network which outputs the predictions of the system.
13795,1," Unfortunately, the paper requires significant improvements, both in terms of substance and in terms of presentation."
13796,1," it missed several recent works in this area:\n- Steerable CNNs, Cohen & Welling\n- Dynamic Steerable Blocks in Deep Residual Networks, Jacobsen et al."
13797,1,It would be excellent if the authors can extend this to higher dimensional time series.
13798,1,"  The would help understanding the proof, and possibly reuse the same idea in different context."
13799,1," For instance in Figure 4a, the task is described as \u201cshowing the changes in the attribute latent variables\u201d which gives the impression that, e.g. for the first row the interpolation would be between a purple triangle to a purple rectangle however in the middle the intermediate shapes also are painted with a different color. It is not clear why the color in the middle changes."
13800,1,\n\nThe main issue I am having is what are the applicable insight from the analysis:
13801,1," Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape?"
13802,1, The use of an auxiliary loss to control the number of state updates is interesting;
13803,1, \nThis constraints may not be necessary if instead they used proximity space representation.
13804,1,  But is there a pattern?
13805,1, The authors train their system with the testdata included which leads to very different visualizations.
13806,1,\n\n- The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack.
13807,1," See for example [Ref1, Ref2, Ref3].[[CNT], [null], [DIS], [GEN]]\nb) Figure 3 is not satisfactory."
13808,1,"  It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent."
13809,1,"\\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL"
13810,1," There is also a bit of overloading going on: at the start of 2.1 it appears f_theta refers to the true function, then later the classification model, and then in eq (4) a linear approximation of the classification model."
13811,1," \n\nAt some point in the first 4 pages it would be good to explain what is meant by ``hard\u2019\u2019 functions (e.g. functions that are hard to represent, as opposed to step functions, etc.) \n""."
13812,1," \u201d\n\nOverall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem."
13813,1," However, the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection."
13814,1, The proposed methods are evaluated on two car datasets.
13815,1, Since recurrence does not guarantee that all information needed is captured.
13816,1, Couldn't the same or a very similar trick be used to correctly rescale $A$ every time one increases the batch size?.
13817,1," \n\n1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387\u201391, 2005.\n2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91\u2013107, 2010.\n3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013."""
13818,1, The basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value function.
13819,1, Is it due to a well crafted search space that is potentially easier?
13820,1," The proposal is to replace the non-linearity in half of the units in each layer with its \""bipolar\"" version -- one that is obtained by flipping the function on both axes."
13821,1,"\n\n- Since the model gets information from the AP and HP before doing any\n  iterations, why not go on and use that to help select candidates?"
13822,1,"\""recent studies\"", \""several studies\"", etc.\n- \""i.e, the improvements\"" -> \""i.e., the improvements\""\n\"
13823,1,  The weights are based on the confidence of the value function of the n-step return.
13824,1,"It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm."
13825,1," Further the claims all over the paper, comparing with the existing works. are over the top and not justified."
13826,1, It is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable examples.
13827,1,did you study the effect of the architectures in terms of striding and pooling how it affects the results?
13828,1," \n\nMajor concerns:\n1.\tThis work brings some modifications to the prediction layer, which is a bit trivial."
13829,1,"\n\n- I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack (such as PGD or region based), or for defenses that know this is a likely attack (see the following comment as well)."
13830,1,"\n\nOverall the work is important, original, well-executed, and should open new directions for deep learning in program analysis."
13831,1,  Overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capacity.
13832,1,"\n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn\u2019t fully utilize the deep net performance."
13833,1, The idea of using similar sentence pairs as cluster-to-cluster translation is interesting.
13834,1, It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those.
13835,1, Building GAN's that operate above-and-beyond moderate spatial resolution is an open research topic.
13836,1," The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed."
13837,1,"\u201d\n\n\u201cEvolutionary Strategies\u201d, at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do."
13838,1, The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm.
13839,1," \n\nDetailed Comments:\n\u201cIn the of NaaA\u201d => remove \u201cof\u201d?[[CNT], [null], [QSN], [MIN]]\n\u201cpassing its activation to the unit as cost\u201d => Unclear.[[CNT], [null], [CRT], [MAJ]] What does this mean?[[CNT], [null], [QSN], [MIN]]\n\u201cperformance decreases if we naively consider units as agents\u201d => Performance on what?"
13840,1," Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious."
13841,1,\n- What are the \u201ctier-1\u201d and \u201ctier-2\u201d models in this section?
13842,1," They second part of their article, where they test the examples created by their models using behavioral experiments was less convincing. "
13843,1, Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.
13844,1, but not sufficient.
13845,1,\n\nI have a favourable impression of this paper
13846,1," So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the \""classification\"" function is pushed down to lower layers, as the upper layers are reduced in size."
13847,1,"\n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015."
13848,1,"  However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks)."
13849,1,\n\n- What kind of safety constraints cannot be expressed by masking actions?
13850,1, Some examples :\n\n\u00ab\u00a0provide a compact representation of a dynamical system\nby representing state as a set of predictions of features of future observations.
13851,1,"""This work re-evaluates complex-valued neural networks: complex weights, complex activation functions."
13852,1,"The nets, in principle, could learn to recognize objects based on shape only, and the shape remains stable when the color channels are changed."
13853,1," The paper's main claim is that this model architecture enables strong\ngeneralization: it allows the model to succeed at the instruction following task\neven when given words it has only seen in QA contexts, and vice-versa."
13854,1," Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs."
13855,1," The main originality seems to be captured in Algorithm 1, which computes the strength between two words."
13856,1," You have very few sequeces/subjects.[[CNT], [SUB-NEG], [DFT], [MIN]] Did  you split by *subject*? I think this is CRUCIAL, and a lot of the results hinge on this answer."
13857,1,\n- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.
13858,1,". The technical novelty is somewhat limited to a minor (but powerful) change in approach from Nakayama and Nishida, 2017; however, the resulting translators outperform this previous method."
13859,1, This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!
13860,1," Since the main contribution is to use an existing algorithm to tackle a practical application, it would be more interesting to tweak the approach until it is able to tackle a more realistic scenario (mainly larger scale, but also more realistic dynamics with traffic models, real data, etc.)."
13861,1," \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality."
13862,1,"""This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm."
13863,1,Problem statement well defined mathematically and understandable for a broad audience\n\t\u2022\t
13864,1," Even better: add the expression \""P(male = 1 | mustache = 1) = 1\""."
13865,1," Intuitively, the randomness in target may achieve certain regularization effect."
13866,1,"n\n-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.\n\nTypo:\nSection 5.1 is build of -> is built of\n"""
13867,1," In fact, ideas for evolutionary methods applied to RL tasks have been widely studied, and there is an entire research field called \u201cneuroevolution\u201d that specifically looks into which mutation and crossover operators work well for neural networks."
13868,1,\n- Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors?
13869,1,"\n\n- In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10)."
13870,1, The authors have only evaluated the method on a synthetic associative retrieval task.
13871,1," \n\nAfter rebuttal:\nThe current version of the paper still needs significant amount of work regarding the experimental part."""
13872,1," In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that?"
13873,1,\n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new.
13874,1,"\n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified."
13875,1,"  However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head."
13876,1,  I find the analysis made by the authors to be very simplistic.
13877,1," I think ideally, I would want to see this on Atari or some of the continuous control domains often used."
13878,1," In the literature, specially in machine learning, there is ``fever\u2019\u2019 about HMC, in my opinion, partially unjustified."
13879,1,  Do the authors think researchers never tried to do this task before then?
13880,1,"  Although it is a common base-line, some choices are not clear: why using a FFNN instead that a CNN which performs better on this dataset; how data is presented in terms of temporal series \u2013 this applies to the Temporal MNIST too; why performances for Temporal MNIST \u2013 which should be a more suitable dataset \u2014 are worse than for the standard MNIST; what is the meaning of the right column of Figure 5 since it\u2019s just a linear combination of the GOps results."
13881,1,\np3. Are you using an L2 weight penalty?
13882,1,"""This paper suggests a \""learning to teach\"" framework. Following a similar intuition as self-paced learning and curriculum learning, the authors suggest to learn a teaching strategy,  corresponding to choices over the data presented to the learner (and potentially other decisions  about the learner, such as the  algorithm used). "
13883,1,"  The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning."
13884,1,My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions
13885,1," However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space."
13886,1," If I understand correctly, only the HDDA model is used to get the final score, and the C^i are only used to make the \\phy(x) representation more class-seperable. Is that right?"
13887,1,"""The paper seems clear enough and original enough."
13888,1,\n4. evaluates different f within MCTS for MiniRTS.
13889,1,\n\n\nEXPERIMENTS:\n\nLavaworld: authors show that pretraining the PATH function on longer 7-11 step policies leads to better performance\nwhen given a specific Lava world problem to solve.
13890,1,"\n5. Present results on larger-scale applications (Text8, Teaching Machines to Read and Comprehend, 3 layers LSTM speech recognition setup on TIMIT, DRAW, Machine Translation, ...), especially because your method is really easy to plug in any existing code available online."
13891,1,"\n - do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains?"
13892,1,"\n* In Section 5.2, what is \""strong inference\""?[[CNT], [null], [QSN], [MIN]] This is not defined previously.[[CNT], [null], [DIS], [MIN]] \n* Have you evaluated on a larger dataset such as CIFAR?"
13893,1, It would be interesting to see these results.
13894,1,"""The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar."
13895,1,"  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging."
13896,1," \nSee for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area."
13897,1,  Experiments are based on the federated averaging algorithm. 
13898,1," Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas."
13899,1, and the proofs are clearly written.
13900,1,  \nThe authors clearly performed an impressive amount of sensitivity experiments.
13901,1, Again the choice of T seems ad hoc and based on computational burden.
13902,1,"\n\nComments:\n1) There are a few details missing, like the batch sizes used for training (it is difficult to relate epochs to iterations without this)."
13903,1," However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs."
13904,1," This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described."
13905,1," Here, w has almost converged to its optimum w* = 1."
13906,1, What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data?
13907,1," Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA)."
13908,1, In particular looking at Riverraid-new is the advantage you have there significant?
13909,1,"""Congratulations on a very interesting and clear paper."
13910,1," I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks."
13911,1,"\nThe method is derived by considering the \""rewiring\"" of an (artificial) neural network as a stochastic process."
13912,1,"  In my opinion, one of the main features of this work is to split the NN computation to local computation and cloud computation, which ensures that unnecessary amount of data is never released to the cloud."
13913,1," Furthermore,\nusing larger batches means fewer parameter updates per epoch, so training is\npotentially much faster."
13914,1," On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning."
13915,1," Consequently, the aggregation of all class-specific core units could include all hidden units of a layer."
13916,1,". Since the authors are extending their work and since these issues might cause training difficulties,"
13917,1,  Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks.
13918,1," \n\nWhile the paper reports superior performance, the empirical claims are not well substantiated."
13919,1," The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent."
13920,1," \n\n(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer."
13921,1," For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled?"
13922,1,"""Learning sparse neural networks through L0 regularisation\"
13923,1,"  BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y)."
13924,1, Prog is conditionally independent of X?
13925,1," but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance."
13926,1,"\u201d However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features."
13927,1, The forward models are not specified.
13928,1, while this is an interesting aspect of the paper: this justifies even more the large scale aspect is the algo need less examples during learning to perform similar or even better classification.
13929,1, \n\nI do have some questions about the training itself.
13930,1," \n- Footnote 1, line 2: \""an exchange\""."
13931,1, The application is to predict errors made by students on programming tasks.
13932,1,"""The idea of the paper is to use a GAN-like training to learn a novelty detection approach."
13933,1,"""This paper studies the issue of truncated backpropagation for meta-optimization."
13934,1," Dead-reckoning (i.e., spatial localization from velocity inputs) is of critical ecological relevance for many animals."
13935,1," If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short."
13936,1,\n\nThe results are in general only marginally improved by the baseline corrected non-negativity constrained approach.
13937,1,"\n\nReferences:\n[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. \u201cFrom A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.\u201d arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.\n\n[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014."
13938,1, Does this sacrifice exploration for exploitation in some quantifiable way?
13939,1,"\nFor the non-deterministic information, they have a residual predictor that uses a low-dimensional latent space. "
13940,1,"  Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training."
13941,1,"\n\n=========================================\n\nAfter the rebuttal I've updated my score, due to the addition of FSGM added as a baseline and a few clarifications."
13942,1, This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time.
13943,1,"""The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder."
13944,1," It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs."
13945,1," Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance."
13946,1," \n\nIn (3), R is not defined."
13947,1,\n-- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN
13948,1,"  The PIR value comes from a separate network not directly accessible during training time, nonetheless I would have been surprised to not see an increase."
13949,1, Feels like a hammer in need of a nail.
13950,1,"\n\nThe description of the model is laborious and hard to follow. Figure 1 helps but is only referred to at the end of the description (at the end of section 2.1), which instead explains each step without the big picture and loses the reader with confusing notation."
13951,1,"\n\nDetailed comments\n  \u2022 [p4, basic health gathering task] \""The goal is to survive and maintain as much health\nas possible by collecting health kits..."
13952,1, \n4. Why is the affine transform assumption valid in biology?  
13953,1," The Curse of Dimensionality for Local Kernel Machines. 2005."""
13954,1, Experimental results on the IHDP dataset confirm the advantage of\nthe proposed approach.
13955,1, The authors provide a version with batch SGD as well.\n\n
13956,1," For example, it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent, and bilinear additive upsampling proposed in Wonja et al., 2017 also demonstrated its effectiveness as an alternative for deconvolution."
13957,1, Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?
13958,1,"  In the empirical section, several RNNs are trained using this approach, using only ~ 100 recurrent parameters, and still achieve comparable results to state-of-the-art approaches."
13959,1, our institutional review board would certainly allow self-certification of the data (i.e. removing the patient identifiers and publishing the first 4 hours of sequences).
13960,1," As well, the considered attacks are in L2 norm, and the distortion is measured in L2, while the defenses measure distortion in L_\\infty (see detailed comments for the significance of this if considering white-box defenses)."
13961,1," Said differently, as there\nis no classification results using images produced by an another\nGAN architecture it is hard to say that the extra complexity\nproposed here (which is a bit contribution of the work) is actually\nnecessary."
13962,1,"""The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step."
13963,1,"  Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG."
13964,1," Differently from (Yogatama et al, 2017), this paper doesn\u2019t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way."
13965,1,\n\n(A) The paper derives the main update equation of W which combines self-organization and label-sensitive learning - Eqn. 15.
13966,1,\n\nOn the whole this is interesting work and the results are very nice
13967,1, It is also unclear why speaker-adaptive training is not needed.
13968,1,"\n\nI like the authors experimented with different benchmarks, but lack of comparisons with existing deep clustering techniques is definitely a weakness."
13969,1,"""This paper presents a method based on GANs for visualizing how humans represent visual categories."
13970,1,"\n\nCons\n- In general, learning a Path function could very well turn out to be no simpler than learning a good policy for the task at hand."
13971,1, This might be worth mentioning.
13972,1," For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w\u2019 \\geq w^{k/k\u2019}."
13973,1, It is quite easy\nto probe the test set to get best performance on these benchmarks.
13974,1,"""The paper proposes to combine the recently proposed DenseNet architecture with LSTMs to tackle the problem of predicting different pathologic patterns from chest x-rays."
13975,1," The method relates closely to prior work on action-dependent baselines,"
13976,1," Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead."
13977,1," In particular, the authors point out that in general, SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise."
13978,1,\n3. I do not see how you apply the model from task i to task j when the two have different output spaces.
13979,1,  Even though this idea in not new
13980,1, The paper presents a good work and is well articulated. 
13981,1," If we are just showing images, then they are evaluating image synthesis, which do not necessarily contain the desired properties in videos such as temporal coherence."
13982,1," Since this is not my main area of research I cannot judge its originality in a completely fair way, but it is original AFAIK."
13983,1,\n\n2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d-1) <= tilde{beta}^(d-1) <= e beta^(d-1) is proven from the property |beta-tilde{beta}|<= 1/d beta (middle of p.4).
13984,1,\n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen.
13985,1, The plot looks like CommNet is still improving.
13986,1,"\n\nNevertheless, the parameterisation of the conditional variational distribution q(\\theta | \\phi, (x, y)) using recognition model is interesting and could be useful in other models."
13987,1," Then, they train generative models over the auto-encoder's latent space, both using a \""latent-space GAN\"" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model."
13988,1,  The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate.
13989,1,\n\n** PAPER SUMMARY **\n\nThe authors propose to regularize RNN for sequence prediction by forcing states of the main forward RNN to match the state of a secondary backward RNN.
13990,1, It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious.
13991,1," The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure."
13992,1, That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples.
13993,1, The taxonomy gives no scientific axioms.
13994,1,"n\nThe paper is quite dense and quite difficult to follow, also due to the complex notation used by the authors.\"
13995,1," \nFor relating properly to the literatue, the experiment for speeding up Hyperband should also mention previous methods for speeding up Hyperband by a model (I only know one by the authors' reference Klein et al (2017))."
13996,1," For instance, when learning trees, the system is tweaked for generating trees."
13997,1,"""The authors propose a new CNN approach to graph classification that generalizes previous work."
13998,1," However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis."
13999,1,"  This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here?"
14000,1, I'd expect that it would but the effects of this changed objective are not discussed in the paper.
14001,1, \nThe synthetic data experiment sec5.1 and fig5 is clearly explained and serves to additionally clarify the proposed method.
14002,1, More trials are also needed to alleviate any suspicion of lucky trials.
14003,1, They propose directly optimizing the time-dependent discrimination index using a siamese survival network.
14004,1," Moreover, mapping features to some appropriate feature space has been widely investigated, including the choice of appropriate mapping."
14005,1,"  Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers."
14006,1, I am puzzled why is the error is coming down before the boundary interaction?
14007,1," For now, it looks like more like a trick than anything else."
14008,1,\n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].
14009,1," Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?"
14010,1,"\n- The sentence \""We define ... to be the embeddings of the l words of the sentence that contains s.\"" is not very clear."
14011,1,"\n\n4. The line before (19) is confusing, since (19) is exact and not an approximation."
14012,1,"   - inside zeta_2 and zeta_3, do you not mean $m^t\"" and $\\bar{m}^t$ ?\n"
14013,1,"\n\n(2) The paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding, which help a lot for understanding how the proposed methods contribute to the improvement."
14014,1, Was there any KL reweighting scheduling as done in the original BBB paper?
14015,1, \nFigure 3 and Figure 5 illustrate the brain maps generated for Collection 1952 with ICW-GAN and for collection 503 with ACD-GAN.
14016,1, \n\n\n--------------\nStrengths:\n--------------\n\n- The paper is fairly clearly written and the figures appropriately support the text.
14017,1,\n\nI am not familiar with the task at hand so I cannot properly judge the quality/accuracy of the results obtained but it seems ok.
14018,1,"\n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2."
14019,1,"\n\n* In table 2, it is not clear how significant these differences are."
14020,1," One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable."
14021,1, The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is.
14022,1,"  Unfortunately, this paper is lack of novelty."
14023,1, Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.
14024,1,"\n\nOn empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is."
14025,1,"\n\n5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015), \""n\"" abruptly appears without proper introduction / context."
14026,1,". They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style."
14027,1,"\n  \u2022 On page 3, where delta_t is defined (the j step return TD error, I think the middle term should be $gamma^j V(S_{t+j})$ "
14028,1,"\n\nThis submission claims that:\n[a] \u201c[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms\u201d,\n[b] \u201cfollowing training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal\u201d,"
14029,1,", and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs."
14030,1, Using a embedding representation based on visual concepts is straightforward.
14031,1,"\n\nThe paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms. "
14032,1,"\"" Probably fairer/safer to say: did not report results on Atari games.\n"""
14033,1,"\"" -> The existing network architecture is used to provide a variational inference framework for I(Z,Y)."
14034,1," After estimating the values of this parametrization, the authors formulate the problem of finding optimal control inputs as a large convex problem."
14035,1," The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation."
14036,1," (c) has no reference equation, does it?"
14037,1, In the experiments the authors generally show improved convergence over SVAE.
14038,1,". For instance, in the case of MNIST, rotations with different degrees are applied"
14039,1,"  The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach."
14040,1,\n7) Some small feedback: The notation $< x_i > = 0$ and $< x_i^2 > = 1$ is not explained.
14041,1," While this may be statistically significant, it is a very small gain nonetheless."
14042,1,"""The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x)."
14043,1, The best GAN models for generating high resolution images are  difficult to train and it is not clear if they would work in this setting.
14044,1,n- Paper is clearly written\
14045,1,"\n(2) It is not clear how \""y^b_n can be expressed as \\sum_{k=1}^K z_{nk}f_k(x_n)\""\nin general."
14046,1,\n\n3) Conditional batch normalization.  I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator. 
14047,1," I found the architecture very intuitively laid out, even though this is not my area of expertise."
14048,1," Other datasets such as ImageNet, Cifar10/100, Celeb A, etc., should also be included."
14049,1,"\n\nFor active learning, the proposed method seems to be specific to the case of obtaining a single label."
14050,1,"   - \""only affects $x_{\\bar{m}^t}$\"": should be $x'_{\\bar{m}^t}$  (prime missing)\n "
14051,1,"\n3) Presentation is suboptimal, and many details are missing. For instance, architectures of networks are not provided."
14052,1," With PCL, one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attacks."
14053,1," On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the \""Two problems with variational EM... \"" paper by Turner and Sahani (2010)."
14054,1,\n\n3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use\nthe concrete transform.
14055,1," While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. "
14056,1,"""The authors test a CNN on images with color channels modified (such that the values of the three channels, after modification, are invariant to permutations)."
14057,1," Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against \""shared\"" adversarial perturbation, in particular against universal perturbation."
14058,1, Doesn\u2019t this support the argument about need in stochastic prediction?
14059,1, TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices. 
14060,1," For example, the higher bound of the memory required can be reduced to about 40% for a Crescendo block with 4 paths where interval = 1.\"""
14061,1," and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique. "
14062,1,"\n\nDuring learning SR and the features, what would be the impact if the gradient for SR estimation were also propagated?"
14063,1," For example, Figure 5-7 show variable sizes of the generated outputs."
14064,1," While the idea is sound,"
14065,1, Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well.
14066,1,"\nIt's also common practice to analyze the representations learned, in\nmany deep learning papers."
14067,1, Could you provide a comparison with EM?
14068,1, This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets.
14069,1,\n\nOptimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters.
14070,1, but nothing really serious.
14071,1," While CrescendoNet seems to slightly outperform FractalNet in the experiments conducted, it is itself outperformed by DiracNet."
14072,1,\n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function.
14073,1,"\n - What about a reward based on a general language model, rather than one that relies on L_{ml} in Equation (14)?"
14074,1,"  To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?"
14075,1,"\n\nThe underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique."
14076,1,  It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader.
14077,1,\n\nThis paper highlights this problem as a fundamental issue limiting meta-optimization approaches.
14078,1,\n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning.
14079,1," The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set. "
14080,1," \n\n-\tThe point brought about CNN\u2019s failing to generalize on same-ness relationships on sort-of-CLEVR is interesting, but it would be good to know why PSVRT provides better generalization."
14081,1,"\n\nMain Comments:\nThis could potentially be a very nice paper, but I feel the current presentation is not ready for acceptance."
14082,1, The results seem to indicate that all layers are basically performing the same.
14083,1, They conclude with empirical observations about the performance of this algorithm.
14084,1, The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima.
14085,1,"The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2."
14086,1,  Is it the optimal solution (x_t)?
14087,1,"\n\nMy biggest concerns that dampen my enthusiasm are some assumptions that may not be realistic in most controls settings:\n\n- First, the most concerning assumption is that of a symmetric LDS matrix A (and Lyapunov stability)."
14088,1, Is it possible to communicate\nthe intuitions behind what is going on?
14089,1," For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real."
14090,1," In my point 4, I was suggesting that in order to have clustering performance, one might alternatively work on the softmax outputs instead of the inputs."
14091,1,"  I'm also unsure whether the windows are over spatial extent only, or over features."""
14092,1,"\nFor ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop."
14093,1,"""In the context of multitask reinforcement learning, this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them, in the form of a task graph."
14094,1,\n4. Does the  Hungarian algorithm used for matching scales to much larger datasets?
14095,1,"  \n\nIn Fig 3. The full batch loss of Adam+ProxProp is higher than Adam+BackProp regarding time, which is different from Fig. 2."
14096,1," First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. "
14097,1," If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication."
14098,1, \n3-\tSo the main benefit of the approach seems to point towards the direction of what possibly happens in real brains.
14099,1,\n\nQuestions: \n- How would the model need to change to account for example difficulty?
14100,1,\n12. The paper mentions the application of image representation but only experiment on 32x32 images.
14101,1,"\n\nThe proposed problem is an explicitly adversarial setting and adversarial examples are a well-known issue with deep networks and other models, but this issue is not addressed or analyzed in the paper."
14102,1, The proposed method was evaluated on a mobile indoor navigation task and a knot tying task.
14103,1, \n\nI have two comments on the experiment section:\n\n- Choice of experiments.
14104,1,\n\nGeneral Review:\n\nMore experimentation with the latent codes will be interesting:
14105,1, and b) The technique still relies on skip connections in a sense so it's not clear that it suggests a truly different method of addressing the degradation problem.
14106,1, The experimental results show the authors have found a way to use second order methods without making performance *worse*.
14107,1,"\n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant? "
14108,1,\n\nCons:\n- There is no new methodology proposed.
14109,1," Why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem);"
14110,1," If I understand what is being evaluated correctly (i.e., best random guess) then I am not surprised the EEN can perform better with enough random samples."
14111,1," The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results:"
14112,1, result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.
14113,1,"  Specifically, estimating the log probability to be proportional to the number of edits in the Levenshtein distance is really not the right thing to do at all."
14114,1,"However, I didn\u2019t see such detailed analysis as in the other papers on controllable image generation."
14115,1,"\n\n3. The other main contribution of the paper is Theorem 3, which shows\u2014via a very particular construction on the generator and encoder\u2014that bidirectional GANs can also suffer from serious mode collapse. "
14116,1, It seems like the method could be more informative than the other methods.
14117,1,"\n- IMO, the structure of the paper can be improved."
14118,1,\nQ2: The authors mention topic alignment without specifying what the topics are aligned to.
14119,1, The idea proposed (learning a selection strategy for choosing a subset of synthesis examples) is good.
14120,1,"\n    2. Section 1, the citation format of \""Bengio et al. (2013)\"" should be \""(Bengio et al. 2013)\""."
14121,1,\n\nIt is not clear what is the stopping criterion for each of the methods used in the experiments
14122,1, The paper also contains grammatical errors and is somewhat difficult to understand. 
14123,1,\n- The paper points out situations when the methods are equivalent
14124,1, Could there be a way to measure the invariances?
14125,1,"  This will be an interesting paper to have at the conference and will spur more ideas and follow-on work."""
14126,1, Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant.
14127,1, Where is this restriction coming from?
14128,1, It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.
14129,1,\nWhat are the underlying mathematical insights that lead towards selecting\nseparable convolutions?
14130,1,"\n3) Sec 2 para 4: \""the reconstruction loss on the validation set was similar to the reconstruction loss on the validation set\"" -- perhaps you could be a little more precise here."
14131,1,"\n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately."
14132,1," For example, in the abstract, \""saliency simply explain how\"" -> explains. Discussion section, \""how xxx contributing to wrong xxx\"" -> contributes to."""
14133,1,"\n\nAlso, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks)"
14134,1,"\n\nFinally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow."
14135,1," With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2)."
14136,1,\n\n- What is the objective of the problem in section 3
14137,1,"\n-  \""to replace the softmax error function (used in deep learning)\"": I don't think we have softmax error function"""
14138,1," - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution?"
14139,1," Given the high variability of deep RL, they have not convincingly shown it performs better."
14140,1,"""The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization."
14141,1," The work itself, however, falls short of the goal."
14142,1,"    \n\nLastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize."
14143,1,"\nTo enable training with SGD, the authors calculate the centre within a mini batch"""
14144,1, \nThe authors claim that this factorization is important and useful but the paper doesn\u2019t\nreally illustrate this well.
14145,1, The problem addressed is one worth solving - building a\ngenerative model of observed data.
14146,1,\n\n- I'm also excited by the result that multi-agent populations tend to improve the rate of convergence and final translation abilities of these models; though I'm slightly confused about some of the results here (see weaknesses).
14147,1,"  For instance, learning via RL + demonstrations was already studied into papers by Farahmand et al (APID, @NIPS 2013), Piot et al (RLED, @ ECML 2014) or Chemali & Lazaric (DPID, @IJCAI 2015) before Hester et al (DQfD @AAAI 2018)."
14148,1," \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning."
14149,1,"\n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning."
14150,1, The paper doesn't do a great job of making that connection.
14151,1,"""The paper introduces a modified actor-critic algorithm where a \u201cguide actor\u201d uses approximate second order methods to aid computation."
14152,1, Is it experiments from figure 3.
14153,1,\n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design.
14154,1,\n5. The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless. 
14155,1,"  You need to add \u00ab\u00a0with probability $1-\\rho$ as in Avron\u2019s paper.[[CNT], [EMP-NEU], [DIS], [MIN]] \n- p12: the derivation of Eq (10) from Eq (9) needs to be detailed."
14156,1, \n\nThe paper also omits any formal discussion on the equilibrium concepts being used in the Markov game setting (e.g. Markov Perfect Equilibrium or Markov-Nash equilibrium) which leaves a notable gap in the theoretical analysis.
14157,1,\n\n(B) Why does the bound in Theorem 2.4 become worse when there are some directions that do not contribute to the cost (the lambda dependence)?
14158,1," A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well."
14159,1,"\n-\tPag7 in 3.2 \u201cdiscussed in 1\u201d is section 1?[[CNT], [EMP-NEU], [QSN], [MIN]]\n-\tPag14 Appendix E, why the labels don\u2019t match the pictures;[[CNT], [EMP-NEU], [QSN], [MIN]]\n-\tPag14 Appendix F, explain better the architecture used for this experiment."""
14160,1, They notably show\nhow physical priors on a given phenomenon can be incorporated in the learning process and propose \nan application on the problem of estimating sea surface temperature directly from a given \ncollection of satellite images.
14161,1," \n\nIn order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF."
14162,1, The table makes me having doubts regarding the competitiveness of S-rRBF.
14163,1,\n\nHaving the luxury of some supervised episodes is of course useful.
14164,1,"\n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well."
14165,1, The final performance of the method ('ours') does not match what is stated in 'Table 2'.
14166,1,The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms.
14167,1, I did not find either to be the case.
14168,1,"\nTherefore, the paper must show that this new method performs better in some way compared with previous methods."
14169,1," If so, this further means that this is not a true gradient."
14170,1," Such survey style paper is not appropriate to for ICLR."""
14171,1, But the authors also need to justify that ISRLU (or ELU) doesn\u2019t need BN
14172,1,"""This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks."
14173,1,\n\nThe conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective
14174,1,"\n\nThe paper is well written, and easy to follow."
14175,1," In addition to examining the effectiveness, authors also performed experiments to explain why OPTMARGIN is superior."
14176,1," Later in the paper they are estimated using a target network, but this is not specified in the derivations."
14177,1, The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs.
14178,1,". I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened. "
14179,1,"""\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL."
14180,1," When the number of objects is very large, the graph could be huge."
14181,1,"     The results over the IMDB dataset in the original paper [2] are higher than the ones reported here, using a simple model (BoW)."
14182,1, Just a horizontal line in blue and red?
14183,1, Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.
14184,1,"""The authors investigate a modified input layer that results in color invariant networks."
14185,1, The improvement is really tiny and a statistical test (not included in the analysis) probably wouldn't pass a significant threshold.
14186,1," It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau)."
14187,1,\n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation.
14188,1," No, z_L is a vector of latent variables."
14189,1," Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions."
14190,1, Have you tried this for high-dimensional models as well?
14191,1,"\n\nI would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision."""
14192,1," Here, the main goal is to use evidence-based arguments to distinguish good from poor local minima."
14193,1,"\n\n5. In Section 4.1, the choice of ER random graph as a baseline is too simplistic."
14194,1,"I know that authors are going to publish the code, but this is not enough at this point of the revision."
14195,1,"  More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.\n """
14196,1,"However, I don\u2019t think it\u2019s sufficient for acceptance."
14197,1,"""This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation."
14198,1, but\n- A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set.
14199,1, The two baseline methods for few-shot learning provide limited insights in solving the few-shot learning problem.
14200,1," Specifically, the authors propose to extend deep neural networks to the case where hidden layers can be infinite-dimensional."
14201,1,\n\nEdit: Thanks for the fixes and clarification of essential parts in the paper.
14202,1,"\n- Section 1, second paragraph: senstence\n- Section 3.1, first paragraph: thorugh\n- Section 5: architetures"""
14203,1,"\n\nAnyway, some other related work:\nLample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes."
14204,1, I suggest you remove them everywhere.
14205,1,\n    The major problem of this paper in my opinion is the total lack of technical details.
14206,1,\n\nThe tested networks seem to perform reasonably well on the task.
14207,1, One candidate for this is the image feature space learned by a deep network.
14208,1, The proposed method is a useful extension of existing methods but needs to evaluated more rigorously.
14209,1, I thought this could use a little more clarity. 
14210,1,\nBy using K > 1 the method is able to leverage information at a farther distance\nfrom the reference node.
14211,1, Please clarify.\n2) It is not clear to me how the model learns to generate specific OOV variables.
14212,1," The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians."
14213,1," Specifically, this comprises a standard word embedding an accompanying local context embedding."
14214,1,"""The paper proposes a novel workflow for acceleration and compression of CNNs."
14215,1," Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown."
14216,1," Comparing the on-policy results, PCL does not show a significant advantage over TRPO."
14217,1," However, it only identifies hidden units that are important for\na class, not what are important for any particular input."
14218,1, This choice could be motivated from the success of residual networks.
14219,1,n\nI was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result: 
14220,1, \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel.
14221,1,"\n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer."
14222,1, As such I'd rate it as borderline; though perhaps interesting enough to be worth presenting and discussing.
14223,1,"  This paper is more about classifying Tumblr posts according to emotion word hashtags than a paper that generates a new insights into emotion representation or that can infer latent emotional state. \n\n\n\n\n\n\n\n\n\n\n\n"""
14224,1, \n\nMutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17).
14225,1,  There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations.
14226,1,"\n\nRegarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this."
14227,1," In terms of numerical stability, though experimental results were reported, there is no theoretical analysis."
14228,1,\n\nCan the authors guarantee that the variational bound that they are introducing (as defined in eqs. (19) and (41)) is actually a variational bound?
14229,1," However, the paper falls short in lacking of theoretical justification and convincing empirical results."
14230,1,"\n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table."
14231,1," The paper first considers the \""Bayes by Backprop\"" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model."
14232,1, Why not choose \\epsion = 9e-4 and \\epsilon=2e-15 for tensorization?
14233,1,"What's the point of designing a network without skip connections?\n"""
14234,1,"""SUMMARY:\n\nThe motivation for this work is to have an RL algorithm that can use imperfect demonstrations to accelerate learning."
14235,1,\n\nWhat is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)? 
14236,1,\n\nThere's not much detail about the data (it is after all an industrial paper).
14237,1,"""The paper presents a means of evaluating a neural network securely using homomorphic encryption"
14238,1," Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data."
14239,1, The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.
14240,1,"  Nothing about the proposed method (e.g. the neural net setup) is specific to images, so this seems quite readily doable."
14241,1, Does it have anything to do with the paper as I didn't see it in the remaining text?
14242,1,"\n\n2. The manuscript contains several typos and grammatical flaws, e.g. \u2018have been widely applied to have the breakthrough\u2019, \u2018The CP decomposition factorizes the tensors into a sum of series rank-one tensors."
14243,1, I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.
14244,1, I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick.
14245,1," The distance that is used is the Euclidean distance over the learned representations, i.e. (z-c)^T(z-c), where z is the projection of the x instance to be classified and c is a class prototype, computed as the average of the projections of the support instances of a given class."
14246,1,\n- Are the experiments single runs?
14247,1,"  The numeric examples, although quite toy, provide a clear illustration."
14248,1,". Some parameters might be redundant,"
14249,1, but close enough to be of interest and certainly indicates that their method is principled.
14250,1,\nEq. (23) uses Q(F|A) to mean the same as P(F|A) as far as I understand. Then why use Q?
14251,1,  This is the technical part of your paper which is a non-standard approximation.
14252,1," As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick."
14253,1, it is hard to conclude much about under what circumstances one representation or model is better than another.
14254,1," Extending control laws through self-exploration under random disturbances has been studied in character control (e.g. \""Domain of Attraction Expansion for Physics-based Character Control\"" by Borno et al.), but the dimensionality of the problem makes this exploration very expensive (even for short time frames, and even in simulation)."
14255,1,"  Clarification is needed. \n"""
14256,1,"    But, perhaps I am misunderstanding something."
14257,1,\n\nI think the paper is well-written.
14258,1," This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset."
14259,1," With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.),;"
14260,1,"\n\n* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without\n  block-diagonal structure)"
14261,1," While the bidirectional loss is a proper loss and optimized as such (by optimizing both E^adv and E), the pivot objective is no loss function, as it does not correspond to any function any optimization algorithm could minimize."
14262,1,"  In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained."
14263,1,\nOptimal control inputs are restricted to be inside the unit ball and overall norm is bounded by L.
14264,1,\nMy main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set.
14265,1,"\n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original."
14266,1, but the experimental setup is not clear.
14267,1,\n\n(significance) This is a promising idea.
14268,1, What are the confidence intervals? 
14269,1," Please clarify how your work is different/new from previous works.\n"""
14270,1, Especially the results on MNIST suggest that this method is most advantageous for very high compression levels.
14271,1,. I update my review to 6
14272,1,"""The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space)"
14273,1,\n\npros:\n(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples.
14274,1,"\n\n* Using raw waveforms as audio modality is very interesting,"
14275,1,"""The paper reformulates the model-agnostic meta-learning algorithm (MAML) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model. "
14276,1," Although there are some (minor) differences between the implementation with Mirowski et al. 2016,"
14277,1,"\n- VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}."
14278,1,"\n\nSec 3\nThe author does not mention the following reference: \""Deep learning for stock prediction using numerical and textual information\"" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks."
14279,1, This is partly because of the writing.
14280,1, One due to the function family of variational posterior distributions used in inference and the other due to choosing to amortize inference rather than doing per-data-point inference as in SVI.
14281,1," Last but not least, it would be more convincing to show the convergence speed of the proposed method."
14282,1, The entropy term is first bounded by conditioning on the previous layer and then estimated using Monte Carlo sampling with a plug-in estimator. Plug-in estimators are known to be inefficient in high dimensions even using a full dataset unless the number of samples is very large.
14283,1,\n\n\nSuggestions:\n- I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state.
14284,1, I am not convinced that augmentation in the proposed manner leads to a greater improvement than just augmenting in the image feature domain.
14285,1," By doing so, they are breaking one of the most fundamental assumptions of inductive machine learning, i.e., the distribution of train and test data should be equal."
14286,1,"\n\nCons:\nIn my opinion, the substance of the contribution is not enough to warrant a full paper and the problem of time-limited learning is not well motivated:"
14287,1,"\nIn addition, it could be very meaningful to provide some experimental results on linguistically distant language pairs, such as Japanese and English, or simply reversing word orders in either source or target sentences (this might work to simulate the case of distant reordering)."
14288,1, The authors demonstrate that the network works well in the semi-white box and black box settings.
14289,1," however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination."
14290,1," \n[4] Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies."
14291,1,"\n\nOverall, I think that this paper could serve as a useful baseline for generating point clouds,"
14292,1, How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)?
14293,1,The approach is thoroughly explained for a large audience.
14294,1, There is nothing interesting or novel about the paper.
14295,1," On the TriviaQA dataset, the proposed model achieves state of the art results on both domains (wikipedia and web)."
14296,1,"\nFirstly, only one toy dataset is used for experimental evaluations."
14297,1, I also have the following questions regarding the theoretical contributions:\n\n(A) The authors emphasize the logarithmic dependence on T.
14298,1," However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all?"
14299,1,"\n\n6. typo: sec 1 parg 5, \u201ccurrent iterate\u201d -> \u201ccurrent iteration\u201d."
14300,1, It is unclear to me that machine learning is the\nbest approach for modeling and solving this problem.
14301,1,\n-- The authors show that filters with different characteristics are responsible for different aspects of image modelling
14302,1," But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates."
14303,1, I don't see why this is true--the quadratic approximation would be likely to capture the narrow basin only.
14304,1," As far as I can tell, the paper never answers the questions: Why do we need a guide actor?"
14305,1, I also like that they try to design experiments to understand the role of specific parts of the proposed architecture.
14306,1," but at least show an improvement over more a naive approach,"
14307,1, \n3. Discussing about the robustness of SRM for different depth is interesting and I suggest to prepare more results to show the robustness of SRM to violation of different hyperparameters.
14308,1,"\n\nIn summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method. """
14309,1,"""The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis."
14310,1, The authors argue that GANs in fact generate the distributions with fairly low support.
14311,1,"""This paper reviews the existing literature on attribute-based collaborative filtering. "
14312,1,\n\nMy main concern with the paper is in the theoretical underpinning of the work.
14313,1," Realizing that the linear mapping is the derivative of network output w.r.t. the input (the Jacobian), the authors proposed to use the reconstruction loss defined in (8)."
14314,1,  This seems to contradict the theorem.
14315,1," I believe explicit was used for Section 4.1, and implicit for 4.2 but again this needs to be spelled out more clearly."
14316,1,"""The authors describe a mechanism for defending against adversarial learning attacks on classifiers."
14317,1," Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E)"
14318,1," For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved."
14319,1,"""The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights."
14320,1, They distinguish theirs approaches into 1) structured updates and 2) sketched updates.
14321,1,"\n \n6. Minor typos\nIn theorem 2.4, ||Q||_op is used for defining rho, but in the text ||Q||_F is used. I think ||Q||_op is right."""
14322,1," Also, in the experiments, the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initialization?"
14323,1, We are just shown some very impressive qualitative results which are indeed admirable but without further details I cannot judge them as true or not.
14324,1,"  For example, the conjugate gradient-based method leading to the Steihaug-Toint point is so much used. [Note: Here, the gradient refers to the gradient of the quadratic model, and it uses only Hessian-vector products.] http://www.ii.uib.no/~trond/publications/papers/trust.pdf."
14325,1," In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?"""
14326,1," However, freezing the layers and continue to train the last layer is of a minor novelty."
14327,1,". Query Learning Strategies Using Boosting and Bagging."""
14328,1," Also, the paper does not clearly mention the attention mechanism part, and needs some improvement."
14329,1," Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, \u201d Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation\u201d)."
14330,1,\n\nThere are a few typos and the presentation of the paper could be improved and polished more.
14331,1," \n \nUpdate:\n\nAfter evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up."
14332,1, the problem also persists here as the gan is discriminating between a continuous and a discrete distribution. 
14333,1,"\nThe authors provide some detail about the actual implementation of their model, section 4, but the in depth details required at ICLR are missing."
14334,1, Current results in Table 1 only compare the amount of data in policy learning.
14335,1," During operation it then switches its strategy depending on a dynamically-calculated threshold reward value (considering variation in agent-specific policies, initial game states and stochasticity of rewards) relative to the total reward of the played game instance."
14336,1,"\n\nMore importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content."
14337,1," Thus, it is already reasonable to\npractitioners that the proposed linear scaling of batch sizes during training\nwould be effective."
14338,1, The proposed loss has been compared extensively against a number of closely related approaches in methodology.
14339,1,"  The framework does include several components and techniques from latest recent work, which look pretty sophisticated."
14340,1, Does higher extractiveness correspond to higher or lower system ROUGE scores? 
14341,1," It is interesting the \""psychological\"" analysis that the authors present in Section 6."
14342,1,\n\n* You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs.
14343,1," It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation."
14344,1, The paper experiments with two types of models \u2013 1) a model which only predicts the span in a document and 2) a model which generates the answer after predicting the span.
14345,1, You need to show the evidence that you do not hurt the failure-free case for a large number of workers.
14346,1, This needs to be addressed.
14347,1,"""A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory."
14348,1, \n\nComments: \n1) Experiments are performed by restricting alternatives to also use a linear classifier for the discriminator.
14349,1,\n\n* But it improves on each noise type when it is trained on that noise type.
14350,1, Wrench map: the fact that the paths taken by the agent are not distributed evenly makes me suspicious.
14351,1,"""The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output."
14352,1, The authors propose to learn subgoals (actually local rewards) to encourage the agent to go towards the same direction as the expert when encountering similar states.
14353,1,The experiments are only limited to bAbI task which doesn\u2019t tell you much.
14354,1,\n* The optimized learning rate in 2.3 is not described. This reduces reproducibility.
14355,1,"\n\nSome typos:\nFirst line in page 3: \u201cbrining\u201d should be \u201cbringing\u201d\[[CNT], [CLA-NEG], [CNT], [MIN]]n\nOverall, I think the current version of the paper is not ready for ICLR conference. "
14356,1," Also, it seems as though none of the full-dataset MNIST models have been trained to convergence, which makes it a bit difficult to interpret some results."
14357,1," One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added"
14358,1,\n\n- The paper is well written overall
14359,1," The baseline seems to be best performing on \""all cars\"" and \""non-red cars\""\n\nIn order to be at an appropriate level for any publication the experiments need to be much more general in scope.\n"""
14360,1,"  On the Fisher test set, the interpolated LM offers very little over the baseline LM in Table 5."
14361,1,"""I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision."
14362,1," Even a pure LSTM achieves (no attention, no memory) 44% and 45%, respectively (Yu et al., 2017)."
14363,1," All the accuracies(unsup dict, unsup, etc) on CIFAR10/CIFAR100 are reported from the paper (Oyallon & Mallat, 2015), ignoring 2-3 years of research that leads to new numerical results."
14364,1,"\n\nI think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs."
14365,1,"\n\nPros:\nThe network is very clean and easy to implement, and the results are OK."
14366,1,"""The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis."
14367,1," \n\nOverall, I think it is really an interesting direction and the proposed method sounds reasonable."
14368,1,"                 \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper. "
14369,1, An externally valid measure would strengthen the results.
14370,1," \n- The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al."
14371,1,"\n\nThough the results are good,"
14372,1," Does it have some support?\n"""
14373,1,"It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs)."
14374,1," Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary."
14375,1," An eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation (SR), which is an occupancy measure induced here by a uniform policy."
14376,1," \n\nThis paper does not compare to the above style of approach empirically,"
14377,1,"\n\nGenerally, I find a jarring mis-fit between the motivation (deep learning\nfor driving, presumably involving millions or billions of parameters) and\nthe actual reach of the methods proposed (hundreds of parameters)."
14378,1, What could explain this phenomenon?
14379,1,\n\nIt is an interesting paper with a novel approach to multi-task learning
14380,1, The regularizer rewards high entropy in the signs of discriminator activations.
14381,1,  What is the influence of the layer on the performance?
14382,1," Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice."
14383,1,\n\nI do have a few questions that might help further improve the draft.
14384,1,"\n\nSecond, it seems that the current model does not use the input-output examples at \nall for training the model."
14385,1,\n\n\nCLARITY\n\nThe details of the memory based kernel density estimation and neural gradient training seemed\ncomplicated by the way that the process was implemented.
14386,1, To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption .
14387,1,\n\nI am also surprised about the baseline choice.
14388,1,\n\nThe contribution of the paper is: \n - some proposed methods to extract a color-invariant representation
14389,1, K>10 is difficult to see from this plot alone.
14390,1, Applying the proposed method on the strong baselines would highlight the author's claims more strongly than just applying on the average performing chosen baselines.
14391,1, the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample.
14392,1," In addition, preliminary experiments comparing among different categories are also provided. "
14393,1,\n- a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFs
14394,1,  Perhaps another problem that has an explicit divide and conquer strategy could be used instead.
14395,1, I think such generalization is interesting but the innovation seems to be very limited.
14396,1,  Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before?
14397,1, The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations.
14398,1, This is particularly apparent in the empirical study.
14399,1,"\""\nHere the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance."
14400,1,\n\nAn analysis or explanation of the following would be desirable: How is the network trained on single descriptions able to generate multiple descriptions during evaluation.
14401,1,"\n\nA few things are still missing to back the strong claims of this paper:\n* Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer."
14402,1,\n\n* The caption for Fig 5 should explain what each of the sub figures is.
14403,1," Another positive aspect of the paper is that the synthesis results can be analyzed, providing insights for the generation process."
14404,1," A note on the QA results: The QA results are certainly good enough to be in the range of \""good systems\"","
14405,1,". A neural network is already trained, and its weights are public."
14406,1, But the authors clearly have only the stochastic min-batch implementation of the algorithm in mind.
14407,1, The evaluation part of this paper is hard to assess due to the unavailability of the 2 datasets and appropriate baselines.
14408,1, I wonder if this is also applicable to the proposed method and how can this be evidenced.
14409,1,\n\nMain weakness of the paper:\n - Missing many IC systems citations and comparisons (see https://competitions.codalab.org/competitions/3221#results)
14410,1,\n\n3. The comparison with Zoph & Le is not fair because their controller is a meta-network and the training happens only once.
14411,1,\n\nIt seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important. 
14412,1,  E-greedy approaches will always struggle to choose the same random action repeatedly.
14413,1, \n\nThe training process looks highly elaborate with a lot of hyper parameters.
14414,1,\n9. Why is the \\epsion in table 3 not consistent?
14415,1,"\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs."
14416,1,"\n\nHowever, I hope that you address some of the concerns I have raised in this review."
14417,1,\n\nIn my opinion writing of this paper requires major revision.
14418,1," All of this analysis provided more insight into the method and helps the reader understand its extents. \n"""
14419,1, This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node.
14420,1,\nbut the general idea of combining different specifications is quite promising.
14421,1," CCNNs do not outperform the prior CNN results listed in Table 2,3,4."
14422,1,\n\nAll in all the paper is very clear and interesting.
14423,1," The reported likelihood results are very impressive though, and would be reason for acceptance if correct."
14424,1,"\n\n-multiagent deep RL has been very active last 1-2 years. E.g., see other papers by Foerster, Sukhbataar, Omidshafiei"
14425,1, Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3?
14426,1,\n\nPro:  The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes.
14427,1," However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative."
14428,1," The teacher also supplies an uncertainty estimate to each predicted label.[[CNT], [null], [SMY], [GEN]] How about the heuristic function?"
14429,1, The presentation is much more complex that need be.
14430,1," However, the authors provided no theoretical study on any of these aspects."
14431,1,\n- The first two sentences of the abstract do not really contribute anything to the paper.
14432,1," The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3). "
14433,1,n\n- A great deal of the analysis and qualitative examples are pushed to the supplement which is a bit of a shame
14434,1," It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3."
14435,1," If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1."
14436,1,\n4. The physical meaning of Eq.(7) is unclear.
14437,1," But if so, then these vectors aren't sparse at all\n  as most values are non-zero."
14438,1,  also how this could outperform classical discrete autoencoders is unclear.
14439,1," The paper studies locally open maps, which preserve the local minima geometry."
14440,1,"""Summary of the paper\n---------------------------\nThe paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training."
14441,1," So, making improvements to help solve grid-worlds better is not so motivating"
14442,1,"""The paper presents a multi-task architecture that can perform multiple tasks across multiple different domains."
14443,1,  The paper is also well written.
14444,1," As they write themselves \""there is no evidence showing that semantic meanings are fully linearly correlated."
14445,1,\n\nThere has been a number of highly relevant papers.
14446,1,"""Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules. "
14447,1, It is not clear to me why this types of theoretical invariance is tested on such as specific dataset.
14448,1," It shows convincingly that standard NMT models completely break down on both natural \""noise\"" and various types of input perturbations."
14449,1," \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"""
14450,1,\n--I suspect the proposed approach is slower than the baselines.
14451,1," The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm."
14452,1,"""The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML."
14453,1,\n - There are no quantitative results.
14454,1,"\nPaper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler."
14455,1,"\n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller,"
14456,1, How does the proposed technique compare to existing methods in terms of runtime?
14457,1," But right now (at least as of the version I'm reviewing), the paper reads as being half-finished. "
14458,1," \n\n* \""In this regard, we are among the first to combine both the centralized perspective and the decentralized perspective\""\nThis is a weak statement (E.g., I suppose that in the greater scheme of things all of us will be amongst the first people that have walked this earth...)"
14459,1,"\nLet || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*.\nIn that case, Lipschitz continuity writes\nf(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||*\nIn the case where || . || is an \\ell-p norm, then || . ||* is an \\ell-q norm; with 1/p+1/q = 1."
14460,1,\n\nOverall I found the paper interesting
14461,1,"\n - p.2: \""folklore within the NLP community\"". I'm not sure whether this is true or not; while pairwise counts have been the method of choice in recent word vector learning methods, it wasn't true of older methods (Collobert and Weston or Bengio's NPLM) and n-gram counts for n > 2 are widespread in pre-neural NLP."
14462,1,  This can then be fed into a classifier.
14463,1," \n\nHowever, when paired with non-cooperative players in the risky PPD game, CCC players lead to an improvement of pay-offs by around 50 percent (see Figure 2, (e)), compared to payoff received between non-cooperative players (-28.4 vs. -18, relative to -5 for defection)."
14464,1, The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule.
14465,1,"\n\nCONS:\n1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction)."
14466,1,\n\n\n**FINAL EVALUATION**\nThe reviewer rates this paper with a weak reject due to the following points.
14467,1, Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set?
14468,1," \n\nMinor issues:\nmake sure that the capitalization in the references is correct (ATM should be capital, e.g., by putting {ATM} - and many more things)."""
14469,1,"\nAdditionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on?"
14470,1,  DAuto does not consider the sequence/ordering of data either.
14471,1, This should not be a surprise.
14472,1," I also think that a discussion of the \""attention is all you need\"" paper by Vaswani et al. is needed, as both articles seem strongly related."
14473,1, The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system.
14474,1," As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. """
14475,1," Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important."
14476,1, Still as discussed below the justification do not address a lots of interesting developments and implications of the method and should better discuss the relation with regularized optimal transport.
14477,1,"\nPage 3, \u201cforgetting is compensate\u201d should be \u201cforgetting is compensated\u201d.[[CNT], [null], [DIS], [MIN]]\nPage 4, \u201cfor one sentences\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 4, \u201cunknow\u201d should be \u201cunknown\u201d.\nPage 4, \u201c??[[CNT], [null], [QSN], [MIN]]\u201d needs to be fixed.[[CNT], [null], [CRT], [MIN]]\nPage 5, \u201cfor the two first datasets\u201d needs to be fixed."
14478,1," Remove the variational component, and phrasing it simply as an auto-encoder."
14479,1," It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding."""
14480,1, It is not clear whether the proposed algorithm will be useful in practice.
14481,1," \n\nAlthough this work and its results are very useful for practitioners,"
14482,1,"  In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact."
14483,1,"\n\nI get it that maybe under different P(z) the difficulty of learning a good generator G can be different, and therefore one may wish to iterate between updating G (under the current P(z)) and updating P(z) (under the current G), and hopefully this process might converge to a better solution."
14484,1, Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder.
14485,1,".\nI am not sure if these are complete baselines or if the baselines need to cover other methods (again, not fully familiar with all literature here).\n"""
14486,1,\n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning.
14487,1,"This is an interesting idea with linguistic validity, and practically possible because of the commonness and promiscuity of prepositions, reflecting their primary grammatical and relational roles (as function words not content words)."
14488,1, Note that the notion of dynamic expert is present in the SEARN paper too.
14489,1," \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy)."
14490,1,"\n- I am confused by the references in the caption of Table 3 - surely the Waibel reference is meant to be for TDNNs (and should appear earlier in the paper), while p-norm came later (Povey used it first for ASR, I think) and is related to Maxout"
14491,1, I think there is some merit in the work.
14492,1," If this is the case, some configurations could easily require more time to evaluate than the others."
14493,1," Additionally, would it be possible to compute this statistic for *real* images?"
14494,1, \n\ntwo questions linger around re practices:\n1. gan is known to struggle with discriminating distributions with different supports.
14495,1,\n[====================================END OF REVISION ================================================]\n\nThis paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning.
14496,1,\n4. Section 4.3: Why does your LSTM in pMNIST performs so poorly?
14497,1,\n\ncons:\n(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive.
14498,1,"\n\n- The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable."
14499,1, Another clarification is the bAbi performance over Entnet which claims to solve all tasks.
14500,1, How could the authors\nget 16% on MNIST with an MLP of any kind?
14501,1," This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n"""
14502,1,\n\n- Quality/Clarity:\nThe paper is well written and pleasant to read\
14503,1,"""The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning."
14504,1," When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance."
14505,1,"""In this paper, the authors propose a method of compressing network by means of weight ternarization."
14506,1,\n\nClarity\nThe paper is clearly written.
14507,1,"\n6.\tTypo: Dataset section, phrases --> phases."
14508,1, This paper had made a good survey.
14509,1,"\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.]."
14510,1,"\n(3) The label embeddings are not directly used for the classification (H(y, z\u2019_1)), but rather as auxiliary part of the objective.  How to decide the test labels?"
14511,1," However, the paper lacks discussion / evidence of how hard it is to optimize for this VGG-based PIR score."
14512,1,\n\n2. The motivation is not sufficient and not well supported.
14513,1," It is an efficient way to mimic what has worked so far for the planar\ndomain but I would not consider it as fundamental in \""closing the gap\""."
14514,1," Although it only holds when light robustness are imposed,"
14515,1,"\n- Page 3. \\Sigma is not defined."""
14516,1, Concretely: Is there any known theory for such objectives?
14517,1,"  \u2028\n- The following statement is unclear to me: \u201cbut building a varying set of objects is challenging in the first place, and the graph model provides a way to do it."
14518,1," From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies."
14519,1," \n\nInstead of saying that in passing why not explicitly state it in key places, including the abstract and title?"
14520,1,\n\nTrust region methods are generally batch methods.
14521,1,"""This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage."
14522,1, Maybe some subsection titles would help make it feel a bit more cohesive.
14523,1, \n\nPros: \n-The results on StarCraft are encouraging and present state of the art performance if reproducible.
14524,1,"\n\nBasically the motivation and method is good, the drawback of this paper is\nits narrow scope and lack of necessary explanations."
14525,1," For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)? "
14526,1, Now it's calculated as the average of the last layer's features
14527,1, How limiting is the learning of a?
14528,1,"  \n\n- The definition of S, the private information set, is not clear. There is no statement about it in the experiments section, and I assume S is the subject identity."
14529,1,\n4.) A recreation of a similar problem in the machine translation context.
14530,1, \n\nA key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer.
14531,1,"\nIn order to improve the paper, I recommend the authors to evaluate on more common datasets and/or use more appropriate reading models."
14532,1," I can't recommend acceptance. """
14533,1," However,\nsome aspects of the paper can be improved by adding more explanations."
14534,1,\n\nCorrectness: There are minor flaws.
14535,1," \n\nAs strong points, the paper is easy to follow and does a good review of existing methods."
14536,1, It could lead to new insight on automating design of neural networks for given problems.
14537,1, The initial assumption of considering factors of variations related to graphics-generated data undermines the relevance of the work.
14538,1, One issue is that basic baselines could more clearly illustrate what is going on.
14539,1,"  This is significantly lower than the two-stream results from [5] (11.9 mAP and 14.3 mAP) and also lower than pretrained AlexNet features averaged over 30 frames and classified with a linear SVM, which [5] reports as achieving 11.3 mAP."
14540,1,  Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.
14541,1,\n\n\n\nStrength:\n- The experimental results on the simple skip RNNs have shown a good improvement over the previous results.
14542,1," The idea proposed in the paper is just a stack of \""better\"" experiments."
14543,1,\n\n? p.3: What parts are pre-trained?
14544,1,\n\nBelow are some suggestions for improving the paper:\n\nCan you enumerate the paper\u2019s contributions and specify the scope of this work?
14545,1, Figure #s are missing off several figures.
14546,1," To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs."
14547,1," The task itself is not too complex which involves 10 objects, and a small set of deterministic options"
14548,1, They also propose an attention mechanism that works better than others (Symmetric + ReLU).
14549,1," The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples."
14550,1, I think at least Machine Translation and other classification results should be added.
14551,1," Sens-FGSM outperforms the adversarial training defenses tuned for the \u201cwrong\u201d iteration, but it does not appear to perform particularly well with error rates well above 20%."
14552,1,\n\nComments:\n\n- In the visual navigation task no numbers are presented on the comparison to slam-based techniques used as baselines although it is mentioned that it will be revisited.
14553,1," In the revised versions, it has been clarified that the tested distribution is not assumed to be uniform but the distribution has \""effectively\"" small support size using an indistinguishability notion."
14554,1," I found only one typo (page 8, \""We also NOTE that...\""),"
14555,1," Furthermore, the equation in algorithm 2 is not well formatted."
14556,1,\nI would also be great to have intuitions on why a single continuous filter works betters\nthan 20 discrete ones (if this behaviour is consistent accross initialization).
14557,1,"""The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss."
14558,1," I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group-wise and per-example posteriors are composed in this model, using Equations and pseudo-code for the main training loop, would have saved me some time."
14559,1,  Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.
14560,1,"\n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here)."
14561,1,\n\nCons:\n1. The name Apprentice seems a bit confusing with apprenticeship learning.
14562,1," Additionally, they also show that the proposed ensemble method results in better performance than other ensemble methods (For example, ensemble over independently trained models)  not only in combined mode but also in individual branches."
14563,1,To use a semi-NMF type of update rule (as proposed by Ding et al .2010) and apply the approach to new spike-train datasets evaluating performance by their decoding ability (decoding also considered in Onken et al. 2016).\n\n
14564,1, This is applied to the relatively simple domain of Atari games video input (compared to natural images).
14565,1,"""The main concern of this submission is the novelty."
14566,1,"   Likewise, we find max-pooling is also not supported by platforms to calculate higher order derivative, one way to walk around is to change all the max-pooling layers to avg- pooling, it hurts accuracy a little bit, albeit this is not our primary concern."
14567,1,"""The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients."
14568,1, I am a bit surprised that you have not discussed it in the paper not to mention provided a baseline to compare your method to.
14569,1," Granted this results in a small difference in timing 0.06s versus 0.01s, however it would seem that avoiding a backward pass is a somewhat small speed gain."
14570,1, Does the solution obtained with the optimization can be run as efficiently?
14571,1,"  For example, you explain how the algorithm for creating the backbone can use unsupervised data."
14572,1,\n\nIt would be great to mention very briefly any helpful intuition as to why F_\\epsilon and H_\\epsilon have the forms they do.
14573,1,"\n\nWhat the authors propose is a simple idea,"
14574,1,\n\n- The paper uses a hard thresholding  in the visual concept embedding.
14575,1,\n\nThe results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surprising.
14576,1," This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches."
14577,1,"\n\n7. Page 2, second paragraph of related work, \u201cPagliardini also introduceD a linear ...\u201d\n\n8."
14578,1," However, there are several issues need to be addressed."
14579,1, The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model.
14580,1, but has room for improvement.
14581,1, The main difference from previous work is restricting the context to be close to a preposition.
14582,1,\n\n- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta.
14583,1, They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize.
14584,1,\n\nThe numerical results are relatively weak.
14585,1," While I fully understand that the length suggestions are not requirements, in my opinion this paper did not make an adequate effort to abide by these suggestions."
14586,1,\n\nTwo local minima are observed: 1) the network ignores stucture and guesses if the task is solvable by aggregate statistics
14587,1," It proposed a \""dynamic fixed point\"" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format."
14588,1,)\n\nThe work is inspired by previous results for feed forward nets and CNNs
14589,1, The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?
14590,1," Therefore, I think this section should be shortened."
14591,1,n2) Report the results of a large study of many of the surveyed models on a large number of datasets.
14592,1, How robust is the defense against samples generated by a different attack network?
14593,1," As an additional (somewhat unimportant) point, the TD-error is usually defined as the negative of what you have."
14594,1, Perhaps the authors can clear this up in the text after sec 4.3.
14595,1, \n\n- It is not clear the how beam search is carried out.
14596,1, The teacher's strategy should also be  learned from data.
14597,1,\n\n\n## Cons / Limitations\n\n- lack of wallclock measurements in experiments
14598,1," This work would fit better a workshop as a preliminary result, furthermore it is too short."
14599,1,"\n\nOn a more technical level the properties of the learned Mahalanobis matrix, i.e. the fact that it should be PSD, are not really discussed neither how this can be enforced especially in the case where S is a full matrix (even though the authors state that this method was not further explored)."
14600,1, I support its acceptance.
14601,1," Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting."
14602,1, The angle the authors took is interesting (essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup).
14603,1,\n\nThe paper is well organized and well written.
14604,1,"""Summary:\nThis paper proposes a data augmentation method for one-shot learning of image classes."
14605,1," If so, shouldn\u2019t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)?"
14606,1,"  Autonomous Agents and Multi-agent Systems (AAMAS), 2016"
14607,1," For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that?"
14608,1,\u201d \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework.
14609,1, but it does not make an infeasible computation feasible.
14610,1,\n- no theoretical analysis of the proposed algorithm is provided
14611,1,"""() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. "
14612,1,"\n\nWeaknesses:\n* Although results are very strong, the proposed models do not outperform the state-of-the-art, except for the models reported in Table 4, which however were obtained by *traditional* ensembling of coupled ensembles. "
14613,1," One can see the merits in employing a hierarchical action space, whereby decision making operates over high-level actions, each associated with low-level controllers, but that the adopted formulation is not fundamental to this abstraction."
14614,1," That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000. "
14615,1,"\n\nHowever, as discussed in the introduction, the reason an efficient\nsampling method might be interesting would be to provide insight\non the components of perception."
14616,1, I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved.
14617,1, This analysis is conducted by drawing on results from the field of critical percolation in physics.
14618,1," Experimental results on MNIST, CIFAR-10 and SVHN are reported to show the compression performance."
14619,1,  These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon.
14620,1,"n\nSpecific comments/questions:\n- The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this."
14621,1,"\nA couple more specific comments:\n- I think that dealing with multimodal distributions of actions with the forward consistency loss is effective for achieving the goal, but not necessarily good for modeling multimodality."
14622,1,\n\nI am curious how the story would look if one tried to push beyond two levels...?
14623,1,\nThe algorithm then chooses optimistically over the distribution induced by the ensemble.
14624,1," If yes, then it's not very surprising that adding the named entities to the vocabulary leads to overfitting."
14625,1, \n\nI have the following questions regarding the experiments:\n1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data?
14626,1," experimental validation is extensive, making it a worthy contribution in my opinion."
14627,1,"""In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence."
14628,1,"\n\n-  Please also discuss the dependence of your algorithm with respect to the starting baseline function \\phi_0."""
14629,1,"  For example, the GAN is trained\non an image data set with many birds and cars but not many\nfire hydrants."
14630,1," However, it remains unclear why SPENs are the right choice for an energy function. "
14631,1,  I think there is\nsome subtle but important discussion needed on how this framework fits into\nmodern distributed systems for SGD.
14632,1,"\n-- In Section 5.4, no details of the question-answer corpus are provided."
14633,1,"  Perhaps future work will see if the results are much different in other languages.\n"""
14634,1,"\n* It could also be welcome to use a more grounded vocabulary, e.g. on p.2 \u201cFigure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources\u201d could be much more simply said as \u201cFigure 1 shows the ellipses corresponding to three sets of R^3 points\u201d."
14635,1," \n- In Figure 2, even though the diff norm fluctuates, the cosine similarity remains almost constant."
14636,1,\n\nQuality: Ok. The claims appear to be sufficiently verified in the experiments.
14637,1, Will this be feasible?
14638,1,"\n(2). In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases. Could you still say that your generated \u201cadversaries\u201d leading to the wrong decision from LeNet? "
14639,1," Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis"
14640,1," Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout."
14641,1, \n\nThe proposed refutation is based on the following experiments:
14642,1,\n\n3. It is unclear how the actual speedup was measured.
14643,1, The generalization bound depends on the spectral norm of the layers and the Frobenius norm of the weights.
14644,1, Goldberg and Nivre just adapted it to transition-based dependency parsing.
14645,1,"\n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n"""
14646,1, but could certainly do with more explanations.
14647,1,"""The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management."
14648,1," This is in part because f is given another meaning in Finn 2017, but also out of general parsimony in symbol use."
14649,1, This makes me feel that there is large room to further advance the paper.
14650,1, Their experiments show that this improve technique can produce complete training sets for three programs.
14651,1, This can be done by evaluating elementary symmetric polynomials at well-chosen values.
14652,1, (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016.
14653,1,"\n\nFinally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18?"
14654,1,"\nCons: \n1. The embedding strategy, especially the representative and discriminative histograms, is complicated."
14655,1,"\n\n- While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem.\"
14656,1," Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015)."
14657,1,"\nThe paper first presents the mathematical form of the proposed activation function (ISRLU), and then shows the similarities to ELU graphically."
14658,1," Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay (\u00ab  Programming Robots Using Reinforcement Learning and Teaching \u00bb, Lin, 1991), something that is not mentioned here."
14659,1," The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons."
14660,1,\n\nI found the paper interesting
14661,1,\n\n It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven.
14662,1,"   Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity."
14663,1,"\n\n  - Finally, although the empirical evaluation is quite extensive and outperforms the state-of the art, I think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned above."
14664,1," The authors provide a rigorous end-to-end analysis for the LDS setting, which is a mathematically clean yet highly non-trivial setup that has a long history in the controls field."
14665,1," Previous approaches to training data mixing are (1) from random classes, or (2) from the same class."
14666,1,\n\n-----------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper considers the use of natural gradients for learning.
14667,1,\n\nSignificance:  The paper lacks of theoretical justification as well as the experiments are not convincing.
14668,1,"\n\n- In the perceptual evaluation procedure, the \u201c1 second\u201d restriction is artificial and makes the evaluated methods appear better than they are."
14669,1,\nii) The three experiments performed showed variety in terms of underlying graph structure hence provides a thorough evaluation of different methods under different settings.
14670,1, \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings.
14671,1, The proposed algorithm is validated on several image classification datasets.
14672,1,"  However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones."
14673,1," FDA is the research field that formulated the ideas about the statistical data analysis of data samples consisting of continuous functions, where each function is viewed as one sample element."
14674,1, And less tuning is needed for these larger datasets.
14675,1, The only real copyediting I noticed was in the conclusion: and be used \u2794 and can be used; that rely on \u2794 that relies on.
14676,1,"\n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked."
14677,1,"\n\nWhile the method is of interest, there are more recent mutation callers that should be compared."
14678,1,"\n\n\nNote: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard."
14679,1," \n\n2) With reference to the previous point, the experiment 1 in Figure 2 provides a standard example of domain shift."
14680,1," Using an architecture to learn how to split the input, find solutions, then merge these is novel."
14681,1,\n\nThe key idea is a smart evolution scheme.
14682,1," This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol."
14683,1,"\n\nInterpretation\nThis is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model."
14684,1,\n\t\nPaper Weakness:\n* Some detail about different fusing method should be mentioned in the main paper instead of in the supplementary material.
14685,1,"  Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.\n\n"
14686,1," \n\nv) During NLP pre-processing (section 4), how do you prune the irrelevant documents?\n"""
14687,1,"  I feel that while the proposed solution is very intuitive, and probably works as described,"
14688,1,"\n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems."""
14689,1,The hypothesis/ the papers goal is clearly stated.
14690,1, I think this issue should\nhave been examined in more depth.
14691,1, How can the proposed method be scaled to handle multiple requested labels?
14692,1,\n\n   Learner: S = \\phi(s) \n   Expert\u2019s i^th state visit:  Ei = \\phi( \\hat{s}_i }  where Ei\u2019 is the successor state to Ei
14693,1,"Therefore, I think the paper cannot be accepted at this stage.\n"""
14694,1,"\"" Could the authors give more detail on this? A reference would be appreciated. """
14695,1," That was particularly misleading, since if we take eq. (17) to be correct (which I did at first), then p(X^L|F^L) cancels out and should not appear in eq. (20)."
14696,1,"Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017"
14697,1," I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats."""
14698,1,\n\n- \\phi in Fig 2 should be explained by the caption.
14699,1,The alignment task  hence reduces to computing a ranking from this similarity.
14700,1, I would assume from the description that the colors are based\n  on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR.
14701,1,"\n\nIf my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: \n\n- ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l"
14702,1," Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell., "
14703,1,"\n\niii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents? "
14704,1,"  Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these."
14705,1,\n-            Sec 6+7: The paper clearly states that it is not the aim to (generally) formulate the MAML as a HB.
14706,1," Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997)"
14707,1, \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem.
14708,1," With no experiments at all showing the benefit of this proposal, this paper cannot be considered complete."
14709,1," As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique."
14710,1,\n\nCons:\n- Considering an adaptive schedule of the learning decay is common practice in modern machine learning.
14711,1,\n[2] A dirty model for multi-task learning (NIPS)
14712,1," \n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\n"""
14713,1," More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper)."
14714,1,"n\n1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers."
14715,1," If it is the local quadratic approximation, how is it correlated to the original function?"
14716,1," Or a simpler genetic algorithm that just preserves the kills off the worst members of the population, and replaces them by (mutated) clones of better ones, etc."
14717,1, Were these experiments run until completion?
14718,1, but there are some very strong assumptions made in the paper that could limit the impact.
14719,1," \n\nThe main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift."
14720,1,\n\nThe first set of questions is about the monotonic attention.
14721,1," However, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known."
14722,1,\n\nClarity: The mechanism of generating the text samples using the proposed methodology has been described clearly.
14723,1,"\n\nFinally, the evaluation of the model in comparison with other models is questionable."
14724,1," It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing."""
14725,1,\n- In fig. 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT
14726,1,"""This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width."
14727,1,\n- Training and verification sets are automatically generated by the proposed method.
14728,1," In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps."
14729,1,\n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams
14730,1,"  If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples."
14731,1,"\nHowever, the result is supported by exhaustive experiments making the result highly convincing."
14732,1,\n\nCon:\n- The manuscript is unclear in many parts -- this should be greatly improved.
14733,1,"""This paper introduces a new task that combines elements of instruction following\nand visual question answering: agents must accomplish particular tasks in an\ninteractive environment while providing one-word answers to questions about\nfeatures of the environment."
14734,1, the authors propose a simple and robust approach for doing it by using the value function estimation network of A3C.
14735,1,\nCan the authors comment on the similarity/differences between the approaches?
14736,1,"\n\nThe model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before)."
14737,1, A feature of previous optimization based methods is that a user may specify the amount of perturbation (epsilon).
14738,1, The paper should stress on this a bit more.
14739,1, \n2. The technique is built on a lot of heuristics without theoretical consideration.
14740,1," In addition, how should \\tau be chosen in these experiments?\n"""
14741,1," As such the method wouldn\u2019t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state."
14742,1," \n\nI am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas."
14743,1, the approach shows that the proposed methods converge faster than existing methods.
14744,1," Because the task is per-position tagging, those added signals are essentially not part of the examples, but the signals of its neighbors."
14745,1,"""\n-----UPDATE------\n\nThe authors addressed my concerns satisfactorily."
14746,1, Hence the paper cannot be read as a standalone document.
14747,1,\n\nThe paper is generally well written and most details for reproducibility are seem enough
14748,1," Given these clarifications in an author response, I would be willing to increase the score."
14749,1,  So the PATH function helps and longer paths are better.
14750,1,"  For example, it seems that the proposed projected gradient descent method leads to better speedup results in VGG as opposed to Resnet, with very similar reduction in accuracy."
14751,1,"  I am sure this idea has been tried before in the 90s but I am not familiar enough with all the literature to find it (A quick google search brings this up: Reinforcement Learning of Active Recognition Behaviors, with a chapter on nearest-neighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997-045/node3.html)."
14752,1, \n\nUsing the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting.
14753,1,"\n- As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported."
14754,1," According to the results the model doesn't really do better than a simple LSTM or GRU."""
14755,1, For text processing the authors use a standard LSTM taking as input GLOVE vectors of words in a sentence.
14756,1,\n\n1. Why is this type of color channel modification relevant for real life vision?
14757,1, \n\nOriginality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper).
14758,1," The application/setting may be novel,"
14759,1,"""This paper presents two methods for imposing a margin on discriminative loss functions, one which uses the margin between the reference transcription and alternatively hypothesized transcriptions (LMLM), and another which compares all alternative candidates and uses a margin between those with a better system objective (WER or bleu) and those with a worse system objective (rank-LMLM)."
14760,1,". As far as I know, even though modern CNNs have reduced convolution\u2019s computation complexity,"
14761,1," But in general there may not be a 2D intrinsic property, or there is a higher dimensional hidden structure - so why not 3D or more? Related to this, why not using an objective that would result in a dynamics similar to a growing neural gas instead of an SOM?"
14762,1," The method is based on a adversarial training: the generator produces filters, and the discriminator aims to distinguish the activation maps produced by real filters from those produced by the generated ones."
14763,1, The integration of generative adversarial networks with auto-encoding loss is not really a novel contribution.
14764,1," When reading the manuscript the first time, I was expecting experiments on images that have regions that are visible and regions that are masked out."
14765,1," One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes."
14766,1," In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments)."
14767,1,"\n \nIn fact given that the proposed scheme applies in the batch case, it seems that other contenders that are very natural are applicable, including BFGS variants for the non-convex case (\n\nsee e.g. Li, D. H., & Fukushima, M. (2001)."
14768,1,"  As the authors note, this kind of character level modelling has been used in many previous works."
14769,1," To this end, slighly novel."
14770,1,"\n* The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture."
14771,1," Though I understand that the point of the paper is the improvement via the adversarial setting, it is hard to gauge how good the numbers are."
14772,1, It feels to premature for publication.
14773,1," The setup focuses on using SPENs as an inference network, but this seems inessential."
14774,1, Theorem 5 follows easily from this.
14775,1, Otherwise in practice how to set these parameters to get better results is not obvious.
14776,1, The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements.
14777,1," The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper."
14778,1, Answering this question may help understand what influence variational inference has on this model.
14779,1, The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task. 
14780,1,Is such improvement really important?
14781,1,  But this cannot be true when the index is a weighted sum of the constituent assets.  
14782,1,  Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?
14783,1, In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper.
14784,1, A domain confusion loss is added to learn domain-invariant feature representations.
14785,1,". However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently."
14786,1," Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty."
14787,1," but some relevant refs are missing (e.g., Kloft and Laskov, JMLR 2012)."
14788,1,\n\n2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation.
14789,1," These are still the main results of the paper."""
14790,1," However, I think there are several main drawbacks, detailed as follows:\n\n1. The paper lacks a coherent and complete review of the semi-supervised deep learning."
14791,1, \n\n\n\n\nTypos: Several citations are unparenthesized when they should be.
14792,1,". For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet."
14793,1,\n\nStrengths\n\n- The proposed model is a generic meta-learning useful for both classification and reinforcement learning.
14794,1, I got confused at several points because it was not clear what was exactly being estimated with the CNN.
14795,1, This will be the subject of future works.
14796,1, Is it over multiple runs or within the same run?
14797,1,"  gan is an interesting idea to apply to solve many problems; it'll be helpful to get the intuition of which properties of gan solves the problem in this particular application to discrete autoencoders."""
14798,1," The details are as follows.\n\n1.  Limited novelty and performance comparison - the idea of unsupervised manifold projection method has been proposed and well-studied in \""MagNet: a Two-Pronged Defense against Adversarial Examples\"", appeared in May 2017."
14799,1," The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU?"
14800,1,"""The paper is well motivated and written."
14801,1,\n\nThere doesn't appear to be a definition of the L1 penalty this paper compares against and it's unclear why this is a reasonable baseline.
14802,1,  So I think the paper is not ready for publication and my opinion remains.
14803,1,"\n2. The delete-and-copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout."
14804,1, The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K.
14805,1,"\n\nThe bottom line seems to be: \""my model and approach works better than the other guys' model and approach\"", but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much """
14806,1, Did you also make changes to the dataset?
14807,1,\n- The papers lacks a more in-depth theoretical analysis.
14808,1,"\n\n- For experiments, they apply k-means clustering in the process so k is one parameter to tune. K needs to be tuned on validation set instead of testing set. "
14809,1,"""Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers."
14810,1, Why not just enforce this constraint by doing projected gradient descent?
14811,1,"  \n\n5. The setting of numerical experiments is not clear, e.g. value of N1 and N2."
14812,1,\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).
14813,1," The reason is that the gradient of proximal is evaluated at the future point, and different functions will have different future points."
14814,1,"  Notations in eqns (2) and (3) are not fully explained (e.g., boldface c)."
14815,1, For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application.
14816,1,\n- How is the system supervised?
14817,1," -- the gradients should still pull the generator *towards* the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finer-grained details."
14818,1, As it is the closet method to this paper it is essential to be compared against.
14819,1," There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting."
14820,1,"""This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport."
14821,1, I think the assumptions made in the results should also be clearer.
14822,1,  Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training.
14823,1,\n\n- What architecture did you use for the prior generator GAN?
14824,1,"""The idea of using cross-task transfer performance to do task clustering is not new."
14825,1," Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct."
14826,1,"""Summary:\nThis paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep networks. "
14827,1,"""This paper proposed to use affect lexica to improve word embeddings."
14828,1,.\n\nWhen it comes to the experiments only one real-world experiment is present
14829,1,"""Quick summary:\nThis paper shows how to train a GAN in the case where the dataset is corrupted by some measurement noise process."
14830,1,"\It shows that in this very simple setting, the \""evidence\"" of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data)."
14831,1," Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models."
14832,1," Also, it is not mentioned what is the loss function to train the network."
14833,1, The introduction of entropy regularization in sec 2.3 seems somewhat odd and obscures the contribution.
14834,1,"\n9. Throughout the paper, some citations are missing enclosing parentheses."
14835,1, It would be helpful to add a few simple and intuitive baselines in the experiments.
14836,1,\n\nThe paper would greatly benefit from a deeper comparison over other techniques.
14837,1,"\n\ncons:\n-Small issues in presentation: \n* Figure 2 \""optimal learning rate\"" -> \""optimal greedy learning rate\"", also reference to Theorem 2 for increased clarity."
14838,1," However, I wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other tasks."
14839,1," First, we really need a more thorough analysis of what this does to the learning dynamics itself."
14840,1, It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding
14841,1," I found the setup for the sequence tagging experiments confusing, tough."
14842,1,\n- The caption in Figure 8 is malformatted.
14843,1, Recent successes in Deep RL--including Atari and AlphaGo all train and test on exactly the same environment (except for random starts in Atari and no two games of Go being the same).
14844,1," though I found some of the figures and definitions confusing, specifically:\n\n- The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.)."
14845,1," Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc."
14846,1,"\n- Getting better performance by switching from VGG19 to ResNet152 is expected, however they obtain worse results than Vinyals et al. 2016 with inception_v3."
14847,1,\n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.
14848,1," Moreover, the hyperparameters are not optimized on different architectures. It is hard to justify the empirically better performance without hyperparameter tuning."
14849,1,\n\n\n=Major Comments=\n* It's hard for me to understand if the performance of your method is actually good.
14850,1,"\nAdditionally, v_i and c_k live in R^d, however, it's not really explained what\n'd' is, is it the number of 'topics', or something else?"
14851,1, The assessment of the method is incomplete and not convincing.
14852,1,\n- The reference of ESIM is not correct.
14853,1,"\nThis reach is NOT inherent in integer programming, per se."
14854,1, The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility.
14855,1,"  You get rid of one, not the other."
14856,1,"  This seems crucial for scaling up this approach""?"
14857,1,"The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization)."
14858,1,.\n2. The experiments are solid to demonstrate this method works very well
14859,1, \n\nPlease explain the reason why STB is better than the proposed method with 100k positive samples.
14860,1,"  But here it seems just to indicate the number of \""CL\"" layers: 2.  And p.1 says that the \""CL\"" layers are those often referred to as \""FC\"" layers, not \""conv\"" (though they may be convolutionally applied with spatial 1x1 kernels)."
14861,1, \n\nWhile the paper is reasonably clearly written and easy to read
14862,1, So I guess it can be thought of as a kind of hierarchical RL.
14863,1,"\n\nSection 4.2:\n- For the second embedding, what exactly was the algorithm trained on?"
14864,1,"However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not."
14865,1,n- figure 2: do these images correspond to each other?
14866,1,"\n\nThe paper is easy to read and organized very well, and has adequate literature review."
14867,1,\n\n6) Section 5 on page 6. Again the stated conclusion here that the iterates do not lead to singular W is much weaker than the claims made early on.
14868,1,  FashionMNIST and MNIST are similar in many ways.
14869,1,"  Besides, comparisons/discussions based on extensive analysis on various deconvolution architectures presented in Wonja et al., 2017 would also be interesting."
14870,1,"  This also leads to unclarity of the text presentation of the model, for example, section 3.2. Which latent variable is used to decode which part?"
14871,1,  It is not the case that the adversarial loss was simply removed.
14872,1,"n- Experiments show improvements over placement by human \""experts"
14873,1,\n\nI also have issues with their experimental setup: why choose to experiment on networks of depth 50 and width 100?
14874,1, Is there any guidelines to choose \\tau?
14875,1,\nIs that correct?
14876,1," \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background."
14877,1,". However, the authors decline to cite many, many important prior work. For example, the tuple extraction described by the authors has significant prior work in the information retrieval community (e.g. knowledge base population, relation extraction). "
14878,1,\n\n- Will the authors release code to reproduce all their experiments and methods?
14879,1, Experiments are conducted on the Mini-ImageNet dataset and the PASCAL3D+ dataset for few-shot learning.
14880,1, The method is clear
14881,1, The experimental results also look promising.
14882,1,\n\nThe paper is fairly clear and these extensions are reasonable
14883,1, I find that recent paper by Tamar et al 2016.
14884,1," Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid."
14885,1," If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set."
14886,1, However there is no direct comparison either theoretical or empirical against them.
14887,1, \n\nQuality: Borderline.
14888,1,\n---cons: 1) Badly presented: the writing of the paper fails in let the reader aware of what the paper actually serves\
14889,1,"\n- as an additional ingredient the authors also propose \""representation learning\"" by mapping x to some representation Phi(x)."
14890,1," In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins)."
14891,1," The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results."
14892,1,\nThe invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG.
14893,1,\n\nThis is a nice paper which I would like to see accepted.
14894,1," From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function."
14895,1," It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small."
14896,1,"\n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \""sampled\"" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments."
14897,1," The authors claim that this approach is better at avoiding \""narrow\"" local optima, and therefore will tend to generalize better than minibatched SGD."
14898,1,"\n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others."
14899,1,"\n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful."
14900,1, but in some settings it seems very useful.
14901,1,"""This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time."
14902,1,  Figure 4 reports layer spectra for SN and WN.
14903,1, This approach (relaxing a strong constraint) has been used for years when learning support vector machines and ranking and a small discussion or at least reference to those venerable methods would position the paper on a bigger picture.
14904,1,I don\u2019t see this as a given in the current formulation.
14905,1,"\n\nSimulation results compare MADQN with Dijkstra's algorithm as a baseline, which offers a myopic solution where each agent picks up the closest customer."
14906,1, Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement?
14907,1,\n\n- Paper presentation quality is clearly below the standard.
14908,1," In contrast, it is less effective to protect against singular perturbations."
14909,1, \n\nAnother task you could try is to learn to perform the same task in two different environments.
14910,1," \nAlso, the criterion of informativeness of Section 2 is split into two sub-criteria in Section 3.3, namely test set NRMSE and Zero-Shot NRMSE: such shift needs to be smoothed and better explained, possibly introducing it in Section 2.[[CNT], [PNF-NEU], [SUG], [MIN]]\n\n*Originality*\nThe paper does not allow to judge whether the three proposed criteria are original or not with respect to the previously proposed ones of [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]."
14911,1,The authors do not give any insight in this regard.
14912,1," Therefore, I rate this paper as a (weak) reject: it is just not (yet) good enough for acceptance."""
14913,1,"""\n\nThis paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating."
14914,1,  Computation of the bound requires integration over multiple layers (equation 15).
14915,1," Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images."
14916,1, This is a rather simple idea that is shown to be effective in Figure 3.
14917,1," For the mobile robot, is the robot learning some form of traversability affordances, e.g., recognizing actions for crossings, corners, and obstacles?"
14918,1," The proposed method presents a possible way of better modeling the future,"
14919,1, \n\nComment:\n\n- The idea of tree2tree has been around recently but it is difficult to make it work.
14920,1,"\n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$."
14921,1, The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN.
14922,1, The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network.
14923,1, The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time.
14924,1,\n-The experiments are lacking and the results are not good enough.
14925,1, The visual Turing test is interesting but not concrete enough to support an ICLR publication.
14926,1,\n\ncons:\n--Change the title! the title is too vague. 
14927,1,"\n\nNovelty:\nPrevious papers like \""beta-VAE\"" (Higgins et al. 2017) and \""Bayesian Representation Learning With Oracle Constraints\"" by Karaletsos et al (ICLR 16) have followed similar experimental protocols inspired by the same underlying idea of recovering known latent factors, but have fallen short of proposing a formal framework like this paper does."
14928,1,"""This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks."
14929,1, It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance.
14930,1," I'd point out that this construction can be done in parallel, so it's less of a computational burden."
14931,1,  Another point of criticism is the way the Amazon Mechanical Turk evaluation was performed.
14932,1," The method uses a learnable character embedding to transform the data, but is an end-to-end approach"
14933,1," That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin\u2019s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments)."
14934,1,"\n\nFinally, the results are not particularly impressive."
14935,1, The authors only experiment with MNIST dataset.
14936,1,"""This paper studies the problem of learning one-hidden layer neural networks and is a theory paper."
14937,1," \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16."
14938,1,\n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs.
14939,1, Maybe bring this detail forward)?
14940,1, it presently isn't a significant enough contribution to warrant acceptance.
14941,1," \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful. "
14942,1, \n\nThe proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled. 
14943,1," Firstly, experiments are the main weakness of the paper."
14944,1," Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014."
14945,1,\n\nEqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v\u2019)?
14946,1,"\n- The discussion in section 4.1 and results in the experimental section 5.2 seem a bit counter-intuitive, especially learning the proposals for SMC using IS."
14947,1, The proposed method simply concatenates a saliency map with the corresponding raw pixel image as an input to adversarial perturbation detector.
14948,1,"""Overall:\nI had a really hard time reading this paper because I found the writing to be quite confusing."
14949,1, This could be of interest for a broader audience.
14950,1," \n\nOverall, I find the paper important for furthering the understanding of fundamental RL algorithms."
14951,1," Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too."
14952,1, The usefulness of the BC technique is proven to a certain extent (see paragraph below) but there is not comparison with state-of-the-art.
14953,1,"""In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\"
14954,1,\n\n6. Possibility to apply to natural images.
14955,1," Note that equation 4 is simply mentioning attention computation, not the proposed positional attention."""
14956,1, \n(b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution\n
14957,1, But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply.
14958,1,\n\nRe: Speed. I brought up this point because this was a bulleted item in the Introduction in the earlier version of the manuscript.
14959,1, The paper presented a solution that is weak compared to these recent results.
14960,1, \n\nThe technical exposition is also relatively poor.
14961,1, yet such information is sorely lacking in this paper. 
14962,1," The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax."
14963,1, We definitely see many very similar images in fairly small sample generated.
14964,1," \n\nOverall, I believe the idea is nice, and the initial analysis is good,"
14965,1, I believe the paper needs a significant rewriting
14966,1," Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea."
14967,1,\n7. Discussion of weight decay on page 5 seems tangential to main point of the paper. Could be reduced to a sentence or two.
14968,1, Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.
14969,1,\n\nThe proposed scheme is applicable to the batch setting when most deep network are learned using stochastic gradient type methods.
14970,1," While previous approaches assumes the order of shared layers are the same between tasks, this paper assume the order can vary across tasks, and the (soft) order is learned during training."
14971,1," The paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas."
14972,1," This paper is not ready for publication, and really feels like it is rushed."
14973,1," It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy."
14974,1," The proposed method performs similar to the baseline in the 25% case, and better than the baseline in the 75% case."
14975,1,"\n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation."
14976,1," The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples."
14977,1,"n\n3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning"
14978,1," I appreciate if authors can provide more results in these settings.\n\n"""
14979,1,"  The exact relationship between the logical formulas and the stochastic trajectories of the MDP is not described in detail here, but relies on a robustness metric, rho."
14980,1,\n\nCons:\n- The main algorithm MCL is only a hueristic.
14981,1,"  In the paper, the author only got around 83 percent accuracy with SGD and 85 percent accuracy with TR."
14982,1,  The paper is well presented and organized.
14983,1," The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next."
14984,1,"""This paper provides a new method for learning representations of prepositions."
14985,1,"\n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented."
14986,1," Additionally, they propose an extension JCP-S, for n-order tensor decompositions."
14987,1, \n\n4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.
14988,1,"n- for Figure 2a it would be clearer to normalize such that 1 is the best and 0 is the random policy, instead of 0 being score 0."
14989,1, I will detail concerns for the specific experiments below.\n\nSection 4.1:\n- How does held-out data fit into the plot?
14990,1,"""The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations."
14991,1," \nAlternatively, can the network be trained once for a domain, and then used for every synthesis problem in that domain (i.e. in your experiments, training one net for all possible binary-image-drawing problems)?"
14992,1," According to the visualizations, the interpreter could generate meaningful attention map given a textual query."
14993,1," On the positive side, the paper is mostly well-written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks."
14994,1,.\n\nThe second question is about the window size $w$
14995,1," \n\nThe paper provides quantitative evaluations under three different settings i) image classification, ii) Time series forcasting iii) Document classification."
14996,1," The work is still great,"
14997,1,"   Overall, I think the drawbacks mentioned in the paper are not common in existing methods and I do not see clear benefits of the proposed method."
14998,1," If yes, this seems like a rather large limitation."
14999,1,\n\nWeakness :\n1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature.
15000,1,"\n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \""good\"" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective."
15001,1,\n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD.
15002,1," In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model."
15003,1, \n* Which kind of error would using a convolution architecture for the encoder decrease?
15004,1,"\n\nOverall, I think it is a borderline paper."
15005,1, Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity.
15006,1, This assumption is not met in the HealthGathering environment as several different states may generate very similar vision features.
15007,1," where you hide some known entries during model training, and evaluate on these entries during test? """
15008,1,"Perhaps a model trained on the human-translated pairs from Task 1 of Multi30k? Obviously, outperforming such a model is not necessary for this approach to be interesting, but it would provide useful context on how well this is doing.\"
15009,1,n\n- I am wondering about the effects of the temperature parameter t. Is that important for training?
15010,1,"\n8. I would like to see some experimental verification that the proposed method significantly reduces the dropout gradient variance (page 6), if the authors claim that tuning dropout probabilities is an area they succeed where others don't."
15011,1,\n+ [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks.
15012,1, They propose three training schemes to train a low precision student network from a teacher network.
15013,1," Even some speculation on how this aspect\n  could be applied would be appreciated (admittedly, many GAN papers could use\n  some reflection of this sort)."""
15014,1,"\n3. Why the authors choose Persian Cat, Container Ship, and Volcano in the experiments?"
15015,1,"\n\nIf I haven't made a mistake in my criticisms above, I strongly suggest removing section 3.1 entirely or replacing it with a completely new example that does not suffer from the above issues.[[CNT], [EMP-NEG], [CRT], [MIN]]\n\n+++ ResNet scaling +++\n\nThere is a crucial difference between VANs and ResNets."
15016,1,"  Is this assuming that the \""initial training set\"" which is used to obtain the \""pre-trained DNN\"" free of adversarial examples?"
15017,1," This is an interesting result, and useful in its own right."
15018,1," Moreover, many problems are known to be undecidable."
15019,1,"  Also, many decision problems for wighted automata are known to be undecidable.[[CNT], [CNT], [DIS], [MIN]]  I am not sure that the paragraph is useful for the paper.[[CNT], [null], [CRT], [MIN]]  A discussion on learning as in footnote 1 shoud me more interesting."
15020,1," Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison."
15021,1,"""This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights)."
15022,1,"The authors tackle a very important problem, the one of learning low precision models without comprosiming performance."
15023,1,  \n\nThe notation that you use is a bit sloppy and not everything is introduced in a clear way.
15024,1," Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected)."
15025,1,"  So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the \""scatter\"" metric described."
15026,1," I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values."
15027,1, This is where I learned something new.
15028,1,"  Also, it is not clear how the graph nodes and connectivity changes after the max-pooling operation."
15029,1,"\n\nMy main concerns with this paper are novelty, reproducibility and evaluation."
15030,1," They perform experiments on MNIST and CIFAR-10, doing classification by scaled regression."
15031,1, I think that it would make the paper stronger.
15032,1,"""his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA."
15033,1,  The paper is very well written and quite clear.
15034,1,"\n\n\nPOST REBUTTAL RESPONSE:\nThe authors have addressed the comments on the MNIST experiments and show better results,"
15035,1," \n\n4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this."
15036,1,"\n-Experiments are not described in detail\n-Experiment design feels \""ad-hoc\"" and unstructured\n-The role and value of the many LR-plots remains unclear to me."
15037,1," \n\nWhat is confusing is that they define\n\n    A( s, a, th^p, th^g, th^v ) = sum_i   gamma^i  r_{t+1}  +  gamma^k  V(  s_{t+k}  ;  th^v  )   -   V( s_t ;  th^v )"
15038,1,"\n\nother questions / comments:\n- \""we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\"" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?"
15039,1, Your paper does not very clearly different itself from Konidaris's work here.
15040,1, The results could make more sense by providing total time consumptions and time cost per iteration.
15041,1,"""The paper \""A Deep Predictive Coding Network for Learning Latent Representations\"" considers learning of a generative neural network."
15042,1, but I think its contribution is on the small side for a conference paper.
15043,1,"""This paper proposes to re-evaluate some of the methods presented in a previous paper with a somewhat more general evaluation method."
15044,1,"""The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices."
15045,1,"  In Bayesian Dropout, there is an explicit variational objective."
15046,1,\n\nI am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset.
15047,1, I'm not quite sure how the authors could externally validate the synthetic data as this would also require generating synthetic outcome measures.
15048,1,"\n\nFurthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs)."
15049,1,"\n\nNext, I disagree with the statement that \""it is not clear how to keep the vertex local property when filtering in the spectrum domain\"".[[CNT], [null], [CRT], [MIN]] Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. See Eq. 18 and 19 in [1]."
15050,1," The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor."
15051,1, It would be useful to clarify whether this is happening.
15052,1,"Furthermore, the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal, if any.\n\n"
15053,1,\n\nConcerns about the paper:\n1.) It is not clear how well the proposed approach works with CNN architectures other than PixelNet
15054,1,\n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.
15055,1,"\n\nWhile the idea is interesting and might be a good alternative to standard CNNs,"
15056,1,"\n\nSection 7.2 on the richness of natural noise is extremely interesting,[[CNT], [EMP-POS], [APC], [MAJ]] but maybe less so to an ICLR audience."
15057,1," The problem was very well-motivated, and the analysis was sharp and offered interesting insights into the problem of maze solving."
15058,1,\n\nRemaining (minor) remarks:\n- It is unclear how iCaRL has been used - it has been proposed as an iterative classification method.
15059,1,\nThe paper is mostly clear and the idea seems nice.
15060,1," As they use a very similar iterative fine-tuning workflow, it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freezing."
15061,1,"\n\nThis paper has many strengths:\n1) The writing is clear, and the paper is well-motivated\n2)"
15062,1, Authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zeros.
15063,1," The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming."
15064,1, I don't think the substance of the algorithmic improvement is enough.
15065,1,"  If so, the training time amortizes to some extent\u2014can you quantify this?"
15066,1,  This means they might have very bad error on false positives.
15067,1," c.f. Eq 3 and Eq 4.\nTherefore, it's unclear whether the gain in generalization is due to an additional \\lambda term or from the post-training training itself."
15068,1," \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work."
15069,1," Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution."
15070,1, I understood the necessity only through reading proofs.
15071,1, If that's the case the whole experimental setup seems flawed.
15072,1, This can work if each layer expands the reachable region (the state) by one pixel if the pixel is not blocked.
15073,1, Some results are counterintuitive if the reader is not familiar with related works (e.g. the Zhang et al. 2016 achieves a lower acceleration with much lower ranks).
15074,1,"\n\nOverall, the method is interesting and the dev set experiments were informative,"
15075,1," However, there is no such guarantee for random walks; indeed, for most Atari games which have several levels, random policies don\u2019t reach beyond the first level, so I don\u2019t see how a Path function would be informative beyond the \u2018portions\u2019 of the state space which were visited by policies used to collect data."
15076,1," The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?"
15077,1,"\n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper."
15078,1, It would also make the paper more clear.
15079,1, I believe the paper could justify this approach better by providing a bit more insights as to why it is required. 
15080,1,\n- 'Non-parametric filter' may not be right word as this work also uses a parametric neural network to estimate filter weights?
15081,1, It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods.
15082,1,"In this method, two new types of kernels are developed, namely the spatial-wise and channel-wise sparse-complementary kernels."
15083,1,"""[Overview]\nIn this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well."
15084,1," It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings."
15085,1," \n\n+ The generated face images are very impressive, especially the improved 512x512-pixel outputs."
15086,1,MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.
15087,1," By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment."
15088,1, The example work and its previous work also accelerated LSTM by several times without significant performance drop on some RC models (including DrQA).
15089,1," The paper first argues that achieving privacy guarantees like differential privacy is hard, and then provides frameworks and algorithms that quantify the privacy loss via Signal-to-noise ratio."
15090,1," By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic."
15091,1,   Also Table 4 and Table 5 on WSJ and  FIsher  show baseline experiments that are quite far away from the state-of-the-art in these tasks.
15092,1," This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. """
15093,1, also applies to table 4.
15094,1," I'm guessing it's because the RNNs don't \""work\"" in all environments with the same initialization (i.e., they either don't look like EC, or they don't obtain small errors in the navigation task)."
15095,1,\n\nThey demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems.
15096,1, \n-\tIssue with noise bursts plot (Input 1+2 attention does not sum to 1)
15097,1," Then: \""[...] the action a_0 should be in a local vicinity of a."
15098,1,\n\nCons:\n1. No SOTA comparison.
15099,1, Does that have any impact on accuracy of OCN?
15100,1,  I would have preferred one domain experiment carried out with appropriately rock solid documentation of the ball-park competitive baseline system to these results.
15101,1,"\n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there."
15102,1, The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.
15103,1,"\n\nOverall, I find this direction exciting and hope the authors would keep pushing in this direction! However, the current manuscript is not ready for publication."""
15104,1," The decoder then maps these new data points into feature space, obtaining in this way the image feature representations that, along with the feature representation of the original (real) image will form the batch that will be used to train the one-shot classifier."
15105,1, though I haven't checked all the details
15106,1,"\nI am not convinced of the necessity of multi-mention reasoning, which the authors use as motivation, as shown in the examples in the paper."
15107,1,", more comparisons and analysis are required to validate the approach."
15108,1,  Also it is not clear how good the classifier is.
15109,1,\n\nThe last two process figures in 1.1 can be improved.
15110,1," When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}."
15111,1, Experimental results are given on different multi-task instances.
15112,1,"\n\nIn addition, the authors report a loss in performance when the gates are not discretized to {0,1}."
15113,1," However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section."""
15114,1,"\n\nSome recent references that warrant a mention in the text:\n- both of these learn optimizers using longer numbers of unrolled steps:\nLearning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017\nLearned optimizers that scale and generalize, Wichrowska et al, ICML 2017\n- another application of unrolled optimization:\nUnrolled generative adversarial networks, Metz et al, ICLR 2017"
15115,1," I noticed that the time and location information was used to generate questions sometime, but sometimes these kinds of questions are ignored."
15116,1," In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768."
15117,1,"\n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out."
15118,1,"\n\nIn general, I think the paper is written clearly and in detail."
15119,1, \n\nComments:\n\nThe findings suggest the effectiveness of that approach. 
15120,1," Anyway, a clearer explanation would be helpful."
15121,1, Corresponding to these two parts are two generators.
15122,1," Maybe so, but we won\u2019t know unless the experimental protocol prescribes a sufficient range of LRs for each architecture."
15123,1, \nPseudo labelling can be obtained by transformations of original input data
15124,1," One cannot hope to do image caption association prediction without capturing the image attributes...\n\n*,"
15125,1,\n- The experiments cover different settings with different task difficulties.
15126,1, \n\n\nMinor details:\nPersonally I\u2019m not a big fan of abusing colons (\u201c:\u201d) instead of points (\u201c.\u201d).
15127,1,  Positive empirial results support the proposed regularizer.
15128,1, Analyzing the actual adaptive algorithm would be very interesting.
15129,1," \n\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.]."
15130,1, This will have the effect of tuning the width of their\nposterior approximation which is directly related to the amount of exploration\nperformed by Thompson sampling.
15131,1," For example, H.-K. J. Kuo, E. Fosler-Lussier, H. Jiang, and C.-H. Lee, \u201cDiscriminative training of language models for speech recognition,\u201d in Proc. ICASSP,\nvol. 1, 2002, pp. 325\u2013328."
15132,1,"  Since only individual images were shown, the evaluation mainly measures the quality of the generated images."
15133,1,"\n\nWhile increasing batch size at the proposed linear scale is simple and seems\nto be effective, a careful reader will be curious how much more could be\ngained from the backtracking line search method proposed in De et al."
15134,1," The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs."
15135,1," We know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one (or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward, which seems not very plausible)."
15136,1," \n\nThe crossover operator is the policy mixing method employed in game context (e.g., Deep Reinforcement Learning from Self-Play in Imperfect-Information Games, https://arxiv.org/abs/1603.01121 )."
15137,1,"  \nThis especially true for adversial perturbations, which have been used as test cases in this work."
15138,1," However, in the implementation, it is restricted to skip connection by addition."
15139,1,"\nDoes not compare existing extensions of SOM."""
15140,1," Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?"
15141,1," The method leads to better performance than using no external resources,"
15142,1,"  The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs."
15143,1, This adds stochasticity as well so why and why not this work? 
15144,1," In practice, with deep D, trained by single gradient update steps for G and D, instead of the \""argmin\"" in Algo 1., the assumptions of the theory break."
15145,1, This is fun but I did not find it particularly surprising from a game-theoretic or from a deep learning point of view.
15146,1, It might be worth adding this in the discussion of the related work.\
15147,1,\n\nI did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision.
15148,1, The paper lacks in clarity as currently written.
15149,1, \n\nUpdate: the revised version of the paper addresses all my concerns about experiments.
15150,1,\n\nThe experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods.
15151,1," Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider."
15152,1," As a running example, they show how this can be be done for bubblesort."
15153,1, \n2) Many notations are not formally defined.
15154,1, Could the authors prioritize clarification to that point !
15155,1," The idea is interesting and the result looks promising,"
15156,1,"More generally, the paper is riddled with non sequiturs."
15157,1,Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model.
15158,1,\n\nCons\n\nThe focus on neuroimaging might be less relevant to the broader AI community.
15159,1, Therefore I recommend reject.
15160,1," \n5. It will be interesting to see the impacts of physics based knowledge on choice of network architecture, hyper-parameters and other training considerations."
15161,1, Then apply any classifier trained on the true distribution on the resulting x* = G(z*).
15162,1, \n\nIt looks that this paper denotes the style of the target domain as y^* and assume that it is shared by all samples in X_2.
15163,1,"  The authors algorithm also seems to be essentially a combination of two other, previously published algorithms."
15164,1,"\n\nUnfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. "
15165,1,"""The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features."
15166,1, \n- Authors do not compare to any state of the art on 3D face representation and reconstruction (e.g. [2]) using public datasets (e.g. BU-3DFE).
15167,1,\n\nEdit: After the authors rebuttal I have increased the rating of the paper:
15168,1,"""This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved."
15169,1," However, a key problem is the following: the nature of the discrete variables being used makes them hard to be inferred with variational inference."
15170,1,"""The main goal of this paper is to cluster images from classes unseen during training."
15171,1," I did not see a discussion about this, given that such problems are not captured by the metrics."
15172,1, I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch?
15173,1," \n1. \u201cIf we consider TD-learning using function approximation, the loss that is minimized is the squared TD error."
15174,1,\n\nThe depth of the TC block is determined by the sequence length.
15175,1, I do not see a comparison.
15176,1," Please, please discuss and cite some papers if required."
15177,1,\n- That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration.
15178,1,"""This paper is about low-precision training for ConvNets."
15179,1, This figure currently very unclear.
15180,1," It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells. """
15181,1,"    Let's assume that o(B Until C) is satisfiable, so the conjunction is satisfiable."
15182,1, One example is Section 4 in [Courbariaux et al. 2016].
15183,1,"\n\n[7] W Chan, N Jaitly, Q Le, O Vinyals, Listen, attend and spell, 2015"
15184,1,"\n? MagNet results were very often worse than no defense in Table 4, could you comment on that?"
15185,1," While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure."
15186,1,\n- The number of layers over which soft ordering was tested was fixed however. 
15187,1,"  The work on dynamic environments was an interesting step:  it would have been interesting to see how the \""models\"" learned for the dynamic environments differed from those for static environments."
15188,1,"""The paper presents a novel representation of graphs as multi-channel image-like structures"
15189,1,\n\n(quality) The experiments are interesting and seem well executed.
15190,1," However, how widespread is this problem across other models or are you simply addressing a point problem for RN?"
15191,1," and in a conv net, just a convolution?"
15192,1, But in the abstract you defined VAN as an architecture without skip connections.
15193,1," What this means is that the agent has a reservoir of n \""states\"" in which states encountered in the past can be stored."
15194,1," Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)?"
15195,1," Therefore, the authors derived the update formulate based on the analytical continuation technique."
15196,1,"\n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs."
15197,1, Would it be able to generate an expander graph?
15198,1, Thus the learned weak learner at this round will make different mistakes.
15199,1, The method is an extension of the GloVe method and in the case of\na single covariate value the proposed method reduces to GloVe.
15200,1,"""This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer."
15201,1," Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper)."
15202,1," E.g. one could imagine cases where jointly training the VAE encoder and decoder finds a local optimum where inference is perfect, but which is still much worse than the optimum that could be achieved if the encoder would have been more flexible."
15203,1,"\n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design."
15204,1," At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn't seem to be that useful (maybe it is for real-time predictions?"
15205,1,"\n\n* Text:\n\nsec 2 para 4. \""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set.\"" ??"
15206,1,- so the failure to be better than the real-valued alternatives seems unremarkable.
15207,1,".\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up."
15208,1,\n\n- What is the impact of the external word aligner quality?
15209,1,\n\nFigure 5: Looks to me like the baseline is actually doing much better than the proposed methods?
15210,1," Then, the proposal is simple and easy to reproduce and leads to interesting results"
15211,1," However, I am concerned about the following points:  \n\n- The improvements are really limited on both the SNLI and the Reverse Dictionary tasks."
15212,1,  It uses established work on network morphisms as a basis for defining a search space.
15213,1,\n\nThis article compares their proposed architecture with RNN (GRU with 10 hidden unit) in few toy tasks.
15214,1,\n    - Parameter noise exploration can be *extremely* bad relative to efficient exploration methods (see section 2.4.3 https://searchworks.stanford.edu/view/11891201)
15215,1," Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication."
15216,1," The message is unclear and the experiments to prove it are of very limited scope,;"
15217,1," \n\nThe theory part seems to be technical enough and interesting,"
15218,1,"""The authors propose to use synthetic data generated by GANs as a replacement for personally identifiable data in training ML models for privacy-sensitive applications such as medicine."
15219,1,"""The paper presents a multi-task, multi-domain model based on deep neural networks."
15220,1," The reason this might be interesting is that when the parameter space is lower dimensional (not an issue for neural nets, but could be problematic for linear FA) the constraint might be too much leading to significantly poorer performance. "
15221,1, However the application to program repair is novel (as far as I know).
15222,1, Finally they learn a model that predicts correctness of syntax in absence of a syntax checker.
15223,1,\n\nThis contribution is not bad for an empirical paper.
15224,1,  I have a few recommendations here:\n* It would be stronger to evaluate results on a larger dataset like ILSVRC.
15225,1," not sure if it would help, but it might"
15226,1," Secondly, why not just do it? \n\n"""
15227,1, Are there existing generative models based on walk paths?
15228,1,"\n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal."
15229,1,\n\n2. Clarification on the unit ball constraints.
15230,1, The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved.
15231,1, The number of centroids learned from NATAC may not be good for k-means clustering.
15232,1, Is it because there is a lot randomness in the stochastic convolutional layer?
15233,1, The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples
15234,1,There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers.
15235,1,"   \n- If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant\n"""
15236,1," I am a weak reject"""
15237,1,"\n\nThe paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea."
15238,1," From a practical computational perspective, the algorithm will be implemented on a machine which processes on finite representations of data."
15239,1,\nNote that item 2) is relevant in many other setups in the deep learning framework and is often overlooked.
15240,1,"""This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints."
15241,1," and does not inherit novel insights due to this derivation.\n"""
15242,1,  There is a\ntiny ounce of novelty in that the authors propose to improve a supervised\nversion of the SOM by using what should have been used in the first place\naccording to modern good practice.
15243,1,"\n\n- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper."
15244,1,\n\nWhy is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution (as stated in Section 2.1)?
15245,1, How do they impact for instance changing the time range in the definition of $\\Phi$) in the performance of the meta-learner?
15246,1, Would that allow improving generalization to varying item or image sizes?
15247,1," So, slight to no novelty."
15248,1, \nd)\tHow is the RW statistically addressing the generation of high-order (subgraph) features?
15249,1," Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks."""
15250,1," I think the authors should spend time on better motivating the choice of invariance used, as well as on testing with different (potentially new) architectures, color change cases, and datasets."
15251,1,\n- Knapsack problem has been added
15252,1,"""The authors propose a new defense against security attacks on neural networks."
15253,1, This average is then fed to a softmax layer for answer prediction.
15254,1," \n-\tTwo times \u201cupdate\u201d in \u201cour true update update as\u201d in Sec. 2.6;[[CNT], [null], [DIS], [GEN]]\n-\tPag3 correct the capital S in 2.3.1\[[CNT], [null], [DIS], [GEN]]n-\tPag4 Figure 1 increase font size (also for Figure2); close bracket after Equation 3; N (number of spikes) is not defined"
15255,1," While it is true that skip connections combat vanishing gradients in certain situations, your example does not capture how this is achieved in VANs."
15256,1," Within this specific setting the authors differentiate their approach from others \nby developing a solution that does NOT estimate an explicit dynamics model ( e.g.,  P( S\u2019 | S, A ) )."
15257,1, Training loss alone likely does not capture the quality of a sketch.
15258,1,"\n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter."
15259,1, \n\nThese works should be discussed in the text.
15260,1,"""Summary:\nThis work is about model evaluation for molecule generation and design."
15261,1, \nSignificance: This paper is somewhat significant.
15262,1,\n\n# Novelty and Significance\n- The problem considered in this paper is interesting.
15263,1," While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward."
15264,1,n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail. 
15265,1,"""This work shows that a simple non-parametric approach of storing state embeddings with the associated Monte Carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards (reacher, half-cheetah, double pendulum, cart pole) (due to the need to threshold a return the algorithms work less well with dense rewards, but with the introduction of a hyper-parameter is capable of solving several tasks there)."
15266,1, Their abstract also claims to utilize a convex programming formulation.
15267,1, The performance of the proposed method and DEPICK are also similar in table 1.
15268,1, \n* Are we sure that the textual description do not explicitly contain the information of the triple to be predicted?
15269,1, \n\nI found the paragraph just above section 5 describing the maze-like\ndeterministic game confusing and not very useful. 
15270,1," ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets."
15271,1,"n\nCOMMENTS:\nThe introduction puzzled me:  the authors, once they stated the problem (the scarceness of the hypernyms' occurrences in the texts w.r.t. their hyponyms), proposed a solution which seems not to directly solve this problem."
15272,1,\n\nThe main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principle.
15273,1," The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned. "
15274,1,\nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions).
15275,1, Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers).
15276,1,\n(c) Depending on the Mask quality GAN can produce low quality samples.
15277,1, Currently it's unclear which type of problems are solved here.
15278,1, In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large.
15279,1, \n\nOverall the authors seems to have captured the essence of a large number of popular CF models and I found that the proposed model classification is reasonable.
15280,1," This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*."
15281,1, However there are issues in the presentation.
15282,1," With both theoretical and experimental analysis, it suggests the optimal batch-size given learning rate and training data size."
15283,1,"""The authors try to combine the power of GANs with hierarchical community structure detections."
15284,1," Please find my comments are below.\n\nOverall it is an interesting  but long paper,"
15285,1,"  It represents an elegant combination of ideas, and a well-rounded combination of theory and experiments."
15286,1,\n\n5. It's not clear whether this is a theoretical paper or an empirical paper.
15287,1,"  For training, apply knowledge distilation for better training followed by fine tuning by reinforce."
15288,1,  The trained residual function can be used to predict a residual z_i for x_i.
15289,1," If the focus is transfer, one could argue that another way of training the PATH net could be by training jointly the PATH net and goal net, with the intend of then transferring to another reward scheme."
15290,1," In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing."
15291,1, I think that it will only be a problem for extremely large numbers.
15292,1, but there lacks of evidence to support some of the arguments that the authors make.
15293,1, The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously.
15294,1, The MSE objective (optionally interpolated with a 2nd-order tensor) is optimized incrementally by SGD.
15295,1, \n - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model.
15296,1,  The experiments should be reproducible given the descriptions in the paper.
15297,1," This is -in part- due to the nature of the chosen datasets, in a 10 class dataset it is difficult to show the influence of the number of unseen classes."
15298,1," \n\n1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2). "
15299,1,but I find (as a non-meta-learner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details).
15300,1,"""Summary:\n\nUsing a penalty formulation of backpropagation introduced in a paper of Carreira-Perpinan and Wang (2014), the current submission proposes to minimize this formulation using explicit step for the update of the variables corresponding to the backward pass, but implicit steps for the update of the parameters of the network."
15301,1,"\n\nIn general, the proposed model has novelty."
15302,1,"  \n(3) In algorithm 1, what exact method is used in determining if \\mu is converged or not?"
15303,1, \n\nThe authors say they compare to DQfD but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit (L2 regularization is also used).
15304,1,"\n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator."
15305,1,"""This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase."
15306,1," However, then what is learned though the proposed formulation?"
15307,1," many good papers contain mistakes, but in my opinion the level in this paper goes beyond what is appropriate for published work."
15308,1,"   \n\n\n[1] Balles, Lukas, Javier Romero, and Philipp Hennig. \""Coupling Adaptive Batch Sizes with Learning Rates.\"" arXiv preprint arXiv:1612.05086 (2016).."
15309,1,   I'm very happy to see good application papers at ICLR.
15310,1, The authors essentially develop an attack targeted to the region cls defense.
15311,1,\n\nWhile the categorization is reasonable
15312,1,\n\nCons:\n- Results about the more efficient densenet* could be shown in the main paper
15313,1,\n\nMajor Weaknesses:\n- Some important technical details about the proposed technique and networks is missing in the paper.
15314,1,"\n\n- Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet."
15315,1,\n\n4. How is the model trained?
15316,1,"\n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR."
15317,1, This claim needs to be discussed clearer as it is not clear to me why this would be the case.
15318,1," but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)"""
15319,1,\n\nReview: \nI very much like the paper.
15320,1," In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method."
15321,1,\n\nThe proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient.
15322,1,"\n\nFurther, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001)"
15323,1," The appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors, is this correct?[[CNT], [CNT], [QSN], [MIN]] \n\n- It is not immediately clear from Figures 3 and 4 that the architectures employed are in fact recurrent."
15324,1,"\n\nMajor comments:\n\nThe paper is well written, and summarizes its contribution succinctly."
15325,1," The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks."
15326,1,This forces the different model instances to become more complementary.
15327,1,"""This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples."
15328,1, The addressed problem is challenging and the proposed idea seems interesting.
15329,1,". Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency"
15330,1," Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity)."
15331,1,"""MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure."
15332,1, Perceptron obtains a better performance than PA algorithms \u2013 which is very odd.
15333,1," Furthermore, while the highlighted contribution is the named entity table, it is always used in conjunction to the database approach."
15334,1,"\n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard."
15335,1," The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay."
15336,1, It is doubtful that the conclusion or results obtained in this small dataset could be scaled up to real-world applications or datasets (millions of subjects and images).
15337,1," Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others)."
15338,1," The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is."
15339,1, It appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.
15340,1, I would have expected the same runtime.
15341,1, I view this as an issue in and of itself.
15342,1, Does no batching (in section 3.2) means a batch size of one utterance?
15343,1,\n-\tSection 3.2 Dataset \u201cAs for TIDIGIT\u201d: \u201cAs for GRID\u201d(?)
15344,1," It would have been better to omit it and use the space to describe HDDA and the specific variant used in this work, as this is the main tool doing the distinction."
15345,1," The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task."
15346,1," Your results show 4 failed tasks, is this your reproduction of Entnet?"
15347,1,"\n\nDetailed comments:\n\n\""learn hierarchical policies\"" -> \""learns hierarchical policies\""?[[CNT], [CLA-NEU], [QSN], [MIN]]\n\n\""n games Mnih et al. (2015)Silver et al. (2016),\"": The citations are a mess."
15348,1,The method is evaluated on heat sink design and airfoil design.
15349,1,"  Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min-max sense."
15350,1," \n    In section 2.1 the authors mention that facial landmarks have been detected using a 'pre-trained ensemble-of-regresion-trees detector (Gerbrands, 1981)'."
15351,1," Unfortunately, all tables with the experimental results are left to the appendix."
15352,1, I am not sure I could redo the work from the provided information.\n\n
15353,1," \n\nWhile it is good to know that using hard negatives improves recall measures on coco, it is not clear that this paper provides enough novel insight to be interesting enough for the ICLR audience."
15354,1," \""Training very deep networks.\"" Advances in neural information processing systems."
15355,1,"\n- Training of CNNs, LSTMs and so on is not clear.(See question regarding whether the models are pre-trained or whether the models are also directly learned from the data.)."
15356,1," This by itself is not enough to boost the performance universally (e.g., if \\Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures."
15357,1," Agents working in this setting therefore, learn the language of the \""teacher\"" and efficiently ground words to their respective concepts in the environment."
15358,1,"  Finally,  I'm not sure I understand the X axis in Figure 2, the (effective) number of examples is much higher than the size of the dataset."
15359,1," Furthermore, I am slightly unconvinced about the authors' claim of efficiency."
15360,1," \""Deeply-supervised nets.\"" Artificial Intelligence and Statistics. 2015."
15361,1," However, if one views the paper in a different light, namely showing some \u201cblind-spots\u201d of current conditional GAN approaches like lack of diversity, then it can be of much more interest to the broader ICLR community."
15362,1," In experiments, results with beta=1.0 need to be presented to assess the importance of network inversion and the reconstruction loss."
15363,1,"""This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks."
15364,1," The testing framework will be made public too, which adds to the value of this paper."
15365,1," Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017)."
15366,1, Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.
15367,1, \n\nTheorem 4.1 seems to be implied by Theorem 4.2.
15368,1, \n\nWhat is the significance of the difference between BBB and BBB with sharpening in the language modelling task?
15369,1,\n- way too long
15370,1, What is the meaning of success rate in here?
15371,1, \n\nAlso note that multidimensional CLT here is glossed over.
15372,1, (The improvement on Zaslavsky's theorem is interesting.)
15373,1," Hence, I was not convinced that the propose KL+Gaussian modeling is suitable for directional relations."
15374,1," In other words, the proposed partial observability assumption requires some further motivation."
15375,1,"\n- I think eq 9 is incorrect, because the decoder is not Markovian."
15376,1," The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models."
15377,1, Auto-regressive VAE is used for decoding.
15378,1,"""The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one."
15379,1,. To some extent this is not consistent with the empirical observation that relu is very important for deep learning.
15380,1,"""Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons."
15381,1,"  The generative approaches based on VAEs and GANs are time consuming, but according to my experience, the training of VAE-based methods are stable and the topology generalization ability of such methods are good."
15382,1," The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally."
15383,1,\n\nIt is nice to see the application of ideas from different areas for learning-related questions.
15384,1,"\n\nIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient."
15385,1," Also, not sure I see why SEARNN can be used on any task, in comparison to other methods."
15386,1," The authors did a good job of using different experiments (filtration number analysis, and teaching both the same architecture and a different architecture) to intuitively explain what their method actually does."
15387,1,\nFor example if we set all the variances to be a unit matrix than the KL is collapsed to be a simple symmetrical Euclidean distance.
15388,1,\n\nOne question left open by the paper is the strategy used to collect data in order to compute the diffusion model (and thus the options).
15389,1," \n\nn Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets."
15390,1,  This paper would be much stronger if it offered some way to exploit this connection.
15391,1,"\n\nMinor:\n- Typo: Page 4 Line 7 \""Note that this algorithm use the similar strategy\"": use -> uses"""
15392,1," \n\nThere are many spelling and grammatical errors.\n"""
15393,1," Also note that subspace identification can estimate (A, B, C, D) matrices which is great for control purposes especially for the infinite horizon LQR."
15394,1, however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios. 
15395,1," But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases. """
15396,1,n3) Provide a common code framework with all methods\
15397,1, These are used in a 1-NN classifier and a (smoothed) Naive Bayes classifier.
15398,1, Such a flaw may affect the final performance of the proposed approach.
15399,1,"\n- \u00ab In principle, Q-learning variants are off-policy methods \u00bb => not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying \u00ab furthermore we are using a multi-step return \u00bb is too vague)"
15400,1,  The use of Thompson sampling for efficient\nexploration in deep Q learning is also not new since it has been proposed by\nLipton et al. 2016.
15401,1, ATreeC is an actor-critic architecture that uses a softmax over TreeQN.
15402,1,"""The authors extend the ResNeXt architecture."
15403,1, The proposed approach uses a syntax-checker to limit the next-token distribution to syntactically-valid tokens.
15404,1,            Sec 4.2: Several ideas are being discussed in Sec 4.2 and it is not entirely clear to me what has actually been adopted here; perhaps consider formalizing the actual computations in Subroutine 4 \u2013 and provide a clearer argument (preferably proof) that this leads to consistent and robust estimator of \\theta.
15405,1," The paper shows that SGD can be understood using stochastic differential equations, where the noise scale is approximately aN/((1-m)B) (a = learning rate, N = size of training set, B = batch size, m = momentum). "
15406,1,"""I was very confused by some parts of the paper that are simple copy-past from the paper of Downey et al."
15407,1," \n\nComments:\n1. It is difficult for the reader to understand a) why Wasserstein is used """
15408,1," \n\nSome minor remarks:\n\n- p3: We use RFs-> RFFs\n- p5: ||X||, you mean |X| the size of the dataset\n- p12: Eq (9)."
15409,1, What insights do you gain by knowing these theorems etc.
15410,1, (I would have liked to have seen what was meant by preserved the function here -- did you mean preserve the same class outputs e.g.?)
15411,1, There are no good explanations for why the method yields better generalization.
15412,1,  So I recommend acceptance.
15413,1, It'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net.
15414,1,"\n\n(2) In Figure 1, I suggest adding an \""N \\times\"" symbol to the left of the Q-P Attention Layer and remove the current list of such layers, in order to be consistent to the other parts of the figure."
15415,1,"  The key to being able to learn these confidence values is to not allow the error of the confidence estimates propagate back though the deep learning architecture.\n\nHowever, the method by which these confidence estimates are refined could be better described."
15416,1," Thus, despite the strong results,"
15417,1, This should probably be noted.
15418,1, They claim to have the following contributions: \n\n1. Using a grammar to guide decoding
15419,1, How does this affect the paper statements and results?\n-
15420,1,"For a GAN noise vector a plausible filter set is created, and for a data sample a set of plausible activations are computed. "
15421,1,"""This paper proposes to automatically recognize domain names as malicious or benign by deep networks (convnets and RNNs) trained to directly classify the character sequence as such."
15422,1, \n- the proposed approach to maintain the budget is simplistic
15423,1,\u201d  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.
15424,1, \n\nThis is mostly a \u201ctheory building\u201d work.
15425,1,"\n\nPros:\n\n- It is interesting that the latent space models are most successful, including the relatively simple GMM-based model."
15426,1," The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally."
15427,1, Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks.
15428,1," In other words, the paper proposes a particular approach for the adversarial defence."
15429,1,"""After the rebuttal:\n\nI do not think I had a major misunderstanding of the paper."
15430,1, Why/when is the 2nd term in (19) small?
15431,1," \n\nIt would've been nice to see an experimental comparison to the algorithm proposed by Arjovsky et al. (2017), since this is mentioned favorably in the introduction."
15432,1," First, if we trained the proposed model with starting from \""zero\"" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?"""
15433,1, \n7. More explanation is needed towards Fig.4c.
15434,1,\n\nCons:\n- The model-based approach is disappointing compared to the model-free approach.
15435,1,"  Ultimately, however, I felt like the paper was\nsomewhat unsatisfying as it left open a large number of obvious\nquestions and comparisons:"
15436,1,  Extensive experiments could be added.
15437,1," The main idea is to learn \""baseline\"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called \""Ranking-based Exponential Similarity Measure\"" (RESM), which is based on the recently proposed APSyn measure."
15438,1,\n\nProof of Lemma 7 seems to have typos/mistakes.
15439,1,"\nIn terms of evaluation, the authors do not compare their result against any other method."
15440,1,"\n\nWeaknesses:\n\n-\tWhile the proposed PSVRT dataset addresses the 2 noted problems in SVRT, using only 2 relations in the study is very limited."
15441,1,"\n\nQ(A) is sometimes taken to mean the true posterior (i.e., eq. (31)), sometimes a Gaussian approximation (i.e., eq (32) inside the integral), and both are used interchangeably."
15442,1," However, this has not been tested or concretely shown in this paper."
15443,1, \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits
15444,1,\n\nMy main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach.
15445,1, Does this mean that you need to convert the raw meshes coming from the 3D camera into a particular topology before you can use this algorithm?
15446,1," \nSentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here?"
15447,1,"\n\nCons\n- \""One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the\n  classification and the gradients are not back-propagated from it."
15448,1,\nThe results shown in the table seems to indicate the \u201cperfect\u201d misclassification.
15449,1,"  As it stands, this is somewhat an unknown, and should be easy enough to demonstrate."""
15450,1,\n-\tWhat is the definition of H the Hadamard matrix in the discrete orthogonal joint definition?
15451,1,\n- What is the exact mechanism of generating a representation for NE EECS545?
15452,1,"""\n# Summary of paper\nThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method."
15453,1,"The sentence \""We will denote the complete augmented...p(d)\"" might be moved to after \""from a uniform distribution\"" in the same paragraph.[[CNT], [PNF-NEU], [SUG], [MIN]] \nIn paragraph starting \""We now update x\"":\n    - specify for clarity: \""the first update, which yields x' \""/ \""the second update, which yields x''  \""\n "
15454,1,". However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as \u201csmaller-norm-less-informative\u201d assumption. "
15455,1, E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1].
15456,1,"""Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape."
15457,1," But comparing to [1], this paper has limited contribution."
15458,1,.\n\nPros:\n- The proposed function has similar behavior as ELU but 4x cheaper.
15459,1,"\n\nIt is highly suggested that the method is called as population-based method as a set of networks is maintained, instead of as \""genetic\"" method."
15460,1, Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors.
15461,1,? What makes this more challenging?
15462,1,. As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors.
15463,1, The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. 
15464,1,"     After all, when we humans generalize to understanding relationships, exactly what variability is present in our \""training sets\"" as compared to our \""testing\"" situations?"
15465,1, Another big issue is the lack of proper validation in Section 3.4.
15466,1,\n\nPros:\n\nThe paper tackles what seems to be both an important and challenging problem.
15467,1," Contributions include a new dual-based algorithm for the fundamental task of computing an optimal transport coupling, the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation, learning a Monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another, and a plethora of supporting theoretical results."
15468,1,"  However, there are quite a number of problems, as explained below.\n\n* The explanation of eqs 1 and 2 is quite poor."
15469,1,- No comparison to prior work on IMDB Sentiment Analysis or UCF-101 activity classification
15470,1," \nOne important contribution of the paper is about optimal batch sizes, but related work in this direction is not discussed.."
15471,1, Does this create any instability in learning?
15472,1,"""Summary: This paper proposes a different approach to deep multi-task learning using \u201csoft ordering."
15473,1, The application to learning from (partially adversarial) demonstrations is a cool idea but effectively is a very straightforward application based on the insight that the approach can handle truly off-policy samples.
15474,1," However, this paper claims both as novelties while not offering any improvement / comparison. "
15475,1,"  The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1], "
15476,1," However, the fact that no convergence rate is given makes the result weak."
15477,1,\n\n- The second weakness listed above might be related to the first one.
15478,1," On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy."
15479,1, The task is to learn whether the center pixel is reachable from the starting point.
15480,1, They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer.
15481,1," \n\nGiven the original number of traces generated is huge, I do not understand, why this method is at all practical."
15482,1, but one problem is that how to obtain the proper solution of equation (3)?
15483,1,\n\nI verified most of the math.
15484,1,"\n\nMinor comments:\n- Sec. 3: \""Language is inherently tree structured\"" -- this is debatable..."
15485,1," I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications."
15486,1," This in itself is not even new, but the authors replace a linear\noutput layer with squared error (proposed in another, earlier paper) by a\nsoftmax layer with cross-entropy."
15487,1,"\n[3] Dai Z, Yang Z, Yang F, et al. Good Semi-supervised Learning that Requires a Bad GAN[J]. arXiv preprint arXiv:1705.09783, NIPS 2017."
15488,1," For example, this work may guide to design better CNN structure for higher accuracy and lower computation cost."
15489,1, The method is tested on standard sequential MNIST variants as long as a class incremental variant.
15490,1,Authors should show these results for SRM.
15491,1, It would be favorable to empirically prove this by designing additional experiments.
15492,1,"\n\nThe main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed."
15493,1," For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1]."
15494,1,  Another problem is that it is not clear at all to me how authors' approach can be extended to non shift-invariant kernels that do not benefit from Bochner's Theorem.
15495,1,Take $\\sigma_k(t) = t for t rational and -t for t irrational$ as an example.
15496,1,"""The authors present a deep neural network that evaluates plate numbers."
15497,1,"\np8. Figure 7. How did you decide which data points to include in the plots?"""
15498,1,"\n\nQUALITY: I understand that the main emphasis of this work is on developing faster computational algorithms, which would handle large scale problems, for factorizing this tensor."
15499,1,The SCAN data-set has the potential to become an interesting test-case for future research in this direction.
15500,1, The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples.
15501,1," \n\nOverall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples."
15502,1,"\nIt would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives."""
15503,1," Experiments show that the method helps to find paraphrases of phrasal verbs, as well as improve downstream performance on preposition selection and preposition attachment disambiguation."
15504,1,"\n\n5. As mentioned in the paper, there are many methods which introduce sparsity in the convolution layer, such as \u201crandom kernels\u201d, \u201clow-rank approximated kernels\u201d and \u201cmixed-shape kernels\u201d."
15505,1,  The need for architectural search is unclear.
15506,1," When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example."
15507,1,"""In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization."
15508,1,"n\n* Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out \""Regularizing neural networks by penalizing confident output distributions."
15509,1," Overall I am weakly inclined to accept this paper."""
15510,1,"\n\nThe random matrix theory part of this paper is intriguing, but left me wondering \""and then what?"
15511,1,"\nThey conduct experiments in 3 datasets where they experiment with augmentation in the image feature space by random noise, as well as the two aforementioned types of augmentation in the semantic space."
15512,1," Because we already have algorithms which better solve this domain, why is your method advantageous?"
15513,1," Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section."
15514,1," So, how can the given observations be used to explain more recent works?"
15515,1,"""In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies."
15516,1," They propose to introduce the noise process into the generation pipeline such that the GAN generates a clean image, corrupts its own output and feeds that into the discriminator."
15517,1," So you use a binary SVM, not one versus rest."
15518,1,"\nThe last paper reports 0.71% error on MNIST-rot, which is slightly better than the PTN-CNN-B++ reported on in this paper."
15519,1," While the learning architecture is not original in itself, it is \nshown that a proper physical regularization greatly improves the performance."
15520,1," Due to the simplicity of the method, this paper could be a useful baseline for future work."
15521,1," It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret."
15522,1,  It would certainly seem strange if this were not the case.
15523,1," ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra-attention)."
15524,1,\nii) The learned latent representation provides an interpretation of generated graph properties
15525,1, The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.
15526,1, \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive.
15527,1," The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty."
15528,1,". It might be cleaner to either treat the general K case throughout, or state the theorem for K = 1.\n\n11."
15529,1,"and (b) although the tensor is non-negative, its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to me."
15530,1," The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context. "
15531,1," Basically, these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail."
15532,1,\n\nThe main weakness of the paper is lack of cohesion in contributions and difficulty in delineating the scope of their proposed approach.
15533,1,"  This expression is used to develop a new layer called a \u201cwarp layer\u201d which essentially tries to compute several layers of the residual network using the Taylor expansion expression \u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with."
15534,1,"\n\nWhile the following are the cons:\n\n  - the mentioned issues with the proposed algorithm, which in particular does not have any theoretical guarantees;"
15535,1,"  Furthermore, this learning extends to large-scale image datasets."
15536,1," While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC."
15537,1,  So there is a possibility for potential regularization but there is definitely a big loss in estimation power.
15538,1,"- Which multi-label classifier is used to classify images in attributes?"""
15539,1," EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs."
15540,1, The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.
15541,1," Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector."
15542,1, Is it to save the additional (small) overhead of using skip connections?
15543,1," I would either try to force the figures to be output at that position (not in or after Section 9) or write \""Figures X-Y show the results\""."
15544,1, Classification is done with soft k-NN on the class prototypes.
15545,1,"""This paper presents a model-based approach to variance reduction in policy gradient methods."
15546,1,\n\n= Major Comment =\n\nI'm concerned by the quality of your results and the overall setup of your experiments.
15547,1,\n- The authors showed benefits compared to a continuous relaxation baseline.
15548,1,"\""  In this task,  the model learns a joint embedding of the images and the attributes"
15549,1,\n\nOne additional question: Skip connections have been shown to be very useful in ConvNets.
15550,1,\n- I am not sure if the attention overt chart is a highlight of the paper or not.
15551,1," Also, tasks may have hierarchical correlation structures."
15552,1," The setup is slightly different from the original theorem in Hazan et. el., 2017 including the noise model, so I strongly recommend to include the original theorem in the appendix, and include the full proof in the appendix."
15553,1, The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically.
15554,1,\n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X.
15555,1," It is generally used a synonym for \""nonlinearity\"", so please use it in this way."
15556,1,". \n\nQuality: The mathematical formulas describing basic complex analysis ideas (e.g.  derivatives of complex functions, definitions of complex versions of standard activation functions) seem reasonable to me"
15557,1, Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing?
15558,1,\nWriting clarity - the paper is very well written and clear.
15559,1," It would, however, seem that the truncated iterations do not result in the approximation being very accurate during optimization as the truncation does not result in the approximation being created at a mode."
15560,1, What is the disagreement between L1 penalty and prediction quality?
15561,1," The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises:"
15562,1,"n\nSince the min-max problem is intractable in general,"
15563,1," Hence, it is\nnot clear why projecting them back into continuous functions is of interest."
15564,1,"  The L1 performance gains are bigger, but the authors only compare L1 on true positives."
15565,1,"\n\nPage 4: \""we apply the technique of variational inference Wainwright et al. (2008)\""."
15566,1," Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space),"
15567,1,"""This work proposed a reconfiguration of the existing state-of-the-art CNN model architectures including ResNet and DensNet."
15568,1,\n\n4. The proposed method only considers the \u201c+-shape\u201d and \u201cx-shape\u201d sparse pattern.
15569,1, \n\n3) The third and fourth columns of Table 5 are identical.
15570,1,"  Two algorithms are presented: NN-1 runs open-loop trajectories from the beginning state, and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state. "
15571,1,"""This paper proposes a technique to improve the output of GANs by maximising a separate score that aims to mimic human interactions."
15572,1," It may sound pedantic, but I don\u2019t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn\u2019t vary too much in the different images of the single object."
15573,1, I think these experiments would strengthen the paper.
15574,1," Could you consider testing the result on more different datasets to verify if the results are generalizable? """
15575,1, Or can this replaced by a single BFGS style step?
15576,1,\n- The second part of the paper seems to be a bit disconnected to the quadratic function analysis.
15577,1," But, as far as I understood, the authors only update loss weights in this paper."
15578,1,. This is achieved by learning to play the game in both directions. Authors show results in a word-level translation task and also a sentence-level translation task. They also show that having more languages help the agent to learn better
15579,1,  Investigating some of these questions would help us understand how well the approach works and in which settings.
15580,1," However, this manuscript is not polished enough for publication: it has too many language errors and imprecisions which make the paper hard to follow."
15581,1,"""The paper proposes a set of benchmarks for molecular design, and compares different deep models against them. "
15582,1,"\n\n- No typos at all, which I find very unusual. Nice job!"
15583,1, The structure allows for fast inference using a spectral approach.
15584,1,\n\n1. The paper is not self-contained.
15585,1, No insight is provided about the kinds of filters that are learned.
15586,1," E.g. learning curves would be useful.\n4) Sec 2 para 5: \""paired with a BNP blood test that is correlated with heart failure\"" I suspect many readers of ICLR, like myself, will not be well versed in this test, correlation with HF, diagnostic capacity, etc., so a little further explanation would be helpful here."
15587,1, The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?)
15588,1,\n- What data do you use for the distillation?
15589,1,"""Well written and appropriately structured."
15590,1," it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters."""
15591,1, \n3. There is only one definition of the reward - related to batch number when the accuracy first exceeds a threshold.
15592,1,"""Summary: \nThe authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix."
15593,1,  \n\n\n\nPros:\n\nImportant and challenging topic to analyze and any progress on unsupervised learning is interesting.
15594,1,  I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.
15595,1,". Hence, the contribution of this paper is limited, although the authors claim two differences between their work and the existing ones."
15596,1, This is unfortunately often the case when dealing with combinatorial search/optimization.
15597,1," I am not sure, how can the auto-encoder model not overfit completely to the training data instances."
15598,1, The architecture achieves impressive results on two tasks: SNLI and the reverse dictionary of Hill et al. (2016).
15599,1,"n\nThe evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings."
15600,1,"""Summary:\n\nThe paper proposes to learn new priors for latent codes z  for GAN training."
15601,1,\nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\n
15602,1,"  The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways."
15603,1,"""This paper presented a multi-modal extension of variational autoencoder (VAE) for the task \""visually grounded imagination."
15604,1," If you look into literature for zero shot learning, work by Z. Akata in CVPR 2015, CVPR2016, the performance on AwA and on CUB-bird goes way above 50%, where in the current paper it is 30.57% and 8.21% at most (for the most recent survey on zero shot learning papers using attribute embeddings, please, refer to Zero-Shot Learning - The Good, the Bad and the Ugly by Xian et al, CVPR 2017)."
15605,1, \n\nPros:\n + simple model\n + strong quantitative results
15606,1,"""The authors consider a Neural Network where the neurons are treated as rational agents."
15607,1,\n\n- Adding results for m=3 to table 2 would bring further insight to the comparison.
15608,1," There is a start at an interesting idea here, and I appreciate the\nthorough treatment of the background, including CEGIS and submodularity as a motivation for doing\ngreedy active learning, although I'd also appreciate a discussion of relationships between this approach \nand what is done in the active learning literature."
15609,1, The authors could better discuss how the approach could be adopted in practice.
15610,1,\n   experiment?
15611,1," The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates."
15612,1,"  However, the author did not suppose this condition."
15613,1,"\n\n\nMinor things:\n-\u201cTS finds the true Q-function very fast\u201d But that contradicts the previous statements, I think you mean something different."
15614,1," While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized."
15615,1,"\n- It is unclear (from Figures 3 and 7) that \""alternative optimization\"" and \""minimax\"" converged fully, and/or that the sets of hyperparameters were optimal."
15616,1," As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. "
15617,1," Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective."
15618,1,". From what I\u2019ve seen, log(tf)-idf LSA seems to perform about as well as LDA"
15619,1," Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed."
15620,1, Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act.
15621,1," Finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance.   \n""."
15622,1,"\n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n"""
15623,1,"  Similarly, your Bleu score on Vietnamese to English translation is way below what were reported (even by the organizer baseline) for the IWSLP conference where the data became available: https://github.com/magizbox/underthesea/wiki/SOTA-Machine-Translation:-IWSLT-2015\n\nGranted, the competing systems also were outperformed by the organizer baseline for that task at IWSLT 2015, but not by the degree to which your system is. "
15624,1, Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.
15625,1,"In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA."
15626,1,".\"" I didn't understand this conclusion, because previous work found complex-valued neural networks to be inferior, which is consistent with the results reported here."
15627,1,\nThe qualitative evaluation on 'Deciperhing the Latent Code' is not enough either.
15628,1,"\n\nOverall, I feel the strengths of the paper outweight its weaknesses."
15629,1," It is shown that simultaneously increasing the momentum parameter and the batch size also allows for fewer parameters, albeit at the price of some loss in performance."
15630,1,  The authors use an energy function based approach.
15631,1,\n\nThe experimental setup is 4x4 grid world with different basic shape or grey level rendering.
15632,1, \n\n\non the negative side:
15633,1, Seems like warmup_time should also be an input to the algorithm.
15634,1, A bi-directional LSTM is used to encode latent space in the training stage.
15635,1,"""This paper introduces a neural network architecture for generating sketch drawings."
15636,1,"  In the present paper these details are omitted and it is unclear how the update rules are derived from the KKT conditions and the Lagrange multiplier and how they differ from standard semi-NMF, this should be better clarified. \n\n"""
15637,1," Given the same receptive field with multiple complementary kernels, is the kernel shape important for the training?"
15638,1, \n\nI don't also see the value of extension to other activation function
15639,1,"  The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR."
15640,1," In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices."
15641,1,"""Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess."
15642,1," If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes?"
15643,1,  It is not safe to say the proposed method is the state-of-the-art by only showing the results in one setting.
15644,1,"\n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available."
15645,1,\nClarity: The paper is clearly written.
15646,1," This extends up to page 4. I would argue, that this is quite a stretch, as the free energy principle is essentially blind to the idea of rewards and preferable states such that all tasks are essentially evaluated in terms surprise reduction."
15647,1,"""This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE."
15648,1," From the analysis in the main paper, I believe the theoretical contribution is correct and sound."
15649,1,"  As is, the applicability of the method is limited."
15650,1," \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?"""
15651,1," For example, verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence."
15652,1," The metrics used in Table 1 are also not clear, they should be explained in the text."
15653,1, but the paper has several strong points:\n\n* The relation to previous work is made explicit and it is show that several previous approaches are generalized by the proposed one.
15654,1,"However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages"
15655,1, The authors may think about including a comment on that issue.
15656,1, I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.
15657,1," However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting."""
15658,1, I agree with the authors that an integrated view of self-organization and learning across layers is presumably required to better understand biological learning.
15659,1,"  Our results below provide more evidence for making this transition.\"" However, no other work on inference networks is directly cited."""
15660,1," It is a very interesting set up, and a novel idea."
15661,1," For now, it looks like more like a trick than anything else."
15662,1," \nWhile the idea of using mask is interesting and important, I think if this\nidea could be implemented in another way, because it resembles Gibbs sampling\nwhere each token is sampled from its sorrounding context, while its objective\nis still global, sentence-wise."
15663,1," The practicability of the method can be controversial, the number of attempts require to build the (meta-)training set of runs can be huge and lead to something that would be much more costful that letting the runs going on for more iterations."
15664,1, It would be interesting to see the results of the new activation function on LSTM.
15665,1,\n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker.
15666,1,  And why limit to 50 epochs then?
15667,1," The task is interesting and relevant, especially for in low-resource language pair settings.\"
15668,1,"\nFinally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation."
15669,1, The derivations look correct to me.
15670,1,". One of the main statements of the paper \u201cOur approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)\u201d is never proved, nor discussed.\"
15671,1,"""The paper proposes a variance reduction technique for policy gradient methods."
15672,1, but explores in particular on-policy fitting and a few other design choices that empirically improve the performance.
15673,1,"\n\nIn addition to introducing a heuristic reward term, the authors propose to alter the Q-function\nto be specific to the subgoal."
15674,1," The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data."
15675,1," It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation."
15676,1,\n\n(8) Below Theorem 2.4: Why is Phi now nk x T instead of nk x nT as in Definition 2.3?
15677,1," Meanwhile, the results are performed on some specific model configurations (like ResNet) and settings. It is difficult to justify whether it can generalize to other cases."
15678,1,\n\nThis is a short paper that contains five pages.
15679,1," Also, what is the epoch number, and why is this 1 for alpha=0?"
15680,1,"\n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods?"
15681,1, The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network.
15682,1, How is the 2d subspace chosen?
15683,1, The work is very light on references.
15684,1,"\nNo statistics over multiple runs are reported, and given the high variance of\nresults on these datasets I would like them to be reported."
15685,1,"\n\n(side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act."
15686,1,"\n\nMinor:\n\n- Fonts in figure 4 are too small.\n"""
15687,1," The survey part is nice,"
15688,1, None of the comparisons in the paper feature any learning.
15689,1,"\n\nThe empirical work is somewhat compelling,[[CNT], [EMP-POS], [APC], [MAJ]] though I am not an expert in this\ntask domain."
15690,1, Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG.
15691,1,\n\ncons:\nThe provided experiments are weak to demonstrate the effectiveness of the proposed method.
15692,1,.\n\n3) The authors argue that their method is preferable to graph kernels in terms of time complexity.
15693,1, It is based on the two-stage method that object proposals are generated from the first stage with attention.
15694,1," \n\nThis paper replaces the manual addition of a distractor sentence with a single word replacement where a \""narrator\"" is trained adversarially to select a replacement to fool the question answering system."
15695,1," As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent."
15696,1,"\n -  would any 7-7 split work just as well (ie, cross-validate over the 14 domains)"
15697,1, Do the authors think that this distinguishability can lead to a defense that uses these statistics?
15698,1,"\n\nThe experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome."
15699,1,  The probability of what?
15700,1, Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science.
15701,1,\n\n=============================================================\n\nThis paper presents an algorithm for training deep neural networks.
15702,1,"\n-While I mentioned it as a pro, it also seems to be that this technique simply doesn't buy you very much as a practitioner."
15703,1,\n(i) Directly performing regression on classification problems is very heuristic and unnecessary.
15704,1," \n\nMinor Comments:\nThe sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed."
15705,1,"\nWhile PSRNN usually uses random features to project the map the states in a new space where dot product approximates the kernel well, the authors proposes to leverage orthogonal random features."
15706,1,")  Also the dynamic programming for Levenshtein is foundational, not required to present that algorithm in detail, unless there is something specific that you need to point out there (which your section 3.3 modification really doesn't require to make that point)."
15707,1," I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  """
15708,1, This seems to be the central part missing to this paper
15709,1," While the algorithm does not have a theory for general non-quadratic functions,"
15710,1,.\n\n\n= Minor Comments = \n\n* You should mention 'Energy Based GANs
15711,1,\n\u2013 Figure 3 is low resolution and difficult to read.
15712,1, The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix.
15713,1," The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator."
15714,1," Unless the authors meant that it \""is similarly decoded to produce $\\mathbf{\\~x}$."
15715,1,"""Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space."
15716,1,"That being said, I believe that \nsome discussions could strengthen the paper:\n - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating\nuncertainties in the observation or physical evolution models."
15717,1,\n\nI'd be interested in how their proposed VGG-based PIR actually correlates with human evaluation.
15718,1," The reduction of performance could mainly cause by seeing fewer raw images, but not the labels."
15719,1," \n3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations."
15720,1,"However, it is very hard to generalize from these toy problems."
15721,1, What are the memory requirements and computational complexity of the proposed method?
15722,1," But as the paper shows, it\u2019s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow."
15723,1, \n- Very interesting dataset to be released.
15724,1, Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.
15725,1,"\n\nOverall, the paper is clearly written and easy to understand the main motivation and methods."
15726,1, Is there a way to fix it besides using ASG?
15727,1,\n\n2. The proposed method is shown to outperform a few baselines empirically.
15728,1, but I would have liked a more thorough analysis.
15729,1, The attempts to demonstrate empirically that the GAN does not memorize training data aren't particularly convincing; this is an adversarial setting so the fact that a *particular* test doesn't reveal private data doesn't imply that a determined attacker wouldn't succeed.
15730,1," The authors provide a possible mechanism to explain these results, by analyzing classification performance as a function of baseline firing rate."
15731,1,\nIt would be interesting to see the accuracy of the fitness approximation during the rank selection procedure.
15732,1,"\n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results."
15733,1,\nEq. (10) uses subscript d where it should be using subscript n\nEq.
15734,1,  \n-      I cannot believe the sum of SVD dimensionality of each feature maps becomes equals to the dimensionality to concatenated feature maps.
15735,1, Why is GLU chosen? Why is dilation used?
15736,1, Is this f(x) the original function (non quadratic) or just the local quadratic approximation?
15737,1,. But the training of the GAN is in itself a problem.
15738,1,\n\nCons and mainly questions:\n1. Missing related work.
15739,1," The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions."
15740,1," \n- Finally, as the matrix is not symmetric, do real eigenvalues always exist?"
15741,1,"\n- the \""blank\"" states do *not* model \""garbage\"" frames, if one wants to interpret them, they might be said to model \""non-stationary\"" frames between CTC \""peaks\"", but these are different from silence, garbage, noise, ..."
15742,1,\n\nThe causality assumption does not seem to apply to the few-shot classification case.
15743,1,"\n\nSentence before Equation (5): I believe there is a typo here, \u201cf takes z_i\u201d should be \u201cf takes u_t\u201d."
15744,1,\n\n(4) It seems that this paper was finished in a rush.
15745,1,"""This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization."
15746,1,"""This paper present a method for detecting adversarial examples in a deep learning classification setting."
15747,1," \n\n2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories."
15748,1, However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n
15749,1,"  As an example of time-travel to the past, the authors talk\nabout RBMs and stacks of auto-encoders as if that was the deep learning\nstate-of-the-art."
15750,1," In few-shot classification, the sequence length can be known a prior."
15751,1,  This very much relates to the method used to generalize over subgoals in the paper.
15752,1,\n- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore.
15753,1,"\n\nReferences:\n\n[1] Grosse, Roger, et al. \""Exploiting compositionality to explore a large space of model structures.\"" UAI (2012).\n[2] Duvenaud, David, et al. \""Structure discovery in nonparametric regression through compositional kernel search.\"" ICML (2013).\n"""
15754,1,\n\nI am also curious with the effect of pre-trained model from ImageNet.
15755,1,"\n\nUnfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling."
15756,1, I wonder how does this approach compare with the proposed method.
15757,1,"   See e.g. Section 2.2 of [Collins et al. JMLR 2008] (this is for the more general multiclass logistic regression problem, but (2) is just the binary special case of equation (4) in the [Collins ... ] reference)."
15758,1," Those approaches should at least be discussed in the related work, if not compared against."
15759,1,"\n\n\n[Yu et al. 2017] Adams Wei Yu, Hongrae Kim, and Quoc V. Le. Learning to Skim Text. ACL 2017\n\n"""
15760,1," \n\nTherefore, I think the current version is not ready to be published. The author can make it stronger and consider next venue."
15761,1," Further, the taxi agents have \u201cbatteries\u201d, which starts at a positive number, ticks down by one on each time step and a large negative reward is given if this number reaches zero."
15762,1, The paper has scope for improvement.
15763,1, but both MNIST and ModelNet40 seem like simple / toyish datasets.
15764,1," The authors cited the no requirement of \""a predefined number of clusters\"" as one of the contributions, but the tuning of alpha seems more concerning."
15765,1," For example, in standard vision systems, low level filters \""V1\"" learn edge detectors (gabor filters) and higher level filters learn angle detectors [1]."
15766,1," But, then, the authors missed to clarify how the definitions of D_i and C_j translate this requirement into math."
15767,1,"   - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \""mask(x',m^t)\""?\n "
15768,1,"\n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b)"
15769,1,"""In this paper a neural-network based method for multi-frame video prediction is proposed."
15770,1," One related point that is perhaps more specific to this particular work is the strategy used to evaluate the options built: the diffusion time, or the expected number of steps between any two states of an MDP when following a random walk. "
15771,1,"""The paper proposes a large experimental analysis with the goal of evaluating the generalization capabilities of CNNs."
15772,1,Can you provide more details about how you run the data assimilation model in the experiments? Did you use your own code?
15773,1," though it involves large amount of experimental efforts, which could be impactful."
15774,1," In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful."
15775,1," However, it is unclear how it means for the graph representation."
15776,1,"\nThe h( \\Phi( x ), v^{k-1} ) sort of makes sense \u2026  value is only calculated with respect to only the observation of the maze obstacles but the policy \\pi is calculated with respect to the joint  observation and agent state."
15777,1, The result from the paper seems to answer with \u201cin all cases\u201d but then that always brings the issue of \u201coverfitting\u201d or parameter tuning issue.
15778,1,". That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization"
15779,1,\n\n- A lot of important experimental details are in the appendices and they differ among experiments.
15780,1,". However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice."
15781,1, The matrix U is learned from the data as well.
15782,1," Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems."
15783,1,"\n\nBesides these major issues, there are also a few minor issues I have with the paper. "
15784,1,  Both picking up the passenger (reachability) and dropping them off somewhere are essentially the same task: moving to a point.
15785,1,"\n\nI agree this relationship with random matrices could be interesting, but it seems too vague right now."
15786,1," To be consistent, notation like \\odot and \\bigodot might be a bit clearer."
15787,1, Alg 1 line 5: 'closed form': there is no closed form in Eq(14).
15788,1,\nThe proposed method has better robustess in different tasks and different batch size setting.
15789,1," Even more puzzling, why does this error go up again for the blue curve (no interaction)? Shouldn\u2019t at least this curve be smooth?\n"""
15790,1,The idea of amortizing inference is perhaps more general. 
15791,1," For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima."
15792,1,"\nBeyond the problem of evaluation, the model the authors propose does not provide new ideas, and rather merges existing ones. This, in itself, is not a problem"
15793,1,\n\n(5) I do not quite understand the reason for the big performance drop on DrQA.
15794,1, Could the authors explain their choices?
15795,1,"  In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method."
15796,1,"\n- The VCs are introduced for few-shot classification, unclear how this is different from \""previous few-shot methods\"" (sect 5)."
15797,1," My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity."
15798,1,"\n  of SMAC, BO, and other approaches that are trying to model the map\n  from these choices to out-of-sample performance."
15799,1," Besides, the idea of using adaptive learning rates are not completely new, and somewhat closely related to second order optimization methods."
15800,1,"\n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN."
15801,1," The interesting part of these experiments is that the noise is not stationary, and this is quite characteristic of real-world applications."
15802,1,"  First, the manuscript is poorly written, to the point where it has inhibited my ability to assess it."
15803,1, Our results answer this question negatively.
15804,1,"\n\nIn the text, k suggests to be identical to the number of entities in the image."
15805,1," b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters."
15806,1," In my opinion, this section could benefit from a little more expansion and conceptual definition."
15807,1," You need to find a better justification for using L2-SVM than \""L2-SVM loss variant is considered to be the best by the author of the paper\"", did you try classical SVM and found them performing worse?"
15808,1,\nbut I think the authors may be overselling these ideas as a brand new algorithm.
15809,1,"\n- Some references to Figures are missing the figure number, eg. 3.2 first paragraph,"
15810,1,"""This paper applies a predictive coding version of the Sigma-Delta encoding scheme to reduce a computational load on a deep learning network."
15811,1, The whole algorithm is tested on a toy problem with 3 repeats.
15812,1,\n* Detailed analysis on different ensemble fusion methods on both training time and testing time.
15813,1,  It would be helpful if the plots of other domains are provided.
15814,1, The work is contrasted to tit-for-tat approaches that require complete observability and operate based on expected future rewards.
15815,1,\n\nPros:\n-\tProvide theoretical guarantee for the use of orthogonal random features in the context of PSRNN
15816,1," \n\nInterestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). [[CNT], [EMP-NEU], [DIS], [MIN]]The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative."
15817,1, So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense.
15818,1,"\n\nIt was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time."
15819,1,"  Given a new dataset, how to determine it for a good performance."
15820,1,\n\nIntro: The name SFC is misleading as the method consists in stopping early the training with an optimized learning schedule scheme.
15821,1,"\n\n5. I don't quite agree with the asserted \""multi-modal structure\"" in Figure 2."
15822,1, Results with linear and non-linear function approximation highlight the attributes of the method.
15823,1," When regularization of the network is not used during training, the trained RNNs no longer resemble the EC."
15824,1, It is repeated multiple times throughout the paper.
15825,1,"\n\nOverall, I think it is important to study control problems from a statistical perspective, and the LDS setting is a very natural target."
15826,1,It is also a bit hard to get the main takeaway messages.
15827,1,"   However, there is no comparison with other approaches in the literature."
15828,1," For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component."
15829,1," However, the checkboard artifact is a well-known problem of deconvolution network, and has been addressed by several approaches which are simpler than the proposed pixel deconvolution."
15830,1, One argument seems to be \u201cfor kernels with specific specific integrand one can improve on its properties\u201d. But this trick can be used for Monte-Carlo as well. And I do not see benefit of this trick in the curves.
15831,1,"  \nIn the end of section 3, it mentioned that \""without normalization,\"" the method will not scale to an arbitrary number of objects."
15832,1," The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments."
15833,1, The algorithm is tested on three datasets.
15834,1,"\n\nThis paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the \""gates\"" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2))."
15835,1, \n2. The property of TR decomposition is that the tensors can be shifted (circular invariance).
15836,1, All of the reported results are what you would expect.
15837,1, What are the class you are interested in?
15838,1,"""Summary: \n\nI like the general idea of learning \""output stochastic\"" noise models in the paper,"
15839,1,. The authors could be more concise when reporting results
15840,1,\n\nFigures 4-7 are hard to interpret and do not convey a clear message.
15841,1,\n\nSignificance \n\nIt is hard to see how the present paper improves on classical or alternative (deep) predictive coding results.
15842,1,"n\nThe model is a residual gan formulation, where the generator generates an image mask M, and (Input + M) is the adversarial example.\"
15843,1, \n(2) Only the policy and value networks specified.
15844,1,\n- the generalization ability of RNNs on longer SCAN commands
15845,1,"""The paper proposes a piecewise linear activation function that is build on ELU."
15846,1,"""\nSummary:\n\nAuthors propose a method which uses a Q-learning-based high-level policy which is combined with a contextual mask derived from safety-contraints and low-level controllers which disable certain actions from being selectable at certain states."
15847,1,\n \n \n3. Background\n \n\u201ca function from the state and the action of an agent to the real value\u201d -> a reward function
15848,1,"\nUsing leaky neurons for encoding and decoding is standard, see e.g.:\nBharioke, Arjun, and Dmitri B. Chklovskii. \""Automatic adaptation to fast input changes in a time-invariant neural circuit."
15849,1, The authors' technique may let us do this data-generation easily.
15850,1," \n- typo, find the \u201cmost orthogonal\u201d representation if the inputs -> of the inputs "
15851,1," The following is the differences that I found from this paper, so it would be important to discuss why such differences are important."
15852,1,. This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings.
15853,1," Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping."
15854,1," It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully."
15855,1,"""This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting."
15856,1,"\n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability."
15857,1,"""\npage 4 sec 3 first para: missing reference at \""a given dialog\""\npage 5 first para: \""Concretly\"" -> \""Concretely\""\nTable 1: \""GMenN2N\"" -> \""GMemN2N\""\nTable 1: what is difference between \""mean\"" and \""average\""?\npage 8 last para: missing reference at \""Iterative Attentive Reader\""\npage 9 sec 6.2 last para: several citations missing, e.g. which paper is by \""Tesauro\""?"
15858,1,"\n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved."
15859,1, Focusing on discrete targets gains the benefits of quantized networks.
15860,1, This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter
15861,1, Isn\u2019t this expected since the baseline uses less filters?
15862,1,"\n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\"
15863,1,"""Summary:  The paper applies graph convolutions with deep neural networks to the problem of \""variable misuse\"" (putting the wrong variable name in a program statement) in graphs created deterministically from source code."
15864,1, Did the authors try such experiment?
15865,1, \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic.
15866,1,\n\n(Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset.
15867,1,\n1) why for the omniglot experiment the table reports the error results? 
15868,1,Authors use the image modality as a bridge between two different languages and the agents learn to ground different languages to same image based on the similarity
15869,1,"\nAlso, why highlight the problem of authorship attribution of Shakespear's work in the introduction, if that problem is not addressed later on?"
15870,1,"\n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017)."
15871,1,"   \n\nThe first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP."
15872,1," However, this does not invalidate the contributions of this manuscript."
15873,1," The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved."
15874,1,"""This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input."
15875,1," In light of these issues, I recommend reject."""
15876,1,\nIn this paper the authors investigate \u201cthe use and place\u201d of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn \u201chigh level tactical decisions\u201d and introduce \u201cQ-masking\u201d a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values.
15877,1,"\n-- It would appear that the baselines could be improved further using standard techniques"""
15878,1, Algorithm 1 is also stated from that thinking and it is a well-known optimization algorithm.
15879,1,  \n\nPros\nThe paper presents interesting ideas regarding unsupervised object discovery\
15880,1,  One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x. 
15881,1, Is it because of the residual connection?
15882,1, The subfigures are not labeled.
15883,1,\n\nSummary\nThe authors present methods for generating synthetic sequences.
15884,1,"  \n\n- I think the authors rush to conclude that \""a small ball around a given input distance can be misleading\""."
15885,1," In general some more nuanced statistical analysis of these results\n  would be worthwhile, especially where they concern human ratings.\n- The dataaset fractions chosen for the semi-supervised experience seem\n  completely arbitrary."
15886,1," Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required."
15887,1,The claimed main contribution of the paper is the taxonomy.
15888,1,  The experiments are complete.
15889,1,"""This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets."
15890,1," Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support."
15891,1,\n- Three new datasets of UI images and corresponding code\
15892,1," If the authors believe that the 50% case is not necessary, please feel free to explain why."
15893,1, The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story.
15894,1, but the iterative inference model has mostly saturated.
15895,1,"""The paper is generally clear, and proposes to use a convolutional autoencoder based on 3D meshes."
15896,1,"\n\u2013 Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency. "
15897,1,"\n* Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)"
15898,1," These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training."
15899,1,"""The problem of numerical instability in applying SGD to soft-max minimization is the motivation."
15900,1,"""\nI thank the authors for the thoughtful response and updated manuscript."
15901,1, The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy.
15902,1, This leads to generalization for 2-layer networks with appropriate bounded size.
15903,1," Still, the paper as it stands is not complete, and I encourage the authors to followup with more thorough quantitative empirical evaluations.\n"""
15904,1, \n\nIt\u2019s hard to take issue with a paper that has such overwhelmingly convincing experimental results.
15905,1," Instead of GMMs and dictionary learning in PCL,  MagNet trains autoencoders for defense and provides sufficient experiments to claim its defense capability."
15906,1,"  Can you provide visualizations of the communities of the interpolated graphs in Fig 7? """
15907,1,"""The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function."
15908,1,"\n\nThe citation style in section 2.4 seems off.[[CNT], [PNF-NEG], [CRT], [MIN]] Also see [4] for a great description of how beam search is done in CTC."
15909,1, These datasets should show whether using more general features (YOLO-9k) helps.
15910,1," \n\nIn summary, getting net2net to work for architecture search is interesting."
15911,1, Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron.
15912,1,\n - On p.3 above sec 3.1: What is u?
15913,1, What were the nearest neighboring posters in the original VGG space? They should not be that bad too.
15914,1, There needs to be a cohesive story that puts the elements together.
15915,1,\n\u2013 The perception-driven control formulation is well-detailed and simple to follow.
15916,1, The main limitation is that the best architectures as currently described are less about discovery and more about human input;
15917,1," In this case, using a teacher model trained on a harder task (CIFAR10) leads to much improved student training on a simpler task (MNIST). Why?"
15918,1,"\n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating."
15919,1,"\n\nMinor comments:\n\nIn appendix C, Table 4 caption: you say target sentence is \u201cTrg\u201d but it is \u201cRef\u201d in the table."
15920,1," In (B), the `without skip-connections' modifies `deep feed-forward networks' and suggests that the network trained has no skip connections. You must mean (B), because (A) is false."
15921,1," The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling."
15922,1, There are many format errors like this throughout the paper.
15923,1," It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough."
15924,1," For example, the authors mentioned sparse or non-sparse loss functions."
15925,1," These experiment results show very small improvement compared to the ML baselines (see Table 2,3 and 5)."
15926,1," The proposed method, which makes use of grouping information, seems reasonable and useful."
15927,1,"\n\nEDIT: I have read the author's comments and am satisfied with their response. I believe the paper is suitable for publication in ICLR."""
15928,1,\\n\n* Section 3.3: Claim 1 is an interesting observation.
15929,1," I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate)."
15930,1, The authors also introduce a specific control variate technique based on the so-called Stein\u2019s identity.
15931,1," However, if you look closely at some pictures, you can see that they are very different though reported as similar."
15932,1," Finally, the paper stops abruptly without any final discussion and/or conclusion. """
15933,1,. These structures are extrapolated  by \n1) mapping the graph nodes into an embedding using an algorithm like node2vec\n2) compressing the embedding space using pca\n3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.
15934,1," That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?"
15935,1,\n\nMy major concerns are as follows.
15936,1,  The writing also needs to be improved.
15937,1,\n- quantitative results (Table 1) too little detail:\n        * why is this metric appropriate
15938,1,"\n\nAdditionally, the example domain makes no sense."
15939,1,"""This paper introduces two new loss functions which can be used along with the existing reconstruction and adversarial losses for language style transfer."
15940,1," As stated previously, the approach is validated with a large number of real Android projects\n4)"
15941,1,\n\nTable 5 is not very clear.
15942,1,\n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency.
15943,1,"\nUnfortunately, I do not believe the paper is ready for publication as it stands, as it suffers from lack of clarity and the experimentation is limited in scope."
15944,1,"""This paper proposes an unsupervised method, called Parallel Checkpointing Learners (PCL), to detect and defend adversarial examples."
15945,1, It is not clear to me immediately how this will affect the output distribution.
15946,1, Gradient descent can converge to a first order optimal solution. 
15947,1,"   Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR)."
15948,1, Same for section 4.6 (which I believe is not used in the experiments).
15949,1, \n\n\nSome more specific comments:\n\nFigure 2: I think this might be clearer if you unrolled a couple of iterations in (a) and (c).
15950,1,  The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset (e.g. some combination of attention between question/context and LSTMs over question/context).
15951,1," this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale."
15952,1,\n--The maths is very rigorous.
15953,1, Can you elaborate?
15954,1," \n-\tWhich patchs are extracted from images, for training and at inference time?"
15955,1,"  This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient."
15956,1," In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model."
15957,1,"  In particular, how to ensure the semantic space u to be same as the vocabulary semantic space?"
15958,1," But, I was a little disappointed by the paper as several details of the model were unclear to me and the paper's writing could definitely be improved to make things clearer."
15959,1, The only other work I found is\nGaussian Quadrature for Kernel Features NIPS 2017. \nThe work is pretty recent so the author might not know it when submitting the paper.
15960,1,"""This paper proposes a neural clustering model following the \""Noise as Target\"" technique."
15961,1,\n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc.
15962,1, It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.
15963,1,\n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\
15964,1,\n\nThis paper is easy to understand
15965,1,\n\n5) bottom of p.4: use hat{L}_gamma = 1 instead of L_gamma =1 for more clarity.
15966,1,\n5) It would be useful to discuss the average reward setting and how it relates to your work.
15967,1,"\n\nQuality and Significance - There is a disconnect between premise of the paper (improving efficiency of fully connected layers by learning sparser structures) and applicability of the approach (slow EM based method to learn structure first, then learn the parameters)."
15968,1, How can such examples be used by a malicious agent to cause damage to a system?
15969,1, The experiments\nperformed show that the Bayesian view of batch normalization performs similarly\nas MC dropout in terms of the estimates of uncertainty that it produces.
15970,1," Please clarify.\n\n---------------\nI have updated my scores as authors clarified most of my concerns."""
15971,1,\n\nThe authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*.
15972,1,  Other systems seem to have development and test performance closer together. 
15973,1, Do redundancy cues which work for multi-document news summarization still work for this task?
15974,1, The experimental results are presented well for a range of Mujoco tasks. 
15975,1,"\n\nCons:\n(1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case."
15976,1,"\n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. "
15977,1,"\n\nOverall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network."
15978,1,"""The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels."
15979,1,"""The paper proposes a method to train deep multi-task networks using gradient normalization."
15980,1,"""The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data."
15981,1,"\nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better."
15982,1," \n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}."
15983,1,"""The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems."
15984,1,\n- Experimental gains on small data sets
15985,1," If no LM was used, then the choice of reporting results in terms of only CER is reasonable,"
15986,1,. The latter would be more exciting.
15987,1,Numerical experiments suggests that the proposed regularization leads to better posed optimization problem and even a slight advantage in terms of inception score on the CIFAR-10 dataset.
15988,1," \n\nAlso, a related article: One article testing rational pedagogy in more ML contexts and using it to train ML models that is\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016)."
15989,1,"\n\nTable 3 and 4. I assume that the training time unit is a minute, I couldn\u2019t find this information in the paper."
15990,1," Thus, if we have the SR matrix we can replace the Laplacian mentioned above."
15991,1,"In summary, the paper lacks novelty wrt technique, and as an \u201capplication-of-attention\u201d paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed."
15992,1," Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method."
15993,1," \n- Right before 6.1.1: \""when the these such\"" -> \""when such\"""
15994,1," It sometimes uses vague/nonstandard terminology (\""parameterless\"") and statement."
15995,1," The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017)."
15996,1," However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about."
15997,1,"\n\nThere's a lot that I like about this paper, in particular the ablation\nstudy to examine which pieces matter, and the evaluation of a couple\nof simple models."
15998,1," They indicate that since Lipton et al. \""do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\""."
15999,1,". The paper postulates that learning should happen on shallower networks first, then on a deeper network that uses the GAN cost function and regularizing discrepancy between the deeper and the small network. "
16000,1," In addition, if this was challenging to optimize, it'd be useful to include lessons for how the authors manage to train their model successfully."
16001,1,"  As presented, the spiking networks are not exactly \""deep\"": I am puzzled by the statement that in the youtube-bb dataset only the top 3 layers are \""spiking\""."
16002,1,"  For the application of images, using text description to refine the representation is a natural and important research question."
16003,1," Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation."
16004,1, Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant.
16005,1,".\n\nComments\n-------------\n- The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems."
16006,1," \n\nThe mutation is a method to sample individual independence of the objective function, which is very different with the gradient step."
16007,1,\n\nI would agree that the idea of using dilated CNN (w/ residual connections) instead of BiLSTM could be a good solution to many online NLP services like document-level classification tasks.
16008,1,\n\nI'm also curious whether using a stochastic latent variable (Z) is necessary
16009,1,". \n3. the experimental setup seems quite unusual to me: \""since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\"". "
16010,1,"\n\nThe decomposition of the LSTM in terms of these weights is useful, and suggests new strategies for comparing existing quadratic time attention-based extensions to RNNs."
16011,1,\n3) When the empirical risk is close to the true risk.
16012,1," It appears that the analysis in this paper threw out more than 90% of the patients in their original dataset, which would present serious concerns in using the resulting synthetic data to represent the population at large."
16013,1, The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further.
16014,1, We are talking about sample variance?
16015,1,\n- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger
16016,1,\n\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.
16017,1,\n\n\nSection 4.3:\n- Many of these distributions don't look sparse.
16018,1,"""This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework."
16019,1, This is surely an important step towards more general deep learning models.
16020,1,"The paper didn't really compare other popular GAN models, especially WGAN and its improved version"
16021,1,"""This paper proposes to use graph neural networks for the purpose of few-shot learning, as well as semi-supervised learning and active learning."
16022,1," Finally there are a few steps that are not explained: for example, no justification is given for the inequality in eqn (13).\n5."
16023,1," This is a weaker result than their claim that ORFs satisfy better error,"
16024,1," It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\\theta | \\phi, x) in eqs. 19-20 parameterised?"
16025,1, This method is a combination of the online variational inference for streaming environment with Monte Carlo method.
16026,1," For instance, why not refer to the seminal paper by Kass & Raferty?"
16027,1," Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\n"
16028,1,"""This paper proposes a new algorithm to generate the optimal control inputs for unknown linear dynamical systems (LDS) with known system dimensions."
16029,1,"\"" --> this does not seem very precise: under what policy is the 60-80% defined?"
16030,1,"\n\nIn appendix:\n\n * Assumption 3, 4: Why is upper superindex d?"
16031,1, This can be improved to help readers understand better.
16032,1," One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound."
16033,1, the motivation behind the semi-supervised and active learning setup could use some elaboration.
16034,1,"\n\nIn section 2.2, it says \""observation that the double centering...\""."
16035,1," Furthermore, such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation (3)."
16036,1, See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper.
16037,1,\n\nOverall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights.
16038,1,"In summary, I recommend acceptance."
16039,1," For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult."
16040,1," Overall, TR seems like an interesting idea,"
16041,1,\n\nSignificance: The problem is important.
16042,1,\n\nii. the equilibrium concepts being considered i.e. does the paper consider Markov perfect equilibria.
16043,1, The first GAN then uses the second GAN as prior to generate the z codes. 
16044,1,"   Some previous work is cited, but I would point the authors to much older work of Parr and Russell on HAMs (hierarchies of abstract machines) and later work by Andre and Russell, which did something very similar (though, indeed, not in hybrid domains)."
16045,1, Having proper mathematical description and good diagrams of what you doing would have immensely helped.
16046,1," They also introduce a \""raw point cloud GAN\"" (r-GAN) that, instead of generating a latent code, directly produces a point cloud.\"
16047,1, The model is trained by a policy gradient algorithm.
16048,1, This is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced above.
16049,1, The authors do not exhibit much knowledge of combinatorial optimization literature (as has been pointed out by other readers) and ignore a lot of previous work by the combinatorial optimization community.
16050,1,\n4) in section 2.2 why is the behavior policy random instead of epsilon greedy?
16051,1," In fact, the \""toy\"" example is so much of a \""toy\"" that I am not sure what to make of it."
16052,1, But at which level?
16053,1, The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments
16054,1,\n\n+ Quality:\n- Simple method leading to great results on ImageNet!
16055,1, but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing.
16056,1,\n\n\n## Pros / Strengths\n\n+ effort to assess momentum / Adam / other modern methods
16057,1,"  It is therefore hard to draw anything conclusive from these results, especially given how many moving parts there are here (from model initializations to training procedures)."
16058,1," The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure 2"
16059,1,"    How do the authors know that humans are effectively generalizing rather than just \""interpolating\"" within their (very rich) training set?"
16060,1,"""This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions."
16061,1,\n\n- The mechanism in 2.2.4 feels a little like\n  http://aclweb.org/anthology/D17-1015
16062,1," However, only one type of tasks is used."
16063,1, \u201cour experiment is extended with additional epochs to fine-tune until the accuracy improvement is smaller than 0.1%.
16064,1," However, the statement \""since we are using at most third order tensors in this work\"" I am further confused."
16065,1,"\nThe paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below)."
16066,1," Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection."
16067,1," Also in Section 11, Figure 13 should be referenced with the \\ref command"
16068,1,"""The paper introduces the notion of continuous convolutional neural networks."
16069,1,\n \nIt is not clear to why the authors mention that \u201cnegative result that the return decreases if we naively consider units as agents\u201d.
16070,1," To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution."
16071,1," \n\n The experimental results seem promising, although not earthshattering."
16072,1," In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow."
16073,1," This year strong improvements over state-of-the-art have been achieved using attention for translation (\""Attention is All You Need\"") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition)."
16074,1," That is, even if there is no way to reach a reward state in 32 steps, an MC value function approximation with horizon 32 can extrapolate from similar looking observed states that have a short path to a rewarding state, enough to be better than a blind random walk."
16075,1, Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic?
16076,1,"""This paper proposes \""spectral normalization\"" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function."
16077,1, There are also many papers on this topic using Gaussian process state-space (GP-SSM) models where an explicit prior is assumed over the underlying dynamical systems.
16078,1,"  Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?"""
16079,1,"  \n\nObjects are matched across consecutive frames using non parametric (not learnable) deterministic matching functions, that takes into account the size and appearance of the objects."
16080,1,"\n \nFurthermore, I would want to question the practical usage of having an 'even faster' method for generating adversarial examples."
16081,1,"  However, under one-shot learning, won\u2019t this  make each class still have only one instance for training?"
16082,1,"Pro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\n"
16083,1," By introducing new branching architecture, coupled ensembles, they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget."
16084,1," However, in the paper there is no motivation about why the architecture was designed like this."
16085,1, Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines.
16086,1,"\n\nIn addition to being quite bold in claims, it is also somewhat confrontational in style."
16087,1," It receives instances in an online setting, where both the prediction model and the relationship between the tasks are learnt using a online kernel based approach."
16088,1,"\n\nIn general, I tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications required."""
16089,1,\n\nOTHER COMMENTS:\n\n- the list of contributions is a bit strange
16090,1,"""Summary:\nThe authors present a paper about imitation of a task presented just during inference, where the learning is performed in a completely self-supervised manner."
16091,1," It is unclear if the proposed model is still helpful when other components are considered (e.g., attention)."
16092,1,  It simply applies hierarchical clustering on the learned similarities and use cross-validation to pick a stopping condition for deciding the number of clusters.
16093,1, They introduce a concept of \u201cscatter\u201d that correlates with network performance.
16094,1, It does not make sense to weaken an existing baseline and then compare with it.
16095,1,\n\nThe introduction was fine.
16096,1,"""This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers."
16097,1,\n\n4. Could the authors indicate the range of values of \\lambda_{rec} and \\lambda_{nonrec} that were examined in the work?
16098,1,"  In this section, why are you not reporting the results from the original Show&Tell paper?"
16099,1, This should be rephrased elsewhere.
16100,1," The group-wise posteriors allow amortization of the information cost KL(group posterior || prior) across all examples in the group, which the authors liken to the \""KL annealing\"" tricks that are sometimes used to avoid posterior collapse when training models with strong decoders p(x|z) using current techniques for approximate variational inference in deep nets."
16101,1, What is your network updating toward?
16102,1,"Of course, it is fine if the assumption is that the attacker has no knowledge of the detector (black box case), but if this is the case, this assumption needs to be explicitly acknowledged up front and attention drawn to the obvious limitations of this approach."
16103,1,. Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)? 
16104,1, They also provide a tight bound for the one dimensional input case.
16105,1," \n\nHowever, there exist several major issues which are listed as follows:"
16106,1,"\n- I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is."
16107,1, Most of the sentences are not grammatically correct.
16108,1," Given that I see only one example I can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having none. """
16109,1, \n\nExperiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples).
16110,1," It certainly doesn\u2019t help that I\u2019m not familiar with the architectures the model is based on, nor with state-of-the-art integer programming solvers."
16111,1," \n\n\nThere is a nice variety of authors and words, though I question if even with all those books, the corpus is big enough to produce meaningful vectors."
16112,1," 2015.\n\n[2] Lee, Chen-Yu, et al."
16113,1,  The results is interesting and novel.
16114,1," (could leave the output of ML-... implicit by writing ML-...(\\theta, T)_j in the $sum_j$; if absolutely needed, use another symbol than f)\n\n*"
16115,1,"  A very important type of dialog act is \""switching topic\"", often done to ensure that the conversation will continue. "
16116,1, It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction.
16117,1," In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae)."
16118,1," The experiments are quite limited.\n"""
16119,1, The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.
16120,1,"\nI like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function, which makes implementation simpler compared to VPN."
16121,1, It was well motivated from a computational cost perspective hence the use of a hierarchical prior.
16122,1," The results seem to indicate that an intermediate level of alpha is best, though I would even question the statistical significance by looking at the curves in Figure 3."
16123,1, It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.
16124,1, Should it allow better classification performance?
16125,1," For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames?"
16126,1," So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. """
16127,1,\n2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because\nactually there is a training phase also at test time.
16128,1, Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions.
16129,1," For instance, what are the implications of using higher-resolution images as input to DenseNet / decreasing the number of layers?"
16130,1,"""This paper borrows the idea from dilated CNN and proposes a dilated convolution based module for fast reading comprehension, in order to deal with the processing of very long documents in many reading comprehension tasks. "
16131,1, \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders.
16132,1, This doesn't really look anything like networks that are trained in practice.
16133,1,"\n\nDuring experiments, the autodidactic returns perform better only half of the time as compared to lambda returns."
16134,1, The transition from the second line of the proof to the third line is not clear.
16135,1,"""This paper focuses on the sub-problem of discovering previously unseen classes for open-world classification."
16136,1,\n\nCons:\n* The assumption that the measurement process *and* parameters are known is quite a strong one.
16137,1,   An adversarial word replacement my in fact destroy the factual information needed to answer the question and there is no control for this.
16138,1," The authors should also avoid uninformative adjectives, clutter, and vague terms throughout the manuscript such as \u2018vital importance\u2019 or \u2018little room for fine-tuning\u2019."
16139,1, Are the number of parameters in the proposed approach and the baseline VAE similar? 
16140,1,"\n  \u2022 On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help."" "
16141,1,"The method resembles quite a lot the Grad-CAM method and the conclusions from the experiment (\""shallow layers are robust enough to adversarial examples and middle layers ....\"") seem shallow as well."
16142,1, Is it due to the network morphing that preserve equality?
16143,1," In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric."
16144,1, It would be useful to explain more why this is needed.
16145,1," This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training \""fast gradient sign\"" method."
16146,1, And does sampling for novel inputs by sampling the residual error collected from the training set. 
16147,1,". It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point."
16148,1," First, the description is mapped to a \""sketch\"" (Y) containing high level program structure but no concrete details about,"
16149,1,"\n\nHowever, the first (theoritical) contribution is to make the link\nbetween matrix decomposition and sampling based objective."
16150,1," what happens if you just randomly pick one of the 10 crops for prediction?"""
16151,1,"""This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly."
16152,1, The authors show that their method enables a higher speedup and lower accuracy drop than existing methods when applied to VGG16.
16153,1,". The datasets considered in the experiments are also large, another plus."
16154,1,"\""\nFrankly, I am unable to decipher what is being said here."
16155,1,\n\nThe experimental results show the results without the cyclic loss.
16156,1, An alternative route is suggested in my detailed review.
16157,1,\n- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks)
16158,1, These are important questions that should have at least an empirical exploration.
16159,1,"\n\nThe main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'."
16160,1,"  It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer."
16161,1,"\n\nFinally, I should say that TAGCN idea is interesting."
16162,1," Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking."
16163,1,"\n- There is a terminology problem in this section.[[CNT], [null], [DIS], [GEN]] Coordinates in a vector are\n  not sparse, the vector itself is sparse if there are many zeros, but\n  coordinates are either zero or not zero."
16164,1," c) perturbed decoded neighbour image, where neighbourhood is searched in the semantic space."
16165,1,"\n\nIn particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation."
16166,1,  The regression loss was also changed from l_1 to l_2.
16167,1, It is also distracting that they call the embedding dimensions topics.
16168,1," When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)"""
16169,1, This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true. 
16170,1," The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context."
16171,1,\n- The authors should clarify which problem they are dealing with.
16172,1," However, I found a lack of motivation for the\nspecific design choices made to obtain equations 9 and 10."
16173,1," If the NPI has access to a black-box oracle, it is not clear what is the use of training an NPI in the first place."
16174,1,"  \n-\tThe authors claim in the last sentence of the section that p(x|W2) is equivalent to 1-p(x|W1), but this is not true: these are two continuous densities, they do not sum to 1, and a model of p(x|W2) is not available (as far as I understand the method)"
16175,1,"\n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points."
16176,1," And even with a single block, does it matter what permutation you use?"
16177,1,"\n\nIn the appendix, the last step of the proof below Eq. 7 is unclear."
16178,1," If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches?"
16179,1," Also, that the \""style\"" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result."
16180,1, Experimental results are reported in a self generated data set.
16181,1, Similar comparison should be done with off-policy fitting in Q-Prop.
16182,1,  Similar problem also exist in results from other datasets.
16183,1," Furthermore, 64 was the smallest batch size considered, but SGD was performing monotonically better as the batch size decreased, so one would expect it to be still better for 32, 16, etc."
16184,1,"\n\nOverall, this is an interesting study on SQuAD dataset."
16185,1, \nThe paper is well presented and I want to underline the importance of this.
16186,1, I like the presentation and writing of this paper.
16187,1,"\n\n- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq. (1), as also suggested in this work."
16188,1, however I am skeptical about whether the results prove the premise of the paper (e.g. multi-mention reasoning is necessary).
16189,1," Moreover, we also see the learned model is consistently improved using the proposed \""Decision-boundary Iterative Refinement Training with a Teacher\"" (DIRT-T) approach."
16190,1,"\n\n+++ Hyperparameters +++\n\nSince the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments."""
16191,1," The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings."
16192,1,"\n- Figure 6 is a bit confusing, the authors do not explain why the \u201cPermuted Order\u201d performs worse than \u201cParallel Order\u201d. "
16193,1, On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences.
16194,1,  We want to infer a posterior density given some data.
16195,1," In light of such results, one might change the policy space to enforce such structure."
16196,1," For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points."
16197,1,"\n\nOn the other hand, I think this paper is not quite ready: it reads like work written in a hurry, and is at times hard to follow as a result."
16198,1, The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations.
16199,1," From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question)."
16200,1,"\n\n\""learn a policy that satisfy\"" -> \""learn a policy that satisfies\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""HRL, We introduce the FSA augmented MDP\"" -> \""HRL, we introduce the FSA augmented MDP.\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\"" multiple options policy separately\"" -> \"" multiple options policies separately\""?[[CNT], [CLA-NEU], [QSN], [MIN]]\n\n\""Given flat policies \u03c0\u03c61 and \u03c0\u03c62 that satisfies \"" -> \""Given flat policies \u03c0\u03c61 and \u03c0\u03c62 that satisfy \"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""s illustrated in Figure 3 .\"" -> \""s illustrated in Figure 2 .\""?"
16201,1,\n \nContributions:\n\n1 The authors proposed Crescendo block that consists of convolution paths with increasing depth.
16202,1," The resulting representations are show to be useful \u2013 they produce SOTA results on preposition selection by a decent margin (on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations) and good (but not quite SOTA) results, without further linguistic features beyond POS tags, on the much more studied task of preposition attachment disambiguation."
16203,1, They also use SRM in combination with a neural network meta-modeling method and a hyperparameter optimization one and show that it can decrease the running time in these approaches to find the optimized parameters.
16204,1,\n- It does not seem clear how the whole training is actually performed (beyond the pre-training policy).
16205,1," This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks. "
16206,1,  It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps. 
16207,1," More specifically, the paper proposed to generate images from attribute and latent code as high-level representation."
16208,1,"n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong."
16209,1," Given these previous works, the contribution and novelty of the paper is limited."
16210,1, The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison.
16211,1," It seems, as the authors also claim in [b], that the work of (Mirowski et al, 2017), which was about navigation in known environments, actually is repeatable."
16212,1," This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout."
16213,1, The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words.
16214,1," I think the motivation and discussion for the training methodology is insufficient,"
16215,1," Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization?"
16216,1, \n\n* The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L = L_d?) and which is impossible to verify in practice (T is not known).
16217,1, The algorithm requires that every step is crystal clear.
16218,1,\n- Authors claim that they operate directly on the gradients inside the network.
16219,1," Also, are ignoring the gradient of the value at the next step?"
16220,1," They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%)."
16221,1," The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration."
16222,1,"\n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices."
16223,1,"  For example, the s_0:m notation is introduced before indicating that s_i would be the symbol in the i_th position (which you use in section 3.3)."
16224,1,\n\nThe presentation of the paper is unnecessarily complex.
16225,1," I recommend it be accepted."""
16226,1," The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. "
16227,1,  What is the upper bound on the size of PATH lengths you can train?
16228,1,\n- Good prediction results
16229,1," The main claimed advantage is that it doesn't require the knowledge of the actions taken by the expert, only observations of states."
16230,1," \nThese are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art)."
16231,1,"\n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \""Rand\"" line for German, or the Swap/Mid/Rand lines for Czech."
16232,1,"\nOverall, I rate this manuscript in the top 50% of the accepted papers."""
16233,1,"\n\nChoice of Datasets\n- If we are thoroughly evaluating the effect of image features, doing so on other datasets is very important."
16234,1," but is not going to scale well,"
16235,1, They key innovation is a convolutional architecture that represents the invariance around the target base.
16236,1,"\n\nFigure 1: \nThis figure is very helpful, however the colour for M->S is wrong in the legend."
16237,1,  It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.
16238,1, It is also unclear whether the model generalizes.
16239,1,My understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connectivity.
16240,1," Moreover, if the network contains pooling layers, different locations of the pooling layer result in different shapes of the feature map, and the soft ordering strategy Eq. (7) does not work."
16241,1, The use of attention heat maps is interesting.
16242,1," Method-wise, the encoder is not novel and decoder is rather straightforward."
16243,1,"\n- Experiments: The experiments are not enough at all, More specifically, the paper didn't establish any baseline to show the difficulty of this problem and the dataset."
16244,1,"""Thanks for all the explanations on my review and the other comments."
16245,1," So, statements like \""as we can see, our embeddings very clearly outperform the random embedding at this task\"" is  an unnecessary inflation of a result that 1) is not good"
16246,1,"\n - It is not clear how the authors achieve to avoid the problem of starting from scratch by \""pre-train the forward model and PSF separately by blocking gradient flow\""."
16247,1," Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by \""twisted\"" and unnatural logics."
16248,1,The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution.
16249,1," However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training."
16250,1,  And even then maximize transfer with respect to what?
16251,1, The authors propose the CrescendoNet which is without residual connections.
16252,1,"\n\n2) I have the questions as follows:\ni) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots."
16253,1,"\n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives."""
16254,1, This is not supported by any strong theory or conceptual idea.
16255,1," Since a classifier can be a highly nonlinear function, it can potentially ignore many aspects of its input distribution such that even poor approximations (as measured by, say, KL) lead to similar test accuracy as good approximations."
16256,1,"?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above."
16257,1,"""The paper analyzed the composition abilities of Recurrent Neural Networks."
16258,1,".  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact"
16259,1,\n\nI also have some minor comments on the paper:\n- There are a lot of typos.
16260,1, The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference).
16261,1, Second - Is your approach applicable to these frameworks?
16262,1,\n* Authors seem to only consider deterministic defogging models.
16263,1, \u00bb: should \u00ab unless \u00bb be replaced by \u00ab if \u00bb?
16264,1,\n+ interesting idea of using the algorithm for RLfD
16265,1," Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers."
16266,1,  \n- I appreciate that it is difficult to find good test datasets for evaluating causal estimator.
16267,1, How many pairs were extracted?
16268,1, \nWhy not train (finetune) the considered models using softplus activations instead of exchanging activation nodes?
16269,1, Could the authors discuss this aspect?
16270,1, Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.
16271,1,".  However, the paper hasn\u2019t shown that this is necessarily possible assuming the hand-designed mockups aren\u2019t pixel-for-pixel matches with a screenshot that could be generated by the \u201cDSL code -> screenshot\u201d mapping that this system learns to invert."
16272,1, Are there any other potential biases brought because the data collection tools?
16273,1," In this work the authors consider substituting the previous penalty by \""\\lambda E_{z~\\tau}}(max( ||\\grad f (z)||-1,0)^2\""."
16274,1," This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup."
16275,1, Did you cap the max episode time to 30mins?
16276,1," What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation."
16277,1,\n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well?
16278,1, More explanations are appreciated.
16279,1," Firstly, it shows experimentally that the same effects appear even for simple models such as linear regression. "
16280,1," They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best)."
16281,1,"\n-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.\"
16282,1,\n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting.
16283,1, I.e. are we talking about a space of 10 total classes or 10000 total classes?
16284,1," The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings."
16285,1,"  With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads."
16286,1, It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague.
16287,1, See the Urmson et al. 2008 paper in the bibliography.) At the very least this technique should be a baseline.
16288,1," \n\nOne way to greatly improve the impact of the paper would be to take the observations made from the simulated data experiments (e.g., MNIST) and use them to make changes to how CNN training is done on another real task (e.g., ImageNet) and show improvements in performance."""
16289,1,"\nThis basically suggests that by ignoring the fact that the data is collected off-policy, and treating it as an on-policy data, the agent might perform better."
16290,1," This is perhaps more important than improving the baseline method by a few point, especially given that the goal of this work is not to beat the state-of-the-art."
16291,1,\nConcerning the first objective the empirical results do not provide meaningful support that the generative model is really effective.
16292,1, I believe at this point combinations of DNN with classical clustering algorithms already exist and comparisons with such stronger baselines are missing. 
16293,1," Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is."
16294,1,"  Given that this is n-best rescoring, how are the N-best lists generated?"
16295,1,"\n\nI didn\u2019t understand the r^in, r&out representation in section 4.1. These are given by the domain?"
16296,1," Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus."
16297,1,  \n\nI found it very difficult to understand the evaluation.
16298,1, At which layer do the features become speaker invariant?
16299,1, The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.
16300,1," \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal."
16301,1," This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks."
16302,1,"""The article \""Do GANs Learn the Distribution?"
16303,1,"\n[2] Larranaga, P. (2002). A review on estimation of distribution algorithms."
16304,1, This needs to be explored.
16305,1," \\alpha in (1) seems to be \\gamma in Alg 1 (line 5). \""L_target is a target objective which can be a negative class probability ..\"" this assumes that the example is a positive class."
16306,1, Why T=2 and not 1 or 10 ?
16307,1," It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position."
16308,1, I am not familiar with the state-of-the-art methods in this field.
16309,1,"\n\n\nIn summary, the paper is not ready for publication in its current form."
16310,1," There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others)."
16311,1," In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections."
16312,1, The authors never mention mini-batch when Algorithm 1 is introduced.
16313,1,"  Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it."
16314,1,"  \n\n- It would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of Section 3 would be to add an additional layer, which would facilitate interactions between the attention-weighted word embedding (\\alpha_i c_i) and aspect embedding (v_t)."
16315,1,"""The authors provide a method for learning from demonstrations where several modalities of the same task are given."
16316,1,\n\nThere are a few minor points which I would like to ask the authors to address:\n\n1. Why cite Kingma and Welling as a source for variational inference in\tsection 3.1?
16317,1, The parameters of the model are an embedding for each word and a local context unit. 
16318,1," Once it is train it will give away the decoder and keep the encoder for sending information.\n\n"""
16319,1, How were these loss functions are combined?
16320,1," Also, Page. 3. What does \u201cpartial descriptions\u201d mean?"
16321,1," In addition, the algorithm in this paper selects the best performing network at each step, which also hampers the discover of the optimal model.\"
16322,1,"\n\nOverall, the proposed approach is novel and achieves good results on a range of tasks."
16323,1,\n\nA second concern is the experimental exploration.
16324,1," While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics."
16325,1," Instead, the conditional density p(y_t|y_{1:t-1|, \\theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM."
16326,1,"""The paper describes learning joint embedding of sentences and images."
16327,1,\n\nThe parameter pruning angle in this paper is fairly weak.
16328,1, Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. 
16329,1, This figure could be moved into the paper's main body with some additional clarification.
16330,1,"""The description of the proposed method is very unclear."
16331,1,"\nThe authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow."
16332,1,"\n\nOn incompleteness, it is not obvious how the classifier is used at test time."
16333,1,\n\nAnother concern is that the authors' approach assumes that all parameters have\nthe same effect.
16334,1, The BC method often performs similarly to or is outperformed by non-BC SbT-NMF.
16335,1," This is even less doable, because the encoder and decoder are not train remotely."
16336,1,"  While the formulation in terms of pre-post spike-times is interesting, the result is clearly different from STDP, and ignores the fact that e_t refers to the backpropagating error (which presumably would be conveyed by a feedback network): applying the plotted pre-post spike-time rule in the same setting as where STDP is observed will not achieve error-backpropagation."
16337,1," Did I miss this, or is it not explained anywhere?"
16338,1, Is it possible to further improve the accuracy by a more careful fine-tuning?
16339,1, \n\nI would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this paper.
16340,1, Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange.
16341,1," but would hope another reviewer is more familiar with the specific application domain than I am."""
16342,1, Previous work in using recursion to solve problems (Cai 2017) used explicit supervision to learn how to split and recurse.
16343,1," Using Bayesian Optimization, search over this space can yield decodings with targeted properties."
16344,1,"\u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept."""
16345,1," However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration."
16346,1, For example: \n(1) The definition of \\bar{A} in Section 4 is broken.
16347,1, This approach amounts to a change of basis - and therefore the resolution invariance is not surprising.
16348,1,  in Theorems 2 and 3 there are no mention to PSRNN.
16349,1," It would be needed to have a way of visually evaluate the similarity between original images and generated images."""
16350,1,"  I\u2019m not sure there is something specific I\u2019m proposing here, I do understand the value of the formulation given in the work, I just find it strange that model based RL is not mention at all in the paper."
16351,1," The architecture is never described,\nit is light on details of the training objective, it's not entirely clear what the DSL used in the\nexperiments is (is Figure 1 the DSL used in experiments), and it's not totally clear how the random\nimages were generated (I assume values for the holes in Figure 1 were sampled from some\ndistribution, and then the program was executed to generate the data?)."
16352,1, Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method (such as FGSM) in generating the adversaries.
16353,1,\n\nCons:\n-The experimental evaluation is not very thorough:
16354,1," \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand."
16355,1, but could be improved.
16356,1,"\n\n- The typos and other issues should be fixed:\np. 3:\nK iteration\nwith capable\np.4:\nclose 0\np.5:\nour our\ns^{t+1} should be defined like the other terms\n\""The state is represented by the coordinates of the agent and 2D environment observation\"" should appear much earlier in the paper."
16357,1,  How does your implementation compare to the original work?
16358,1, Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour.
16359,1,"   I can't point to exactly what would have to be different to make things \""work\"", because it's really hard to do that ahead of actually trying to do the work. "
16360,1,\n* sec3.1 task-specific parameters $\\phi_j$ (I would avoid writing just \\phi altogether to distinguish in usage from \\theta)
16361,1, Do you run the narrator model with all possible obfuscations and pick the best choice?
16362,1," I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are."
16363,1,"  The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality."
16364,1," However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),"
16365,1,"\n\nOriginality\n\nWhile others have pointed out limitations before, this paper considers relational networks for the first time."
16366,1,"  \n- Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others)."
16367,1," \n\nI believe that the current method can only learn and track simple objects in a constant background, a problem which is  well-solved in computer vision."
16368,1,"\n\nThis is true for the CIFAR net but the opposite is true for ResNet, right?"
16369,1, The only thing the player knows is the best adversarial policy.
16370,1," but would require more justification, or at least a more developed experimental section."
16371,1," They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models."
16372,1, The weights of the NN like structure are optimised using a genetic search algorithm which optimises the cross-validation error of a nearest neighbor classifier.
16373,1, \n\n The paper is not well written.
16374,1,"\n\n- The claim that \""implicit SGD never overshoots the optimum\"" needs more supports. Is it proved in some previous papers? "
16375,1, \n\n\n\nThere are some imprecisions in the writing.
16376,1,\n\nWhat is the computational complexity of BBB with posterior sharpening?
16377,1," \u201cIn the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al\u201d\nThe architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)\n\n- Page 6-Sec 4.2)"
16378,1," with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods."
16379,1,\n\n2) FID in real data. The numbers in Table 1 appear favorable to the projection model.
16380,1," \n\n2, In the paper, there is an assumption about the peak of random feature \""it is a natural assumption on realistic data that the largest peaks are close to the origin\""."
16381,1," This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores."
16382,1,"""* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label."
16383,1,"""This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN."
16384,1,"\n\nDue to these two citations, the novelty of both the problem set-up of learning different embeddings for each covariate and the novelty of the tensor factorization based model are limited."
16385,1, \n\nThe high-level policy is learnt via fairly standard Q-learning (epsilon-greedy exploration policy and a NN function approximator.)
16386,1,"\n\nHowever, the exposition needs significant improvements to warrant acceptance."
16387,1, Doesn't this also prevent optimal results ?
16388,1,\n- The concept of data probability density function seems weird to me.
16389,1,"\n\n[9] H Tang, L Lu, L Kong, K Gimpel, K Livescu, C Dyer, N Smith, S Renals, End-to-End Neural Segmental Models for Speech Recognition, 2017"""
16390,1,"\n* The baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the \""fixed\"" version which is fed into them."
16391,1,"""The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion."
16392,1,"\n\nOverall, the paper is well-written."
16393,1,"""This paper proposes a new learning method, called federated learning, to train a centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections."
16394,1,"\n\u2013 The paper is poorly written, containing several typos and incomplete, unintelligible sentences, incorrect captions (eg. Table 4) etc.\n"""
16395,1, The approach is then experimented on various image and text tasks.
16396,1,"""This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer."
16397,1," Also it would be very useful to say that a physician was consulted and that the critical values were \""clinically\"" useful."
16398,1," However, the contributed framework did not account for previously proposed metrics (such as equivariance, invariance and equivalence)."
16399,1," That is, it might rate many tasks as dissimilar even when they are not."
16400,1,\n\nReview:\nQuality: The quality of the work is high.
16401,1,"\n\n\n- Other comments:\n\nIn Fig. 5, use a consistent naming for the axes (bias and variances)."
16402,1,"\n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3."
16403,1,"  Also, what about if we include the bias term so that b + w a is the preactivation value?"
16404,1,"\n\n\""from the first m domain corpus\"" -> \""from the first m domains\""?[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n\""may not helpful\"" -> \""may not be helpful\""\n\n\""vocabularie\"" -> \""vocabulary\""\n\n\""system first retrieval\"" -> \""system first retrieves\"".[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nCOMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines."
16405,1, \n\nThe impact of this paper may be limited in this community since it is a survey about a fairly niche topic (a subset of recommender systems) that may not be of central interest at ICLR. 
16406,1," It would be interesting to consider also a single construction, instead of the composition of two constructions.."
16407,1,"""This is a fine paper that generally reads as a new episode in a series on motion-based video prediction with an eye towards robotic manipulation [Finn et al. 2016, Finn and Levine 2017, Ebert et al. 2017]."
16408,1," it is not clear how the various cluster sets have been obtained\nand what are their influence on the performances (if they are randomly initialized, it \nwould be great to see standard deviation of performances with respect to initializations)."
16409,1," Also, I believe the Z should be a vector, not a set. "
16410,1, but the work is not adequately validated and the novelty is somewhat limited.
16411,1, The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline.
16412,1," \n\n+ The authors perform ablation experiments, which are always nice to see."
16413,1,"       \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off)."
16414,1,  I didn\u2019t get the contrast with method one.
16415,1,recent NN toolbox can easily compute the exact gradient and use it for the penalization but this is not clearly discussed even in appendix.
16416,1," Is performance maintained only on the last 2 tasks, or all previously seen tasks?"
16417,1, The proposed model achieved STOA performance on Stanford Question Asnwering Dataset
16418,1, I suggest to put more information into its caption.
16419,1,"\n\n- The attacks are all based in L2, in the sense that the look for they measure perturbation in an L2 sense (as the paper evaluation does), while the defenses are all L_\\infty based (since the region classifier method samples from a hypercube, and PGD uses an L_\\infty perturbation limit)."
16420,1,"\n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel."
16421,1," The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently)."
16422,1," It would have been nice to experiment with increasing model complexity to study such effect. """
16423,1, \n- the results in the  figure 4: it's very unlikely that the differences reported are actually significant.
16424,1,  They use a sparse reward function of +10 for reaching the goal and -10x(lane difference from desired lane) as a penalty for failure.  This simple reward function is possible because the authors do not require the ego car to obey speed limits or avoid collisions.
16425,1," The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details."
16426,1,\n\nCons: \n-The grammar in the paper is pretty bad.
16427,1, \n\nI like the way they handle the nonconvexity component of the model.
16428,1,"\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). "
16429,1, A different question is whether that regime is actually useful.
16430,1,"""- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right"
16431,1," \n\n- in page 2, what do the numbers mean at the end of each sentence? Probably the figures? "
16432,1, The clarity of the paper has improved;
16433,1, Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner.
16434,1," Especially, because the authors\ntune the amount of data from the replay-buffer that is used to update their\nposterior distribution."
16435,1,I think that this general approach deserves further attention from the community.
16436,1," Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy."
16437,1,"""The paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making."
16438,1," Is there an economic motivation?[[CNT], [CNT], [QSN], [MIN]] Is it just a different way to train a NN?"
16439,1," (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure;"
16440,1,"   It would be helpful if there are comparisons to these models, and use similar datasets. "
16441,1, Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder.
16442,1, There is a reference that appeared twice in the bibliography (1st and 2nd).
16443,1,  \n\nMain comments:\nIt would strengthen the paper to also compare all these network learning based approaches to variational ones.
16444,1," In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations."
16445,1, RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning.
16446,1,"\n-            Sec 5.1: While the qualitative example is useful (with a bit more text), I believe it would have been more convincing with a quantitative example to demonstrate e.g. the convergence of the proposal compared to std MAML and possibly compare to a std Bayesian inference method from the HB formulation of the problem (in the linear case)"
16447,1,"\n[3] Stanley, K. O., & Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies."
16448,1, Could you perform ablation without contextualized embedding (CoVe)?
16449,1," although the idea of the extensive experimental comparison is good.\n\n\n"""
16450,1, The authors apply an extension of this\nmethod to topic and sentiment transfer and show moderately good latent space\ninterpolations between generated sentences.
16451,1,\n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart.
16452,1,"\n\nAlso, the claim that \""we show efficient designs\"" is very thin to me since there are no experimental comparisons between the proposed method and existing works."
16453,1,"\nFinally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator."
16454,1,\n\n---\n The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices).
16455,1,"\n\n* Sec. 3. 4 mentions that the \u201conly image\u201d encoder is used to obtain the representation for the image, but the \u201conly image\u201d encoder is expected to capture the \u201cindescribable component\u201d from the image, then how is the attribute information from the image captured in this framework?"
16456,1,\n* Section 5. The discussion on the difficulty of training shoud be emphasized and connected to the --missing-- description of the model architecture and its hyperparameters.
16457,1, Would the authors be willing to comment on the importance of the value of h?
16458,1,\n\nThe paper also presents several methods for negative samplings and according to table 4 there is a lot of performance variability based on the method that is used for selecting negative sampling.
16459,1,  A short discussion of this result would make the paper stronger.
16460,1, CNN is known to be usually unable to capture long-range correlations in natural language (unless enhanced with attentions).
16461,1, It is also impressive how much faster their model performs on tasks without sacrificing much performance.
16462,1,I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA).
16463,1,"\n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}"""
16464,1,"  In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3)."
16465,1,"\n4. Add computation time (wall-clock) for all the experiments, to see how it compares in practice (this could definitively weight in your favor, since you seems to have a nice CUDA implementation)."
16466,1,"  \n\nOn the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author\u2019s own baselines."
16467,1,\n\nEquation 1 has a symbol E in it.
16468,1," \n\nPros:\nOne of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions."
16469,1,\n\nThe biggest weakness of the paper (and the reason for my final decision) is that the paper completely goes easy on baseline models.
16470,1, Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.
16471,1," Page 9, conclusion, the beginning sentence of the second paragraph is erroneous."
16472,1, I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework.
16473,1,". However I am not convinced by the distribution of \\|z^\\prime\\|^{2} in the first place (eqn (2)): the samples from the gaussian will be approximately orthogonal in high dimensions, but the inner product will be at least O(1)"
16474,1,\n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading.
16475,1,"""CONTRIBUTION\nThe main contribution of the paper is not clearly stated."
16476,1,"\n\n3. In section 5, the authors make a connection with the work of Ge et al. 2016."
16477,1," We didn't find anything \""intelligent\"" in the proposed mapping."
16478,1, \n4.\tOne of the observations made in the paper is that \u201ctraining on one dataset and evaluating on the other results in a drop in the performance.
16479,1, The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function.
16480,1," If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter."
16481,1," I found this surprising, since it\u2019s not at all clear why dataset size should lead to \u201cstrong inference\u201d."
16482,1,\n\nATARI 2600 games: I am not sure what state restoration is.
16483,1," \n\nOverall, I think there is potential with this work but it feels preliminary."
16484,1, See more comments below.\n\n= Originality / Significance = \n\nThe paper presents a clever idea that could help make SPENs more practical.
16485,1," \n- IMO, Section 5.3. should be rewritten (also, maybe include another reference for BEGAN).[[CNT], [PNF-NEU], [DIS], [MIN]]\n- There is a reference to Lemma 15. However, I have not found that lemma."
16486,1," Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients."
16487,1," The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible."
16488,1," The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets)."
16489,1,The authors are advised to do a thorough survey of the relevant works in Multimedia and computer vision community.
16490,1,"""The authors has addressed my concerns, so I raised my rating."
16491,1,"  This eliminates the need to restart training when making an architectural change, and drastically speeds the search."
16492,1,"""The authors are motivated by two problems: Inputting non-Euclidean data (such as graphs) into deep CNNs, and analyzing optimization properties of deep networks."
16493,1, \n\n2. Can the author provide analysis on scalability the proposed method?
16494,1,The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. 
16495,1, These theoretical results justify the objective function\nshown in Equation 8.
16496,1, Does it come from a high variance?
16497,1,n\nThe paper presents an interesting approach which achieves good performance.
16498,1," And btw, don't you need to put E_z[ ... ] around the 2nd term on the r.h.s. ?"
16499,1,\n2. The pairwise similarity measure appears to be one that might have a high false negative rate.
16500,1, I am not convinced that these changes will lead to a significant difference.
16501,1," For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6)."
16502,1, The paper is a experimental paper as it has more content on the experimentation
16503,1,\n\nThe results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper.
16504,1,(c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function\u2019s optimization
16505,1," Just as you said right before presenting Eq. (1), typically the goal of learning a DGM is just to match Q_x with the true data distrubution P_x. It is **not** however to match Q(x,z) with P(x,z)."
16506,1,\n\nPros: \nOverall the paper is well-written
16507,1, They show superiority of their algorithm over SSTE.
16508,1, \n\n3. Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result? 
16509,1,- The method is specifically designed for online learning with limited hardware ressources.\n\n
16510,1,"  Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings."
16511,1,"""\nSummary:\n- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function."
16512,1,\n\nThe main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot
16513,1, A loss function based on the Wasserstein distance is used. \nThe paper is interesting
16514,1,"  Out of curiosity, is it intended as an experiment to verify the need for better baselines? Or as a 'fair' training procedure?"
16515,1,  The paper should work harder to motivate why adapting lambda as a function of state---which has been studied---is not sufficient.
16516,1, This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task.
16517,1, An auxiliary \u201clabel difference cost\u201d was further introduced to encourage class information captured by the foreground generator.
16518,1, The proof of lemma 1 only establishes the Lipschitz constant of the CNN function.
16519,1,"""This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training."
16520,1, The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly. 
16521,1,"""The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective."
16522,1, This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches.
16523,1, but are not very well on the DrQA model.
16524,1,"""This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks."
16525,1, I checked the numbers in table 2 and the numbers aren't on par with the recent methods.
16526,1, The derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potential.
16527,1," For example, Lemma 4 is not correct as written \u2014 an invertible mapping \\sigma is not necessarily locally open. "
16528,1," Therefore, the novelty of the proposed method is somewhat weak."
16529,1, It uses the notion of a locally open map to draw equivalences between local optima and global optima.
16530,1,\n\n3) Regularizing deeper networks: in the experiments of varying the length did you see if the results can be stabilized using dropout with deep networks and small feature maps?\n\n4) between training g and h ?
16531,1, Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation.
16532,1, I think slack variables come from (Cortes et al 95). 
16533,1,\n4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices.
16534,1,  \n3) There is no mathematical definition of memory addressing mechanism used in this paper.
16535,1,"\n\nNotes: I did not check the proofs of the theorems in detail. \n"""
16536,1, Only the topology of the cells themselves are designed.
16537,1,"""This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1."
16538,1,"\n- In table 3, what if we use the right-branching tree-lstm with attention?"
16539,1,  I am looking forward to the reply to my questions above; my final score will depend on these.
16540,1,\n* The word \u201cview\u201d in this paper is misleading.
16541,1, but there is no good and detailed experimental analysis that help explain these observations.
16542,1,"\n\nIt would also be nice to see some visual representations of images perturbed with the new perturbations, to confirm that they remain visually similar to the original images."
16543,1," This could have been done via empirical work, for instance:\n- Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1."
16544,1,\n\nWhile the experimental results are interesting
16545,1, The authors study several discrete questions about the aforementioned inference gaps and how they vary on MNIST and FashionMNIST.
16546,1,"\n\n\nReferences:\n[1] Chelsea Finn, Ian Goodfellow, and Sergey Levine."
16547,1,"\n\n2) In the paragraph above Section 4.1, the paper made two arguments. I might be wrong, but I do not agree with either of them in general. "
16548,1, It shows comparable results with standard LSTM.
16549,1,"\n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning."
16550,1,"\n\nSimilarly, the second experiment in Table 1 compares classification\naccuracy between different sampling methods, but it does not provide\nany comparison as done in Vondrick (2015) to a classifier trained\nin a conventional way (such as an SVM), so it is difficult to discern\nwhether the learned distributions are informative."
16551,1,".\n- Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations."
16552,1,.\n\nPros:\n- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas.
16553,1," The author's inclusion of the latent time variable as a part of the agent's observations reconfirms this well-known fact, but doesn't tell us anything new."
16554,1," The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem."
16555,1,"  However, the author did not suppose this condition."
16556,1,"\n\nThe sentence containing \""assume that the network model can be shared\"" had me puzzled for a few minutes."
16557,1," The authors apply their technique to two datasets, namely, the Omniglot dataset and the TIMIT dataset and show that their model does a reasonable job in these two tasks."
16558,1," \n\nBy focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question."
16559,1," It's ok not to provide the answer if it's hard to analyze, but if that's the case the paper should provide some numerical case studies to show this bound either holds or the gap is negligible in the toy example."
16560,1," Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster."
16561,1,"Overall I think your intuitions and ideas are good,"
16562,1, but I would not say it was well written. 
16563,1," Visual Question answering is mentioned several times in the paper, however no evaluations are done in this task."
16564,1,\n\nEvaluation on two datasets is not sufficient to provide insight into whether the proposed metric is useful.
16565,1, \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN. 
16566,1," Furthermore, the authors also provide the probabilistic interpretation of the models."
16567,1," \n\nThe trick to reset $\\mu$ after half an epoch at the end of Section 3 is too heuristic.[[CNT], [null], [CRT], [MIN]] There lacks of explanation.[[CNT], [null], [CRT], [MIN]] \n\nThe experimental results are not convincing."
16568,1, \n(2) Pure MC methods can outperform TD methods when the rewards becomes noisy.
16569,1,\n  I recommend to add some distance-based outlier detection methods as baselines in experiments.
16570,1, Time series modelling is performed via convolutional LSTMs.
16571,1," I could not verify its novelty, but this seems to be a great contribution"
16572,1,\n\nSignificance: Average.
16573,1, It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy.
16574,1," For the Office experiment, the LCO appears to be trained on ImageNet data."
16575,1,\n\nDoes Figure 8 show an example input after the extractive stage or before?
16576,1,"\nA comparison the Graves 2013 is absolutely required, more comparisons are desired."
16577,1,\n\n- the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite.
16578,1,"\n\nOriginality:\nIt would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer."
16579,1," Perhaps I am mistaken, but there seems to be n^2 spans in the document that one has to score."
16580,1," \n\nFor the CIFAR experiments, the experiment design is reasonable for a general comparison."
16581,1,\n\nThis paper is clearly written
16582,1, but there are some issues with the discussion of previous work.
16583,1, That is perfectly fine -- and this work is still valuable.
16584,1," The architecture does seem to work well on the associative retrieval task, but it is not clear yet if this will also be true for other types of tasks."
16585,1, Can the authors clarify how does the decoder learns to generate these words.
16586,1,"Without good reasoning from the authors, I see no reason why the entries in the row of a matrix should have a normal-like distribution."
16587,1," \nIn this case, perhaps expressing heuristic rewards as potentials as described in Ng\u2019s shaping paper might solve the problem."
16588,1," I think you simply mean the gradient of the value function, for the given s_t, but its not clear. "
16589,1," From figure 2, I don't see much advantage of Checkhov GAN."
16590,1, A few typos.\n\nOriginality\n\nThe approach is a straightforward extension of the MCMCP approach using generative models.
16591,1,"""The authors present a scalable model for questioning answering that is able to train on long documents."
16592,1, (a) why are non-overlapping architectures so common?
16593,1," \n\nAlso the results do not clearly demonstrate the advantage of the proposed method, in particular the benefit of using PCN."
16594,1," Returns with higher confidence should be weighted higher, according to the confidence estimate around the value function estimate as a function of state? "
16595,1, it is a shame to see the overall quality of the paper very weak.
16596,1,"  These two sections would benefit from a more careful layout of the process, what is going on in a forward pass, a backward pass, how does this interact."
16597,1,"\u201d  The main benefit of the GP is not the uncertainty in the predictions, but the marginal likelihood which is useful for kernel learning."""
16598,1,"""Summary of the paper: The paper analysis how well difference target probation (DTP) - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like CIFAR-10 and ImageNet. "
16599,1,\n\n4. I'm not sure whether it makes good sense to apply an SVD decomposition to the \\hat{z} vectors.
16600,1,"'\n- Figure 2 should be shifted to the next page, since it is not self-explanatory and requires more context.\n"""
16601,1," These would be the same\nif the covariate is one hot coded, however, this isn't obvious in the paper right now."
16602,1," Could you provide some further comments on this?\n\n"""
16603,1, A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper.
16604,1,\n\nThe paper is quite well written aside from some grammatical issues.
16605,1,"\n\nA second concern is the presentation of the paper, which can be confusing at some points."
16606,1,"\ It does so from the perspective of \""Bayesian model comparison\"", where two models are compared based on their \""marginal likelihood\"" (aka, their \""evidence\"" --- the expected probability of the training data under the model, when parameters are drawn from the prior)."
16607,1,"""1. Summary\n\nThis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change."
16608,1,\n\n\nThe proposed assumptions are not well motivated and seem arbitrary.
16609,1," The authors suggest this is needed because \""the dynamics of training are different at the very start compared to later stages\"", which is a bit vague. Perhaps the authors can expand upon  this point?"
16610,1,. \n\n- The baselines are not thorough and lack proper justifications
16611,1,"\n\n1. The framework uses the class information, i.e., \u201conly data samples from the normal class are used for training\u201d, but it is still considered unsupervised."
16612,1, This should be moved to the appendix. You should also add a short summary of the TLM architecture to the main paper body.\
16613,1, This is exciting as it could significantly push the state of the art in sketch understanding and generation.
16614,1," In short, the insight of having an end-to-end differentiable function based on a dynamic-programming chart is pretty common -- the idea is in the air."
16615,1,"\n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks."
16616,1," \n\nReview: \nThe claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise."
16617,1," Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal."
16618,1," why not attend to the entire segment, i.e., from the current boundary to the previous boundary"
16619,1," Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? \nIf these are separate models, how are they used in conjunction to give a final density score?"
16620,1, Also it only has been tested on very simple datasets.
16621,1,"\n\nThe authors rightly say that one of the skills an autonomous car must have is the ability to change lanes, however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles."
16622,1, It is important to clarify this point.
16623,1, The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf\n\n3.
16624,1, \n\nThe experiments feel lacking.
16625,1,"\n\nThe paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge."
16626,1,"""# Paper overview:\nThis paper views the learning process for stochastic feedforward networks through the lens of an\niterative information bottleneck process; at each layer an attempt is made to minimise the mutual\ninformation (MI) with the feed-in layer while maximising the MI between that layer and the presumed-endogenous variable, 'Y'."
16627,1, The authors use multiple data sets to study different aspects of the proposed approach
16628,1,\n- Regarding to the human experiments with AMT: how do the authors deal with noise on the workers performance?
16629,1,"\n\n  The paper uses LSH structures, computed over the set of examples,"
16630,1,\u201d The conclusion is obvious when we train the network to make it invariant to colors and textures.
16631,1,".  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value."
16632,1, \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.
16633,1,"  Below I list some of the key problems.[[CNT], [null], [DIS], [GEN]]  First of all the authors claim in the introduction that their algorithm is very fast and with provable theoretical guarantees."
16634,1, which may beg the question if the algorithmic contributions are buying much for their added complexity?
16635,1, Method description is clear.
16636,1,\n----------- EDIT -----------\nAfter reading the publications mentioned by the other reviewers as well as the following related contributions\n\n* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)
16637,1, The key idea is that the agent learns a shared representations for tasks with different visual statistics
16638,1," I'm thinking in particular of:\n\nProximal Policy Optimization Algorithms (Schulman et. al., 2017)"
16639,1," In practice, how large must the weight matrix be to observe this behavior?"
16640,1," \n- Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization"
16641,1,"""The paper develops a technique to understand what nodes in a neural network are important\nfor prediction."
16642,1,\n- One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit.
16643,1,The method part is clear and well-written.
16644,1,"\n\nMinor point:\n- Figure 1 is unclear and requires a better caption. """
16645,1,"n4) Add a discussion on more structured sources of covariates (e.g., social networks)."
16646,1," In Eq. 3, it is unclear to me where the constraint 0 \\leq \\alpha \\leq 1 comes from."
16647,1,"\n-- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text."
16648,1," These rejected images could be clustered to identify the number of unseen classes; either for revealing the underlying structure of the unseen classes, or to reduce annotation costs."
16649,1,"""After reading the revision:\n\nThe authors addressed my detailed questions on experiments."
16650,1, 2-Using multiple models for short and long term memory. 
16651,1," Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays."
16652,1,  This derivation should be treated more thoroughly and carefully.
16653,1,. Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all
16654,1, It is only mentioned that the data is augmented with translations and horizontal flips.
16655,1," The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices. "
16656,1,\n\nRe: High resolution.
16657,1,". So my main problem with this paper, lack of novelty, is addressed and my score has changed."
16658,1,\n\nI enjoyed reading this paper.
16659,1," During evaluation, what is a step?"
16660,1, The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally.
16661,1, Currently the novelty is not obvious.
16662,1,\n\nBaselines: why is Shin et al. (2017) not included as one of the baselines?
16663,1,\n\nI am confident that point 1 has been used in several previous works.
16664,1, Maybe the manuscript part with the definition of the accuracy measures may be skipped.
16665,1,\n5. What is the memory consumption for different solvers?
16666,1," GCN looks at the nearest neighbors and the paper\nshows that using also the 2-ring improves performance.\n"""
16667,1, This (the space between x and x') I think is more interpretable as the invariance corresponding to the space between z and z_k. Have you tried that?
16668,1,\n\nThe paper is moderately well written and structured.
16669,1,"""This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance."
16670,1, This loss of detail could be a limitation.
16671,1,\n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning.
16672,1,"\n\nFinally, there is also an issue with the synthetic evaluation dataset."
16673,1,"\n\n-- A suggestion: As future work I would be very interested to see if this method can be incorporated into common few-shot learning models to on-the-fly generate additional training examples from the \""support set\"" of each episode that these approaches use for training."""
16674,1," As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution."
16675,1, I didn't like that two types of experiment are now presented in parallel.
16676,1,"\n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not."
16677,1," The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations."
16678,1,"\n\n4. Proof of theorem 3.3.\nTheorem 3.3 is one of the key results in this paper, yet its proof is just \""noted\""."
16679,1, The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.
16680,1," The fact that multiplication often involves public weights is used to speed up computations, wherever appropriate."
16681,1,"""This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks."
16682,1," On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N)."
16683,1," \n\nTheorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify)."
16684,1," However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:\n\n-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action."
16685,1,"\n2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights)."
16686,1," For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner."
16687,1,"\nAdditionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models."
16688,1," If that is indeed the case, that's a surprising and interesting result that deserves having more attention called to it (I appreciated the analysis in the results section to this effect, but you could call attention to this sooner, here on page 7)."
16689,1,\n\n3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM.
16690,1,"The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies."
16691,1," A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions."
16692,1,"\n- Same for 3.4, which seems like the biattention (Seo 2017) or coattention (Xiong 2017) from previous squad work."
16693,1," First and foremost, a result for a single neural network does not constitute enough evidence to justify the authors' conclusions."
16694,1, How sensible is the algorithm to these hyperparameters?\
16695,1,"  Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains."
16696,1,"\n- red and green should be avoided on the same plots, as colorblind people will not perceived any difference..."
16697,1,  This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective.
16698,1,"\nAs the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y)."
16699,1," It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds."""
16700,1,"\n\nHowever, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the \nproposed term depending on the parameters of the model (and this depends on the model!)."
16701,1,"""This is an interesting idea, and written clearly."
16702,1, For a class project this could get an A in a ML class!
16703,1,"\n\nAs a side note, the k-NN MOA is central to for the evaluation of the proposed approach."
16704,1,", \nno other approach is considered besides the prothotipical network and its variants. "
16705,1, Do you have any guesses as to why this might be?
16706,1,\n\nDetails of the \u201cwhite components\u201d in Figure 2 are not mentioned at all.
16707,1," Given the paper title, I would have expected some experiments in a generative context."
16708,1,"\n\n\nDetailed comments and questions:\n\n\nThe distribution of connections in \""windows\"" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image."
16709,1," The numerical experiments are motivated as a way to \""understand the capacity of the network with regards to modeling the external environment\"" (abstract)."
16710,1,"  \nBut, given that the method is solving a formulation that leverages second order information, it would seem reasonable to compare with existing techniques that leverage second order information to learn neural networks, namely BFGS, which has been studied for deep learning (see the references to Li and Fukushima (2001) and Ngiam et al (2011) below)."
16711,1, Could you please provide more justification for such assumption?
16712,1,".\n- Fig. 5 could be more convincing; \""bushy eyebrows\"" is a difficult attribute to judge"
16713,1, Would be good to see the performance against other forms of adversarial attacks as well if they exist.
16714,1,"\n\u201cIn game theory, the outcome maximizing overall reward is named Pareto optimality."
16715,1,\n\n+ Insights on how different modalities affect the prediction results.
16716,1,"""Summary:\n\nIn this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA)."
16717,1,"  For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop."
16718,1,"Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem."""
16719,1,"\n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments."
16720,1," And if they are in reality much lower, then competitiveness of s-rRBF in terms of classification results to these systems is questionable."
16721,1,"""This paper proposes a method for parameter space noise in exploration."
16722,1, some of those questions are left unanswered in the subsequent work as detailed above.
16723,1, I wish to see how they can be used to improve binary networks.
16724,1," My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer."
16725,1,\n\n4. The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation. 
16726,1," The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier). "
16727,1,"""\n\nThis paper presented interesting ideas to reduce the redundancy in convolution kernels."
16728,1," If so, I think it would be more effective to label the plot with that."
16729,1, The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed.
16730,1,\n\n3. Why do you motivate the learning method using self-play?
16731,1, The proposed method does not lead to any relevant improvement.
16732,1, \n\nComments:\n1. The writing of the paper could be improved.
16733,1,". Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN."
16734,1," In this case, the means are standard deviations do not even make sense to me."
16735,1, The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below).
16736,1,"\n- Regarding the evaluation, you wrote:\""In order to evaluate the interpolation capability of the autoencoder, we split the dataset in training and test samples in the ratio of 1:9."
16737,1,\n\nThe authors present techniques that are of similar flavor to quantized+sparsified updates.
16738,1, can it be a trainable parameter?
16739,1," There are several minor typos and formatting errors (e.g., at the end of Sec. 3.3, the authors mention Figure 3, which seems to be missing, also references [Egorov, Maxim] and [Palmer, Gregory] are bad formatted)."
16740,1," To provide an analysis of why it works and quantitative results, is part of the same contribution I would say."""
16741,1, The resulting image can be subtracted from the original encoding to highlight problematic areas.
16742,1," It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms."
16743,1,\n+ New dataset for robot arm pushing objects.
16744,1, What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}?
16745,1,"""The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text."
16746,1,\n- I am a bit confused by the requirement that all meshes need to have the same adjacency matrix.
16747,1," I have a few, mostly cosmetic, complaints but this can easily be addressed in a revision."
16748,1,\n4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when.
16749,1, I am supposing this is at the test time. 
16750,1," So the results are not strongly convincing, and the paper lacks any mention of newer work on attention."
16751,1, The authors also show that these inferred quantities can be used to generate more effective attacks against the targets.
16752,1,\n\n6. I did not understand very well the label difference loss in (5).
16753,1," For example, 1/3 of the training results are worse than the test results in Table 1."
16754,1,"\n\n(3) What is the relation between the \""PhaseCond, QPAtt+\""\b in Table 2 and the \""PhaseCond\"" in Table 3?"
16755,1,"\n\nRegarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets."
16756,1,"\n\nReference\n[a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017"""
16757,1,"\""\n\nThe experiment on fast Hyperband is very nice at first glance, but the longer I think about it the more questions I have."
16758,1,"""This paper presents a method for classifying Tumblr posts with associated images according to associated single emotion word hashtags."
16759,1," \n\nThe proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters,"
16760,1, i.e. one small dataset with the only experiment purportedly showing generalization to red cars.
16761,1,"\n\nTogether, these negatives imply the proposed approach is not yet at the point of being useful in practice."
16762,1,"\n\nIn summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison"
16763,1," Both papers can handle sigmoid/tanh, but cannot handle ReLU."
16764,1,\n\nThe organization and presentation of the paper need some improvement.
16765,1,but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations
16766,1,"""The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference."
16767,1,\n\nSec 5.2 states \u201cneuron partitions the inputs in two distinct but overlapping categories of quasi equal size
16768,1," Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN."
16769,1,"\n\n*******************************\n\nMy problem with this paper that all the theoretical contributions / the new approach refer to 2 arXiv papers, what's then left is an application of that approach to learning form imperfect demonstrations."
16770,1, It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better.
16771,1,\n\n3. Clarification question: For the WSJ experiments was the model decoded without an LM?
16772,1," At each layer, the targets can be chosen using a variety of search algorithms such as beam search."
16773,1,Paper written so that it's easy for a reader to implement the methods\n\t\u2022\t
16774,1, I recommend to follow the notation E[variable] the authors been using throughout the paper in the proof instead of dropping these brackets.
16775,1," However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway."
16776,1," Also, near-optimality would refer to some parameters being chosen in the best possible way."
16777,1," By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels."
16778,1," Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset?"
16779,1,They evaluate the suggested model on synthetic data and outperform the current state of the art in terms of accuracy.
16780,1,"\n[3] Tishby, Naftali, and Noga Zaslavsky. \""Deep learning and the information bottleneck principle."
16781,1, What could explain this difference in the performances?
16782,1," Here, the image is considered to be an effect of all the labels."
16783,1," Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method."
16784,1,"The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing."
16785,1," In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed ."
16786,1," However, many related works are missing in the literature, for example, Highway Networks [1],  Deeply-Supervised Nets [2] and Deep Networks with Stochastic Depth [3], etc."
16787,1," but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting."
16788,1, The main idea behind the proposed test is very insightful. 
16789,1," Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO."
16790,1,\n\nThe paper demonstrates improvements in a number of public datasets.
16791,1,"\n\nDetailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks."
16792,1,"\n\nThe paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed."
16793,1, How this current work compares with the existing such literature?
16794,1,\n\nThe paper has a technical focus.
16795,1," At least, it should show the generation texts were affected about DAs in a systemic way."
16796,1,"\n[d] \u201cthis state-of-the-art result is shown to be successful on only one map, which brings into question the repeatability of the results\u201d,"
16797,1,"\n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n"""
16798,1,\n\nThe contribution of this approach could be better highlighted.
16799,1," Overall, I think that the paper proposes some\ninteresting ideas,"
16800,1,"""This paper investigates an effect of time dependencies in a specific type of RNN."
16801,1,  First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task.
16802,1,\n\n3) The evaluation is on fairly small workloads (CIFAR-10).
16803,1,"\n\nSome points:\n1. The introduction uses \""scalability\"" throughout to mean something closer to \""ability to generalize.\"" Consider revising the wording here."
16804,1, It is claimed that all possible types of graphs can be learned which seems rather optimistic.
16805,1,"\n\nWeaknesses:\n\n1.\tIt would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset, other than the size."
16806,1," Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard."
16807,1,"\n\nOverall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.\n"""
16808,1," Since reordering windows restrict the context of each position to a limited number of neighbors, it may not capture distant information enough."
16809,1," but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD."
16810,1,"""Although the problem addressed in the paper seems interesting,"
16811,1,\n\n3) This omission of related work also weakens the experimental section.
16812,1, We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge?
16813,1,"  Also since $\\rho_\\pi$ is define with a scaling $(1-\\gamma)$ (to make it an actual distribution), I believe the definition of $\\eta$ should also be multiplied by $(1-\\gamma)$ (as well as equation 2)."
16814,1,"\n\nSide notes:\n- DCN is already quite commonly used abbreviation for \""Deep Classifier Network\"" as well as \""Dynamic Capacity Network\"", thus might be a good idea to find different name."
16815,1,\n\n1. Why is universal perturbation an important topic (as opposed to adversarial perturbation).
16816,1," \n - p.7. Why such an old Wikipedia dump? Most people use a more recent one!\[[CNT], [null], [DIS,QSN], [MIN]]n - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model."
16817,1,"""pros:\nThis is a great paper - I enjoyed reading it."
16818,1,\n\n8) the fine-tuning experiments do not bring significant novelty.
16819,1," Ideally, a more reasonable form of parameter-crossover (see references) could be compared to -- the naive one is too much of a straw man in my opinion."
16820,1,\n\n- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.
16821,1,\nPage 5:\n-\tthe description of the various networks used for text generation is insufficient for understanding:\no\tThe AREA is described in two sentences.
16822,1, The belief is that this learning style mimics human learners
16823,1,\n-            Sec 5.2: The abstract clams increased performance over MAML but the empirical results do not seem to be significantly better than MAML ?
16824,1," Hence, the apparent semantic coherence in what the authors call \""topics\""."
16825,1,\n\nThe introduction covers relevant literature and nicely describes the motivation for later experiments.
16826,1," Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points."
16827,1, It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots.
16828,1,\n\nSecureML: A System for Scalable Privacy-Preserving Machine Learning\nPayman Mohassel and Yupeng Zhang.
16829,1,  \n\n\nI have several concerns:\n\nMetrics are not very reasonable to me: \n- It does not measure how well the content is preserved.
16830,1,\n\nCons: \n\n- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices 
16831,1,"  Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage."
16832,1,"""This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric,"
16833,1,"\nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase."
16834,1," As a result, it is not convincing that the system design has good generalization."
16835,1," \n\nThis problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue."
16836,1," I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods."
16837,1,\n\nThis is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function.
16838,1,"\n\nConclusion\nThis paper brings in an interesting idea, is it possible to cluster the unseen classes in an open-world classification scenario? "
16839,1,\n\nThe results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables.
16840,1," \n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization."
16841,1,"\n\n+++ writing issues +++\n\nTitle:\n\n- \""VARIABLE ACTIVATION NETWORKS: A SIMPLE METHOD TO TRAIN DEEP FEED-FORWARD NETWORKS WITHOUT SKIP-CONNECTIONS\"" This title can be read in two different ways."
16842,1,"\n\nIn general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life."
16843,1,"""This paper investigates the effect of adversarial training."
16844,1," \nThe first step, virtual observations, are used to provide stand ins for inputs and outputs of the GPN."
16845,1,"\n\nTable 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive. "
16846,1,"  In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54"
16847,1, The use of these techniques for compressing is still unclear and their quality today is too low.
16848,1, \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?
16849,1," ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)"
16850,1,\n\n== Presentation ==\nThe paper is readable but not well polished
16851,1,\n\nConcerns.\n\nThe paper is hard do tear and it is deficit to identify the precise contribution of the authors.
16852,1,Otherwise it\u2019s not very clear to me why the numbers of parameters are so much higher for the baseline models.
16853,1,"\n\nI would have liked to see a discussion in the related work comparing the proposed approach to the long history of reasoning with subtasks from the classical planning literature, notably HTNs."
16854,1,"   The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent space."
16855,1, Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)? 
16856,1,. It is hard to justify any of the results after these strategies
16857,1, The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).
16858,1,"""Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper."
16859,1," For example, it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments, and the discussion at the end of Section 3.3 also sounds confusing."
16860,1,\nThe paper shows nice results on a number of small tasks.
16861,1,. Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\
16862,1," Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7)."
16863,1,"\n\nFirst of all, I was surprised on the short length of the discussion on the state-of-the-art."
16864,1,"\n\nRebuttal Response: I am still not confident about the significance of contribution 1, so keeping the score the same."""
16865,1," The authors also miss the most standard defense, training with adversarial examples."
16866,1," Why is this?\n\n* I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?\n\n\n\n\n\n\n"""
16867,1," The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations."
16868,1, \n\nThe authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower).
16869,1," I think that the problem would be better handled that way than with the proposed strategy,;"
16870,1," \n\nIn addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for"
16871,1," But before that, there are several issues need to be addressed."
16872,1,"\n- Section 4:  How is \""as soon as possible\"" encoded in this objective?"
16873,1,\nEven a linear regression would get at least half of that.
16874,1,"  It would be nice to have a succinct summary of how all of the pieces presented fit together, e.g. the original victim network, fine-tuning loss, per-class dictionary learning w/ OMP."
16875,1," Important information on the experiment settings are missing, e.g., how the model is parallelized."
16876,1,\n(b) Using a novel in-filling procedure to overcome the complexities in GAN training.
16877,1,\n- Le and Zudema use pooling and this paper uses weighted sum.
16878,1," For the approximation (20), it isn't clear if this is a good approximation."
16879,1,"""\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas:"
16880,1,"\nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar."
16881,1,\n- The definitions of the statistics and features (state and observation features) look highly elaborated.
16882,1,"  However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words."
16883,1," They apply this approach to multi-modal (several \""intentions\"") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks."
16884,1," Training the classification-features along with reconstruction-features does not seem to give any significantly new insights. """
16885,1, Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable.
16886,1,"To be more informative they should be reported by number of epochs, in addition or not to percentage."
16887,1,It is unclear if they are comparing to strong baselines.
16888,1, The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task.
16889,1,\n\nRecommendation\nI tried to read the paper several times and I accept that it was very hard to me.
16890,1,  So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).
16891,1," Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models."
16892,1, What happens if you start with random vectors?
16893,1,"  However, the aspects mentioned make me think the paper needs some improvements to be published.\n"""
16894,1," Certainly, the claim in the Discussion \u201cshow these to be equivalent to a form of STDP \u2013 a learning rule first observed in neuroscience.\u201d is inappropriate."
16895,1, What is the motivation to just average the inverse covariance matrices to compute S_C? 
16896,1,"  First, there are two RNNs present in the LSTM- one controls the gates, one controls the content update."
16897,1, Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally.
16898,1," Empirical results on ResNet50 on CIFAR show promising results for simulations with slow workers and servers, with the proposed approach."
16899,1, \n5. Only early stopping seems to constrain their model to be near identity.
16900,1,"\n\nThe abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc."
16901,1,"\n\nGiven labeled samples from a source domain and unlabeled samples from a target\ndomain, this paper proposes to minimize the risk on the target domain by \njointly learning the shift-invariant representation and the re-weighting \nfunction for the induced representations."
16902,1," The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al."
16903,1,"\n\nIn Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing\nthe proposed approach can reach comparable accuracies to previous work at even\nfewer parameter updates (2500 here, vs. \u223c14000 for Goyal et al 2007)\n"""
16904,1,"  However, I have found the current paper quite preliminary and incomplete."
16905,1,\n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance.
16906,1," but Table I shows that the improvement is not statistically significant, except in two games, DefendCenter and PredictPosition."
16907,1,"\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations"
16908,1,"""This paper presents simple but useful ideas for improving sentence embedding by drawing from more context. "
16909,1, Why is using a permutation of each pixels' color a good idea?
16910,1, Does it also change the location of the Nash equilibria?
16911,1," In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption."
16912,1," In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \""expert\"" trajectories can be generated in unlimited quantities."
16913,1, When that is done it would make an exciting contribution to the community.
16914,1,"\""\n\nHere one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct."
16915,1,More comments and further exploration on this results should be done.
16916,1,"\n(2) the results on MNIST and CIFAR 10 are not good enough for practical deployment."""
16917,1,"""This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples."
16918,1,"\n\nPros:\nThe paper is well-written and clear, if a bit verbose."
16919,1," For instance, does the same phenomenon happen for different datasets?"
16920,1,"This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803)."
16921,1,"""*Summary*\n\nThe paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs)."
16922,1," None of the comparison in table 4 is really meaningful, because none of the other systems have parameters as many as 19 layers of convolution."
16923,1,  How is this achieved in practice?
16924,1, What is a_t in\nequation 9?
16925,1,\n\nThe model and algorithm in this paper are simple and straightforward.
16926,1,.\n\nThis paper shows improvement over baselines.
16927,1, This will be clear if you provide the pseudo-code for learning.
16928,1, Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.
16929,1, \nSo the two can be presented together in the same section.
16930,1,\n2. Should state and verify conditions for application of implicit function theorem on page 2.
16931,1," It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep."
16932,1,\n- Scales to high-resolution images.
16933,1,"\nThe paper is well written, quality and clarity of the work are good."
16934,1,"""The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces."
16935,1, However the MSE obtained when not using regularization is the same (or even smaller) than when using it.
16936,1," I don\u2019t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2),"
16937,1, however it suffers from the following weaknesses that prevent the experiments from being convincing especially as they are not supported with more realistic setups.
16938,1,"""The authors present 3 architectures for learning representations of programs from execution traces."
16939,1, Being less grandiose would make the value of this article nicely on its own.\n*
16940,1," For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\"
16941,1, \n\nMajor comments:\n\nNo major flaws.
16942,1," Though it is quite common in the literature to assume this, it would have been interesting to see if there's a way to handle the case where it is unknown (either the process, parameters or both)."
16943,1,"  For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result."
16944,1," \n\nOriginality/Significance\nWhile the architecture is new, it is based on a combination of previous ideas about fast weights, hypernetworks and activation gating and I\u2019d say that the novelty of the approach is average."
16945,1, I did not find much insight in this paper.
16946,1," From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance."
16947,1, Should $\\phi^*$  be $\\hat \\phi$ ?
16948,1,"\n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported."
16949,1,\n3. Add an experiment where you vary 'pf' and 'qf' (and keep the hidden size fixed) to show how the optimization/generalization performances can be tweaked.
16950,1,\n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.
16951,1,"\n\n\nOverall, the paper is well-written but the novelty and applicability seems a bit limited."
16952,1, \n\nEquation 3: please define H.
16953,1," I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods,"
16954,1, As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed.
16955,1,  It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood.
16956,1, and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters.
16957,1, The authors may want to consider the following comments:\n\n1. I did not really understand the analogy with STDP in neuroscience because it relies on the assumption that spiking of the post-synaptic neuron encodes the backpropagating error signal.
16958,1,"""This paper utilizes ACOL algorithm for unsupervised learning."
16959,1,"""This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics."
16960,1," To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors)."
16961,1," Section 2 of the same paper also notes it (depthwise convolutions can be traced back to at least 2012)."""
16962,1," \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task."
16963,1," The used Visual Concepts (VCs) were already introduced by other works (Wangt'15), and is not a novelty."
16964,1," For e.g. \""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks\"", \""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\"", \""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\""."
16965,1," This makes it very hard to reproduce the experimental results (in order to verify it), and its practical value becomes quite restrictive (very few organizations can afford such a cost)."
16966,1,  how was the net initialized? 
16967,1, The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image.
16968,1," However, I don't find the paper of high significance or the proposed method solid for publication at ICLR."
16969,1,  Experiments search for effective CNN architectures for the CIFAR image classification task.
16970,1,\n\nExamples of odd language choices:\n\t-\t\u201cThe idea also does not immediately scale to nonlinear function approximation.
16971,1,"\n\nThe proposed method seems effective, and the proposed DSAE metric is nice, though it\u2019s surprising if previous papers have not used metrics similar to normalized reduction in log-ppl"
16972,1," The so-called \""chicken-and-egg\u201d issue concerns the causality dilemma: two causally related things, which comes the first."
16973,1," In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer."
16974,1, The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach.
16975,1,. Any references here
16976,1, \n\nIn conclusion:\nThis paper presents a method for creating features from a (pre-trained) ConvNet.
16977,1,   Why is this.
16978,1, (The background is all black - did something go wrong with the pdf?)
16979,1,"\nVocabulary is wrong in other places, for example the word semi-supervised\nis wrongly understood and used. "
16980,1,". The paper shows results on the MNIST, SVHN, and Celebrity Faces datasets."
16981,1, This group of related work should also be discussed.
16982,1,"   I'm a little worried that it's \""in the eye of the beholder\"" whether a given generalization should be expected to work or not."
16983,1, Please proof read.
16984,1,"  This source of error is often ignored in the literature,"
16985,1,"\n\nAlthough related work is discussed in detail in section 1, it remains unclear how exactly the proposed algorithm overlaps with existing approaches."
16986,1,"n\nA flaw of this paper is that kennen-i and io appear to require gradients from the network being probed (you do mention this in passing), which realistically you would never have access to. (Please do correct me if I have misunderstood this)"
16987,1," \n- It looks like the typos in the equations got fixed\n- The new phrase \""enables to learn to plan\"" seems pretty awkward."
16988,1," There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties."
16989,1, Each subtask execution is represented by a (non-learned) option.
16990,1," If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales."
16991,1,"\n(4) suggests using MC marginalization and also using the \""average\"" action to improve computational feasibility"
16992,1, \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations.
16993,1," They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation."
16994,1,"""This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis."
16995,1," Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations."
16996,1,"""This paper proposed a training strategy for fully connected neural network, where per class, per weight learning rate is used."
16997,1,"\n\nIn summary, I feel that while there are some issues with the paper, it presents\ninteresting results and can be accepted."""
16998,1, What are the standard errors?
16999,1," The idea of \u201csoft ordering\u201d enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular."
17000,1, What do you mean by re-initializing and retraining the narrator? Isn\u2019t it costly to reinitialize the network and retrain it for every turn?
17001,1,"\n\n\nMinors:\n1. Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems."
17002,1," It is unclear how the approach improves on the original predictive coding formulation of Rao and Ballard, who also use a hierarchy of transformations."
17003,1,\nI don\u2019t really get the intuition behind this formulation.
17004,1, The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model.
17005,1,\n- Due to the weird sparsity terminology Table 1 is very confusing.
17006,1," \n\nFurthermore, the paper is very badly written since it keeps postponing the actual explanations to later sections (while these  sections eventually refer to the appendices)."
17007,1, It seems to me that a human subject experiment to somehow compare the two methods is required.
17008,1, Some exploration of this issue or commentary would be valuable.
17009,1," If the contribution of the paper is the \""output stochastic\"" noise model, I think it is worth experimenting with the design options one has with such a model."
17010,1, (b) why only slight overlap is used in practice?
17011,1," \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation)."
17012,1,\n4. Authors stated that the last step is to build a mapping from the GPS features into the response Y.
17013,1,"\nThen the expected expert direction for learner state S is:\n\n     SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 )"
17014,1,.\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].
17015,1,\n\nThe running times in Table 9 are also skeptical.
17016,1," In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision."""
17017,1, Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU.
17018,1, But the paper cannot establish such a result. 
17019,1,"\n\nSecond, the title of the submission, \u201cDo Deep Reinforcement Learning Algorithms Really Learn to Navigate\u201d makes a broad statement [e] that cannot be logically invalidated by only one particular set of experiments on a particular model and environment, particularly since it directly targets one specific paper (out of several recent papers that have addressed navigation) and one specific architecture from that paper, NavA3C+D1D2L (incidentally, not the best-performing one, according to table 1 in that paper)."
17020,1,"\n\nGenerally, I think that the paper is written well (except some issues listed at the end)."
17021,1, All of the approaches seem to perform about the same
17022,1,  I think the idea of representation learning using a somewhat artificial task makes sense in this setting. \n\nI have several concerns for this submission.
17023,1," The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant."""
17024,1,"\nDespite its limited novelty, this is a very interesting and potentially impactful paper."
17025,1,"\n- format for images should be vectorial (eps or pdf), not jpg or png..."
17026,1, There is no comparison to any such simple baselines.
17027,1," I still think the novelty, significance of the claims and protocol are still perhaps borderline for publication (though I'm leaning towards acceptance),"
17028,1,"\""consistent\"" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2.[[CNT], [CLA-NEG], [CRT], [MAJ]] You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel ."
17029,1," A major rework should be considered."""
17030,1,  It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.
17031,1,"   This property is visible in I(Z_2,X) for PIB in Figure 3, but otherwise absent."
17032,1,\n\nWhy is the stride for the convolutional/deconvoluational layers set to 2 (as stated in Section 2.1)?
17033,1,\n\nMinor:\nDefinition of g in the beginning of Sec 3.1 seems to be a typo.
17034,1," If yes, is it given during evaluation as well?"
17035,1," My biggest recommendations for this paper are for the authors to 1) articulate one theme and then 2) look at each part (whether that be section, paragraph, or sentence) and ask what does that part contribute to that theme."
17036,1,"\n5) \""We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\"" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model."
17037,1," While I can now clearly see the contributions of the paper, the minimal revisions in the paper do not make the contributions clear yet (in my opinion that should already be clear after having read the introduction)."
17038,1,"\n\nThe left side contains th^p and th^g, but the right side does not."
17039,1,. A higher rank would correspond to more dependence across time
17040,1, The resulting architecture outperforms vanilla deep nets and sometimes approaches the performance of ResNets.
17041,1,"  But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995."
17042,1,\n\nMy opinion is that this paper is not ready for publication.
17043,1, \nThe only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes.
17044,1,"""The authors introduce a novel novel for collaborative filtering."
17045,1,"\n\nThird, the experiments with synthetic noise are significant to a reduced extend."
17046,1," The authors also admit that when two factors are dependent, this method might fail."
17047,1,"\n\n\n*This relates also to this: \n\n\""Later we empirically verify that, even when the overall in-\nformation revealed does not increase per se, an independent master agent tend to absorb the same\ninformation within a big picture and effectively helps to make decision in a global manner."
17048,1,"\nIt is true that the authors use strong regularization techniques (drop out, external knowledge of words embedding...)."
17049,1,"\n\n\nPros:\n- results\n- novelty of idea\n- crossover visualization, analysis\n- scalability;"
17050,1," The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used)."
17051,1," This looks very noisy and non-interesting.\n\n"""
17052,1,  This work is neither mentioned in the related work nor compared to.
17053,1," but it is not clear what the strengths are, and the presentations needs to be done more carefully."
17054,1,\n\nI've been disappointed by the authors have incorporated the feedback/reviews - I expected something a little more clear / honest.
17055,1," It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model."""
17056,1, The idea is interesting and insightful.
17057,1,"and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception"
17058,1,\n+ The results seem interesting too
17059,1, Have authors performed any experiments ?
17060,1,"""The paper introduces a neural translation model that automatically discovers phrases."
17061,1, Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)?
17062,1, I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.
17063,1, The authors argue that using different number of particles for learning the proposal parameters versus the model parameters can be beneficial.
17064,1,"n- the method is a simple feed-forward network, so it is very fast to compute\"
17065,1,"  Moreover, the transformation to the minimization form crucially required the closed form computation of the dual function (with w* just defined above equation (2)), and this is limited to linear discriminators,  thus ruling out the use of the proposed approach to more powerful discriminators like deep neural nets."
17066,1,  \n\nThere are essentially three scenarios of generalization discussed in the paper:\n        (a) various generalizations of image parameters in the PSVRT dataset\n  
17067,1," The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha."
17068,1," Finally, how performance is influenced by dimensionality P and L should also be clarified.\n\n"
17069,1," Do these all use the same minibatch, at each iteration?"
17070,1,"""SUMMARY\nThe paper deal with the problem of RL."
17071,1, Please expand on this.
17072,1,"\n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms."
17073,1,"\n4. Under the experiments, different variations of Majority Vote, EM and Oracle correction were used as baselines."
17074,1," Also, it is assumed that the number of possible inputs is finite (also true for the recursive NPI paper), and it is not clear what techniques or lessons of this paper might transfer to tasks with perceptual inputs."
17075,1,"""The paper is clearly written, with a good coverage of previous relevant literature."
17076,1," So, does the training data have a particular string as the ground truth answer for such questions, so that a model can just be trained to spit out that particular string when it thinks it can\u2019t answer the questions? "
17077,1, I recommend the paper\u2019s acceptance for this reason.
17078,1,\no\tWhat is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA?
17079,1, (mixture pointer mechanism + REINFORCE)
17080,1,"""This paper is concerned with video prediction, for use in robotic motion planning."
17081,1," For example, could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural?"
17082,1," However, the way the results are presented/plotted, namely the linear plot of the (shifted) relative performance of CAR (and LRA3C) versus A3C, visually inflates the importance of tasks on which CAR (and LRA3C) perform better than A3C, and diminishes the importance of those tasks on which A3C performs better. "
17083,1," Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function."""
17084,1,"\n- Also, several details are missing in toy experiments"
17085,1, Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features.
17086,1," The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \""style\"" and \""content\"", is an interesting and long-standing problem."
17087,1," The difference in performance for Dual-AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right?"
17088,1,"\n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion"""
17089,1," Instead of using a fixed or Gaussian parametric filters, this work proposes to predict filter weights using a multi-layer perception."
17090,1,"\n\nd) The authors found hard to regularize the gradient $\\nabla_x D(x)$, even they tray tanh and cosine based activations."
17091,1,n\n\nPros:\n\n\nThe paper is clear and the proposed problem is novel and well-defined.
17092,1," \n\n The choice of the face domain is also suspicious, since all faces are topologically the same graph (even though there are geometric variations)."
17093,1,   I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.
17094,1,\n\n3) Cons:\nOverall architectural prediction network differences with baseline are unclear:\nThe differences between the proposed prediction network and [1] seem very minimal.
17095,1,"\n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one."
17096,1,\n- Quantitative results are missing.
17097,1, I would have also liked to see more innovation in evaluation.
17098,1,"\n\nEDIT: I read the author's rebuttal, but it has not completely addressed my concerns, so my rating has not changed."""
17099,1,\n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?
17100,1, The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class.
17101,1," \""\nThis is right. I am just surprised is has not been done before, since it requires only few lines of derivation."
17102,1,\n\nPROS:\nThe idea is interesting. 
17103,1," Finally, this paper doesn\u2019t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference."
17104,1," The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is)."
17105,1," The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed. "
17106,1,"\n\nsection 3.1:\n- replace \""to to\"" by \""to\"" in the second line[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nsection 4:\n- \""This may be a result of the ensemble nature of ResNets (Veit et al., 2016), which does not play a significant role until the depth of the network increases.[[CNT], [CLA-NEG], [CRT], [MIN]]\"" The ensemble nature of ResNet is a drawback, not an advantage, because it causes a lack of high-order co-adaptataion of layers."
17107,1,\n\n[after rebuttal: revised the score from 7 to 8].
17108,1, \n\n2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN?
17109,1, Is this because of the MCTS approach?
17110,1, \n\nThe mathematical basis for this paper is actually introduced in [3] and a single-layer version of the current model is developed in [4].
17111,1, It would be useful to comment on that aspect.
17112,1,"""This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy."
17113,1, I do not think using it in DNN is a big contribution.
17114,1,"""The paper attempts to extend the predictive coding model to a multilayer network."
17115,1,"\n\nOn the practical side, the chosen baseline is very poor."
17116,1," The first is including time remaining in the state, for domains where episodes are cut-off before a terminal state is reached in the usual way."
17117,1," but with dense connections (Huang et al., 2017) and with a classifier at each layer."
17118,1, and the language model options (Table 1 row 9 and footnote 4) may reduce the model accuracy as well as it works not so effectively.
17119,1, It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these.
17120,1,"The experiments seem extensive, using many recently proposed RL methods, "
17121,1,"\n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks."
17122,1, It is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding (classification) relies on artifacts or confounds.
17123,1," We find this \u201chybrid\u201d method is both fast and accurate, for both small batch and large batch."
17124,1,"However, they only show the reduction of computation complexity for convolution, and speed comparison between ReLU, ISRLU and ELU on high-end CPU"
17125,1," The idea of counting the number of regions exactly by solving a linear program is interesting,"
17126,1," First, the paper proposes a dual algorithm to estimate Kantorovich plans, i.e. a coupling between two input distributions minimizing a given cost function, using dual functions parameterized as neural networks."
17127,1,\n\nI\u2019d like to see more discussion of why the second stage supervised problem is beneficial.
17128,1,"\n\n\""or in the case of heart failure, predicted BNP level\"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level?"
17129,1,"   Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined. "
17130,1, The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks.
17131,1, \n\n- I would be keen to see eigen options being used inside of the agent.
17132,1," However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important."
17133,1, Must it be trained for each new synthesis problem?
17134,1,.\n\nOriginality:\nThe presented idea seems novel.
17135,1,\n\n\nThe writing and organization of the paper is very well done. 
17136,1," The main difference\nappears to be that rather than using the classic GAN loss to shape the\naggregate posterior of an autoencoder to match a chosen, fixed distribution,\nthey instead employ a Wasserstein GAN loss (and associated weight magnitude\nconstraint, presumably enforced with projected gradient descent) on a system\nwhere the matched distribution is instead learned via a parameterized sampler\n(\""generator\"" in the GAN lingo)."
17137,1," Despite being a simple approach, the experimental results are quite promising."
17138,1," The next step of the algorithm is less easy to follow, and presentation of the ideas could be much better."
17139,1,"""The paper proposed an extension to the fast weights from Ba et al. to include additional gating units for changing the fast weights learning rate adaptively."
17140,1, Epsilon-greedy methods\nhave already been shown to be less efficient than Bayesian methods with\nThompson sampling for exploration in q learning (Lipton et al. 2016).
17141,1,\n\nI understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring.
17142,1,"  You can make an argument for why the one that was ablated was \u201cmore interesting\u201d, but really this is an obvious empirical question that should be addressed."
17143,1," \n\nThere is a small typo in (eq 5).\n"""
17144,1," After characterizing the kernel function, this allows us to use the GP framework for prediction, model selection, uncertainty estimation, etc."
17145,1, I believe there are far more caveats in this analysis than what is suggested in the paper and the authors should avoid over-generalizing the results based on a few domains and the analysis of a small set of algorithms.
17146,1, although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.
17147,1," It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation?"
17148,1," \n\nThe expression \n\n   h_{aid} ( \\phi(0), v )   =   <  Wa,   [ \\phi(o) ; v ]   >   +   b\n\nmakes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions)."
17149,1," \n\nFor the latter question, the authors propose using a \""query network\"" that based on the current state, pulls out one state from the memory according to certain probability distribution."
17150,1,"\n\nThis paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited."
17151,1, This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance.
17152,1," \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose."
17153,1,"\n2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct."
17154,1," The call for papers states that \""we strongly recommend keeping the paper at 8 pages\"", yet the current submission extends well into its 10th page."
17155,1,"\n\n1. After equation (5), I don't understand how the gradient of L(tilde_W) w.r.t. B(i) is computed. B(i) is discrete."
17156,1, There is no clear basis for the main equations 1 and 2.
17157,1,. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task
17158,1,"If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful."
17159,1,\n\n(Assessment)\nBorderline. Refer to the Cons section above.
17160,1," Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections."
17161,1, \nCons: nothing major.
17162,1, The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.
17163,1, It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.
17164,1, The papers are written well and easy to follow.
17165,1,"\n\nThe abstract and the conclusion should be revised, they are very vague."
17166,1," \n\nFurther, please explicitly support your claim via experiments."
17167,1," Finally, there were various typos throughout (one example is \""neglect minimua\"" on page 2 should be \""neglect minima\"")."
17168,1,"    \n- The list of \""prior work on learning causal graphs\"" seems a bit random."
17169,1," The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks"
17170,1, \n- Some notations and equations are broken.
17171,1," Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified."
17172,1,"\nICLR Workshops, 2016\n\n* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering."
17173,1,"\n- The multi-level attention is novel and indeed seems to work, with convincing ablations."
17174,1," \n\nIn Section 4.2, you need to refer to Table 2 in the text."
17175,1,\n\nThe paper suggests normal pruning does not necessarily preserve the network function.
17176,1,\n\n\nConclusion:\nI think the paper presents an interesting idea which should be exposed to the community.
17177,1, Both RNNs are trained jointly and only the forward model is used at test time.
17178,1," The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems."
17179,1," Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE-style bounds (w.r.t. the inference parameters) grows with their accuracy."
17180,1," Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model."
17181,1," I fail to see how this would lead to superior performance compared to conventional CNNs."""
17182,1,"\n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}?"
17183,1,\n\n3). The results in Table 3 are a little confusing.
17184,1,\n\n\n8. Figure 5 is hard to understand.
17185,1,  Is there a PIR per image?
17186,1," It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework."""
17187,1,  This makes reading the paper very hard.
17188,1, Currently most domains are terminated by failure/success conditions rather than time.
17189,1,"""This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector."
17190,1, The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks.
17191,1, Here are some specific questions:\n\n1. How are the keys generated? 
17192,1, The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary.
17193,1, but the different proof technique might be a good enough reason to accept this paper.
17194,1," The stated goal is to address some of the issues related to the role of depth in deep networks, but I think it should be clarified which specific issues in particular are relevant to this method and how they are addressed."
17195,1," However, some implementation details are missing, which makes it difficult to assess the quality of the experimental results."
17196,1,"""The authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperature."
17197,1,"  The model is simple: tensor factorization, where the covariate can be viewed as warping the cosine distance to favor that covariate's more commonly cooccuring vocabulary (e.g. trump on hillary and crooked)"
17198,1,\n\n* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side).
17199,1,"\n\nMoreover, the numerical experiments look to be realized in the context of targeted attack."
17200,1,\n\nComments on the details of the paper:
17201,1,\n\nIt seems that there could be more things to show in the experiments part.
17202,1,"""This paper proposes a convnet-based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed-up."
17203,1, I think that this is a good approach to the problem that could be used in real-world scenarios.
17204,1," \n\nOverall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that."
17205,1,"I think this is a pitfall that many deep network papers fall, where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution (where the latent have prescribed semantics). I would argue that is rarely the case."
17206,1,"\n\nThe idea of using task decomposition to create intrinsic rewards seems really interesting,"
17207,1,"\n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data."
17208,1," If in many applications the sentence-level processing is already good enough, the motivation of doing speedup over LSTMs seems even waker."
17209,1,\n\nFigure 2 is complex and confusing due to the lack of proper explanation in the text.
17210,1," If the authors are evaluating on the test set and reporting numbers, then it is odd that they `skipped' reporting SPICE numbers."
17211,1," There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense."
17212,1,"If the noise vector is perturbed and a new plausible filter set is created, the input data can be optimised to find the input that produces the same set of activations."
17213,1, The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique.
17214,1," For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices."
17215,1," A variant on this, which also incorporates the ranks of the imposters sorted by a metric such as edit distance or BLEU metric with respect to the ground truth is also introduced."
17216,1," Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs."
17217,1,"Were the word representations fixed, or fine tuned during training?"
17218,1,"\n3. Formulate complex batch normalization as a \""whitening\"" operation on complex domain"
17219,1," Indeed, this largely regulates the hard problems (i.e., controlling the low-level actions of the vehicle while avoiding collisions) to a separate controller."
17220,1, it\u2019s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks
17221,1,\n\nTable 4 is confusing.
17222,1, It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.
17223,1," Please cite the conference version if one is available, many arxiv references have conference versions."
17224,1, The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations.
17225,1, I do not think this working point is useful for a real application
17226,1, And when data can be sparse. 
17227,1," Based on the magnitude of this profile coefficient, which determines the importance of this `filter,\u2019 individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner."
17228,1," \n[Ref2] Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht. Multiclass Total Variation Clustering."
17229,1, These limitation should be discussed more explicitly and more thoroughly.
17230,1, It would be interesting to compare these algorithms as well.
17231,1,"  Maybe providing rather original headings is better?[[CNT], [PNF-NEU], [QSN], [MIN]]  It's a style issue that is up to tastes anyway so, again, it is minor.[[CNT], [PNF-NEU], [DIS], [MIN]]\n\n- \""However, sharing latent variables across an entire class reduces the encoding cost per element is significantly\"": typo.[[CNT], [CLA-NEG], [CRT], [MIN]]\n\n- \""Figure ?? illustrates\"".\n"""
17232,1," I would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resources."""
17233,1, This would help interesting applications inform what is wrong with current theoretical views.
17234,1, \n\nDimension of c_i / o ?
17235,1,"\n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read."
17236,1, Might this be a typo in 4.1?
17237,1,\n\nOriginality: I am not familiar enough with adversarial learning to assess the novelty of this approach.
17238,1," \n- I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture."
17239,1," The network is to be evaluated over a private input, so that only the final outcome of the computation-and nothing but that-is finally learned."
17240,1,"\n\n3. The proposed two kernels introduce sparsity in the spatial and channel dimension, respectively. "
17241,1, Why does CTC fail when trained without the blanks?
17242,1," In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution."
17243,1, \n\nThe attack model is too rough.
17244,1,"\n\nThat said, overall the paper is a nice contribution to dialogue and QA system research by pointing out a simple way of handling named entities by dynamically updating their embeddings."
17245,1,   \n\nWould this proposed strategy have thwarted the Russian tank legend problem?
17246,1,  I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only.
17247,1," The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset),;"
17248,1, Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results.
17249,1,"\"".... Device Placement\"" seems to suggest that one is placing devices when in fact, the operators are being placed."""
17250,1," In particular, articles are frequently missing from nouns."
17251,1, \n\nThe method proposed by the authors is very similar to Lipton's approach.
17252,1,. This again leads to poor readability and it is hard to read and compare the corresponding numbers from Table 1.
17253,1, Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin.
17254,1,"\n\nI\u2019m also not very happy with the term \u201ccounterfactual\u201d. As the authors mention in footnote, this is not the correct use of the term, since counterfactual means \u201cagainst the fact\u201d."
17255,1," The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the \""flat vs sharp\"" dilemma, but is lacking here."
17256,1," The authors report improved paraphrasing of phrasal verbs, and state-of-the-art accuracy in correcting grammatical errors involving prepositions, and good results on prepositional phrase attachment."
17257,1, \n\n2. It will be great if the author could provide some discussions with respect to the analysis of information bottleneck [3] which also discuss the generalization ability of the model.
17258,1," However, it is more interesting and important to test on more advanced networks."
17259,1,\nThere is no new novel contribution on the methods side.
17260,1, It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear.
17261,1,.\n3. The writing is easy to follow.
17262,1," which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).\n\n\n\n"""
17263,1,"\n\nI'm on the fence about this work: I like the ideas and they are explained well,"
17264,1,nIt would be nice to see empirically how much of computation the proposed approach takes during training. 
17265,1,"\n\n3. Negative images are fast to obtain, but they are oversimplified, and can be obtained via linear transform which is easy for neural networks."
17266,1," The motivation in the appendix is very informal and no clear derivation is provided.[[CNT], [CLA-NEG], [CRT], [MIN]] The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states."
17267,1,  What is the main objective of Eq. (7)?
17268,1,"""The authors proposed a supervised learning algorithm for modeling label and worker quality. "
17269,1,"\nEntropy-regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017]."
17270,1,"\n - According to \""SPICE: Semantic Propositional Image Caption Evaluation\"" current metrics used in image captioning don't correlate with human judgement."
17271,1, It is not clear how the authors operated the choices of these figures.
17272,1, That could help readers to better understand the effect of TR.
17273,1,"\n\nThe authors break down the \""inference gap\"" in VAEs (the slack in the variational lower bound) into two components: 1. the \""amortization gap\"", measuring what part of the slack is due to amortizing inference using a neural net encoder, as compared to separate optimization per example."
17274,1,"\n\nThis reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains."
17275,1, The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM).
17276,1,"  Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016)."
17277,1, It generates images in a layer-wise manner.
17278,1," I'm usually quite happy to see connections being made to other fields,"
17279,1,"  Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \""A sensitivity analysis of (and practitioners guide to) CNNs...\"" Zhang and Wallace, 2015.)"
17280,1,. This explicit duration modelling captures multiple scales of temporal resolution.
17281,1," \u2026\u201d\n\nOn a related note, I don\u2019t like the term \u201cSelection Probability\u201d for the quantity it describes."
17282,1,"\n\nThe way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on."
17283,1," However, I think that it should be for pair in the same layer. It is not clear in the paper."
17284,1,"  The authors demonstrate the method on just two datasets, and effectively they show results of training only for Feed-Forward Neural Nets (the authors claim that \u201cthe entire spiking network end-to-end works\u201d referring to their pre-trained VGG19, but this paper presents only training for the three top layers)."
17285,1," For example, the authors describe RandWalk (Arora et al. 2016) but how their work falls into that framework is unclear."
17286,1," And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%."
17287,1, \n\nThe paper did not specify how often the neural net must be trained.
17288,1," While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all."
17289,1,"\n\n1. Apply the sum-of-squares (SOS) method.\nThe paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy."
17290,1, except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3);
17291,1," The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset."
17292,1,\n\nPrior convergence proofs: I think the way the paper is currently written is misleading.
17293,1, \n\nThe paper is chiefly concerned with analysing these local minima by expanding the cost function about them.
17294,1,\n\nIs your equation 1 correct? I understand that your logits are reciprocal of mean squared error.
17295,1,"   I don\u2019t expect to see state-of-the-art performance on Charades; the point of the experiment is to demonstrate that Skip-RNNs perform as well or better than their non-skipping counterparts, which it does."
17296,1,\n\nWhat is C in eq. 9?
17297,1," \n- it is unclear how joint training might help, given that the objectives do not influence each other"
17298,1," This goal is to achieve a similar effect to that of natural gradient, but with lighter computation."
17299,1," \n\nThe contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks."
17300,1,n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs.
17301,1, I would also be quite interested to see the performance of the proposed algorithm for different values of parameters (such as the butch size).
17302,1,"   I believe the latter, but this implies that the vocabulary dimension (V) is the same as the number of aspects, since A is apparently shared."
17303,1,"  Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?"
17304,1,\n\nStrengths\n- Use of graph neural nets for few-shot learning is novel.
17305,1, I would suggest to \nput the two together since they both demonstrate that the network can learn to be invariant to a certain\ndomain aspect as far as data augmentation is used to cover that aspect for at least a part of the observed\ncategories.
17306,1," the analysis seems correct,"
17307,1,\u2028\n(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare.
17308,1, \n\nSignificance:\n\nIt is hard to determine how significant the work is since the authors only\ncompare with a single baseline and leave aside previous work on efficient\nexploration with Thompson sampling based on variational approximations.
17309,1,"""This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially."
17310,1," the paper is not well executed, and I cannot understanding how gradient descend is performed based on the description of Section 4."
17311,1, The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work.
17312,1,".  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \""generic\"" tensor."
17313,1," If so, how is the partial language description as a target handled since the description for a different entity in an image might be valid, but not the current target."
17314,1," \nMy only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself."
17315,1, Then the method is tested in several image and text data sets.
17316,1,"""The paper applies tools from online learning to GANs. "
17317,1, The paper is very well written and has nice visualisation of demonstrating weights.
17318,1,"It is easy to follow, and succinct while being comprehensive."
17319,1,\n\nThe main contribution appears to be the optimisation procedure for the binary masking tensor g.
17320,1," To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline."
17321,1, 2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios);
17322,1," AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S."
17323,1, Nonetheless I find the results interesting to the RL community and a starting point to further analysis of the MC methods (or adaptations of TD methods) that work better with image observation spaces.
17324,1,  Finally the authors stack these warp layers to create a \u201cwarped resnet\u201d which they show does about as well as an ordinary ResNet but has better parallelization properties.
17325,1,"\n2) What is the exact heuristic in \""Text Styles\"" in section 3.1? Should be stated for replicability."""
17326,1," \n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation."
17327,1," In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives."
17328,1, It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model.
17329,1,\n\nMinor comments:\n- Paragraph below Equation 6:  The meaning of $\\phi(\\psi)$ is unclear
17330,1, They are very close to existing algorithms
17331,1,\n2. Only small scale experiments (although this factorization has huge potential on larger scale experiments)
17332,1, There are a lot of undefined notation.
17333,1,\n\n2. Missing important related works.
17334,1, \n\n# Clarity\n- The paper is overall easy to follow and the description of the proposed method is clear.
17335,1,"\n\nThe draft is well-written, and the method is clearly explained."
17336,1,"""This paper provides a survey of attribute-aware collaborative filtering."
17337,1," See https://sites.google.com/site/neighborembedding/mnist\nc) For 20 Newsgroups dataset, NATAC achieves 0.384 NMI."
17338,1,\nThe baseline is random forests and feature engineering.
17339,1," That\u2019s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair."
17340,1,  It proposes a non-parametric approach that maps trajectories to the optimal policy.
17341,1," Different algorithms have different cost for \""each iteration\"" so comparing that seems not fair."
17342,1,"""The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results."
17343,1, There is no experimental result to verify this.
17344,1,\n(b) Searching in the latent space z could be strongly dependent on the matching inverter $I_\\gamma(.)$.
17345,1, However as explained above the final conclusions are also significantly weaker than this prior literature so it\u2019s a bit of apples vs oranges comparison.
17346,1,  The empirical evaluation demonstrates the effectiveness of the approach.
17347,1, and if the authors are able to address it satisfactorily I will increase my score. 
17348,1, What about other potential reward definitions?
17349,1,\nYou shouldn't claim such a universal revolution to exploration / RL / evolution because I don't think that it's correct.
17350,1," \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy."
17351,1," It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5."
17352,1," However, the experimental evaluation is globally limited,  hyperparameter tuning on test which is not fair."
17353,1,"\n2) The authors state in the abstract (and elsewhere): \""... showing that (model free) policy gradient methods globally converge to the optimal solution ...\"". This is misleading and NOT true."
17354,1,"\nBut this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1."
17355,1,\n\nWord2gauss also evaluates on similarity and relatedness datasets.
17356,1,"n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance)."
17357,1,  This is crucial in order to follow / judge the rest of the paper.
17358,1, but I'm wondering if this is because more augmented data is used overall.
17359,1,"""The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs."
17360,1,"""This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network."
17361,1, All proofs are provided and easy to follow.
17362,1,"  arXiv preprint arXiv:1709.06560.\n\n[2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016)."
17363,1,"\n\nFirst, the paper does not compare against a very similar approach of Parisotto et al.\nNeuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network\nfor generating the program tree incrementally by decoding one node at a time."
17364,1, In order for the Taylor approximation to be good?
17365,1,"\n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions."
17366,1," My suggestion would be to completely separate these three parts: present a general method first, then use heat sink as the first experiment and airfoil as the second experiment."
17367,1,\n\nMy major concern is that the clustering task is not extensively explored.
17368,1,"\n\nThe paper is severely weakened by not comparing experimentally to other learning (hierarchical) schemes, such as options or HAMs."
17369,1,The experiments are clear and the three different schemes provide good analytical insights.
17370,1," For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from \""Attention is All You Need\"")."
17371,1,"\n\nSection 3.2.2 shall be placed later on, and clarified.[[CNT], [CLA-NEU], [DIS], [MAJ]]\n\nDiscussion on mixing more than two sounds leads could be completed by associative properties, we think... ?\n"""
17372,1,"\n\nOverall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score."""
17373,1,\n\nOriginality: Above average.
17374,1, I got confused in this section because eq.9 defines f(x) as a quadratic function.
17375,1,"The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014)."
17376,1,"\n\n- Fig 2 was not that convincing since the figure with time showed that either usual BackProp or the exact ProxProp were best, so why care about the approximate ProxProp with a few CG iterations? "
17377,1, The paper uses the right composition tools like moments accountant to get strong privacy guarantees.
17378,1," \n[2] G. D. Konidaris, S. Niekum, and P. S. Thomas. TD\u03b3: Re-evaluating complex backups in temporal difference learning."
17379,1," However, I couldn\u2019t find any details on how the phrasal verbs were chosen, or what (if any) held out data was used for tuning the models."
17380,1,"  The idea presented seems to have merit ,"
17381,1, \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded.
17382,1,"\n\nUnder the theme of sketched updates, they examine quantized and sparsified updates with the property that in expectation they are identical to the true updates."
17383,1,\n\nReproducibility in continuous control is particularly problematic.
17384,1,"\n - Figures 3 and 4 take 2 pages, but what should one see there?"
17385,1,\n- \u201cthe gradient at s_{t+1} that will change the value the most\u201d  - This is too colloquial.
17386,1,\n- DNN was used in Section 2 without being defined.
17387,1," To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered."
17388,1,"\n\n- The writing should be careful, e.g., \nvalue iteration is presented as a learning algorithm (which in my opinion is not)"
17389,1, I find it quite difficult to support the specific claim in the abstract from the results without adding a comment about the significance.
17390,1," One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3)."
17391,1," I would not like to see this work in the proceedings, due to the lack of originality and poor technical discussion."
17392,1," Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing."
17393,1," but I am not sure that the contribution is significant enough for acceptance.\n"""
17394,1,\n- how is this an end-to-end approach if you are using an n-gram language model for decoding?
17395,1,"  Neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter."
17396,1,"""This paper investigates a new approach to prevent a given classifier from adversarial examples. "
17397,1,  This was totally unclear until fairly deep into Section 3.
17398,1,\n\nTheoretical analysis is presented to show the performance of any selected subset using the geometry of the data points.
17399,1, However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain).
17400,1, Why not do lots of random initializations for the optimization?
17401,1,(3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase.
17402,1," I couldn't see any emerging trend/useful recommendations (like \""if your problem looks like X, then use algorithm B\"")."
17403,1," This is the bad side of the recent deep nets hype, and ICLR is particularly susceptible to this."
17404,1," Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?"
17405,1, The core model is Variational Autoencoders (VAE) with an integrated visual attention mechanism that also generates the associated text.
17406,1," The technique consists of first training a model by constraining its trace norm, which allows it to be well-approximated by a truncated SVD in a second fine-tuning stage."
17407,1,"\n- you say there is no \""complexity\"" incrase when using \""logadd\"" - how do you measure this? number of operations? is there an implementation of \""logadd\"" that is (absolutely) as fast as \""add\""?"
17408,1," In fact, the performance of CrescendoNet is worse than most of the variants of residual networks, e.g., Wide ResNet, DenseNet, and ResNet with pre-activation."
17409,1," TAGCN with K=2 has\ntwice the number of parameters of GCN, which makes the comparison not entirely\nfair."
17410,1,"\n\nMy main concern about this paper is that although the presented techniques work well in practice,"
17411,1, \n- The authors did a great job summarizing prior work and motivating their approach.
17412,1,"If the deep network match the shallow one , this can be understood as a form of \u201cdistilled content \u201c loss? and the disc of the deep one will take care of the texture , style content? is this intuition correct? \n\n- original cyclic reconstruction constraint is in the pixel space using l1 norm usually, the regularizer introduced matches in a feature space , which is known to produce better results as a \u201cperceptual loss\u201d, can the author comment on this? is this what is really happening here, moving from cyclic constraint on pixels to a  cyclic constraint in a feature space  (shallow network)?\n\n-  *"
17413,1, \n\n- The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction.
17414,1,"\n\nThe distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes."
17415,1,\n\n*Clarity*\nThe explanation of the theoretical framework is not clear.
17416,1," On a conceptual level since S_c should be a PSD matrix it can be written as the square of some matrix, i.e. S_c = A_c^TA_c, then the Mahanalobis distance becomes (A_c z - A_c c)^T ( A_c z-A_c c), i.e. in addition to learning a projection as it is done in Snell et al, the authors now learn also a linear transformation matrix which is a function of the support points (i.e. the ones which give rise to the class prototypes)."
17417,1,"   The objects discovered could be discovered with mosaicing (since the background is static) and background subtraction. \n"""
17418,1,"  For example, the probability p_1 of 'pkoe' versus p_2 of 'zoze' as likely versions of 'poke' (as, say, the prefix of pokemon, as in your example) should be such that p_1 >>> p_2, not equal as they are in your model."
17419,1, This needs significantly more detail and explanation.
17420,1,"""The paper describes a method for detecting adversarial examples using a second detector classifier."
17421,1,"\n\nSummary:\n\u2014\u2014\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method."
17422,1," The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations."""
17423,1,"If it does actual data augmentation, it should perform better"
17424,1, Did the author experiment with a comparable architecture?
17425,1, It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet.
17426,1,"\nThe authors propose to first embed each image into a feature space, and then feed this learned representation into a auto-encoder that handles the projection to and from the semantic space with its encoder and decoder, respectively."
17427,1,"\n* Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD. "
17428,1,"\n\n4. Please, could you explain the last sentence of Sec. 4.3 that says \""The drawback here is that the agents will not be able to generalize to other unseen maps that may have very different geographies.\"" In particular, how is this sentence related to partial observability?"""
17429,1, The paper discusses the influence of the label redundancy both theoretically and empirically.
17430,1,\n[r1] CNNpack: packing convolutional neural networks in the frequency domain.
17431,1," There are some typos in it, e.g.:\n1st page, senstence --> sentence\n4th page, the the ..."
17432,1, It is fair to say that this paper contains almost no novelty.
17433,1, The running times are not in favor of the proposed method.
17434,1," I think the idea is intuitive and reasonable, the result is nice."
17435,1, RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.
17436,1,"The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing."
17437,1,"n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth)"
17438,1, The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.
17439,1,\n\nBreaking down the inference gap into its components is an interesting idea and could potentially provide insights when analyzing VAE performance and for further improving VAEs.
17440,1,"\n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims."
17441,1,"""This paper proposes a fast way to learn convolutional features that later can be used with any classifier."
17442,1," As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different."
17443,1,"\n\nFor the structured QA task, there are 400 training examples, and 100 named entities. This means that the number of training examples per named entity is very small."
17444,1,. How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network?
17445,1,"\n\n5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10."
17446,1,"  Again, your best performing system (using your new methods) has performance far below the worst reported competing system. "
17447,1,\n(4) MC methods tend to degrade less when the reward signal is delayed.
17448,1,"\n\nOverall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown."
17449,1,"""This paper presents a so-called cross-view training for semi-supervised deep models."
17450,1,"\n-The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for \u201cdiscriminativeness\u201d and seems like something that can be gamed."
17451,1,"\n(b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses,"
17452,1,"\n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads."
17453,1,"""This work proposes to replace the gradient step for updating the network parameters to a proximal step (implicit gradient) so that a large stepsize can be taken."
17454,1," It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration."
17455,1,"\n(ii) Given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages."
17456,1,"\n\n* \""aim to learn what parameter values of the base-level learner are useful\n  across a family of related tasks\""\n\nIf this is essentially multi-task learning, why not calling it so?  \""Learning\nwhat to learn\"" does not mean anything."
17457,1,"  They use a prior model proposed in Finn et al. 2016, make several incremental architectural improvements, and use an adversarial loss function instead of an L2 loss."
17458,1,"""The paper studies a combination of model-based and model-free RL."
17459,1,   Which notation in the figure corresponds to which variable is not clear at all.
17460,1,"  For all the text datasets, there were no comparisons with k-means on the features learned from the auto-encoders or clusterings learned from similar number of clusters."
17461,1,"\n\n1. It seems the concept of \""binarized activation patterns\"", which the proposed regularizer is designed upon, is closely coupled with rectifier nets."
17462,1,"""This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (\""n-grams\"") which can be queried efficiently."
17463,1, \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded.
17464,1, \n- What corpus did you use to pre-train word vectors?
17465,1,"\n- \""One benefit of the ARAE framework is that it compresses the input to a\n  single code vector."
17466,1, The model first encodes the document via tuple extraction.
17467,1," By adding more filters or layers in the model while keeping the same FLOPs and parameters, the models with the proposed method outperform the regular convolution models."
17468,1, A novelty of this work seems to be transforming a graph into an image.
17469,1, People have also specifically tried to train a siamese network and use its features as input to the SVM.
17470,1,"Again, Tamar et al. 2016 deals with this 3 points."
17471,1,"   I'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings, but still catches some errors."
17472,1,". Hopefully, the suggested studies will improve the quality of the paper in the future submission."
17473,1, This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model). 
17474,1,"""1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter."
17475,1," They ablate the choice of K, the binary nature of the gate weights."
17476,1, Experiments on 3 different tasks showcase the potential of the proposed method.
17477,1," For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features);"
17478,1,"\n- In fig. 5 left I cannot find the 4% and 8% higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs, as mentioned in section 5.1 anytime prediction results"
17479,1," Empirically, they show the performances are better than random feature and the LKRF."
17480,1," For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different."
17481,1,\n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors. 
17482,1,"\n\nOverall, this seems like a natural and effective approach, and achieves good results."
17483,1, Instead of performing the optimisation to find x' have you tried visualising the real data sample that gives the closest activations?
17484,1,"""This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization."
17485,1,"\n\nThe paper lacks novelty, just reports some results without proper analysis or insights."
17486,1, It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small).
17487,1,n\n2) The computational aspects of DFM are not clear in the paper.
17488,1,"""The authors did extensive tuning of the parameters for several recurrent neural architectures"
17489,1," Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations."
17490,1, but it is to the best of my knowledge the first explicit application of this idea in neural architecture search.
17491,1, This could probably more or less easily be added as a subsection using the current classification.
17492,1," In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category."
17493,1," Generally, the result is interesting and the presentation is easy to follow."
17494,1, but the repeated references to these tables suggest that these experimental results are crucial for the authors\u2019 overall points.
17495,1,\nThe exposition of the model architecture could use some additional detail to clarify some steps and possibly fix some minor errors (see below).
17496,1,"""I was asked to contribute this review rather late in the process, and in order\nto remain unbiased I avoided reading other reviews."
17497,1,\n\n- The first mention of partial observability can be moved to the introduction.
17498,1,"""Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks."
17499,1," \nThe authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator."
17500,1,"\n\nSection 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid)."
17501,1,\nThe proposed methodology is evaluated on some standard benchmarks in vision.
17502,1,\n(2)\tThe CW-SC kernel (Figure 2 (c)) is very similar to interleaved group convolutions.
17503,1,"\n3. No wall-clock time that show the speed of the proposed parametrization."""
17504,1, It circumvents the traditional tradeoff between search space size and complexity of the found models.
17505,1," However, many of dynamic systems are inherently discontinuous (collision/contact dynamics) or chaotic (turbulent flow)."
17506,1, What exactly is the purpose of this paper? 
17507,1,"\n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al."
17508,1, Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods.
17509,1, There is some originality in the proof which differs from recent related papers.
17510,1," With more experiments and interpretation of the model, including some sort of multilayer analysis, this can be a good acceptance candidate."
17511,1, For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice.
17512,1," The numbers in the tables are good but I have several comments on the motivation, originality and experiments."
17513,1,"\n\n+ the paper is well written, well organized and overall easy to read"
17514,1,\n\nCons:\n- missing background\n- missing ablations\n- missing details;
17515,1,"\n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. "
17516,1," It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work."
17517,1, To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word).
17518,1,\n* Is the assumption that \\sigma has Taylor expansion to order d tight?
17519,1,"\n\n(1) My biggest concern is about the motivation of the paper: \n\nFirstly, another popular approach to speed up reading comprehension models is hierarchical (coarse-to-fine) processing of passages, where the first step processes sentences independently (which could be parallelized), then the second step makes predictions over the whole passage by taking the sentence processing results."
17520,1,After a revision I would consider to increase the score.
17521,1,". From this paper's literature survey, they dont exist."
17522,1,"\n\nPage 2\n\""convolutional neural networks(CNN)\"" -> \""convolutional neural networks (CNN)[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""related works\"" -> \""related work \"""
17523,1,"""The article proposes to use dense skip-connections on the \""vertical\"" (between-layers) connections of recurrent networks."
17524,1, Can you make this more precise?
17525,1," Since L is NON Convex, it could not be automatically considered as bounded."
17526,1,"\nReferences:\n[1] Vincent Vanhoucke, Andrew Senior, and Mark Mao, \u201cImproving the speed of neural networks on cpus,\u201d in Deep Learning and Unsupervised Feature Learning Workshop, NIPS, 2011.\n[2] Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin, \u201cOn the efficient representation and execution of deep acoustic models,\u201d Proc. of Interspeech, pp. 2746 -- 2750, 2016.\n\n9. Minor comment: The authors use the term \u201cwarmstarting\u201d to refer to the process of training NNs by initializing from a previous model."
17527,1," Here, the sharing is essentially meaningless, because the variance of the weights in this derivation shrinks to zero."
17528,1,"\n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger."
17529,1,"\n- In second paragraph Section 4.3, it should be Table 1 instead of Figure 1."
17530,1, The contributions of the work are in general very limited.
17531,1,  It is not very clear to me how y^* is chosen in Section 3.3. Please make that part clear.
17532,1,\nThe results are only limited to single toy task.
17533,1," I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans."
17534,1, \n\nThe paper evaluates popular GAN evaluation metrics to better understand their properties.
17535,1,\n\n\nComments:\n\nThe paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed.
17536,1,"  It shows better generation errors by trust region methods than SGD in different tasks, despite slower running time, and the authors speculate that trust-region method can escape sharp minima and converge to wide minima and they illustrated that through some hybrid experiment."
17537,1,"""This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents."
17538,1," This means that the promise of rotation and scale equivariance holds up only along (x,y)."
17539,1, Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach
17540,1, the authors have failed to convey to me why this direction of research is relevant.
17541,1," By employing the cluster, this work propose a joint source/target modeling by varying how sampling is performed, e.g., draw independently or conditionally, and how the cluster are constructed, e.g., model-wise or non-model."
17542,1,That are the functions used?
17543,1," Are there any good automated metrics, and how well do they correspond to human judgement?"
17544,1," The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs."
17545,1," Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity."
17546,1,"\n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior."
17547,1,"  \n2) Considering this is an unsupervised domain adaption problem, how do you set the hyper-parameters lambda and the kernel width?"
17548,1," But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima."
17549,1," \n\n[1] Interleaved Group Convolutions. Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. ICCV 2017. "
17550,1, A few additional comments are as follows:\n\n  \u2022 The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches.
17551,1, \n\n2. The whole standpoint of the paper is quite vague and not very convincing.
17552,1,", as it gives some intuition about how much overlap might 'suffice'.  \n\nI recommend weak accept."
17553,1,"The paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network. "
17554,1," As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level."
17555,1,". Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points."
17556,1, A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged.
17557,1,"\n\nIn the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck)."
17558,1," For these reasons, I cannot recommend this paper for acceptance."
17559,1," Moreover, these tables do not even cite more recent higher-performing CNNs."
17560,1,\n\n- I think a bit more analysis is needed in section 4.2.
17561,1,"  As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations)."
17562,1,"Finally they find a \""a feature vector for each plate by summing up the output of the last recurrent layer overtime."
17563,1,"\n\n- page 3: \""(cite a couple of them)\"" should be replaced by some actual references :)"
17564,1," The justification given is that it is \""to address the difficulty of training due to the complex nature of the problem\"" but this is not really satisfying as the problems are not that hard."
17565,1,"  Specifically:\n- I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution."
17566,1,\n\nPros\n\nWell-written and original contribution demonstrating the use of GANs in the context of neuroimaging.
17567,1, The method is demonstrated on a toy example and on the task of unsupervised domain adaptation.
17568,1,"\""gives good performance\""\n- \""Recent works\"", \""several works\"", \""most works\"", etc.-> "
17569,1," When the system is learned end-to-end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree. And this is not by showing final performance on a game."
17570,1,\n\n2- How sensitive is the results to the number of -1 in the diagonal matrix?
17571,1,"""In this paper, the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs, in particular when they are conditionally independent ('factored')."
17572,1, The game is parameterized with the rewards of +1 and +3.
17573,1,"  \nE.g., the statement at the end of 2.1 is unclear at that point in the document. [[CNT], [CLA-NEG], [CRT], [MIN]] How is the PIR drawn exactly?"
17574,1,"\n\nI think this is good and potentially important work,"
17575,1,"\na. Sigma-Delta model of spiking neurons has a long history in neuroscience starting with the work of Shin. Please note that these papers are much older than the ones you cite: \nShin, J., Adaptive noise shaping neural spike encoding and decoding. Neurocomputing, 2001. 38-40: p. 369-381. \nShin, J.,The noise shaping neural coding hypothesis: a brief history and physiological implications. Neurocomputing, 2002. 44: p. 167-175."
17576,1, These functions are trained to maximize expected squared jump distance.
17577,1, however the current framework has several weaknesses:\n1. The whole pipeline has three (neural network) components: a) input image features are extracted from VGG net pre-trained on auxiliary data;
17578,1,"\n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case."
17579,1, The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing.
17580,1, The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.
17581,1," For instance, is designing alternative loss function useful in practice? "
17582,1, You show that it outperforms simulated annealing. Is this the state of the art?
17583,1," 2) solid theoretical ackground,[[CNT], [null], [APC], [MAJ]] even if no methodological novelty has been introduced (this is also a cons!)"
17584,1,"  Besides, as Veit showed, ResNet also shows ensemble behavior."
17585,1,  Do the papers that prove similar theorems about ConvNets able to handle general L
17586,1," The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are \""begin learnt\"", just from different datasets."
17587,1,"\n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention."
17588,1,"  In particular, they look at the problem of maze testing, where, given a grid of black and white pixels, the goal is to answer whether there is a path from a designated starting point to an ending point."
17589,1,Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier.
17590,1,\n\nResult interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used.
17591,1," For instance, it could describe more the advantages over reinforcement learning."
17592,1," Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n"""
17593,1,"\n\nFinally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently."
17594,1, Which kernel is better and how to choose between them in a deep network?
17595,1," Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2."
17596,1,"\n\nHowever, this is definitely not the first work on semi-supervised formed few-shot learning."
17597,1,"\n3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified."
17598,1," Specifically, the authors propose using approximate posteriors shared across groups of examples, rather than posteriors which treat examples independently."
17599,1,"\n\n(4) there is some vague connection to random matrices, with some limited experiments that are consistent with this observation but far from establish it, and without any theoretical analysis (Martingale or Markov chain theory)"
17600,1,\n* Section 4.3. The generation process is adapted for generating trees which seems to be cheating.
17601,1," Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives."
17602,1,\nHow is this actually done in practice?
17603,1," The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method."
17604,1," \n- As the proposed method was successful for the QA task,"
17605,1,"  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task."
17606,1," This can be seen in some of the illustrative examples in Figure 5: there *is* a coarse-scale positive curvature, but this would not necessarily come through in a quadratic model fit using the hessian."
17607,1," \n(vi) \""Exponentially large\"", \""reasonably prior\"" model etc. is very vague terminology"
17608,1,"\n\n3. When choosing the orthogonal matrix, I think one obvious choice is to sample a matrix from the Stiefel manifold (the Q matrix of a random Gaussian). This baseline should be added in additional to H and B."
17609,1, What are the theoretical advantages of using reservoir sampling?
17610,1," \nFor this purpose, authors employed a reinceforcement learning approach to\noptize a prediction from masked text."
17611,1, For this reason I cannot recommend publication as I am not sure how to evaluate the paper\u2019s contribution.
17612,1,"\n\n- Related to the above, it should be clarified what is meant by dropping a\n  class."
17613,1,"\n\n* The word \""layers\"" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder."
17614,1,"\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016)."
17615,1," For a reference, see Peters, Janzing, Scholkopf: Elements of Causal Inference: Foundations and Learning Algorithms (available as pdf), Definition 6.32."
17616,1,\n\n\n3. High level technical\n\n- I'm confused at the first and second lines of equation (19).
17617,1,"\n - To provide a fair comparison authors, should compare their results with other paper results."
17618,1,   This seems to be the fundamental question -- do random projects help in the train_D | test_C case?
17619,1,  \nIn addition a relaxation is performed allowing each constituent to deviate a bit from unitarity (\u201csoft unitary constraint\u201d).
17620,1, Transferability of the learned low precision models to other tasks is not discussed\n\n
17621,1,"  Expanding out, each term is a product of some of the r_i and some of the identities I."
17622,1,"""1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection\n2."
17623,1,\n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.
17624,1,\n\nDoes the unit norm normalization used to construct the covariance disallow ARD input selection?
17625,1," For example, it is unclear how the performance of the presented quantization algorithms compares to say  QSGD [1] and Terngrad [2]."
17626,1,\n\n[Pros]\n- The paper is easy to follow.
17627,1,"\nIs it possible to perform the CP decomposition by minimizing the activation reconstruction loss (like proposed by Zhang et al. 2016), and not the tensor reconstruction loss (as usual)?"
17628,1,\n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other.
17629,1," I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations."""
17630,1," Specifically, the author argue that the moment-generating function for the pointwise kernel approximation error of ORF features grows slower than the moment-generating function for the pointwise kernel approximation error of RFM features, which implies that error bounds derived using the MGF of the RFM features will also hold for ORF features."
17631,1,". The authors have started with right motivation and the initial section asks the right questions, however,"
17632,1,"\n- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data."
17633,1, but you should be careful to distinguish this type of method from other approaches.
17634,1, are the results significantly different?
17635,1, The authors define a Wasserstein Distance Network to find  a suitable affine transformation that reduces the nuisance factor.
17636,1, I think a much more complicated data would be more interesting.
17637,1,"""EDIT: The rating has been changed. See thread below for explanation / further comments."
17638,1,"\n- I understand that there is no mixing in the test phase, perhaps it would be useful to recall it."""
17639,1," It is not clear whether a different MLP is used for different channels and for different layers, to predict the filter weights."
17640,1,"""Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur."
17641,1," The problem being solved is not literally the problem of decreasing the amount of data needed to learn tasks, but a reformulation of the problem that makes it unnecessary to relearn subtasks."
17642,1," In the abstract, what does \u2018two-pass decomposition\u2019, \u2018proper ranks\u2019, \u2018the instability problem\u2019, or \u2018systematic\u2019 mean?"
17643,1," To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages."
17644,1," Since the graph presentation does not use color channels,  pre-trained model is used different from what it was designed to."
17645,1," It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm."
17646,1,\n\nCons:\n- This work may be better reviewed at a more GPU-centric conference
17647,1, The proposed method is then tested on two image data sets.\n\n
17648,1,"  So the simpler the structure of this latent space, the easier it should be to train a goal function, and hence quickly adapt to the current reward scheme. "
17649,1," But this makes the train-test split described in 4.1 rather odd, since there is no overlap of subjects in the train-test split. We need clarifications on these experimental details."
17650,1,\nHow does the likelihood of the model behave under the circumstances?
17651,1, Typos didn't influence reading.
17652,1," Therefore I've upgraded my rating, and due to better understanding now, als my confidence. """
17653,1,"\n\nIn short, the only comparable dataset is CBT, which has too low accuracy compared to a very simple baseline."
17654,1," Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting."
17655,1, They find that the complex-valued networks do not in general perform better than real-valued networks.
17656,1, Could authors please clarify this?
17657,1,"  They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic)."
17658,1," \nIn some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length."
17659,1," The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \""strong\"" penalty parameter."
17660,1,"  Cars follow a fixed model of behavior, do not collide with each other and cannot switch lanes."
17661,1, I found the yelp transfer results particularly impressive.
17662,1," Maybe you can verify this experimentally (as much of the paper consists of experimental findings)\n(iii) Eq. 13. Maybe you want this form to indicate a direction you want to move towards,  by I find adding and subtracting the gradient in itself not a very interesting manner of illustartion."
17663,1,\n\nSome things which could limit the significance of the work:\n1) The paper does not provide a way of measuring the (approximate) evidence of a model.
17664,1, I would enjoyed more digging in this direction.
17665,1,  That is to say that I still have some reservations (though less than I had before).
17666,1,\nUsually the size of a covariate would be the dimensionality.
17667,1," This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form."
17668,1,"\n- The plots in Section 6 are interesting, it would be nice if the authors had an explanation of why the loss surface changes the way it does when skip connections are added."
17669,1," If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable? "
17670,1,  Chebyshev polynomial is used for faster computations.
17671,1, \n\n4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3).
17672,1,"  \n\nThere is a lot of white space around the plots, which could be used for larger more clear figures."
17673,1, In my view this particular result is an immediate corollary of the result for linear networks.
17674,1,\n\n--Sub comment\nNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result.
17675,1, for which the reason could be a multitude of issues probably related to hyper-parameter tuning.
17676,1, The proposed approach does not rely on attention based model.
17677,1," It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side."
17678,1," \n\nSection 4, while interesting, appears to be somewhat disconnected from the rest of the paper..[[CNT], [EMP-NEG], [CRT], [MIN]] \n\nIn Theorem 2.3. explain why the two layer case is limited to n=1.."
17679,1, The experimental results show that the model is able to achieve the state-of-the-art performance on CNN/Daily Main and New York Times datasets.
17680,1,"  \n- Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful."
17681,1," The sentence that \""the influence of the prior diminishes as the size of the training data increases\"" is debatable for something as over-parametrized as a DNN."
17682,1," Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum."
17683,1," Maybe add a sentence regarding the large/Small eigenvalues?"""
17684,1,\n+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks
17685,1,"\n- Besides, how are D and W exactly defined?"
17686,1,"  In fact, it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation, or, frankly, the way in which the experimental results are described."
17687,1, \n- I don\u2019t understand the objective function (eq 4 and 5).
17688,1, \n(ii) What is a one-hot label?
17689,1,"\n - p.8. For various reasons, the coreference results seem less useful than they could have been,"
17690,1, \n\nThe main issue with this paper is novelty.
17691,1, Dividing them makes more complicated to \ndraw general conclusions from this particular data augmentation setting.
17692,1,"""Summary\n - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism."
17693,1,  The motivation is clear and the idea is simple and effective.\
17694,1,"n\n- The paper should include more analysis of how this method helps interpret the\n  actions of the neural net, once the core units have been identified."
17695,1," Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal."
17696,1," The authors argue that it is not a data augmentation technique, but rather a learning method."
17697,1," Moreover, there is no link between notations used for the swan part and the ones used in the reordering part."
17698,1,". Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time."
17699,1,"""Here are my main critics of the papers:\n\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)?"
17700,1," Consistent use of the A, T, and S' dataset abbreviations would help."
17701,1,"""This paper describes AdvGAN, a conditional GAN plus adversarial loss."
17702,1, \nA gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent.
17703,1,\n\n\nminor comments\n---------------\n\n* the paper employs vocabulary that is not common in ML.
17704,1, And using the procedure to test and improve current version of GANs. It demonstrated some interesting stuff.
17705,1, The author also reports state-of-art results.
17706,1, Is there any difficulties with power law graphs?
17707,1," I would have appreciated a comparison to other methods for guiding discriminator representation capacity, e.g. autoencoding (I'd also imagine that learning an inference network (e.g. BiGAN) might serve as a useful auxiliary task?)."
17708,1,\n- The relation with other recent work in entropy-regularized RL should be expanded.
17709,1," \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data."
17710,1," Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?"
17711,1, Convolutions/downsampling-convolutions/upsampling that are demonstrated in the paper basically boil down to function prediction on the same exact global graph.
17712,1,\n  The text unfortunately does not explain why the loss function looks so vastly different\n  with different look-ahead.
17713,1,"\n- Not clear what \""Ensure\"" means in the algorithm description."
17714,1,"\n\nMinor:\n-            Sec 4.1 \u201c\u2026each integral in the sum in (2)\u2026\u201d eq 2 is a product\n"""
17715,1,"\n\nComments:\n1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5)."
17716,1," \nIf the expert\u2019s first few transitions were easily approximable,\nthe learner would get local rewards that cause it to mimic expert behavior."
17717,1,"\n\nThird, TripAdvisor: the dataset paper by Wang et al. (2010) is not evaluated on accuracy (rather on ranking, etc.)."
17718,1, The inception score fails to capture this property. 
17719,1,"\n- The rest of the comments are below:\n\n- 3.1: I got a bit confused over what X actually is:\n -- \""We would like to learn a generative model for **sets X** of the form\""."
17720,1,\n \n- the two losses introduced are not really new.
17721,1,"\n\nConcerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion."
17722,1,"\nFinally, extensive experiments are conducted and they are in accordance with the theory."
17723,1,"""This is quite an interesting paper. Thank you."
17724,1," These results seem conflicting, or at least raise more questions than they answer."
17725,1, There aren't any new technical ideas here.
17726,1,\n(b) generating adversarial examples for image classification as well as text analysis.
17727,1," \n\u2013 The use for a genetic algorithm for optimization is not motivated, and no comparison is made to the performance and efficiency of other approaches (like standard backpropagation)."
17728,1,\n- Novel task of generating UI code from UI screenshots
17729,1," Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches."
17730,1,"\n\nThe consistency properties are nice,"
17731,1," Empirical performance is shown on 3 downstream tasks: Product-type classification, Sentiment Classification and Aspect Extraction."
17732,1, Is there a way to avoid this?
17733,1,"\n\nDespite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR."""
17734,1," \n\nThe proposed idea is not exceptional original,"
17735,1,"""This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images."
17736,1,\n\n- No clear novelty
17737,1," This method is not learning-based, doesn't need training data in a simulator, generalizes to **any** exit and lane configuration and variants of this basic technique continue to be used on real-world autonomous cars."
17738,1, but the model needs further development.
17739,1,"""This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017)."
17740,1,"""This paper proposes a new method for estimating optimal transport plans and maps among continuous distributions, or discrete distributions with large support size."
17741,1," \nHowever, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart. "
17742,1," The notation is not much used after sec. 3.1, for example, figure 1 does not use it."
17743,1,"\n\nAlso, I am a bit disappointed by how \u201ccascades\u201d are actually implemented."
17744,1,\n\nThere are no fancy new methods or state-of-the-art numbers in this paper.
17745,1,  Which parameters are fit using the fine-tuning loss described on page 3?
17746,1," Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results."
17747,1, \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model? 
17748,1,  The authors present a very intriguing novel approach that  in a clear and coherent way. 
17749,1,"\n(iv) I am not sure in whoch way g is \""measured\"", but I guess you are determining it by comparing coefficients. "
17750,1,"\n\n5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully.\n"""
17751,1,"""This work proposes an LSTM based model for time-evolving probability densities."
17752,1," However, the previous settings can be reinterpreted in the authors setting."
17753,1,"\""\nI do not agree with them: latent representations from RNN are fine-grained & context dependent; latent representations from Socher et al. are also fine-grained & target dependent: the position in the latent space -modeling context- has an impact on the estimated sentiment."
17754,1," In this paper, the authors proposed a conditional way to generate images compositionally."
17755,1, Too early to tell about significance.
17756,1," For example, \n- Section Method, equation (4)"
17757,1," The data is derived from an online repository of ~1500 Android apps,"
17758,1,".\n\nOverall, it\u2019s nice a nice study of the query completion application."
17759,1," The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.\n"""
17760,1," The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines."
17761,1," To see why, suppose that the options were constructed based on data collected by a non-random policy that only visits a subspace of the state space."
17762,1, It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.
17763,1,"  Does the occupancy grid account for sensing limitations (e.g., occlusions)?\n\n"""
17764,1,\n- There are many spelling errors
17765,1,"\n\nFor the newly added case of VAN(lambda=0), please note the strong similarity to https://arxiv.org/abs/1611.01260 (ICLR2017 reviews at https://openreview.net/forum?id=Sywh5KYex)."
17766,1," At any rate, it is rather unconvincing when the only evidence you cite is the paper by Sharan and Valiant, where AFAIK, neither author has ever published an NLP paper or attended  an NLP conference....\n - p.5 FEC should be FCE; Ng et al. (2014) should be (Ng et al., 2014)\n\n\n"""
17767,1," As a result, the results are not suggesting significance or generalizability of the proposed method."
17768,1, \nHowever the writing is not very clear and the paper is not self-contained at all.
17769,1,\n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals.
17770,1, Even the datasets are outdated (from the 90s?).
17771,1, Any differences in terms of theory and experiment?
17772,1," \n\nI am not an expert in this area, so it is difficult for me to judge the technical merit of the work."
17773,1,\n\n[Weakness]\n\n1. The definition of explicit grounding is a bit misleading.
17774,1," I acknowledge that some real human demonstrations are used but there is not much about them and the experiment is very shortly described. """
17775,1,\n\nAlso I found the symmetric matrix assumption on A is quite limited.
17776,1," They also do not require pre-training and re-training, but just a single training procedure."
17777,1, otherwise I don't actually have any comments on the text.
17778,1,\n+ Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches.
17779,1," This assumption is not very natural \u2014 though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels."
17780,1, I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever.
17781,1,"\n- a network predicting a polar origin,"
17782,1,  Please strengthen the motivation and originality of the paper.
17783,1,"\n\nTypos / Form\n------------------\n\n1. sct 1, par 3: \""using Householder reflection vectors, it allows a fine-grained\"" -> \""using Householder reflection vectors, which allows a fine-grained\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n2. sct 1, par 3: \""This work called as Efficient\"" -> \""This work, called Efficient\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n5. sct 1, par 5: \""At the heart of KRU is the use of Kronecker\"" -> \""At the heart of KRU, we use Kronecker\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n6. sct 1, par 5: \""Thanks to the properties of Kronecker matrices\"" -> \""Thanks to the properties of the Kronecker product\""\n7.[[CNT], [CLA-NEG], [CRT], [MIN]]  sct 1, par 5: \""vanilla real space RNN\"" -> \""vanilla RNN\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n8. sct 2, par 1: \""Consider a standard recurrent\"" -> \""Consider a standard vanilla recurrent\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n9. sct 2, par 1: \""step t RNN\"" -> \""step t, a vanilla RNN\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n11. sct 2.1, par 1: \""U and V, this is efficient using modern BLAS\"" -> \""U and V, which can be efficiently computed using modern BLAS\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n12. sct 2.3, par 2: \""matrices have a determinant of 1 or \u22121, i.e., the set of all rotations and reflections respectively\"" -> \""matrices, i.e., the set of all rotations and reflections, have a determinant of 1 or \u22121.\""\n13. sct 3, par 1: \""are called as Kronecker\"" -> \""are called Kronecker\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n14. sct 3, par 3: \""used it's spectral\"" -> \""used their spectral\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n15. sct 3, par 3: \""Kronecker matrices\"" -> \""Kronecker products\""\n[[CNT], [CLA-NEG], [CRT], [MIN]]18. sct 4.4, par 3: \""parameters are increased\"" -> \""parameters increases\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n19. sct 5: There is some more typos in the conclusion (\""it's\"" -> \""its\"")[[CNT], [CLA-NEG], [CRT], [MIN]]\n20. Some plots are hard to read / interpret, mostly because of the round \""ticks\"" you use on the curves."
17784,1," Either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf."
17785,1,"\n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE."
17786,1,\n\nClarity:\n\nThe paper is clearly written.
17787,1,"\n\n1.  The paper misses some more recent reference, e.g. [a,b]."
17788,1,"\nClarity: this paper is clear,"
17789,1,"\n\nThe paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic."
17790,1, The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence).
17791,1,\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning.
17792,1," Namely, the following questions need to be answered:\n\n1. Does using linked-word-pairs truly raise the state of the art?"
17793,1,\n\nMy first remark regards the presentation of the technique.
17794,1,"\""\n\n* I didn't find Table 1 particularly illuminating."
17795,1, Please elaborate on this.
17796,1," Looking at fig 3, its clear the kangaroo is skewing the results, and that overall the new method is performing worse."
17797,1, The final optimization problem that is used for training of the propose VAE should be formally defined.
17798,1, No caption or figure number is provided.
17799,1, \n\nI find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity.
17800,1,"""The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach"
17801,1,\n\nCons\n------\n- kennen-i seems like it couldn't be realistically deployed\n- lack of an intermediate difficulty task\n
17802,1,\n- Can't see why SEARNN can help with the vanishing gradient problem.
17803,1, I am not aware of any evidence for this.
17804,1,\n\n3 The authors proposed a path-wise training procedure to reduce memory requirement in training.
17805,1," Moreover, to justify the effect of the randomness, the paper should have empirical experiments."
17806,1," However, I don\u2019t see any demonstration of this in the experiments section."
17807,1," \n(v) Why is an incorrect \""pseudo\""-set notation used instead of the correct vectorial one?"
17808,1, I think this is a key experiment that should be run as this result would be much easier to compare with the other methods.
17809,1,\n\nPros:\n- The presentation is clear and easy to follow.
17810,1,"\n\nIs there a specific use scenario for the prefix splitting, other than for the evaluation of unseen prefixes?"
17811,1," However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance."
17812,1," The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path."
17813,1,"\n\nIntroduction\n- The introduction to the paper could be made clearer - the authors talk about the language of captioning datasets being repetitive, but that fact is neither used or discussed later."
17814,1," Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?"
17815,1, A discussion on the difference would be helpful.
17816,1,\n \n\u201cQ_it maximizing the equation is designated as the optimal price.\u201d Which equation?
17817,1," However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective."
17818,1, The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers
17819,1," For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training."
17820,1, This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models.
17821,1," In Balestriero & Baraniuk, it is shown that any DNN can be approximated via a linear spline and hence can be inverted to produce the \""reconstruction\"" of the input, which can be naturally used to do unsupervised or semi-supervised learning. "
17822,1, This I do not think is a reference to a facial alignment method bu t rather a set of general purpose linear algebra methods.
17823,1,\n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain.
17824,1," The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.\n\n\n\n\n\n\n\n\n\n"""
17825,1, The performance is investigated in terms of wall clock time. 
17826,1, Training in this plot is very unstable.
17827,1, No theoretic results are present to support this design.
17828,1, \n\n***************\nUpdates: \n***************\nThe authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper.
17829,1,. The proposed model is novel but incremental comparing to existing frameworks.
17830,1,\n\n4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy?
17831,1,. The results are interesting.
17832,1," Even though the significance of the work is apparent given the good results of the proposed neural network,"
17833,1, If so they should clarify this otherwise it confuses the reader a bit.
17834,1," For instance, what is s, r, t."
17835,1,"  If not, what is the difference between s_0:m and s_1:m?"
17836,1," Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?"""
17837,1,\n\nIs this work extending the applicability of baselines to new types of problems?
17838,1," I therefore disagree with the conclusion that this paper has shown that S-rRBFs are \""comparable to the best performer for most of the diverse benchmark applications\"" (last paragraph in Conclusion)."
17839,1, Is that correct?
17840,1," Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail."
17841,1,"\n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain."
17842,1,"\n\nAs s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2."
17843,1, That said I found the intuitive story a little bit difficult to follow;
17844,1," Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation."
17845,1," Unfortunately, I do not think their scope (single synthetic, plus a single UCI dataset) is sufficient."
17846,1, Please check the minor comments.
17847,1, \n\nUnfortunately the evaluation falls short.
17848,1,  The authors have made their synthetic dataset publicly available.\
17849,1," The mechanism is called \""bias-corrected moment estimate\"" in the Adam paper, arXiv:1412.6980."
17850,1," It may be the case that even a \""bad\"" generative model (according to some other metric) can still result in a classifier that produces reasonable test accuracy."
17851,1," I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?"""
17852,1,    \n       \nIn general I like the idea and I believe that it can lead to a very useful model.
17853,1,\n\n(2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter \\alpha (although only fixed value is used in the experiments) to control the negative saturation zone.
17854,1, The clustered representations are the visual concepts.
17855,1,\n\n(7) In Definition 2.3: What does the parenthesis in \\phi_j(1) denote?
17856,1,"\n\nTherefore, it is not clear that the proposed methods improve over previous approaches."
17857,1,"\nAs far as I remember, there exists also some paper from the nineties that\nlearn the parameters of RBF networks but unfortunately I have not been able to\ngoogle some of them."
17858,1," I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer."""
17859,1,"""This paper views graph classification as image classification, and shows that the CNN model adapted from image net can be effectively adapted to the graph classification."
17860,1, \n\nThe dev perplexity quoted in Section 4 for a 5 gram LM is very high.
17861,1," You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates."
17862,1," Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation."
17863,1,The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers?
17864,1,\n\nExperiments\n\nYou should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training).
17865,1," \n\nNovel: I am not familiar enough with adversarial deep learning to assess novelty or impact. """
17866,1,\n\nQuality: SeaRnn is a well rooted and successful application of the L2S strategy to the RNN training that combines at the same time global optimization and scalable complexity.
17867,1,"\nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin."
17868,1,"\n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments."
17869,1,"\""\n\n> Table 1\n\nWhy is the second best method on CIFAR (\u201cHier. repr-n, random search (7000 samples)\u201d) never tested on ImageNet? "
17870,1,  I've described many of the points I was confused by in more detailed comments below.
17871,1, The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix.
17872,1, They also should compare to Strelka whic\nh interestingly they included only to make final calls of mutations but not in the comparison.
17873,1," This is unfortunate and will certainly confuse readers; I advise carefully changing this throughout the entire paper (eg Algo 2,3,4, eq 1, last eq in sec3.1, eq in text below eq3, etc)\n\n*"
17874,1," Similar idea has been explored in early 2000 by Finley and Joachims in their ICML paper titled \""Supervised Clustering with Support Vector Machines\""."
17875,1, Did these hyper-parameters also include the sizes of the models? 
17876,1,\n+ The authors show that one of the existing methods can fairly successfully fool humans to believe its synthetic results are actual human faces.
17877,1, The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards.
17878,1,"\n- In general, the graphs are difficult to read; fonts should be improved and the graphs polished."
17879,1,"\n\n-I feel the model is seems overly complicated for the small gain (i.e. 75.7->76.0 Exact Match), especially on a relatively exhausted dataset (SQuAD) that is known to have lots of pecularities (see anonymous comment below)."
17880,1," In section 3.3 part Ablation Study on Features Sets, line 5, the sentence should be \u201cAp are more important than HP\u201d.\n"""
17881,1," The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework."
17882,1," Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G."
17883,1, The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree.
17884,1, \n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations.
17885,1," The paper should make this connection to the literature more clear and discuss what is missing in our existing understanding of this case, to motivate your work."
17886,1, but the idea is not fully explored (in terms of reasonable variations and their comparative performance).
17887,1," As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed."
17888,1, any comment on this?
17889,1,"- as stated in the related work section, effectively the same convnet+RNN architecture has been in common use for image captioning and other vision applications."
17890,1,"  If the rank is > 1000 I wonder how meaningful it actually is.[[CNT], [null], [DIS], [MIN]]  For the usual analogies task, you can usually find what you are looking for in the top 5 or less.[[CNT], [null], [DIS], [MIN]]  \n\nIt seems that table 1 is the only evaluation of the proposed method against any other type of method (glove, which is not a tensor-based method)."
17891,1," \n\nFirst of all, the improvement is rather limited."
17892,1,  \n\nSecondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013).
17893,1,\n\nI personally miss a more technical and detailed exposition of the ideas.
17894,1," It's true that Le and Zuidema take a parse forest from an existing parser, but it still contains an exponential number of trees, as does the work in here."
17895,1,\n\nThe paper is very hard to read.
17896,1,"""In this work, the objective is to analyze the robustness of a neural network to any sort of attack."
17897,1,\n\nI will be happy to revisit the rating if the experimental section is enriched.
17898,1,"""SIGNIFICANCE AND ORIGINALITY:\n\nThe authors propose to accelerate the learning of complex tasks by exploiting traces of experts."
17899,1," It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.\n"""
17900,1,"\n\n-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as\n\nM."
17901,1," Why spend so much time on a step-by-step derivation anyway, as this is all \""classic\"" and has been carried out many times before (in a cleaner write-up)?"
17902,1, So the novelty is limited.
17903,1,\n- the second GAN solution trained on reverse codes from real data is interesting 
17904,1,\nline 6: Decay O(1/t^\\beta). This is indeed vague albeit easy to understand.
17905,1,\nSince the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary. 
17906,1, A domain-level translation function is jointly optimized with an instance-level matching objective.
17907,1,It seems that it would be naturally to model the baseline by including mean values in the model rather than treating the baseline as a preprocessing step.
17908,1,"\n- It would have been convincing to see an experiment showing actual use of the proposed method for navigating the face space, e.g. for finding criminals based on a description."
17909,1,\n\nHow the 4000 hypernyms have been selected?
17910,1,\n\n2. Non attribute part (Z) is explicitly modeled in the framework.
17911,1," This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation."
17912,1, This raises the question whether the named entity table can only work in this context.
17913,1," Indeed, (a) I mentioned in the previous paragraph the issues with the symmetric CP decomposition and (b) although the paper is motivated by the recent algorithm proposed by Sharan&Valiant (2017), the algorithms proposed in this paper are not based on this or other known algorithms with theoretical guarantees."
17914,1,"  (you don't need them, but also why number if you never refer to them later?"
17915,1,.\n* The overall idea is interesting and has many potentials.
17916,1, They also show that they can train deep residual network architecture on CIFAR without the use of BN.
17917,1," \n\nFurther, the phenomenon is only tested on random initialization. When the network is trained for several iterations and becomes more settled, it is not clear how \""angle affect\"" affects gradient vanishing problem."
17918,1," The paper does use skip connections, but the difference is that they are phased out over training."
17919,1,".\n  Please see the paper:\n  -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)\n  This paper shows that the performance gets even better for higher dimensional data if each feature is relevant."
17920,1,"\"" Why then try to link \""meanings\"" to basis vectors for the rows of A?"
17921,1,"\n2. Based on mixture assumption, I suggest the author add one more comparison to other method,"
17922,1," \n\nIn the last paragraph, beginning with \""Note that this is essentially saying...\"", I don't agree with the argument that the \""base embeddings\"" decompose into independent topics."
17923,1,"""This paper presents an embedding algorithm for text corpora that allows known\ncovariates, e.g. author information, to modify a shared embedding to take context\ninto account."
17924,1,"  However, this doesnt seem to be the case, given that it is impossible to construct VC activations for specific images from this definition."
17925,1, \n\nNotes:\n- I find the description of the Q_MC method presented in the paper very confusing and had to consult the reference to understand the details.
17926,1,"\n\n2. Experiments do not evaluate run time, memory use, computational complexity, or stability."
17927,1, The observation on multiplicative compositionality is the main strength of the paper.
17928,1, It is wrong to cite (Boser et al 92) for the soft-margin SVM.
17929,1," \n- In eq.(3) Choosing \\sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol."
17930,1, The attack model involves a standard l_inf norm constraint.
17931,1,"  Sometimes predictions are described as the output of SPENs (Tables 2, 3, 4, and 7), sometimes as inference networks (Table 5), and sometimes as a CRF (Tables 4 and 6). "
17932,1,  The connexion to PSRNN is very tenuous since the main results are about the regression part.
17933,1," As such, I do not really see any real grounds for acceptance."
17934,1,"\n1) The novel criteria are not compared with existing ones [Goodfellow et al. 2009, Lenc & Vedaldi 2015, Cohen & Welling 2014, Jayaraman & Grauman 2015]."
17935,1, \n\n\nMinor remarks\n- Table 1: the reported results of the Matching Network are different from the results in the paper of Vinyals (2016).
17936,1,"\n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \""Improved deep reinforcement learning for robotics through distribution-based experience retention,\"" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952."
17937,1, It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches.
17938,1, Such optimizations are very common in deep learning field while less known in the field of security.
17939,1,\nA (not too large) translation of the input image therefore does not change the log-polar representation.
17940,1, \nThese are all points that require discussion which is currently missing from the paper.
17941,1, But certain kinds of safety constraints like 'do not drive in the blindspot of other vehicles' sometimes require the ego car to speed up for a bit beyond the speed limit to pass the blindspot area and then slow down.
17942,1,"  However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one."
17943,1,\n\n2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid.
17944,1,"""This paper studies the control of symmetric linear dynamical systems with unknown dynamics."
17945,1," For instance, one can find SOTA results for CIFAR100 around 16% and for CIFAR10 around 3%. Similarly, one can find SOTA results for IWSLT2015 around 28 BLEU"
17946,1," \n\nWith regard to the learning rule: while the rule is formulated in terms of spikes, it should be noted that for neuron with many inputs and outputs, this update will have to be computed very very often, even for networks with low average firing rates."
17947,1,  This isn\u2019t too surprising.
17948,1, but I do not agree that other techniques like Dropouts are not useful.
17949,1,"\nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this?"
17950,1," In this light, the performance relative to the baselines is particularly important."
17951,1, \n\nExperiments on the IHDP dataset demonstrates the advantage of the proposed\napproach compared to its competing alternatives.
17952,1,". What happens if you instead treat it as a regression task, would it then be able to hint at intermediates (a batch size of 96) or extremes (say, 512).\"
17953,1, Are you thinking of the gradient coming back through PATH as a reward signal?
17954,1, This is an unfair comparison
17955,1,... I'd want to understand how/why and whether we should expect this universally.
17956,1, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance
17957,1, but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice.
17958,1, The experiments are OK 
17959,1, Was sharpening used in the image caption generation task?
17960,1," There is no clear explanation of what exactly the authors want to achieve in the paper, what exactly is their approach/contribution, experimental setup, and analysis of their results. "
17961,1,"\n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W."""
17962,1," Three NN architectures are explored, which leverage program semantics rather than pure syntax."
17963,1," External/internal?\n \n\u201cSpecifically, it is applicable to various methods as described below \u2026\u201d Related papers should be cited."""
17964,1, Reporting that would help interpret the numbers.
17965,1,"\n\nThe text says that rand+cegis selects 70% of examples of the proposed approach,\nbut figure 4 seems to suggest that the numbers are very close -- is this initial\nexamples only?"
17966,1,". I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: \""SMD trajectories (red) during meta-optimization of initial effective ...\""."
17967,1,"""The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,"
17968,1,"\nIdeally, the train and test urls would also be different in time."
17969,1," However, such assumption limits the generality of the work."
17970,1,"""This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization."
17971,1," Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper."
17972,1,"\n\nThe number \""124x ESS\"" in sec5.1 seems at odds with the number in the abstract, \""50x\""."
17973,1,"Once getting into the details of the proposed approach, \nthe quality takes a downturn, unfortunately."
17974,1,"\n- Section 4 and Alg 1: S we do not really care about the \""labels/targets\"" of the examples."
17975,1,It would also be interesting to understand more fully how performance scales to larger networks.
17976,1, They also claim that this analysis can be used to improve the performance of the models.
17977,1,  Is there any way to extend the current results to non-symmetric systems?
17978,1,"""This paper introduces a new design of kernels in convolutional neural networks."
17979,1," Intuitively from Figure 5, the network generates black images (i.e. all values close to zero) whenever the attention is on no entity and, hence, when attention is on an entity the latent space represents only this entity and the image is generated only showing that particular entity."
17980,1,\n\n- The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives).
17981,1,\n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class.
17982,1," This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation."
17983,1, In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?).
17984,1, The analysis with the toy network is interesting and helps illustrate the method.
17985,1,"  In particular, they exhibit some high frequency artefacts."
17986,1, It could be that this paper belongs to another venue that is more appropriate for survey papers.
17987,1," \n-- Figure 9,10, need to specify which noise model was used.\n\n\n\n\n\n"""
17988,1, Visual analysis of the attention map is convincing.
17989,1,"""This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally."
17990,1,"\n\nCons:\n1) Experimental evaluation is very substandard. The experiments on invariances seem to be the highlight of the paper, but they basically do not tell me anything. "
17991,1,\n\nFigure 1: Use the same y-axis scale for all subplots (if possible) to simplify comparison.
17992,1, \n\nThe proposed approach is well founded and the experimental evaluations are promising. 
17993,1, \n\nThe work is put in context and related to some previous relaxation approaches to sparsity.
17994,1,\n\n== Evaluation \n\nI found this paper quite clear to read and enjoyed reading it.
17995,1, The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations).
17996,1, It\u2019s better to discuss about this paper as well as a reference.
17997,1, The weight assigning (write) network is optimized for maximize the expected rewards.
17998,1, Is that correct?
17999,1,"\n[2] Zichao Yang, Alexander J. Smola, Le Song, Andrew Gordon Wilson. A la Carte \u2014 Learning Fast Kernels. AISTATS 2015."""
18000,1,\n\nMajor comments:\n1) Spatial resolution. What spatial resolution is the model generating images at?
18001,1," \nFurthermore, the manuscript seems to suggest, that the simulation results are somehow related to human vision as it is stated:\n\u201cThe model provides apparently realistic saccades, for they cover the full range of the image and tend to point over regions that contain class-characteristic pixels.\u201d\nbut no actual comparisons or evaluations are provided. """
18002,1, The small step is in the direction of gradient when top class activation is taken as the objective.
18003,1," In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN)."
18004,1,\n\nCons:\n(1) No direct comparisons with other methods are provided.
18005,1," \n\nPros:\n- Simple approach based on nearest-neighbors, likely easier to train compared to GANs."
18006,1, to be sought for here?
18007,1,"\n\nSection 5.3:\nAmortization error is an important contributor to the slack in the ELBO on MNIST, and the dominant contributor on the more complicated Fashion MNIST dataset."
18008,1,"  Otherwise, the reference \""ideal\"" distribution would be modeling a **rotated** version of the \\hat{z} samples, which imo only introduces unnecessary discrepancies."
18009,1,\n- Could you elaborate on the last sentence of section 4.1?
18010,1, The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss.
18011,1,\n- Approximating a cluster with a single sample (sec. 2.3) seems rather crude.
18012,1, The experimental results could also include results without style discrepancy loss.
18013,1,"\n\nI am assessing this paper as \""7\"" because despite the merit of the paper, the relevance of the reformulation of MAML, and the technical steps involved in the reformulation, the paper does not eg address other forms (than L-MAML) of the task-specific subroutine ML-..., and the benchmark improvements are quite small."
18014,1," However, the previous settings can be reinterpreted in the authors setting."
18015,1,\nThe method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided.
18016,1, The latter is important for the GAN-like training.
18017,1," \n4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong."
18018,1,\n\n7) the experiment in section 6.4 should be presented as an extreme case of that of figure 7.
18019,1, There is also no analysis of what happens for adversarial examples for the detector.
18020,1,\n- Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful.
18021,1, It is not clear the better performance comes from reservoir sampling or other differences.
18022,1,"\n \nOn the theoretical side, the discussion could be improved. Namely, Section 3 about \""limitation of domain adversarial training\"" correctly explained that \""domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity\"". "
18023,1," In particular the paper uses a Caffe reference model on top of the adjacency matrix, rather than learning a method specifically for graphs."
18024,1,"\n\n----------------------\n\n\nThis paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs."
18025,1," They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights."
18026,1,\n\nThe optimization procedure is unclear.
18027,1,"\n\n2. According to the description, referring to an existing named entity must be done by \""generating a key to match the keys in the NE table and then retrieve the corresponding value and use it\""."
18028,1, The test loss in question looks like a noisy version of the base test loss with a slightly lower mean.
18029,1," \n- The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper."
18030,1,"  \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,"
18031,1," In particular, I have serious problems understanding:\n\n1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ?"
18032,1,\n\nThe paper is rather preliminary in its examination.
18033,1,"\""\n-p9 Fig 3 -- the caption is mismatched with the figure.. top/bottom/left/right/etc.... Something is confusing there..."
18034,1," Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned)."
18035,1, but we only see results compared to DQN ... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods.
18036,1,"""This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements:"
18037,1,\nResults for deep linear neural networks are puzzling.
18038,1," \n\n- Second, it appears that the proposed methods may rely on running the dynamical system several times before attempting to control it."
18039,1," They present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy."
18040,1,\n\nMy concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model.
18041,1," On the other hand, the authors of this paper seem to be not aware of this pioneering work and claim \""To the best of our knowledge, our proposed PCL methodology is the first unsupervised countermeasure that is able to detect DL adversarial samples generated by the existing state-of-the-art attacks\"", which is obviously not true."
18042,1,"\n\nOverall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found \""in the wild\""."
18043,1, The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.
18044,1," The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017),"
18045,1," In addition to the incoherent presentation, the proposed method lacks proper justification."
18046,1,"  To be competitive, deeper CCNNs would likely need to be trained."
18047,1,"\n\nThe proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games."
18048,1, The GAN is a WGAN trained on the train set (only to keep the generator).
18049,1,\n\n\n\nPresentation issues:\n- in printed black and white versions most figures are meaningless.
18050,1,It seems easy enough to treat the alphas as just more parameters to optimize with gradient descent.
18051,1,"\n\nHowever, this paper is mercilessly difficult to read."
18052,1,  I would be interested to see if layerwise\ninput skip connections (i.e. between each network layer L_i and the original input variable 'X') hastened the 'compression' stage of learning e.g. (i.e. the time during which the intermediate layers minimise MI with 'X').
18053,1,  In eq. (1) many of the notations remain rather unclear until later in the text (and even then they are not entirely clear).
18054,1,"\n2. In the introduction, the authors mentioned \u201climitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings."
18055,1," However, in [4] the authors manage to avoid the additional Q(A) approximation that breaks the variational bound."
18056,1, \n\n2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset.
18057,1," If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear."
18058,1,\nThe empirical evaluation seems rather mixed.
18059,1, I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1.
18060,1,\n\n5. Proof of lemma 3.1.\nI found it's hard to keep track of which one is inside the expectation.
18061,1,\n\nPros\n\n1. It is new to apply quadrature rules to improve kernel approximation.
18062,1, It mostly leverages the existing literature on primal-dual subgradient methods to modify the GAN training procedure.
18063,1," \n\n While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. "
18064,1,".\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E). "
18065,1," They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure."
18066,1,"  \n\n- Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$."
18067,1," \"" -- please clarify; better results on a development set, I hope?"
18068,1,"\n\n* Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold."
18069,1,"The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients."
18070,1," Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2."
18071,1,"\n\n\n\n\n\nTypos or question related to notations, details etc:"
18072,1,"\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process."
18073,1, It would be helpful if the authors could comment on the dependence between T and L.
18074,1,\n\n4) The metric of quality is particularly ad-hoc.
18075,1,  The code specifying the models is also excessive for the main text -- it should be moved to an appendix or even left for a code release.
18076,1,"\n\nFrom \""CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\"":\nTo demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong \nwhich makes the game into a social dilemma."
18077,1, Maybe also rotating the codes with the singular vector matrix V or \\Sigma^{0.5} V?
18078,1, Two literatures of note are intelligent tutoring and machine teaching in the computational learnability literature.
18079,1," The human evaluation only compares two alternative models for preference, which is not enough to support this claim."
18080,1," However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished."
18081,1,"  \n\n- The gains here appear to be consistent, but they seem marginal."
18082,1,"""The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps."
18083,1, It is very probably more costful.
18084,1,"\n\n[1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017.\n[2] Tri Dao, Christopher De Sa, Christopher R\u00e9. Gaussian Quadrature for Kernel Features. NIPS 2017"""
18085,1," It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward."
18086,1,\n\u00a0\nPositives:\n\u00a0\n\t\u2022\tNovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed\n\t\u2022\tVery good abstract:
18087,1,. The detailed comments are as follows:\n\n- The main contribution of this paper is to apply word pairs instead of words to RBM models
18088,1, Is the neighborhood predefined?
18089,1,\n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality.
18090,1, It is very difficult to decipher these details from the short descriptions given in the paper.
18091,1," If that was the case, future work should first explore higher capacity (which may require larger-memory GPUs...)."
18092,1,"""The main issue is the scientific quality."
18093,1, the experimental part does not really show what are the advantages of the propose RIN.
18094,1," From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations."
18095,1, Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here.
18096,1,\n\nminor comments:\n- on page five in the last paragraph there is a left parenthesis missing in the inline formula nabla dot w_t(x))^2.
18097,1,"\n\nTypos :\n- In Eq 3, the numerator has z_t. Should that be z_y ?"
18098,1,\n\n(4) Please specify which version of the SQuAD leaderboard is used in Table 3.
18099,1, Figure `1 is a fantastic illustration that presents the core idea very clearly.
18100,1," The analysis becomes considerably more complicated,"
18101,1, Experiments are conducted on both Omniglot and miniImagenet datasets.
18102,1,\n\n\nI found the paper difficult to read.
18103,1," The authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach."
18104,1,"\n\nd) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it's kind of impossible to scale to large datasets? \n"""
18105,1," The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings."
18106,1, Seems like there are two identical terms.
18107,1,(ii) how to make use of the episodic memory when deciding what action to take.
18108,1, Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.
18109,1, This is therefore confusing and I would be interested in the author's point of view to this issue.
18110,1,\n\nA few recent works have directly tested increasing batch sizes during\ntraining.
18111,1," This will use significant resources and is much more difficult,"
18112,1," However, they're sometimes to briefly described to be understood by most readers."
18113,1, It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling.
18114,1,"\n- Eq 1: \n  *label \""y_i\"" has two different semantics (L_ocn it is the class label, while in L_pcn it is the label of an image pair being from the same class or not)"
18115,1, And what would they potentially lead to? 
18116,1," \nIgnoring my verbose comment, another view is that the baseline are disadvantaged to the treeQN, because they have less parameters (and are less deep which has a huge impact on the learnability and expressivity of the deep network)."
18117,1,"\n\n* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted."
18118,1," I understand that since the learner algorithm is an NN, this is not the case - but more explanation is necessary here - does your method also reduces the empirical possibility to get stuck in local minima?"
18119,1," \n\nSeveral minor comments:\n\n1. Please avoid the use of \u201cinformation theory\u201d, especially \u201cclassical information theory\u201d, in the current context."
18120,1, What guarantees can we hope to achieve?
18121,1, What is the reason that we need to run adversarial attacks 'even faster'?
18122,1,".\n\nAt a high-level, hyperparameter optimization (for the challenging case of discrete variables) can be seen as a black-box optimization problem where we have only access to a function evaluation oracle (but no gradients etc.)."
18123,1,"\n\nI am not sure that the discussion in page corresponds to the actual number on Table 3, I did not understand what the authors wrote."""
18124,1, Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval.
18125,1," For example, see \""Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\"" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)"""
18126,1," The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm."
18127,1,\n\niii) The experiments are insufficient in terms of details. How is the loss calculated? How is the detection accuracy calculated?
18128,1," If yes, what is its value?"
18129,1," \nTo learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal."
18130,1, This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable.
18131,1, \n(1) The topic of this paper seems to have minimal connection with ICRL.
18132,1,"\n\n5) the experiment in section 6.1, figure 5 is just slightly different from that in figure 3."
18133,1,"\n\nOverall, this reviewer leans towards rejecting this paper due to its limited contribution/novelty and the incomprehensive experiment results."
18134,1,\n4. I would like to see more explanation for the figure in Appendix A.
18135,1, How does it compare to other methods?
18136,1," the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work. """
18137,1, The paper claims that the approach can deal with the partial observable domain better than the standard methods.
18138,1, Please clarify this.
18139,1," Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster?"
18140,1,\n\nIn its current shape it has a couple of major flaws (but those can be fixed during the revision/rebuttal period):
18141,1,"""The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data."
18142,1," \n\nAs far as I understand, the methods for learning backbone structure and the skip-path are performed independently, i.e. there is no end-to-end training of the structure and parameters of the layers."
18143,1,"""This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems."
18144,1, This is not surprising.
18145,1,\n\nThis is a little too unprincipled for my taste.
18146,1,"Once these details are done correctly, the experiments support the relatively well-accepted hypothesis that flat minima generalize better. "
18147,1, The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations.
18148,1," First, the improvement of the proposed model over the previous state of the art is limited."
18149,1, This idea has been around for a long time (although I\u2019m having a hard time coming up with a reference).
18150,1," Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).33.07"
18151,1," If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself."
18152,1, The paper is well written
18153,1," Probably better to introduce branch for key methods, parallel sampling/ translation broadcasting and inadaptive or adaptive model."
18154,1," On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used."
18155,1," In fact, Vinyals et al. note this very point in their paper."
18156,1,"\n8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research."
18157,1," Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague,"
18158,1, This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data.
18159,1,\n6) The writing needs improvement and the paper in general needs some copy editing.
18160,1, though I do not view the results as particularly strong. 
18161,1," This should be explicitly said and the parameter value should be stated.\nPage 4: \n-\tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers."
18162,1,"  There are good results on the convex hull problem, which is promising."
18163,1,\n\nThe algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability.
18164,1, The paper organization needs to be revamped with emphasis on the proposed ideas of the paper and how it differs from the rich related work.
18165,1,"\n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017)."
18166,1," In fact, an interesting experiment here would have been to do ablation studies e.g. by replacing the LSTM with a feed forward fully connected network."
18167,1, This three networks need to be clearly described; ideally combined into one end-to-end training pipeline.
18168,1,"\no\tIn general: a block diagram showing the relation between all the system\u2019s components may be useful, plus the details about the structure and optimization of the various modules."
18169,1," Considering that the DrQA is a better system on both SQuAD and TriviaQA, the speedup on DrQA is thus more important."
18170,1,"Traditionally, for determining optimal execution plans, one may resort to cost-based optimization (e.g., database management systems)."
18171,1," This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated. """
18172,1," However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used)."
18173,1,\n- I also expect performance and  error analysis of the task results.
18174,1,"""Update (original review below):\nThe authors have addressed several of the reviewers' comments and improved the paper."
18175,1, What is the major structural difference to their Eqn. 13 which is discussed along very similar lines as Eqn. 15 of this paper.
18176,1, In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions.
18177,1, \nIt would be interesting to see if this can improve the results.
18178,1," \n2. The policy network would have to output a probability for each datapoint in the dataset U,"
18179,1,"\n\nComments:\n\n-\tIn page 2, authors suggest that from that G\u00fcl\u00e7ehre, Bengio (2013) that for visual relations \u201cfailure of feed-forward networks [\u2026] reflects a poor choice of hyper parameters."
18180,1, Lots of people have tried pre-training a neural network on auxiliary task(s) and using the features from it as input to the final SVM classifier.
18181,1," I do not see why this should be formulated as a \""sequential\"" decision problem."
18182,1,\n\n\nCons:\n\nThis is mostly an application of an existing method to a new domain -
18183,1, scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3.
18184,1,\n\nCan the proposed approach help when we have recurrent value functions?
18185,1, CTC is a sequence training criterion
18186,1,\n\n## Paper Summary\n\nThe paper examines the influence of batch size on the behavior of stochastic\ngradient descent to minimize cost functions.
18187,1,"  Furthermore, even if suitable datasets are not available, the authors could have chosen to train different architectures."
18188,1, This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5.
18189,1,"  I also liked the idea of having the section \""A geometric view of embeddings and tensor decomposition\"", but that section needs to be improved."
18190,1, \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight\u2019s learning rate is somewhat novel.
18191,1,n- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison;
18192,1,The discussion on the deficiencies of the naive LP approach is mostly well done.
18193,1,"\n[6] Gomez, F., Schmidhuber, J., & Miikkulainen, R. (2006). Efficient nonlinear control through neuroevolution."
18194,1, A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets.
18195,1," At the very least, I would consider using regret to the model of the task, and compute some quantiles on that which is still suspect in the matrix completion setting."
18196,1,"""This paper presents an algorithm for clustering using DNNs."
18197,1," The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform."
18198,1," Whereas neither of these components are new, to my knowledge, nobody has combined all three of them previously."
18199,1,\n\n1) A stronger motivation for this model is required.
18200,1,"\n\nMy main concern (and complaint) is not technical, but application-based."
18201,1,  See specific comments below.\n- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.
18202,1," Interpolating\n  between two sentences sampled from the prior is a neat parlour trick, but the\n  model as-is has little utility."
18203,1," but unfortunately the efficacy is not well justified in theory.[[CNT], [CNT], [CRT], [MAJ]] The empirical study is not always convincing, and did not compare with many state-of-the-art baselines."
18204,1, It is not clear why that is not used or at least compared to the method presented.
18205,1," The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search."
18206,1,\n3. Experimental performance is convincing.
18207,1," However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks."
18208,1,". For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge."
18209,1, \u2028\n(TODO) I would recommend that the authors test their approach on such setting.
18210,1," However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space."
18211,1," However, the analysis is done on small examples without any theoretical analysis."
18212,1,"\n\nIn the following paragraph, what is *contraction*?"
18213,1,"\n\nTo learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm."
18214,1," These are all novel contributions, but each one seems incremental in the context of previous work on this and similar algorithms (E.G. Nokland, Direct Feedback Alignment Provides Learning in Deep Neural Networks, 2016; Baldi et al, Learning in the Machine: The Symmetries of the Deep Learning Channel, 2017). \n\n"""
18215,1,"  It can be traced back to [3], used in PGM setting in [4] and used in VAE setting in [1]."
18216,1," Once the signals from different windows are intermixed, how do you even define the windows?"
18217,1, Results look like significant improvements over standard learning setups.
18218,1,\n\n\nUnclear evaluation:\nThe way the Amazon Mechanical Turk experiments are performed are unclear and/or not suited for the task at hand.
18219,1, I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization. 
18220,1,"You spend too much space talking about specific hyperparameter ranges, etc."
18221,1," For example, with sparse and block diagonal matrices, reducing the size of the matrix to fit into the cache on the GPU must obviously make a difference, but this did not seem to be investigated."
18222,1, The results in Section 5.2 seem to make a lot of sense and\n  show the big contribution of the model.
18223,1,"\n \nThe 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found?"
18224,1, Is there a connection to subtasks rewards in earlier HRL papers?
18225,1, It is commendable that the authors also discuss the memory requirements and increased wall clock time of the model.
18226,1, No new insight is being discussed.
18227,1,"""The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset."
18228,1,"""The paper describes a deep Q-learning approach to the problem of lane changing, whereby the action space is abstracted to high-level maneuvers that are then associated with low-level controllers."
18229,1, \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison.
18230,1,\n\nThis is clearly an application paper.
18231,1, The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a \u201cgate\u201d to the information flow
18232,1, \n+ The paper is fairly clea
18233,1,"\n\nSecond, even though the proposed architecture proved to perform empirically better that the considered baselines, the extent to which it advances RL research is unclear to me."
18234,1,"\"" In Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015."
18235,1," As mentioned by the authors in section 1, any relaxation in synchrony brings more noise and higher variance to the updates, and also may cause slow convergence or convergence to a poor solution."
18236,1,"\n\nSo at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution."
18237,1,  \n\nI think table 5 is also a bit strange.
18238,1,"\n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description)."
18239,1," However, the paper is not really stand alone."
18240,1, This does not seem to be reasonable.
18241,1,"""This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators)."
18242,1,\n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty.
18243,1," In particular, the formulation can be seen in a different way."
18244,1," \n\nThere are many terms in R_e in (5) which appear to have no effect on optimization, such as a(x) and b(y) in the denominator and \""- 1\"". It seems like R_e boils down to just the entropy."
18245,1," \n- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate?"
18246,1," In caption, is it mean accuracy or maximum accuracy?[[CNT], [null], [QSN], [MIN]]\nPage 6, \u201cdataset was achieves\u201d needs to be fixed."
18247,1,"  It has been pointed out before in various ways, however:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class)."
18248,1,\n\nPros:\n1. The paper is well written and easy to read.\n2.
18249,1,  What is the heuristic here?
18250,1,"\n\n\""In theory, this system alone could be used to compute anything securely.\"" This is informal and incorrect."
18251,1," However, my main concerns with this paper are related to motivation and experiments."
18252,1,  They were only given \u201ca few trials\u201d to learn how to operate the controls before the test.
18253,1, Experiments are carried out very carefully.
18254,1,"""The paper proposes, under the GAN setting, mapping real data points back to the latent space via the \""generator reversal\"" procedure on a sample-by-sample basis (hence without the need of a shared recognition network) and then using this induced empirical distribution as the \""ideal\"" prior targeting which yet another GAN network might be trained to produce a better prior for the original GAN."
18255,1,"  The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation."
18256,1,"\n\n* The experiments are sufficient to demonstrate the viability of the approach,"
18257,1,\n2) I would have liked to have seen an ablative study where the detectors are trained on pixels alone and directly compared with the detectors trained with saliency as an input channel.
18258,1,Can this be explained by comparing the variational free-energy.
18259,1, The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples.
18260,1,  I don't fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN's failure (GANs are not easy to train and this seems to be reflected in the results);
18261,1, No new theory development.
18262,1,"  I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area."""
18263,1, The main weakness is that I would have liked to see more analysis and comparisons in the evaluation.
18264,1, \n\nAlso the main assumption is that there is an easy way to compute similarity between states.
18265,1,"""The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator."
18266,1,  for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .
18267,1, although it outperforms GAR on 7 out of 13 categories;
18268,1,\n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance.
18269,1,"\n\nOn the overall, while the idea may be of interested,"
18270,1, \n\nPositive points:\nAuthors tackle irregular data feature extraction and learning using CNNs which is a hot topic in deep learning.
18271,1,"  \n\nThere are lots of small typos, please fix them."
18272,1,\n\n\nPOST REVISION COMMENTS:\n\n- I didn't reread the whole thing -  just used the diff tool. 
18273,1, What makes this problem particularly challenging is the need to predict/respond to the actions of other drivers.
18274,1,. Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing
18275,1,"\n* Gaussian-noised\n* approximation of the it objective\n* before eq9: \""that solves\"": well, it doesn't really \""solve\"" the minimisation, in that it is not a minimum; reformulate this?"
18276,1," Unless these two are done, we cannot assertively say that the proposal seems to do interesting things."
18277,1," As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah."
18278,1,"\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases."
18279,1, It is hard to understand how the agent utilizes the external memory without such an analysis.
18280,1, Results are in line with hypothesis\n\t\u2022\tThorough appendix clearing any open questions \n\u00a0\n
18281,1,"While this is NP-hard problem in general, the greedy algorithm is 2-approximate"
18282,1, The results presented are state-of-the art.
18283,1, \n\nThe entire dataset is based on 4 patients.
18284,1,"\n4. However, this paper introduces two different types of networks for \""parametrization\"" and \""physical behavior\"" mapping, which is interesting, can be very useful as surrogate models for CFD simulations."
18285,1, Figures are very hard to read because of small font.
18286,1, \nThe goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance.
18287,1,"""Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning. "
18288,1,\n\nAlso the paper lacks comparisons to other methods (including ones from before 2016) which have tackled this problem.
18289,1,"            Sec 4.1, p5: Last equation: Perhaps useful to explain the term $log(\\phi_j^* | \\theta)$ and why it is not in subroutine 4 ."
18290,1,"\n\n3) \""The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward\""\nIndeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider."
18291,1, How realistic is this set-up and in what application is it expected that this will show up?
18292,1, How do you deal with words (or even the whole string) for which you have no word embedding?
18293,1,"  In additional, there is no ablation study analyzing impacts of each design choices"
18294,1,"  Here are a few comments: \n\nI think when talking about modelling the dynamics of the world, it is natural to discuss world models and model based RL, which also tries to explicitly take advantage of the separation between the dynamics of the world and the reward scheme."
18295,1,"http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed\u2026\n\nOverall Assessment: \n\n"
18296,1,.\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model?
18297,1," The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets."
18298,1,  There is a PIR per level and feature defined in C4.
18299,1,"""This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points."
18300,1,"\n\nFurthermore, I would like to see two additional analysis."
18301,1,"\n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture."
18302,1,"\n\n4) Two drawbacks of previous methods motivate this work, including the bias of\nrepresentation learning and the high variance of re-weighting."
18303,1," This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n"""
18304,1,"\n\nOne complaint I had was inconsistent clarity: while a lot was well-motivated and straightforward to understand, I got lost in some of the details (as an example, the figure on page 4 did not initially make much sense to me)."
18305,1,"\n\nThe experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model."
18306,1,"\n\n\nWeaknesses:\n\n1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch."
18307,1," This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection."
18308,1," The loss ratios depend on initial loss, which is not important for the final performance of the system."
18309,1, \n\n====================\nClarity:  I found the paper clear and easy to understand. 
18310,1, Is their any helpful intuition about what goes wrong?
18311,1, I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?
18312,1,"""This should be the first work which introduces in the causal structure into the GAN, to solve the label dependency problem."
18313,1,"\n\nIn the definition of the prototypes the component wise weigthing (eq. 5) works when the Mahalanobis matrix is diagonal (even though the weighting should be done by the \\sqrt of it), how would it work if it was a full matrix is not clear."
18314,1,\n- introduces a simple branch-and-bound approach.
18315,1, Or is there some form of supervision involved?
18316,1," During research, we have multiple executable oracles and need to produce good training data from them."
18317,1," \n\nLastly, The insights regarding label leaking are not compelling."
18318,1,  How does the game do on the original task?
18319,1,"\n - more experiments showing a comparison of the proposed metric to others (e.g. inception score, Mturk assessments of sample quality, etc.) would be necessary to find the metric convincing"
18320,1, The paper rightly mentions that existing reading comprehension datasets (e.g. SQuAD) where the current methods are already performing at the human level largely due to large lexical overlap between question and document.
18321,1,  The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks.
18322,1,"\n\n*Based on the rebuttals and thorough experimental results, I modified the global rating."
18323,1,"The actual word embedding method, therefore, has a big influence on performance (as you show)."
18324,1,"\"" It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.)"
18325,1," A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent."
18326,1," However, I don't think that enough information is provided, because, given the author's approach, the value of this parameter most probably depends on other parameters, such as the bach size."
18327,1,"The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016),"
18328,1,\n\nSignificance: Hard to say.
18329,1,"""Paper summary:\nThis paper proposes a technique to generalize deconvolution operations used in standard CNN architectures."
18330,1," However they advocate that a better candidate is what they refer to as \""semantic space\"" formed by embedding the (word) labels of the images according to pre-trained language models like word2vec."
18331,1," Unlike their claim in the paper, the idea of combining supervised and RL is not new."
18332,1,"\n- What is the trade-off when the size of the coreset increases?\n"""
18333,1,"-  The examples in the paper seem to be cherry picked to illustrate dramatic effects.[[CNT], [CNT], [APC], [MAJ]]   The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs."
18334,1," It is interesting to see how DAs are used for conversational modeling,;"
18335,1," The number of layers increases exponentially in the number of paths, but not in the depth of the network."
18336,1, \nThe paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it.
18337,1,\n\nPlease refer to below for more comments and questions.
18338,1,"\n\nI believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al."
18339,1, Actually t-SNE on raw MNIST pixels is not bad at all.
18340,1,"  \nMy main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper. The efficiency of such a search method decreases very fast when the dimensions of the z-space increases."
18341,1,  This is a greatly restricted setting compared to real images.
18342,1, I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before.
18343,1," However, the writing made it challenging and the experimental protocol raised some serious questions."
18344,1, Would be good to provide its training curve.
18345,1," Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened."
18346,1," Therefore, contributions of lower blocks vanish exponentially."
18347,1, Bhatnagar et al. (2009) propose a solution by projecting the error on the tangent plane to the function at the point at which it is evaluated.
18348,1, Here there could be a better image reconstruction network that does a better job of reconstructing images than the ones used in the paper.
18349,1, The authors also study the analytic form of critical points of a single-hidden layer ReLU network.
18350,1, That said I agree this is an interesting avenue for future work.
18351,1, It is unclear what these word features are. Are they one-hot encodings or embeddings or something else?
18352,1," \nBased on this definition, and assuming identifiability (i.e. uniqueness up to invariants), and for networks with Leaky ReLU activations,  the authors prove that if the number of mappings which preserve a degree of discrepancy (density preserving in the text) is small, then the  set of \u00ab\u00a0minimal\u00a0\u00bb mappings  of complexity C   that achieve the same degree of  discrepancy is also small. "
18353,1," In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion."
18354,1," They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article."
18355,1, \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches.
18356,1,"    The implementation of SPL seems to hurt performance in some cases (slower convergence on the IMDB dataset), can you explain it?"
18357,1,\n\nThey consider learning VAEs using two different choices of inference networks with (1) fully factorized Gaussian and (2) normalizing flows.
18358,1," \n\n\nQuestions:\n- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor?"
18359,1,"  The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels)."
18360,1,"\n\nTo be honest, I didn't really \""get\"" this paper.\n*"
18361,1,"\n\nI recomend this paper for a workshop presentation.\n"""
18362,1, Overall action-dependent baseline outperform state-only versions.
18363,1," However, when x is not repeated frequently, both RAML and SQDML are biased."
18364,1," \n\nPros:\n+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing"
18365,1," First, using the term\n\""size m\"" for the number of values that the covariate can take is a bit misleading."
18366,1,\n\nThe experiments in this paper are also lacking.
18367,1, The experiments are only performed on MNIST.
18368,1, Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time.
18369,1," This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice. "
18370,1,n-\tEmpirical evaluations seem to use relatively small datasets composed by few dozens of temporal trajectories.
18371,1,\n\n* Please clarify whether the objective value shown in the plots is wrt the training\n  set or the test set.
18372,1, \n3. The result of optimal batch size setting is useful to wide range of learning methods.
18373,1,", there is no proposed new work beyond the existing approaches."
18374,1, And I write this as a card-carrying computational linguist.
18375,1,"\n\nSummary of evaluation\n\nThere is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade,"
18376,1," This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \""unused\"" until the present task in hand.\n\n"
18377,1,"  When proposing yet another simplification, some sense for how these different reductions is useful, so I would recommend comparison to those."
18378,1, \n\nPros:\n+ The paper is generally very clearly written.
18379,1, In the PPD the only (jointly) winning move is not to play.
18380,1," Differently, it can memorize the training data and performs as good as the baseline model. "
18381,1,"""The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor."
18382,1," (17) includes p(X^L|F^L) in the definition of Q(...), but it shouldn't."
18383,1, Scalable Bayesian optimization using deep neural\nnetworks.
18384,1,"\n\nThe authors write that artificial intelligence has mostly overlooked the role of teaching, but this claim is incorrect.[[CNT], [CNT], [CRT], [MAJ]] There is a long history of research on teaching in artificial intelligence."
18385,1,\n- The introduction also states that the authors will propose ways to improve image captioning.
18386,1, The authors certainly make a convincing statement about the internal validity of the method.
18387,1,"""- This paper is not well written and incomplete."
18388,1,"\n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN."
18389,1, At least local minima issues could be assessed using multiple random initializations.
18390,1,"""The paper studies the expressive power provided by \""overlap\"" in convolution layers of DNNs. "
18391,1,"\n\nFollowing the author's response and revisions, I have raised my grade.\n"""
18392,1, The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension.
18393,1,\n3. Efficient CUDA implementation (not experimentally verified)
18394,1," \n[3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017\n[4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015"""
18395,1, How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results?
18396,1," Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used."
18397,1, The first to publish the application of VBP to NNs was Werbos in 1982. Please correct. 
18398,1, I was not very sure as to why the proposed method is more general than existing approaches.
18399,1," The short experiment in Appendix E seems to try and answer this question, but it's results are anecdotal at best."
18400,1, Nevertheless the paper is quite well written.
18401,1,"\n- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners."
18402,1,  Assuming the privacy claims are reasonable (which I have some doubts about below) then this paper is clearly useful to any company wanting to do privacy preserving classification.
18403,1," \n\n- The authors refer to \""predicting the error patterns\"", but again don't define what an error pattern is."
18404,1, It might be obvious to the authors but it will not be to the ICLR audience.
18405,1," \n\nThe authors demonstrate novel approaches for generating real-valued sequences using adversarial training, a train on synthetic, test of real and vice versa method for evaluating GANS, generating synthetic medical time series data, and an empirical privacy analysis."
18406,1,\n\n2. It is not clear to me: in which dimension of the tensors are we saving the scale factor?
18407,1,\n- The paper is clearly written and key contributions are well present.
18408,1,"\"" : This does\nnot seem to be true."
18409,1,"""I enjoyed reading the paper."
18410,1,"\""; and \""[...] the action a_0 should be similar to actions sampled from pi_theta(a|s).\"" What do you mean \""should\""?"
18411,1,"  In particular, in Sec 2.5 the feedback pass for weight updating is computed."
18412,1,"  Much of the notation introduced in section 3.1 is not used later on.[[CNT], [PNF-NEG], [DFT], [MIN]]  There seems to be a bit of a disconnect before and after section 3.3.[[CNT], [PNF-NEG], [DFT], [MIN]] The algorithm in deep RL could be explained a bit better."
18413,1," However, the novelty of the paper is limited and the experiments are not sufficient."
18414,1," The key idea is exploiting saliency maps as additional inputs to detector, which reveals importance of each pixel in classification."
18415,1," A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task."
18416,1,\n\nThe main idea of  using upper bound (as opposed to lower bound) is reasonable.
18417,1," What would happen if the higher reward was +2, or more interestingly, if the game was extended to allow agents to fish medium-sized fish (+2), in addition to small and large fish."
18418,1,\n\n\n1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity. 
18419,1," \n\nConsulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar."
18420,1," but the current version of the paper contains several typos."""
18421,1, The presentations of the ideas are pretty clear.
18422,1,"  \nWith respect to the comparison vs. human operators of the car simulation, the human operators were not experts."
18423,1,\n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation).
18424,1," If the latter, then the experimental results are far weaker than what I would find convincing."
18425,1,"\n\n3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer."
18426,1,"  Since the prediction score is given in probabilty values, the reviewer assumes the final layer of the model is a SoftMax activation."
18427,1, My comment is mainly about its importance for large-scale computer vision applications.
18428,1, The theoretical arguments are interesting but lacking in rigor.
18429,1,"\n\nThe main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e.,"
18430,1," Moreover, the authors themselves suggest how to proceed along this line of research with further improvements."
18431,1,\nIt seems like the derivations presents several equations that score a given span.
18432,1," Any optimal policy would therefore need to spend some time e in the danger state, on average."
18433,1, \n - The paper is well written and the contribution is clear.
18434,1," Could the authors generate many wrench maps (same topology, random size, random wall textures) to make sure there is no bias? """
18435,1,\n\n---\n\nThe authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey.
18436,1,\n+ The idea to keep the decision boundary in the low-density region of the target domain makes sense
18437,1, I have no reference point to understand these graphs
18438,1, \n\nThere is also a general lack of attention to detail.
18439,1," \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem"
18440,1," \n\nRegardless - an important note to the authors is that it's a particularly long and verbose paper, coming in at 16 pages of the main paper(!) with nearly 50 (!) pages of supplementary material where the heart and meat of the proofs and experiments reside."
18441,1,"\n\nIt is a well-written paper,"
18442,1, They choose the alignment of the kernel to data as the objective function to optimize.
18443,1," I'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap."
18444,1,I think this paper does not connect very well with that literature.
18445,1," While it is better than a \nconterpart of MLE (MaskedMLE), whether the result is qualitatively better than\nordinary LSTM is still in question."
18446,1," \n- Figure 1, 4 and 5 need confidence intervals or standard errors."
18447,1,\n\n3) I have the same questions about Partial Episode bootstrapping: Is there a task in which we find our RL agents learning in time-limited settings and then evaluated in unlimited ones?
18448,1," The proposed method was evaluated on the SQuAD dataset only, and marginal improvement was observed compared to the baselines."
18449,1, And the whole paper seems as if written\ncirca 2010 or 2012.
18450,1,\u201d This statement is problematic for two reasons: \n\n(i) It is rather standard in the field of computational neuroscience to start from reasonable assumptions regarding patterns of neural connectivity then proceed to show that the resulting network behaves in a sensible way and reproduces neuroscience data.
18451,1," \""Non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain.\"""
18452,1, The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences.
18453,1,"""This paper tried to analyze the subspaces of the adversarial examples neighborhood."
18454,1, \n\n3. Experiment evaluation is not sufficient. 
18455,1," A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points."
18456,1,\n\nComments:\n1. Paper should cite Domke 2012 in related work section.
18457,1, But this procedure is not justified: does each step minimise the loss?
18458,1," \n\nThe images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind."
18459,1," In particular, a tree structured LSTM is taken and modified."
18460,1," For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values."
18461,1,"\n\nIn figure 2, top row, please display the learning rate on a log scale."
18462,1, The proposed model just simply sums up all the occurrences of candidate answers throughout the full document.
18463,1,"The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts."
18464,1," Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training."
18465,1,\n- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers?
18466,1,"  However, in section 4, in (7), the authors claimed the target vector v_t will affect the context shifting their representation to c\u2019_i."
18467,1,". \n\n(2) Authors only experimented with two very simple CNN architectures and with three different nonlinear activation functions, i.e., ISRLU/ELU/ReLU and showed their accuracies on MNIST"
18468,1, \nThe method assumes that it is possible to first execute 100 evaluations up to the total number of epochs.
18469,1," Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids."""
18470,1,"""In this paper an alternating optimization approach is explored for training Auto Encoders (AEs)."
18471,1,\n\nThe major weakness of this paper is the unclear presentation.
18472,1," Arguably, disentangling is a step towards concept learning and concepts might be harder to formalize than the approach taken here where in the experiment the variables are well-behaved and relatively easy to quantify since they relate to image formation physics.\n4. For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation."
18473,1, My main concerns are the following:\n\n1) One motivation of DFM is that in many applications data is a discretization of a continuous process and then can be represented by a function.
18474,1," I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here."""
18475,1, \n\nHow do you execute a trajectory?
18476,1," \n\n\nReferences\n[1] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside\nconvolutional networks: Visualising image classification\nmodels and saliency maps. arXiv preprint arXiv:1312.6034, 2013.\n[2] Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N De Freitas, Dueling network architectures for deep reinforcement learning arXiv preprint arXiv:1511.06581\n"""
18477,1,"""The authors propose to decompose reinforcement learning into a PATH function that can learn how to solve reusable sub-goals an agent might have in a specific environment and a GOAL function that chooses subgoals in order to solve a specific task in the environment using path segments."
18478,1,A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting.
18479,1," and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.\n\nThe presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive."
18480,1,"\n- The first paper to propose weight sharing was not Han et al., 2015, it was actually:\nChen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. \""Compressing Neural Networks with the Hashing Trick\"" ICML 2015\nAlthough they did not learn the weight sharing function, but use random hash functions."
18481,1,".\u201d This experiment only shows that this is true in aggregate, not for specific neurons?"
18482,1," \n* I am surprised by the results in Table 2, which suggest that the optimal number of nets in the ensemble is remarkably low (only 3!)."
18483,1,"  2) how the tune related weight of the different objective functions.  """
18484,1,"it is clear how this capability can help practical applications, especially no such examples are shown in the paper."
18485,1,"  In addition, the authors should compare with more baselines such as [1], [2], [3] and try with deeper networks as many networks used in the experiments are not very deep, with only 8 layers or less."
18486,1," ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011)."
18487,1,"""The paper introduces a method for learning graph representations (i.e., vector representations for graphs)."
18488,1,"\n\n(2) An advantage over prior work, this approach integrates architectural evolution with the training procedure. "
18489,1, \n\n* Reproducibility and evaluation\nThe description of the network is quite superficial.
18490,1,"""Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention."
18491,1, I would thus suggest that the authors update the paper accordingly.
18492,1,\n\n2. The authors implement the proposed sparse-complementary convolution on NVIDIA GPU and achieved competitive speed under the same computational load to regular convolution.
18493,1,\n\nThe work should not be evaluated from a practical perspective as it is of a theoretical nature.
18494,1,\n\n\nCRITICISM:\n\nMy central criticism is that the introduction of the L_2 norm as a replacement of KL divergence is completely ad-hoc; how it is related to KL divergence remains unclear.
18495,1, And is the *optimal accelerated rate* the same as *convergence rate* mentioned above?
18496,1,"""The authors present a new variation of autoencoder, in which they jointly train (1) a discrete-space autoencoder to minimize reconstuction loss, and (2) a simpler continuous-space generator function to learn a distribution for the codes, and (3) a GAN formulation to constrain the distributions in the latent space to be similar."
18497,1," Just the\n  book, or the whole corpus?"
18498,1, Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO.
18499,1,"  The basic idea is to use a multi-step dynamics model as a \""baseline\"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased."
18500,1," Are they all the continuous control tasks available in open ai?\n\n\n \n\n\n\n\n"""
18501,1,  Did the authors try experiments with less embedding matrices?
18502,1," For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi)."
18503,1," A number of design decisions (such as instance normalization) seem to help yield better results,"
18504,1,or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner.
18505,1,\n4. It seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix.
18506,1,\n\n\nA few more specific comments:\n\n2.2.1: The \\cdot seems extraneous to me.
18507,1," as I explain below, I believe there is an underlying assumption that is not considered."
18508,1, Experiments on MNIST are provided to analyse what this approach learns.
18509,1," Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains."
18510,1,\n- Figure 6 / sect 4.2: which model-free agent is used?
18511,1, I am looking forward to seeing work on the research goals outlined in the Future Directions section.
18512,1,  The comparisons for the Twitter dataset were even based on character-level with word-level.
18513,1, I found the formulation of the \\alpha to be non-intuitive and confusing at times.
18514,1,"\n\n[3] Huang, Gao, et al. \""Deep networks with stochastic depth.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n"""
18515,1," \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way."
18516,1," In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network."
18517,1,  \n- I think Section 5 can be put in the Appendix - it's essentially an illustration of why the weight scaling is important. 
18518,1,"\n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods."
18519,1, The claim is that different prepositions should have representations that are sufficiently distinct from each other.
18520,1, Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.
18521,1," there several major problems with this paper, including lacking clarity of presentation, insights and substantiations of many claims."
18522,1,"""This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images."
18523,1,"\n\n\nNumerical experiments\n====================\n\nGlobally, the numerical experiments are in favor of the presented method."
18524,1,\n\n1. It is unclear to me from Algorithm 2 how the weight vectors w are estimated. This is not adequately explained in the section on estimation.
18525,1,\n- The algorithm requires tuning of quite a few hyperparameters (sec. 3).
18526,1,\n\nThis is a strong paper that presents a significant improvement in document summarization.
18527,1,"\n2. For the same accuracy, coupled ensembling yields significant parameter savings."
18528,1," A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of \""optimal\"" under eq.6 is not explained."
18529,1, Several articles are wrongly placed and even some meaning is unclear.
18530,1,\nThe resultant policy is a function of the expert traces on which it depends.
18531,1," Conclusions drawn by testing on out of sample data may not be completely valid."""
18532,1,\nThe algorithm learns faster than unassisted DQN as shown by learning curve plots.
18533,1, Sensitivtiy-n seems to be an extension of the region perturbation idea to me.
18534,1,"\n\nI am willing to adjust my rating when the questions and remarks above get addressed."""
18535,1," For me, this is not the pain point in this tasks."
18536,1," Moreover, I do not think that node2vec is more efficient than, e.g., Weisfeiler-Lehman refinement used by graph kernels."
18537,1,"\n\nStrengths:\n\n- Reasonable approach, quality is good"
18538,1," \n\ncons/suggestions: \n- the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail."
18539,1,"\n\nIndeed, the authors have commented: \""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures."
18540,1," Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted."
18541,1," Even with the best hardware, computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible."
18542,1,\n2. How is the forward model / value function used in MCTS?
18543,1,"  It would be helpful to make the description here more explicit and clear."""
18544,1,"\n\nRead the rebuttal and revision and slightly increased my rating."""
18545,1,"""In conventional boosting methods, one puts a weight on each sample."
18546,1,\n\n(2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp. 
18547,1," As a reader I wouldn\u2019t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2. "
18548,1,\n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime.
18549,1,\n\nMy major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments.
18550,1,"\n\nSome other smaller points:\n- \""zero-shot skill composition\"" sounds a lot like what used to be called \""planning\"" or \""reasoning"
18551,1,\n\nComments:\n\n-The weak convergence results are interesting.
18552,1,"""This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs. "
18553,1," \n\nOn clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons."
18554,1,"\n* It appears like the beginning of the second paragraph in Section 5.2 describes figure 1. Is this correct?"""
18555,1," Given the recent interest in (deep) reinforcement learning (combined with the lack of theoretical guarantees in this space), this is a very timely problem to study."
18556,1, Did the authors try any values besides 0.5?
18557,1, \n\n- How robust are the eigen options for the Atari experiments?
18558,1,"\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. "
18559,1, Therefore correlation of the image features with the captions is weaker that it could be
18560,1,\n\nQuestion:\n-\tWhat is the cost of constructing orthogonal random features compared to RF?
18561,1,"  The authors describe these confidences variously as: \""some notion of confidence that the agent has in the value function estimate\"" and \""weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015)\""."
18562,1," Authors are then testing the model using test sets that do not follow the same distribution than training data, example,  longer sequences."
18563,1," However, many related works are missing in the literature, for example, Highway Networks [1],  Deeply-Supervised Nets [2] and Deep Networks with Stochastic Depth [3], etc."
18564,1,"  Meanwhile, the evaluation could be improved with the use of a better baseline (e.g., using an existing planning framework such as a predictive RRT that plans to the goal)."
18565,1, It also evaluates the number of regions of small networks during training.
18566,1,\n3. Are words that are linked via a dependency better than commonly co-occuring words?
18567,1," \n\nOverall, I think the idea presented in the paper has merit,"
18568,1," Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms."
18569,1, It contains a few errors.
18570,1, Am I missing something about the algorithm?
18571,1," \n\nAnother reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data."
18572,1,"  In the AP, HP and\n  AP+HP cases of Table 2, it is essentially the same predictive setup"
18573,1," However, if this \u201cflow\u201d is computed the same way [1]  does it, then the title is misleading.[[CNT], [null], [CRT], [MIN]]\n\n\nAdversarial training objective alone is not new as claimed by the authors:\nThe adversarial objective used in this paper is not new."
18574,1,"  In Sec 2.6 e_t is termed the postsynaptic (pre-nonlinearity) activation, which is confusing as the computation is going the other way (post-to-pre)."
18575,1,"\n\ncons:\n\n- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement. "
18576,1," Also, the reduction to 40% does not apply e.g. to vanilla SGD because the computed gradient can be immediately added to the weights and does not need to be stored or combined with e.g. a stored momentum term."
18577,1,"""The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules. "
18578,1," and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale."
18579,1,"\nLikewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability,"
18580,1,"  A picture would help here, showing where the depth-layers are divided between blocks.\"
18581,1, \n\nStrengths:\n- The method is simple but novel.
18582,1, The paper describes a classifier that predicts the dialog act of the next utterance.
18583,1,\n  \n- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7.
18584,1,"""This paper proposes a device placement algorithm to place operations of tensorflow on devices."
18585,1, The reference to a GAN architecture seems very forced and out of the scope.
18586,1,\n\nThe paper uses three manners of adversarial attacks and formulate saliency as the gradient that is particularly influential to the final classification output.
18587,1," Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings."
18588,1, A plot demonstrating the tradeoff of \naccuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance.
18589,1," I wonder if this is due to the very small batch size used (\""a small batch size of 4 \"").\n\n\n"""
18590,1,.\n\nMy main concern with this paper is a lack of technical novelty.
18591,1," The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al. 2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1-norm of the layers instead of the Frobenius norm."
18592,1, This is important as stacking LSTMs is a common practice.
18593,1," Specifically, it is unclear how the model correctly treats word connections between segments without any global language model."
18594,1, A baseline of regular Q-learning should be included for these simpler domains.
18595,1," Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales."
18596,1," In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method."
18597,1,"  \n(1) By using the objective in eq.(14), how to learn the embeddings E?"
18598,1, Your choice of IBM 2 is wired since it doesn't include fertility.
18599,1," \n\n(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation?"
18600,1,\n- It'd be great if Plot (a) and (b) in Figure 5 are swapped.
18601,1,\n\n== Detailed comments ==\nThe proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region.
18602,1," Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from."
18603,1," Some findings in this submission indeed look interesting,"
18604,1, \n\nPros:\ni) Detailed review of the existing work and comparison with the proposed work.
18605,1," I found that the answers are usually very short, which is more like factoid QA."
18606,1,"\n\n\nIn your experiments, you do not compare with any state-of-the-art RL or hierarchical RL algorithm on your domain, and use a new domain which has no previous point of reference."
18607,1,"""This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied."
18608,1," As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better."
18609,1, Would running the problem on quadratics with different condition numbers be insightful?
18610,1,", there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper."
18611,1,\n- One major bottleneck of the model is that the proposals are not jointly finetuned.
18612,1,"\nReferences:\net al.\nYI WU\n"""
18613,1, The results using this particular encoding are not better than any previous work.
18614,1," In this sense, I agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy, however, the protocol proposed requires to train the model several times, therefore, it does not scale properly to large datasets that are the common rule in the deep learning world."
18615,1," As far as I can see, the variable g_i^LRP wasn\u2019t defined, and the use of Eq. 5 to achieve this last could be better explained."
18616,1,   \n\nQuality:  I'm intrigued by but a little uncomfortable with the generalization metrics that the authors use.
18617,1," On a related note, the line in the abstract stating that \u201c... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise\u201d implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren\u2019t given the chance."
18618,1,which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\
18619,1, But one concern that I have is regarding the evaluation metrics used for it.
18620,1," Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale."
18621,1, I do not know whether using a centralized network where each agent has a window of observations is a novel algorithm.
18622,1,"\n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case."
18623,1,\n-- Details of the validation sets used to tune the models.
18624,1, This result is formally stated and proved in the paper.
18625,1, but ReLUs network perform well in practice.
18626,1, Any defense can be *evaluated* against samples generated by any attacker strategy.
18627,1,"  Given the variance in deep RL training curves, it is hard to make definitive claims from single runs."
18628,1, What problem does the guide actor solve?
18629,1,"and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3]."
18630,1, This makes the speedup less impressive.
18631,1, This is a direct result from definition of conditional probability.
18632,1,"""The manuscript proposed to use prefix codes to compress the input to a neural network for text classification."
18633,1," Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field."
18634,1, It is still unclear how those uncertainties \ncan be integrated in the model
18635,1, Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points.
18636,1, I would still like to have a better justification on why should we care about RWA and fixing that model.
18637,1,"\n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases."
18638,1," This allows the authors to turn the standard min max problem of adversarial training into a single minimization problem, which is easier to "
18639,1,"  For example, what is the output of the Eq. (7)?"
18640,1," So it is unclear why GA makes for a better choice of optimization, if at all."
18641,1,\n\n2. Why is the adversary called narrator network?
18642,1,"""- This paper shows an equivalence between proto value functions and successor representations."
18643,1," They do constitute a good starting place to test a model,"
18644,1," The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \""temperature parameter\"". "
18645,1," None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject."
18646,1," Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision."
18647,1,"\n- It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL, and it seems unclear when the algorithm converge and how to increase/decrease it over iterations."
18648,1," but it is a hack, not a well established well-grounded theoretical method (the authors claim that in their experiments they found it easy to find informative peaks, even in hundreds of dimensions, but these experiments are limited to the SVM setting, I have no idea how these empirical findings would translate to other kernelized algorithms using these adaptive features)."
18649,1," Is this for compressing the networks (for which there are alternate procedures), or anything else."
18650,1, \nThe paper provides for a good read.
18651,1," It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$."
18652,1,"\n\nThat said, I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples."
18653,1," it adds an interesting perspective to the discussion of how network optimization \""works\"", how it handles local optima and which role they play, and how the objective function landscape is \""perceived\"" by different optimizers."
18654,1,\n\nThe notation for equation 8 is not explained.
18655,1," Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a \""Gaussian\"", and thus the class probabilities are the reseponsibilities proportional to the class Gaussian."
18656,1,"\nThey should also present early their model and their mathematical motivation: in what sense is their new penalty \""preferable\""?"
18657,1," For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters."
18658,1," but I think the evaluation, especially against other methods, needs to be stronger."
18659,1, \n\n- Some figures and tables are overlapping in the experiments. Just keep one is enough.
18660,1,\n\nClarity:\nThe paper needs major revision w.r.t. presenting and highlighting the new main points.
18661,1,"\n  As such, I think it's a reach to claim the model is learning interpretable topics."
18662,1," Then there is the additional question, why should we care?"
18663,1," For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2."
18664,1," The annealed importance sampling technique of Wu et al (2017) for\nestimating bounds on a generator's log likelihood could be easily applied in\nthis setting and would give (for example, on binarized MNIST) a quantitative\nmeasurement of the degree of overfitting, and this would have been preferable\nthan inventing new heuristic measures."
18665,1," Finally, while this method introduces a principled way to remove mean baseline activity from the sensory-driven response, this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response (Destexhe et al., Nature Reviews Neuroscience 4, 2003; Gutnisky DA et al., Cerebral Cortex 27, 2017)."""
18666,1, The curves in Fig. 5(a) are not well explained.
18667,1,  The paper argues that the recurrent state should be high-dimensional (in order to be able to encode the input and extract predictive information) but the recurrent dynamic should be realized by a low-capacity model.
18668,1, I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.
18669,1," The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST."
18670,1,\n4) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far (not parameter updates).
18671,1,\n- p.3: We use an appropriate encoder is repeated twice.
18672,1," That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014)."
18673,1," The interesting issues are referred to future works.\n"""
18674,1,"http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf"""
18675,1, The authors propose a reservoir sampling algorithm for drawing samples from the memory.
18676,1," The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns, and use policy gradient to update the teacher parameters."
18677,1,"""In this paper, the authors propose a new approach for learning underlying structure of visually distinct games."
18678,1,\n\nThe paper is to be commended for the following aspects:\n1) Detailed description of GGNNs and their comparison to LSTMs
18679,1," The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations, argue experimentally that they occur in (some) neural networks of practical interest."
18680,1,"  This analysis does not seem very different from Twitter analysis, because although Tumblr posts are allowed to be longer than Twitter posts, the authors truncate the posts to 50 characters."
18681,1, (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.
18682,1," One could argue that TreeQN is learning an \""abstract\"" planning rather than \""grounded\"" planning."
18683,1,\n\nThe paper is well-written and authors explain related work adequately.
18684,1," These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity. "
18685,1,\n(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels.
18686,1, The approach is evaluated for several attacks on MNIST data.
18687,1, What precisely do we learn here?
18688,1,"\n\nThere is also a work in this area that the authors do not cite or contrast to, bringing the novelty into question; please see the following papers and references therein:;"
18689,1," Thirdly, it is not clear what Figure 5 means in terms of goodness of learning."
18690,1," Also the data problem is extremely simple, and it is not clear the didactic benefit of using it."
18691,1, The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps.
18692,1,\n- why focus only on extractive QA?
18693,1,\n-  The study is limited to just one dataset.
18694,1," \n- To make a fair comparison, the results in Table 1 should consider the amount of data used in pre-training the forward models."
18695,1,There are some concerns about this network that need to be clarified:\n1. sigma is never clarified in the main context or experiments\n2.
18696,1,"  The second to bottom row of Table 5 seems to correspond with the first row of Table 5, but other methods like slack rescaling have higher performance."
18697,1, \nWould the learner then just prefer to go back towards those states that it can approximate and endlessly loop?
18698,1,"\n- instead of the \""numerical result show the performance of the new model\"", give some numerical results here, otherwise, this sentence is useless."
18699,1,"\n\nThe paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures."
18700,1," The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated."
18701,1,"\n\nAlbeit the above caveats, we iterate the paper offers a nice construction for an important problem."
18702,1, The paper is written clearly and is easy to follow.
18703,1, The paper is well-written: I enjoyed the mathematical formulation (Section 3).
18704,1," However, a few points are not clear to me:\n- Is the result new or not? "
18705,1,)\n\n- Minor: in equation 2- is the correct exponent not t'?
18706,1," But more importantly, I think that the proposed method has its limitation about what kind of physical systems it can model."
18707,1," I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect. "
18708,1,\n\nOverall the paper is well-written and quite clear.
18709,1," Importantly, the model achieves state-of-the-art performance of the SQuAD dataset."
18710,1, The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution.
18711,1, I can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model.
18712,1,Section 4 extends\nanalysis to include SGD methods with momentum.
18713,1,\n\nThe section 3 is quite long and could be compressed to improve the relevance of this experimental section.
18714,1, It's not clear if Pruned FNNs are the most suitable baseline for evaluating the results.
18715,1,\n\nThis is an extremely impressive manuscript and likely to be of great interest to many researchers in the ICLR community.
18716,1," \n\nSpecific Comments on Sections: \nComments: Sec 1,2\nIn these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data."
18717,1,"\n\n2. For the RL approach, I think it is very unclear as a formulation of an estimator."
18718,1, \n\nThe paper is written well and provides good insights (mostly taxonomy) on the existing methods for neural network-based clustering.
18719,1,"\n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator."
18720,1,"\nIn any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point."
18721,1," However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0."
18722,1,"  The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space. "
18723,1,"\n\""\\pi_\\theta described in the previous sections\"", notation \\pi_\\theta appears the first time here...\n3x3 -> 3 \\times 3\nofB\nV_{\\theta^t w^t}[[CNT], [CLA-NEG], [DIS], [MIN]]\np.6:\nthe the\nFig.2's caption:\nWhat does \""both cases\"" refer to? They are three models."
18724,1,\n\nStrengths:\nSimultaneous text and image generation is an interesting research topic that is relevant for the community.
18725,1, The agent is tasked with driving the ego-car down the n-lane highway and stopping at \u201cthe exit\u201d in the right hand lane D km from the start position.
18726,1,"  \n\n- In Eq 3, please be explicit as to whether the '+' is here a concatentation or an actual (element-wise) sum?"
18727,1,"\n\nThe authors might discuss more how to extend their model to image recognition, or at least of other modalities as suggested."
18728,1," but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.\n"""
18729,1,  \n* I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now.
18730,1," Is there a guarantee that a same named entity, appearing later in the dialog, will be given the same key?"
18731,1,"  It's not how the paper is written, though. "
18732,1, Is there a reason that these models have not been as successful in other domains?\
18733,1, All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*
18734,1,"\n\nWith regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations."
18735,1,"It would have been great had the authors mentioned that u_j \\in {0,1}. "
18736,1," The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs."
18737,1," I wonder if you could put this in the context of \""training with input noise\"", which has been studied in Neural Network for a while (at least since the 1990s)."
18738,1," Providing more structure to the text would also be useful vs. long, wordy paragraphs."
18739,1, It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks).
18740,1,"  No standard deviations of nSV and Time are reported.\n"""
18741,1," The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though."
18742,1," However, I did notice that the majority of these changes were superficial re-orderings of the original text."
18743,1,\n\nClarity\n\nThe rationale in the paper is straightforward.
18744,1," Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion."
18745,1," Also I'm not sure about the first equality in this equatiion, is I^2 = 0 or is there a typo?"
18746,1, Finally the third technique learns to weight the samples according to their distance to the original prototypes.
18747,1, This seems unlikely due to the sampling.
18748,1,"\n\nMinor comments\nPage 1: forcing an compact -> forcing a compact\n\u201cand and\u201d =>and\nSection 2: mention that I is mutual information, it is not obvious for everyone\n\nFigure 3: circles/triangles are too small, hard to see \nFigure 5: not really convincing."
18749,1,2014\n\nPros:\n- Propose a sound approach to mix two complementary strategies for domain adaptation.\n- Great empirical results.
18750,1,\n\n- Page 4. 3.3) the importance of texture and shape is disregarded.
18751,1," On the contrary, the model is heuristic, and simple, but the descriptions are unclear."
18752,1,"\n\nCOMMENTS\n\nThe introduction is well-written, clear, and concise."
18753,1,"  Finally the cost effectiveness training (3c), how come that the same \""car wheel\"" (as in 3b) is discovered by the EM clustering? Is that coincidence?"
18754,1," In my opinion, selecting good loss functions and hypothesis spaces are much harder problems than data teaching - except maybe when one use a pre-defined set of possible loss functions and select from it."
18755,1,.\n\u2022\tVery impressive results
18756,1,"\n\nThe literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments."""
18757,1,\n\t2.2) A synthetic dataset is only considered.
18758,1,.  The other issue concerned the validation of the approach on databases other than MNIST.
18759,1,\n\nI think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at.
18760,1, \n\nOne question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches.
18761,1, It might be better to add errorbars to those curves
18762,1,\n\nThe second step is to train the goal function for a specific task.
18763,1,"\n\nThough experiments, they show that there are two kinds of minima, depending on whether we allow negative initializations in the convolution kernels."
18764,1,"\n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair."
18765,1," In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience."
18766,1,"  The idea of using class label to adjust each weight\u2019s learning rate is interesting and somewhat novel,"
18767,1," Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach."
18768,1,"""In the medical context, this paper describes the classic problem of \""knowledge base completion\"" from structured data only (no text)."
18769,1,"  It was reported that the human participants \u201cdid not feel comfortable\u201d with the low level controller on, possibly indicating that the user experience of controlling the car was less than ideal."
18770,1,  d\n\nDefinition of u (eq. 3) => v?
18771,1,"""This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process."
18772,1,"\n\nFig 3: This figure studies \u201cthe first and last layers of each network\u201d. Is the last layer really the last linear layer, the one followed by a softmax?"
18773,1, What is u? This is revealed in the latter sections but should be specified here.
18774,1," As a matter of fact, under some scenarios performance even decreases."
18775,1," \n\nThe primary claims of the paper are as follows:\ni) The proposed approach is a generative model of graphs, specifically producing \""sibling\"" graphs"
18776,1, The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem.
18777,1,"""The authors study cases where interpretation of deep learning predictions is extremely fragile."
18778,1," As far as I can tell, this paper has nothing to do with GANs."
18779,1,. There exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed setting. 
18780,1,"\n\nGenerally speaking, this paper is well written and easy to follow."
18781,1, There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').
18782,1,"\n\n6. The generator reversal procedure needs to be carried out once for each data point separately, and also when the generator has been updated, which seems to be introducing a potentially significant bottleneck into the training process."""
18783,1," The work is valuable,"
18784,1, The presented method does not consider this and hence does not analyze invariances.
18785,1,"""This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al."
18786,1,It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods.
18787,1,  The goal function tries to predict states in this latent space.
18788,1,"\n\n3. The writing could be significantly improved, both at the grammatical level and the level of high level organization and presentation."
18789,1," \n\nWhat is proposed seems to be twofold:\n- instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound."
18790,1," Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels."
18791,1," An alternative could be: \nSec 1: Introduction \nSec 1.1: Related Work\nSec 2: Causal Models\nSec 2.1: Causal Models using Generative Models (old: CIGM)\nSec 3: Causal GANs\nSec 3.1: Architecture (including controller)\nSec 3.2: loss functions \n...\nSec 4: Empricial Results (old: Sec. 6: Results)\n- \""Causal Graph 1\"" is not a proper reference (it's Fig 23 I guess)."
18792,1,"\n2) To my knowledge, the proposed approach is new"
18793,1, Well within the remit of the conference.
18794,1," Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this."
18795,1,  Section 2 should be clearer and used to better explain\nrelated approaches to motivate the proposed one.
18796,1,"\n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens."
18797,1," I cannot quite imagine in what kind of applications we can get \u201ca set of pairs of intra-class (same class) examples, and the negative training data consists of a set of pairs of inter-class\u201d."
18798,1,"\nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal."
18799,1, Is it Word2vec or something else?
18800,1,"\nExample 1 from abstract: \n\u201cWe show that for a wide class of differentiable activation functions (this class involved \u201calmost\u201d all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.\u201d\n\nThis is certainly not proven and in fact not formally stated anywhere in the paper."
18801,1,  The proposed method appears to have a modest improvement for few-shot learning.
18802,1,"\n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise."
18803,1, \n-   Equation (5)  should be - O(\\alpha^2)
18804,1,"The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs."
18805,1," eg. I am not sure what\noccupancy values, or inducing points are.[[CNT], [null], [DIS], [MIN]] \n\n* Supposingly that the authors properly consider computation in RKHS, then \\Sigma_i\nshould be definite positive right?"
18806,1,". The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\\gamma}, which can be regarded as the inverse function of the original GAN G_{\\theta}, is trained to learn a map from the original input data space to the latent z-space."
18807,1," If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum."
18808,1, How do this marginals help solving the NLI task?
18809,1,"  If so, it would interesting to highlight the key obstacles that a naive dual-based approach would encounter and how these are overcome.\n\nIn algorithm 1, it is confusing to use u to mean both the parameters of the neural net and the function represented by the neural net."
18810,1, More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method.
18811,1," \n\nFinally, there are many further simplifications of LSTMs that could have been explored in the literature: coupled input-forget gates (Greff et al, 2015), diagonal matrices for gates, GRUs."
18812,1, Would NATAC-k need a different number of clusters than the one from NATAC?
18813,1," This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective."
18814,1," In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time."
18815,1,"""* EDIT: Increased score from 5 to 6 to reflect improvements made in the revision."
18816,1, \n\nSee also comment [*] later on the presentation.
18817,1, The claim is that the found input represents the non-linear transformations that the layer is invariant to
18818,1," \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%."
18819,1,"  However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters."
18820,1,"\n\nBesides, some papers should be mentioned in the related work such as Kingma et. al. 2014."
18821,1,". First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network."
18822,1, It extends results of Ge and collaborators for the fully observed symmetric case to non-symmetric problems.
18823,1," Is the latter a proxy for the former? How are they related?"""
18824,1," Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model.\n"""
18825,1, i.e |\\partial L / \\partial z \\partial w|<= B.
18826,1,\n\nMore details are needed about Figure 3.
18827,1, So you train in (a) on the steps task until 350k steps?
18828,1,\n-            It is not clear from the text or experiment how the learning parameters are set.
18829,1," But it seems that the extent of these guarantees are comparable to those of several other generative models, including WGANs, the Sinkhorn-based models of Genevay et al. (2017, https://arxiv.org/abs/1706.00292/), or e.g. with a different loss function the MMD-based models of Li, Swersky, and Zemel (ICML 2015) / Dziugaite, Roy, and Ghahramani (UAI 2015)."
18830,1," For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem)."
18831,1,"  Through a series of experiments and a newly defined dataset, it exposes the short-comings of current seq2seq RNN architectures."
18832,1,"  Essentially, a normalization vector is maintained an updated separately."
18833,1," Also, important references in active learning literature are missing."""
18834,1, The number of state updates that the model learns to use can be controlled with an auxiliary loss function. 
18835,1," Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017)."
18836,1,"""The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning."
18837,1,"\n\nFundamentally, a convolutional filter stands for a operation within a small neighborhood on the image."
18838,1, How does the timing compare to Isola et al\u2019s Conditional GAN?
18839,1,"\n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc."
18840,1,\nThere are a couple of highly related work with multi-view VAE tracking similar problem have been proposed in the past.
18841,1,"\n\nMost of your empirical success is obtained by taking a pretrained CRF energy function and using this as a teacher model to train a feed-forward inference network. You have have very few experiments using a SPEN energy function parametrization that doesn't correspond to a CRF, even though you could have used an arbitrary convnet, RNN, etc."
18842,1, What problem does this particular set of contributions solve that is not solvable by the baselines? 
18843,1, Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled.
18844,1, \n \n2. The depth is so limited.
18845,1," As it is, I am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere."
18846,1,\n- local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs.
18847,1,"but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation."
18848,1,"\n\n(3) Most importantly, in this derivation, we see that the kernels lose the interesting representations that come from depth in deep neural networks."
18849,1," While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture."
18850,1," Each node is represented with (dx, dy) along with one-hot representation of three different drawing status."
18851,1,\n\n3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief.
18852,1," Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks."
18853,1, The study seems fairly thorough (both vanilla and cold-start experiments are reported).
18854,1,"""This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information."
18855,1," Appendix C: space missing after \""Section 5.1\"""
18856,1,\n\n* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). 
18857,1," For example:\n\n1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning."
18858,1, I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.
18859,1," To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations."
18860,1, Showing such results would be more convincing.
18861,1,"  That is really not very informative (the authors refer to the Appendix so I carefully read that part of the Appendix, but it is extremely vague, it is not clear at all how the Langevin dynamics can be \""emulated\"" by a discrete Markov chain that mixes fast; the authors do not provide any justification of that approach, what is the mixing time ?;"
18862,1, It seems that A2 should also be similar with A since only one bit in A2 and A1 is different.
18863,1, but given the experimental results it just is not clear that it is working.\
18864,1,"\n- In 3.1, Socher et al. do not use lstm"
18865,1, The experiments show that the architecture outperforms a couple of related models on an associative retrieval problem.
18866,1,\n\nThe suggested methods are empirically evaluated on a number of settings.
18867,1," Also, the qualitative results in Table 3 do not convince me that the embedding dimensions represent topics."
18868,1, Would your regularizer still be helpful there?
18869,1," Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful."
18870,1," Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity."
18871,1," Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top."
18872,1, This means that a liner combination with the real-valued learned gate parameters is suboptimal.
18873,1, I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here?
18874,1,"""The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning."
18875,1,"""This paper aims at robust image classification against adversarial domain shifts."
18876,1," Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization."
18877,1,"\n\nExperiments\nTypo: Should say \u201cThe data distribution is p_d(x) = 1{x=1}\u201d.\n"""
18878,1,\nMore precisely there is tradeoff to achieve between the complexity of the model and its simplicity.
18879,1, Is it a PCFG?
18880,1, Because Gumbel Softmax doesn\u2019t scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experiments.
18881,1,"""The paper studies the global convergence for policy gradient methods for linear control problems."
18882,1,\n\nThe other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verbs.
18883,1," The paper empirically evaluates this idea on Atari games with deep non-linear state representations, compared to state-of-the-art baselines."
18884,1," When combining multiple types of augmentation the results are better,"
18885,1," \n\u201cFor future work, a possible study is to investigate what classes of problems DRN can solve."
18886,1, \n- The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail.
18887,1, I am satisfied with the improvements to the paper and have changed my review to 'accept'.
18888,1," Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small."
18889,1," In fact, you lose valuable structure when you take these limits, and what you get is very different than a standard deep neural network."
18890,1,"\n\nAnother broader question I have is in the distinction between lower and upper layers (those referred to as \""feature extracting\"" and \""classification\"" in this paper)."
18891,1," \n\nThe fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space."
18892,1," Moreover, authors argue that the \nquality of generated texts is not appropriately measured by perplexities,\nthus using another criterion of a diversity of generated n-grams as well as\nqualitative evaluations by examples and by humans."
18893,1,"""This paper describes a method to induce source-side dependency structures in service to neural machine translation."
18894,1, I would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method.
18895,1,"\""\n- The function rho is originally defined on whole trajectories but in eq 7 it is only on a single s':  I'm not sure exactly what that means."
18896,1,. This is not yet achieved with the results presented in the submitted paper. 
18897,1," Every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners, clearly stating discrepancies."
18898,1, Do you mean \u201csolves/considers both issues in a principled way\u201d?
18899,1,"However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures."
18900,1,"\n- It is considered that the GAN are trained without any problem, and therefore work in an optimal regime"
18901,1, Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks.
18902,1,"""This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL."
18903,1,"\n6. Section 4: How does the KRU compares to the other parametrizations, in term of wall-clock time?"
18904,1, Similar questions for the 5\n   models in the 3D rendering experiment.
18905,1,"  \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum."
18906,1," If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results."""
18907,1,"\n\n\""with 2 hidden layers each of 64 relu\"": Missing word?Or maybe a comma?[[CNT], [CLA-NEG], [QSN], [MIN]] \n\n\""to aligns with\"" -> \""to align with\"".[[CNT], [CLA-NEG], [CRT], [MIN]] \n\n\"" a set of quadratic distance function\"" -> \"" a set of quadratic distance functions\""."
18908,1,\n\n- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator? 
18909,1,\n\nThe contribution is clear
18910,1," The same method was proposed in\n\nSnoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., ... &\nAdams, R. (2015, June)."
18911,1," Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures."
18912,1," \n\n# Summary of review\nI find the contribution to be incremental, and the validation weak."
18913,1,"It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer."
18914,1,"  \n\nThe paper is clear in most points, with some parts that could use further elucidation."
18915,1,\n\n* The L-BFGS results reported in all Figures looked suspicious to me.
18916,1," \n\n[3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015)."
18917,1, The objective function is a linear mixture of the cost of generating the tree structure and the target sentence.
18918,1,"\n2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000)."
18919,1,  Multilingual Distributed Representations without Word Alignment. ICLR 2014.
18920,1, This is in turn optimized (approximately) by alternating between gradient-based loss minimization and submodular maximization.
18921,1, I prefer to have discussions on experimental results with both datasets.
18922,1,"  Besides \u201cfooling\u201d the discriminator, the generator objective is to maximise user interaction with the generated batch of images."
18923,1, The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.
18924,1,"  Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty."
18925,1," Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder."
18926,1," Given the small size of the datasets, it is also unclear how generalizable the approach is."
18927,1, A few minor presentation issues:\n- ReLu --> ReLU\n\n+
18928,1, \n\nClarity: Clear.
18929,1,\n\nThe proposed concept is only analyzed in MLP with Sigmoid activation function.
18930,1, The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning.
18931,1," \n\nother question: In Eqn.4-5 , the terms $O(\\alpha)$ and $O(\\alpha^2)$ are omitted, however, since $\\mu_w^j$ are updated with its own learning rate, would it be better to increase the learning rate $\\alpha_{\\mu}$ and use the term $O(\\alpha^2)$ in the gradient update (6) as it would better approximate the gradient direction?"
18932,1, Do we get similar results?
18933,1,\n\nThe discussion on graph sampling does not include how much of the graph was sampled.
18934,1,\n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\
18935,1, A Gaussian model is used for representing these confidences as covariance matrices.
18936,1, The current analysis is too simple. 
18937,1," \n\n- As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM."
18938,1,"http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper.pdf"""
18939,1,  In this regime of greedy decoding it is hard to see any difference between the different ConvNet features used for captioning - Table 1 shows meteor scores within 0.19 - 0.22 for all methods.
18940,1," The writing is clear, concise and easy to follow."
18941,1, It seems to me this is a very general claim.
18942,1,"""This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution."
18943,1," But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work."
18944,1,"""This paper proposes a new method, called VCL, for continual learning."
18945,1, Thus they represent weak baselines for a fully centralized method such as MS-MARL.
18946,1,"\n\nThe description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages."
18947,1,"""This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs."
18948,1,  And the experiments are described with very little detail or discussion about the experimental setup.
18949,1," The \""super-convergence\"" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models."
18950,1, First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions.
18951,1,"\nThis line of work is specially relevant since it attacks one of the main bottlenecks in learning complex tasks, which is the amount of supervised examples."
18952,1," It is really interesting, so improving the quality of this figure will really help."""
18953,1,"\n\nOverall, while this is an interesting and important problem, and the engineering details are interesting and reasonably well-motivated, the main contribution of the paper is based on a pretty flawed approach to modeling correction probability, which would limit the ultimate applicability of the methods."
18954,1, \n- Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better.
18955,1,  The second step of sampling is ignored in the description of VAEs in the first paragraph.
18956,1, The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p.
18957,1,"  As the authors point out, hashtags and words may be used sarcastically or in different ways from what is understood in emotion theory."
18958,1," \n\nIn the comparison to previous work, please explicitly mention the EMD algorithm, since it's used in the experiments."
18959,1," While the experiments are well done,"
18960,1,  \n\nOverall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak.
18961,1, \n\nThe paper shows that is possible to learn the between domains mapping using GAN only without a reconstruction (cyclic) loss
18962,1, There is no experimental proof in the paper.
18963,1,". Also, training errors should be shown, since I suspect that underfitting may be happening especially in the case of ImageNet."
18964,1,"\n\n7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations."
18965,1,\n\nCons:\n- The link between the intuition and reality of the gains is not obvious
18966,1," This allows them to perform the vector products needed per layer; two's complement representation also allows for an \""easy\"" implementation of the ReLU activation function, by \""checking\"" (multiplying by) the complement of the sign bit."
18967,1," Also, other FDA approaches for operator learning should be discussed and compared to the proposed approach.\n"""
18968,1,\n\nThe paper does not discuss the computational cost of the method.
18969,1, This is a strong contribution
18970,1," But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging."
18971,1, The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).
18972,1,"\n\n\n[1] QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks https://arxiv.org/abs/1610.02132"
18973,1,"""This paper is well written and it was easy to follow"
18974,1,"""The paper is relatively clear to follow, and implement."
18975,1," Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8."
18976,1," As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al. NIPS 2016."
18977,1, A neural network that finds a naturalistic solution for the production of muscle activity.
18978,1," \n\npage 15:\n- \""we wish to compute\""-> we aim at showing?"
18979,1,\n- Experiments on multiple tasks and datasets confirm the efficacy of the method
18980,1,"\nOverall, fine work, well organized, decomposed, and its rationale clearly explained."
18981,1,"\nThe analysis is also performed by varying the network architectures, considering data augmentation and/or fine tuning."
18982,1,"""This paper examines the problem of optimizing deep networks of hard-threshold units."
18983,1, It would be great if this analysis could be performed for MNIST as well.
18984,1,"\n\n8. In figure 2, how are rounds constructed?"
18985,1, It is possible the gains just come from having more parameters.
18986,1,"  Regardless, the authors should report the micro-F1 in addition to the macro-F1."
18987,1,"There are many approaches to devising optimal execution plans, e.g., rule-based, cost-based, learning-based."
18988,1,"\n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?"
18989,1,\n- Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent. 
18990,1," In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution."
18991,1," The set is optimized to be as hard as possible (maximize loss), which results in a min-max problem."
18992,1,. The paper should be checked for typos.
18993,1," \nIt clusters features from a specific pooling layer, and then creates a binary assignment between per image extracted feature vectors and the cluster centroids."
18994,1, Is the time budget different for each new generated environment?
18995,1,"\n- In Eq 5, the denominator has z_y."
18996,1, \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format?
18997,1,"\n- Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics."
18998,1,"\nThis \""hybrid\"" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time."
18999,1," There is not unstructured datasets (e.g., images, text documents, speech) tested, which is where deep learning is making sense."
19000,1,\nThe authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices.
19001,1, It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results.
19002,1, This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.
19003,1," it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable."""
19004,1, It does not have skip connections yet performs quite well.
19005,1, Making Neural Programming Architectures Generalize via Recursion.
19006,1, The main limitation is that the best architectures as currently described are less about discovery and more about human input;
19007,1, Is there an elbow that indicates the threshold cut?
19008,1,"\n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation?"
19009,1, Or in other words why should we call this zero-shot imitation instead of simply reinforcement learnt policy being learnt and then used.
19010,1, Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity?
19011,1,. The max-pooling operation is then used to obtain a vector representation of size d.
19012,1,\n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from.
19013,1,\n\nPros\nExperiments seem well done.
19014,1,". However, the main techniques such as RBM, parser to extract word pairs, tf-idf for filtering, and k-means for clustering, are all existing standard techniques."
19015,1,\n\nOnly one dataset is used for the evaluation and it seems to be very limited and small.
19016,1, The second paragraph of introduction listed some related work and yet failed to compare with them well. 
19017,1,"\"" in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods."
19018,1,\n\nCons\n\n1. I don\u2019t find the theoretical analysis to be very useful.
19019,1, Given the ongoing review decisions/issues I'm putting my review slightly below accept..
19020,1,"\""\n\""experiments and theroy analysis are done\""\n"""
19021,1," However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters."
19022,1," Did the authors tried their approach to non-DA tasks, such as generating images, as often done with GANs?"
19023,1," In addition, the authors of MagNet also compared their performance in white-box (attacker knowing the reformer), gray-box (having multiple independent reformers), and black-box (attacker not knowing the reformer) scenarios, whereas this paper only considers the last case."
19024,1, I would like to see proper discussion here. 
19025,1,"\n\n-\tAuthors reason about biological inspired approaches, using Attention and Memory, based on existing literature."
19026,1," The paper did not get a chance to test these differences, and I can only raise a few hypotheses."
19027,1, The writing is excellent.
19028,1,\nCons\n-\tThe method is not presented clearly enough: the main component modeling the network activity is not explained (the HDDA module used)
19029,1,\n\nI had some difficulty to understand the paper because of its presentation and writing (see below).
19030,1,\n\n2. More details on how node embedding vectors are initialized.
19031,1,"\n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental."
19032,1, \n\nLet me state the main interesting results before going into criticisms:\n1. TR method seems to generalize better than mini-batch SGD.
19033,1," \n\nThe previous paper (Mirowski et al. 2016) introduced a deep RL agent with auxiliary losses that facilitates learning in navigation environments, where the tasks were to go from a location to another in a first person viewed fixed 3d maze, with the starting and goal locations being either fixed or random."
19034,1,  Instead these are emotion words that a person chooses to broadcast along with an associated announcement.
19035,1," Especially, the authors propose a visual Turing test to evaluate the synthesize quality of three generative models: WGAN-GP, DFC-VAE, and Pixel VAE."
19036,1,\n\n* I cannot understand figure 7 without more explanation.
19037,1, Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable.
19038,1, The authors should add more information about previous work on this topic so that their results can be understood with respect to previous studies.\
19039,1," That's a good idea, but problem reformulation is always hard to justify without returning to a higher level of abstraction to justify that there's a deeper problem that remains unchanged."
19040,1,  There is no meaningful caption for this important figure.
19041,1,"""This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate."
19042,1," The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks."
19043,1, There should be a single definition for p(F|X).
19044,1," Some typos and minor issues are listed in the \""Cons\"" part below."
19045,1,"\n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract."
19046,1," Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works."
19047,1,"  Samples are shown for CIFAR 10, MNIST, and OMNIGLOT. "
19048,1, Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.
19049,1," It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2. "
19050,1, And I would quantify speed tuning via something like the slope of the firing rate vs speed curves.
19051,1," Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless?"
19052,1,\n- The methods are not well motivated. 
19053,1," Other propositions e.g. Theorem 1, proposition 6, could do with clearer intuition leading to them."
19054,1,. This is achieved by forcing the output of some channels being constant during training.
19055,1,"  If it is not, then perhaps the defogger would not help so much at winning."
19056,1,\n\nI think the paper could do a better job differentiating from those other papers.
19057,1, \n2. The listed site for videos and additional results is not active.
19058,1,"\n\n# Minor errors\n- sec1: \""The sampler is trained to minimize a variation\"": should be maximize\n\""as well as on a the real-world\"""
19059,1," Experiments show improved generalization over mini-batch SGD, which is the main positive aspect of this paper. "
19060,1,"\n\nOriginality: Low-rank tensors have been used to derive features in many prior works in NLP (e.g., Lei et al., 2014)."
19061,1,"The authors point out that the original \""value iteration network\u201d (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network."
19062,1,"\n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial). "
19063,1," Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance."
19064,1,"""The main idea of this paper is to replace the feedforward summation\ny = f(W*x + b)\nwhere x,y,b are vectors, W is a matrix\nby an integral\n\\y = f(\\int W \\x + \\b)\nwhere \\x,\\y,\\b are functions, and W is a kernel."
19065,1," It would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted HALS which admit negative values in the optimization, see also the review by N. Giles\nhttps://arxiv.org/abs/1401.5226\nas well as for instance:\nNielsen, S\u00f8ren F\u00f8ns Vind, and Morten M\u00f8rup."
19066,1," Specifically, there is an issue about the speech recognition part of the experiment."
19067,1,"""This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks."
19068,1," In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations."
19069,1," \n\n* The paper overstates the contributions of Q-masking, emphasizing improvements to data efficiency among others."
19070,1,"   However, at least in some key cases in the recent literature, parameter numbers *were* controlled (see e.g. Table 4 of Trabelsi (2017))."
19071,1, A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for.
19072,1,". How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?"""
19073,1,"I\u2019m not an expert, but I assume there must be some similar idea in CNNs."
19074,1, The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way.
19075,1, What is the intuition behind the random walks and graphs of Fig 6?
19076,1,\n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers.
19077,1,"""SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent."
19078,1,\n\nThe proposed method is using Elman nets as the base RNN. 
19079,1," If you say something like \""they did not estimate a true posterior\"" then you should quantify this and (presumably) justify the implication that taking a gaussian approximation to the final layer is a *true* posterior."
19080,1," In the experimental part, it is shown that the loss function and test accuracy depend only on the schedule of the decaying noise scale over training time, and are independent of whether this decaying noise schedule is achieved by a decaying learning rate or an increasing batch size."
19081,1," \n\nFurther, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase."
19082,1,  Thus the significance appears a bit limited to me.
19083,1," \n\nPersonally i would not use the term \""convergence\"" in this setting at all as the runs are very short and thus we might not be close to any region of convergence."
19084,1,"\n\nA very bad manner, which unfortunately is often performed by deep learning researchers with limited pattern recognition background, is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is reported."
19085,1, The readers can hardly reproduce the results or evaluate possible performance by reading the paper.
19086,1,  Experiments show its usefulness 
19087,1,"\n\nIn discussing the domain adaptation results, you mention that the L2 regularization \""works very well in practice,\"" but don't highlight that although it slightly outperforms entropy regularization in two of the problems, it does substantially worse in the other."
19088,1,"""\n This was an interesting read."
19089,1," During model evaluation only the forward decoder is used, with the backward operating decoder discarded."
19090,1,The multi-scale representation allows for better performance at early stages of the network.
19091,1, Why is it assumed that human intuition is necessarily good? 
19092,1,"  What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?"
19093,1," I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches."
19094,1," Also, the presentation is quite clear and the paper well written."
19095,1,"\n\n[Summary]\n\nI think this is a good paper which integrates vision, language, and actions in a virtual environment."
19096,1, In particular I appreciate that the authors stress the importance\nof 'non-equilibrium physics' for understanding the SGD process.
19097,1," If yes, it would be good to show them pairwise.\n- figure 5: please explain why the saliency map is relevant."
19098,1,"""This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations."
19099,1," The authors removed a clearly flawed sentence, but the analysis still assumes that two densities (of 'good' and 'bad' examples) are modeled, while in the work presented only one of them is."
19100,1,"""This manuscript proposes a method to improve the performance of a generic learning method by generating \""in between class\"" (BC) training samples."
19101,1," This is not to say that demonstrating that these patterns can arise as a byproduct is not important, on the contrary."
19102,1,"\n\nAs a matter of fact, one of the issues of the presented quantized techniques (the fact that random rotations might be needed when the dynamic range of elements is large, or when the updates are nearly sparse) is easily resolved by algorithms like QSGD and Terngrad that respect (and promote) sparsity in the updates."
19103,1," \n\nFirst, I did not see any novel idea presented in this paper."
19104,1, -- finding a more efficient search path would be an important next step.
19105,1,"""The paper addresses an important problem in multitask learning."
19106,1, The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task.
19107,1,"\n\n- \""... is more computationally and ...\""\n\n- \""... our results for performing final ...\""\n"
19108,1," \n\nHowever, the results are weak."
19109,1, \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5).
19110,1,"\nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates."
19111,1," \n\nOn the negative side there is very limited innovation in the techniques proposed, that are indeed small variations of existing methods. \n"""
19112,1, Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops.
19113,1,\n\nThe AIR paper also contains references to relevant previous work.
19114,1, \n- The modifications add complexity.
19115,1," some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26\u201330."
19116,1,\n- Sec 4.6: The explanation for why the accuracy drops for all models is not clear.
19117,1,"  It seems to assume that you don\u2019t have much unlabelled text (or you\u2019d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well."
19118,1,\n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling)
19119,1,\n\nThe tables look ugly.
19120,1, but it would need a lot of work to address the issues raised above.
19121,1, Many mistakes in the presentation and experiments.
19122,1,\n\n4. It would be useful to discuss the implementation of the method as well. 
19123,1,\n\nAnother question that I had is why use a L1 loss when in the evaluation you're using L2?
19124,1," But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling?"
19125,1, It would be much more helpful (and easier to read) if it were enlarged to take the full page width.
19126,1," This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations."
19127,1," It is said that *a* random direction was used from each minimiser to plot the loss, so how the 2D directions obtained ?"
19128,1,\n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy.
19129,1,"  This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent.[[CNT], [PNF-NEG], [CRT], [MIN]]  It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities.[[CNT], [PNF-NEG], [CRT], [MIN]]\n\n* As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4)."
19130,1,"  The introduction is particular well written, as an extremely clear and succinct introduction to optimal transport."
19131,1," However, there are several important negative points which should prevent this work, as it is, from being published."
19132,1," Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences."
19133,1, I searched a bit but it is not possible to find any kind of similar results.
19134,1,"\n\u201c.. we demonstrate that the agent obeys to maximize its counterfactual return as the Nash Equilibrium\u201c => Perhaps, this should be rewritten as \u201cAgents maximize their counterfactual return in equilibrium."
19135,1,"""[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner."
19136,1," Moreover, many of the multi-task baselines obtain a worse performance than a simple perceptron (which does not account for multi-task relationships). \n"""
19137,1,\n\n(3) I like the idea of data augmentation with paraphrasing.
19138,1,"\n\nHow the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?"""
19139,1,There are also many missing details in the experimental section: how were the number of \u201cactive\u201d components selected ?
19140,1,\n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution.
19141,1," I can't see how this approach would be tractable in more standard program\nsynthesis domains where inputs might be lists of arrays or strings, for example."
19142,1,"\n\nThe inference gap is log p(x) - L[q], the approximation gap is log p(x) - L[q^* ] and the amortization gap is L[q^* ] - L[q]. The amortization gap is easily evaluated."
19143,1,"  \n\nAs an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance."
19144,1,"\n\nTo keep my text readable, I assume we are working in feature space\ninstead of state space and use different letters for learner and expert:"
19145,1,?\n- How were the 5k source words for Procrustes supervised baseline selected
19146,1,"The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information."
19147,1," Is (b), (d),(c) in a sequence or is testing moving from plain to different things?"
19148,1,"\n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines."
19149,1,"\nCons:\n-\tEmpirical evaluation only on small scale datasets.\n"""
19150,1, An innovative training criterion based on that certificate is proposed.
19151,1,It would have been good to have a summary/conclusion/future work section\n\u00a0\nSUMMARY: ACCEPT.
19152,1," In particular, perhaps the deviations from Brownian motion could also be due to the discrete\tnature of SGD (i.e. that the continuous-time formalism is only an approximation of a discrete process)."
19153,1,"\n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling)."
19154,1,"\n\nCons\n-------\n\n1. Some experimental setups are unfair, and some other could be clearer"
19155,1, though they don't provide much insight into the rate at which epsilon should be decreased with n or similar properties.
19156,1, This reviewer is a bit sceptical to the methodology.
19157,1,"\n\nPros: new and interesting result, theoretically sound."
19158,1,"\n\nPage 7: \""In the supervised setting...\"" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside."
19159,1,  The empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distortions.
19160,1,"\n- For the interpolation results you say \""we output the argmax\"", what is meant?"
19161,1,\n- Show the effect of the reparameterization trick on estimator variance.\n- Compare the bias and variance of TRPO estimates vs the proposed method.
19162,1," For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically."
19163,1," Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques."
19164,1," I believe this paper needs to focus on the working of the VCs for few-shot experiments, showing the influences of some of the choices (layer, network layout, smoothing, clustering, etc)."
19165,1," Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots."""
19166,1, As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model.
19167,1,"\n\nMINOR: \nFirst paragraph of Section 3: \n- The definition of a mesh (F=(V,E,A)) is not correct: Both E and A essentially define the same connectivity."
19168,1,"  Since the authors combine their method with a GAN, it is not surprising that the generated images look more realistic."
19169,1,"\n\nThird, the paper is rather vague or imprecise at points."
19170,1," However, I have a few major and minor comments.\n\nMajor comments:\n\n1. The proof of Lemma 1 is incomplete."
19171,1," The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017)."
19172,1,"\nGiven this architecture, the authors focus on characterizing the objective landscape of such a problem."
19173,1,"\nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper. "
19174,1,"\n\nFor the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine \u201ccontexts\u201d that were not observed during training."
19175,1,"\n  * relation between the p(y_i = 1) (in PCN) and g(x_p,x_q) in Eq 2 could be made more explicit, PCN depends on two images, according to Eq 1, it seems just a sum over single images."
19176,1,\n\nThe title is misleading.
19177,1,\n\n\u201cWe can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.
19178,1,"""This paper analyzes the effect of image features on image captioning."
19179,1," As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100."
19180,1,\n\nAdditional Comments:\n- Why in training you used logistic loss instead of the more common cross-entropy loss?
19181,1,"\n* The practical advantages of the proposed approach are twofold:\n1. Given a fixed parameter budget, coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets."
19182,1,\n4. The paper seems to misuse the term \u201cFalse positive rate\u201d as the y label of figure 3(d/e/f).
19183,1, \n2. Lack of motivation for IE or UIE.
19184,1,"  But one could use a small value of K (say, K=5)."
19185,1,"""The authors propose a mechanism for learning task-specific region embeddings for use in text classification."
19186,1, The method is simple and shows to be extremely effective/accurate in practice.
19187,1,"\n\n(2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights."
19188,1,\n\nReferences issues:\n- harmonize citations: if you add first name for some authors add them for all of them: why writing Harold W. Kuhn and C. Vilani for instance?
19189,1,"  This doesn't strike me as the most effective way to try to assess the seen/unseen distinction, since, as I understand the procedure, you will end up with very common prefixes alongside less common prefixes in your validation set, which doesn't really correspond to true 'unseen' scenarios."
19190,1," The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to \u201cparallel ordering\u201d and \u201cpermuted ordering\u201d and show the performance gain."
19191,1,Cons:\n(-) The gap with full precision models is still large \n(-)
19192,1,"\n-- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages)."
19193,1," (This is in part, addressed in the CIFAR80 20 experiments in the appendices)."
19194,1, The proposed approach captures several popular few-shot learning approaches as special cases.
19195,1, They all seem to be equally capable.
19196,1," While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer."
19197,1, This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks.
19198,1, Is that correct?
19199,1," Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions."
19200,1, There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works.
19201,1,"\n\nI was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data."
19202,1, The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction.
19203,1,"""Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and"
19204,1," However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat. "
19205,1, There isn't really much novelty in the work.
19206,1," For example, it is difficult to discover residual network denovo."
19207,1,"\n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU."
19208,1," If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)?"
19209,1,\n\nThe suggested techniques are nice and show promising results.
19210,1," This discussion does not appear at all in the manuscript and it would be an important limitation of the method, specially when dealing with large-scale data sets."
19211,1,"""Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention."
19212,1," Next, the authors consider the problem of learning a time-unlimited policy from time-limited episodes."
19213,1,"""This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector)."
19214,1," I would encourage the authors to implement colocRL [1] on their own hardware and software systems, and make direct comparison. "
19215,1," Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible)."
19216,1, It seems the implicit SGD approach is better in the experimental comparisons. 
19217,1, but not as high performance as using Glove embeddings.
19218,1,"\nBesides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture."
19219,1,"\nHowever, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks."
19220,1," However, even for that sort of setting I think the paper requires some additional work, as some final parts of the paper have not been tested yet (the interesting part of explanations)."
19221,1,  \nWhile the paper is very well-written
19222,1," I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3."
19223,1,"""The paper proposes a method for identifying representative examples for program\nsynthesis to increase the scalability of existing constraint programming\nsolutions."
19224,1," For example, if we use the single shared question embeddings, but increase the size from 128 to some larger number like 192, can we observe similar improvement."
19225,1," \nHere are my concerns:\n1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model."
19226,1," \n\nIntroduction:\n- \""Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\"""
19227,1,\n\n2. I also have concern with the time efficiency of the proposed method.
19228,1, My feeling from reading the paper is that it is rather incremental over Cai et al.
19229,1,Conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketches. 
19230,1, Nothing was given about the combination weights of the method.
19231,1,\n- The experimental results are impressive in their outperformance.
19232,1,"\n\nThe experiments on NYUv2 involves non-standard settings, without a good justification."
19233,1," Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer?"
19234,1, \n\nClarity: The paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribution.
19235,1,"\n\nIn fact, this is already appearent both from the model architectures and the\ngenerated examples: because the model aims to fill-in blanks from the text\naround (up to that time), generated texts are generally locally valid but not\nalways valid globally. This issue is also pointed out by authors in Appendix\nA.2."
19236,1," Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier."
19237,1," The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting."
19238,1, I could not find any technical contribution or something sufficiently mature and interesting for presenting in ICLR.
19239,1, I thus recommend a clear rejection.
19240,1," Question for the authors: in the limit of infinite training\ndata and model capacity, will the neural network training lead to a model that will reproduce the\nprobabilities in 3.3?"
19241,1,"""Authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information."
19242,1," On the other hand, \ngiven the expression of the proposed regulatization,\nit seems to lead to non-convex optimization problems which are hard to solve. Any comment on that?"
19243,1,"It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based. "
19244,1,\n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance)
19245,1," While they provide some good references to support this statement it would have been interesting to show whether they actually improve TTA under image parameter variations\n"""
19246,1, It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S.
19247,1,"  The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range)."
19248,1, The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance.
19249,1, The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next.
19250,1, and (2) not specific to face images.
19251,1,"  The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function."
19252,1,\n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100?
19253,1," 5) The results presented in this paper given the complexity of the method are just not great -- for example, WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU (Google's Neural Machine Translation System) -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that? . "
19254,1," However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable."
19255,1,\n\nCould you also run experiments on the real-world datasets used by the 3BE paper?
19256,1, Is it the same as the generated representation?
19257,1, I\n\nThe model is inspired by the variational autoencoder.
19258,1," The graph (wedge, claw, etc) characteristics are good (but simple) metrics; however, it is unclear how a random graph with the same size and degree distribution (configuration model) would generate for the same metrics (it is not shown for comparison)."
19259,1, It is also unclear to me what are the sources of the uncertainties captured.\
19260,1," \nAlso in terms of experiments, there is not enough exploration of simpler sparse learning methods such as heavy regularization of the weights. """
19261,1," Why not IBM 4, for instance ?"
19262,1," Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled."
19263,1," For me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task."
19264,1, The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration.
19265,1,"""This paper proposes a computationally fast method to train neural networks with normalized weights."
19266,1," Typically, the regularization depends on both hyperparameters and parameters."
19267,1,\n- The evaluation is a bit limited to two specific downstream prediction tasks.
19268,1, Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout.
19269,1,"\nIn particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons."
19270,1, Bridging the gap between stochastic gradient MCMC and stochastic optimization.
19271,1," \n\nThis paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task."
19272,1," On the other hand, the proposed method is also specific to navigation tasks in 2D or 3D environment, which is hard to apply to more general memory-related tasks in non-spatial environments."
19273,1, Authors should pay attention to explain more detailed analysis about this point in the paper.
19274,1, 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers.
19275,1, The paper is an extension of Kawaguchi'16.
19276,1,"PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer."
19277,1,\n* Figure 3 suggests that most of the time between the first and second change of the noise scale (approx. epochs 60 to 120) are spent on overfitting.
19278,1, Extremely low bit neural network: Squeeze the last bit out with ADMM.
19279,1, This is a merit compared with existing protocols used in generation evaluation.
19280,1,\n\n4. The sequence ordering is important.
19281,1," \nFor the base line Encoder+HC, was the encoder trained independently?"
19282,1," Although the performance on automated metrics is encouraging,"
19283,1,"\n - the authors define color invariance as a being invariant to which specific color an object in an image does have, e.g. whether a car is red or green, but they don't think about color invariance in the broader context - color changes because of lighting, shades, ....."
19284,1," Correspondingly, the derivative of C is a small negative value."
19285,1," In my personal opinion, the explanations are opaque and unsatisfactory."
19286,1,The authors have listed a few related algorithms in the last paragraph on page 1.
19287,1," The author follow the reasoning that '... these ideas were not explored in the context of survival analysis', thereby disregarding the significant published literature based on the Concordance Index (CI). "
19288,1,"\n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods. "
19289,1,  All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.
19290,1,  A skeptical perspective on the paper is that the adversarial loss just makes the images look prettier but makes no difference in terms of task performance (control).
19291,1,\n\nThis paper is reasonably readable.
19292,1," There are some clarity issues related to the use of the term \""activation function\"" and a typo in an equation but the authors are already aware of these."""
19293,1,"""The topic is interesting however the description in the paper is lacking clarity."
19294,1," Furthermore, in Section 4 a new method is proposed, that is to combine the best parts of the already existing models in the literature."
19295,1, but I think the paper would be a lot stronger and more convincing with some additional work.
19296,1,"""The paper extends the prototypical networks of Snell et al, NIPS 2017 for one shot learning."
19297,1," However, I find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality."
19298,1,"  The actual setup seems somewhat arbitrary,"
19299,1,"  This paper helps others to better understand the vulnerabilities of DNNs."""
19300,1, it is a bit too applied and specific to the particular task they studied than is appropriate for a conference with as broad interests as ICLR.
19301,1,".\n\nMinor issues:\n\n1) In Eqn. (4), it would be better to use a vector as the input of softmax.\n\n"
19302,1," The authors a) extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights, and b) and algorithm to determine the rank of each convolutional filter."
19303,1,"\n\nAt the beginning of section 3, the discussion about the independence of the terms in the decomposition (6) is not completely relevant: alpha terms embedded the relation between the target and the context i"
19304,1,"\n\n- My main concern is that I am really trying to place this paper with respect to doing reinforcement learning first (either in simulation or in the real world itself, on-policy or off-policy) and then just using the learnt policy on test tasks."
19305,1," More specifically, while the profile coefficient is applied for every input element in Eq. (1), it\u2019s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer\u2019 is more confusing than clarifying.\n\n3."
19306,1,"\nThe emphasis on the computational demand of 1-3 minutes for LCE seems like a red herring: MetaQNN trained 2700 networks in 100 GPU days, i.e., about 1 network per GPU hour."
19307,1, however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks
19308,1,  \n- It is not clear how authors uses PCA to reconstruct faces in the test set.
19309,1, A proper embedding would have mapped $x$ into a function\nbelonging to $\\mH$.
19310,1, Updates are made to the parameter representing the # of bits via the sign of its gradient.
19311,1," The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why."
19312,1," They define three criteria: Disentanglement, Informativeness, and Completeness."
19313,1," \n- The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported."
19314,1,"\n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \""over a variety of viewpoints\""."
19315,1,"\n\n- In the experimental section an emphasis is made as to how small the number of recurrent params are, but at the same time the input/output projections are very large, leaving the reader wondering if the workload simply shifted from the RNN to the projections."
19316,1,"  There is a PIR per level and filter (as defined in C4) but in the setup the L_{PIR} was mentioned to be a scalar function, how are the values then summarized?"
19317,1, A batch or an epoch or other?
19318,1,"  The approach is validated using programming assignments from an online course, and compared against syntax based approaches as a baseline."
19319,1," In those scenarios, the proposed method might not work well and we may have to resort to the gradient free methods. "
19320,1,\n\nAnother motivation of the paper is that targets are given as 1s or 0s while soft targets should work better
19321,1,\n\n3) Controlling the amount of distortion.
19322,1,"\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks."
19323,1,";\n- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.\n\n"""
19324,1,\n-- What are the sizes of the train/test sets derived from the OpenSubtitles database?
19325,1,"\n- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis."
19326,1,"  Combining with an reconstruction objective and \""delete-and-copy\"" trick, it is able to cluster the data points into different groups and is shown to give competitive results on different benchmarks."
19327,1," Some of the baselines seems to perform poorly, and some comparisons with the baselines seems unfair (see the questions below)."
19328,1,"\nCons: the method needs to be compared with typical ensembles of models going only forward in time, it may turn that it using the backward RNN is not necessary\n"""
19329,1, \n\n- Technically it was a bit unclear to me how the objective is defined.
19330,1," Because this paper is not comparing to the state-of-the-art, no specification of the leaderboard version may confuse the other reviewers and readers. "
19331,1,\nNo uncertainty of the mean is stated for any of the results.
19332,1,\n\nWeaknesses\n\n- Design choices made for the reinforcement learning setup (e.g. temporal convolutions) are not necessarily applicable to few-shot classification.
19333,1, (ii) the large amount of data required to learn the policy; 
19334,1,\n\nNegatives\n- While the exhaustive analysis is extremely useful
19335,1, What does \u201cusing game rules to infer the existence of unit types\u201d mean?
19336,1,"\n\nClarity: The paper is relatively clear,"
19337,1,"  \n\n\nReferences\n\n[1] Le et al, \u201cA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u201d, arXiv 2015\n[2] Arjovsky et al, \u201cUnitary Evolution Recurrent Neural Networks\u201d, ICML 2016\n[3] Cooijmans et al, \u201cRecurrent Batch Normalization\u201d, ICLR 2017\n[4] Zhang et al, \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS 2016\n[5] Sigurdsson et al, \u201cHollywood in homes: Crowdsourcing data collection for activity understanding\u201d, ECCV 2016\n[6] Sigurdsson et al, \u201cAsynchronous temporal fields for action recognition\u201d, CVPR 2017"""
19338,1,"This is\ncryptic, just show us that this is the case.[[CNT], [null], [DIS], [GEN]]\n\nRegarding the experiments there needs to be more discussion about how the\ndifferent model parameters were determined."
19339,1, The second step also shrinks the number of targets over time to achieve clustering.
