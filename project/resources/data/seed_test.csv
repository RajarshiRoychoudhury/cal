,Class Index,Description
0,3," Marking my review as \""educated guess\"" since i didn't have time for a detailed review]"
1,3," The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path."
2,1,  Figures 3 and 5 are\nconvincing evidence that MCMCP compares favorably to direct sampling of\nthe GAN feature space using the classification images approach.
3,3," The variation in an autodidactic update of a value function (Equation (4)) depends on a few things, the in variation future value function estimates themselves being just one factor."
4,3,"  A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features."
5,1," For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation."
6,3,".\n\nThough the authors claimed that they used 3 techniques to accelerate synchronous SGD, only partial pulling is proposed by them (the other 2 are borrowed straightforwardly from existing papers). "
7,3,"""The paper uses a lesser-known interpretation of the gradient step of a composite function (i.e., via reverse mode automatic differentiation or backpropagation), and then replaces one of the steps with a proximal step."
8,3,"\n\n* \""The generated bottleneck samples are then used to estimate mutual information\"" -> an empirical estimation of I(Z,X) would seem a very high variance estimator; the dimensionality of X is typically large in modern deep-learning problems---do you have any thoughts on how the learning process fares as this varies?"
9,3,"\n\nIn Fig. 1, I didn't find the meaning of the acronym NN with no specified width."
10,3," \n\nFor deep nonlinear networks, the results require the \u201cpyramidal\u201d assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points."
11,3,\nI don\u2019t really get the intuition behind this formulation.
12,1,\n\nI find the basic idea quite compelling.
13,3,\n\nA second concern is the experimental exploration.
14,1,\n\n=Clarity=\nThe overall setup and motivation is clear.
15,3,"\n\nThe paper reports a comparison of real-valued and complex-valued neural networks, controlling for storage capacity (with an interesting discussion of controlling for capacity in terms of computational inference)."
16,3," \n\nAt the beginning of section 2.1, I think the authors suggest the PATH function could be pre-trained independently by sampling a random state in the state space to be the initial state and a second random state to be the goal state and then using an RL algorithm to find a path."
17,2,The work that this group does is a disgrace to science
18,1, I recommend the paper\u2019s acceptance for this reason.
19,3,"""This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport."
20,1,". However, if this is the case, a simple solution would be to move the server to a different node."
21,3,"""This paper revisits the idea of exponentially weighted lambda-returns at the heart of TD algorithms."
22,1," Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods."
23,3, At each timestep the model emits an update probability; if this probability is over a threshold then the next input and state update will be skipped.
24,3,"\n\nOverall, I think it is important to study control problems from a statistical perspective, and the LDS setting is a very natural target."
25,3," That said, I think the authors would need to convincingly address issues of clarity in order for this to appear."
26,3,"  Cars follow a fixed model of behavior, do not collide with each other and cannot switch lanes."
27,3,"  It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer."
28,1," This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K."
29,3,"  The  expert demonstrations are sequences of consecutive states that do not include actions, which is closer to a real setting of imitation learning."
30,3,\n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?
31,3,"  The incremental architectural changes, different dataset and training are responsible for most of the other improvements."""
32,1,"""Quality\nThe theoretical results presented in the paper appear to be correct."
33,3," When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient."
34,3,\n\nBelow are some suggestions for improving the paper:\n\nCan you enumerate the paper\u2019s contributions and specify the scope of this work?
35,3," Secondly, why not just do it? \n\n"""
36,1,"\n\nGenerally speaking, this paper is well written and easy to follow."
37,2,The paper is presented as a rather undigestible and tortuous collection of disparate results
38,1, The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.
39,1," The call for papers states that \""we strongly recommend keeping the paper at 8 pages\"", yet the current submission extends well into its 10th page."
40,3, Ho and Ermon 2016 extensively study the fact that imitation is not possible in stochastic environment without the knowledge of the actions.
41,3,\n\nI think you may want to consider minimising ||a(x'|z) - a(x|z_k)|| instead to show that moving from x -> x' is the same as is invariant under the transformation z -> z_k  (and thus the corresponding movement in filter space).
42,3, \n\nThe authors propose a simple method of mixing the global model with user specific data.
43,3,\n\nMy main concern with the paper is in the theoretical underpinning of the work.
44,3, The interesting part here is that the linear projection is a function of the support points.
45,3," \n\nMy primary suggestion is that I would like to know a lot more (even qualitatively, does not need to be extensively documented runs) about how sensitive the results were--- and in what ways were they sensitive--- to various hyperparameters."
46,3," (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)"
47,3,\n\nThis is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function.
48,3,"  This approach extends the method presented on Arxiv on Sigma delta quantized networks (Peter O\u2019Connor and Max Welling. Sigma delta quantized networks. arXiv preprint arXiv:1611.02024, 2016b.)."
49,2,"The research claims these phenomena are understudied, as though there is some amount of study they should endure."
50,3, The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule.
51,3, A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training.
52,2,Such a prestigious journal can surely make better use of its limited space.
53,3,"""This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models)."
54,3,"\n\nIn contrast, the paper \""Coco-Q: Learning in Stochastic Games with Side Payments\"" by Sodomka et. al. is an example where they took a well-motivated game theoretic cooperative solution concept and explored how to implement that with reinforcement learning."
55,3,"""This paper presents a reading comprehension model using convolutions and attention."
56,1,The hypothesis/ the papers goal is clearly stated.
57,1,"""This is quite an interesting paper. Thank you."
58,3, Specifically they take text and add common sense knowledge from concept net.
59,3," \n\nThe paper includes a detailed comparison of the existing formulations from the traditional label propagation scheme to more recent more graph convolutions (Kipf & Welling, 2016 ) and geometric convolutions  (Monti et al. 2016)."
60,3," Even if we ignore that for most tasks only the sparse reward (which favors this algorithm) version was examined, these author's only demonstrate success on 4, relatively simple tasks."
61,2,I also do not feel that the lead PI is qualified to undertake this workâ€¦she needs to be academically successful first.
62,3, For example the ordering in Figure 6 would be inverted if plotting as a function of sample complexity?
63,1," In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution."
64,3,"""1) This paper proposes a method for learning the sentence representations with sentences dependencies information."
65,3,"\n2. The definition of v and delta_TD w.r.t. to v seems unnecessary, since you only use Q."
66,3," \n\nI also like the choice of the two regularisers and wonder whether the authors have tried to make this more general, considering other regularisations ? "
67,3, Their abstract also claims to utilize a convex programming formulation.
68,3,"\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017)."
69,1,"\nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n"""
70,3, This seems like an important baseline to report for the image caption ranking task.
71,3,"""[Main comments]\n\n* I would advice the authors to explain in more details in the intro\nwhat's new compared to Li & Malik (2016) and Andrychowicz et al."
72,2,"There is no novelty in the work, the approach or the results and the implications in this context are quite possibly irrelevant."
73,1,"\n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller,"
74,1," However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n"""
75,3,\n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning.
76,3," The theory is significant to the GAN literature, probably less so to the online learning community."
77,3,"\n- Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017?\n"""
78,3,\n\nOne claim of the paper is that a generative model of fMRI\ndata can help to caracterize and understand variability of scans\nacross subjects.
79,1, \n\nExperiments on the IHDP dataset demonstrates the advantage of the proposed\napproach compared to its competing alternatives.
80,3, Numerical experiments that demonstrate the diversity increment on the generated samples are shown.
81,3,"\n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016)"
82,3,\n\n- One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks.
83,1,\n\nI did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision.
84,1,"  Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity)."
85,3,"\n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more."
86,3, \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks.
87,3,"\n3.) Since the paper describes a computer-vision image synthesis system and not a new theoretical result, I believe reporting the actual run-time of the system will make the paper stronger."
88,3,\n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015).
89,3,"  The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper."
90,1, It was an interesting read.
91,3, I would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plots.
92,3, Is it the same as the generated representation?
93,3," The authors discuss the advantages of SMC and say that is scales better than other methods,"
94,3," I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated?"
95,3," In some cases, MNIST is too easy to consider the complex structure of deep architectures."
96,3,\n \nThis work attempts to bring several fundamental principles in game theory to solve neural network optimization problems in deep learning.
97,1,"The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629\n). "
98,3," The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively."
99,3,\u201d Didn't see how the initial (well constant here) step size was tuned?
100,1, They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks.
101,1,"""Well written and appropriately structured."
102,3," There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller \""effective\"" d', you only have to figure out a generating system for this subspace and carry out optimisation inside)."
103,3," Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.\no\tSNLI data should be described: content, size, the task it is used for"
104,1," \n\nPros:\n+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing"
105,3,"   Even if there is randomness, can the adversary take actions that account for that randomness?"
106,3, They also propose an attention mechanism that works better than others (Symmetric + ReLU).
107,3," The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > \\sqrt{N}."
108,3, The authors study several discrete questions about the aforementioned inference gaps and how they vary on MNIST and FashionMNIST.
109,1, The idea is introduced clearly and rather straightforward.
110,3,"\n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available."
111,3," Both papers can handle sigmoid/tanh, but cannot handle ReLU."
112,3,"\""\n\n* You should add citations for the statement \""In these and related settings, gradient descent has started to be replaced by inference networks."
113,1," \n \nUpdate:\n\nAfter evaluating the response from the authors and ensuing discussion as well as the other reviews and their corresponding discussion, I am revising my rating for this paper up."
114,3,"\n\nIn Section 5.3, experiments look at Inception-ResNet-V2 on ImageNet, showing\nthe proposed approach can reach comparable accuracies to previous work at even\nfewer parameter updates (2500 here, vs. \u223c14000 for Goyal et al 2007)\n"""
115,3,"""This paper proposes a model for adding background knowledge to natural language understanding tasks."
116,3," Finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years."
117,3,"""Summary:\nThe manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model."
118,3," Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one."
119,3,"\n\nThe literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments."""
120,2,"While this study represents a substantial amount of work, it is not all that clear why the work was..."
121,2,"While I think there is good reason to have performed this work, it does make for unexciting reading."
122,3,"   The paper uses a hard decision on the top-k objects (there can be at most k objects) in the final object list, based on the soft object presence values (I have not understood if these top k are sampled based on the noisy presence values or are thresholded, if the authors could kindly clarify)."
123,1,"\nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark."
124,3, It allows the subgoal to adjust the value \nof the underlying model?
125,3,"\n\nI'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images."
126,3,"  In NIPS 2013.\n[Ref3] Zhirong Yang, Jukka Corander and Erkki Oja. "
127,1," \n\nIt is interesting that although L2 regularization does not lead to low \\nu parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization."
128,3," CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection."
129,3,"\n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques, "
130,3, An experiment illustrating this effect could be illuminating.
131,3," More state-of-the-art baselines are needed, e.g. [1] and [2]."
132,1,\n\n3) The evaluation is on fairly small workloads (CIFAR-10).
133,1,"  However, while results on convex hull task are good,"
134,3," The paper does not present a new method, it only focuses on analyzing learning situations that illustrate their main ideas."
135,2,This study does not go much deeper than a sunday morning kitchen table calculation
136,3,  This very much relates to the method used to generalize over subgoals in the paper.
137,3,"\n\n(3) Regarding the ablation in Table 2, my first concern is that the improvement seems small (~0.5%). "
138,2,"I appreciated how the author seemingly had in mind that a goodly percentage of the readership are not native speakers, so anything too academic or erudite might be lost on them"
139,3,\n\nWhy is the paper focused on these specific contributions? 
140,3, The authors evaluate AdvGAN on semi-white box and black box setting.
141,3,\n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2.
142,1, \n\nOverall I think that this paper is decent.
143,3, The experimental results could also include results without style discrepancy loss.
144,3,"  Perhaps the TAXI problem, but you have two different taxis that require different actions in order to execute the same path in state space."
145,1,"\n\nOverall, I thought the paper was clearly written and extremely easy to follow."
146,3,"""\n1) Summary\nThis paper proposes a recurrent neural network (RNN) training formulation for encouraging RNN the hidden representations to contain information useful for predicting future timesteps reliably."
147,1, The authors apply an extension of this\nmethod to topic and sentiment transfer and show moderately good latent space\ninterpolations between generated sentences.
148,3, 1D interpolations and 2D contour plots can be described in a few sentences each.
149,3," For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training."
150,3, I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models.
151,3," The Marchenko-Pastur theorem seemed to even be imprecise, since if y>1, then a < 0, implying that there is a nonzero chance that the positive semi-definite matrix XX' has a negative eigenvalue."
152,3,"   Moreover, what support vectors should be removed by optimizing Eq. (7)?"
153,3,\nThe results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models.
154,3," The 3 ConvNets all have a different goal:\n1. an Open Classification Network (per class sigmoid, trained 1vsRest, with thresholds for rejection)"
155,3," So all classes, popular and rare, have equal weight for Bayesian classification."
156,3,  Why did the authors make this choice?
157,3,"""Summary: \nThe paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem"
158,3," If so, shouldn\u2019t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)?"
159,1,\n\n\nQUALITY\n\nAblation studies show that the guidance rewards are important to achieving the improved performance of the proposed method which is important confirmation that the architecture is working in the intended way.
160,3,\n\nThe AIR paper also contains references to relevant previous work.
161,3,Is there a more sophisticated method that can avoid redundancy without this heuristic?
162,1," \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction."
163,3, I have no reference point to understand these graphs
164,3,"  How this \""most interpretable\"" were selected?"
165,3, \n\nThe main concern is that this looks like a class project rather than a scientific paper.
166,3,"""The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets."
167,3,"\n3. You could consider giving the Discriminator, real data etc in Fig 1 for completeness as a graphical summary."
168,3,"\n\nIt may be that the methods in this paper can outperform previous ones -- that would be interesting,"
169,1,"\n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,"
170,3," The proposed method was evaluated on the SQuAD dataset only, and marginal improvement was observed compared to the baselines."
171,1,"\n\nThe paper is well written,"
172,3, Would be good to know if the authors have any intuition why is that the case.
173,1, The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy.
174,3, The objective function is a linear mixture of the cost of generating the tree structure and the target sentence.
175,3,\n\nThe contribution of the paper is: \n - some proposed methods to extract a color-invariant representation
176,1, I think this is the main contribution of this work.
177,3,"""This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights)."
178,2,This left me somewhere between scratching my head and pulling my hair out
179,3, The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary. 
180,1,\n\nContributions:\n- The paper proposes a cheaper activation and validates it with an MNIST experiment.
181,1,\n\nIt is nice to see the application of ideas from different areas for learning-related questions.
182,3," I couldn't see any emerging trend/useful recommendations (like \""if your problem looks like X, then use algorithm B\"")."
183,3,\n\ncons:\n(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive.
184,2,I found the entire premise of the work to be utterly theoretically bankrupt
185,3,"\nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions."
186,3," In this work, they explore the effect of using partial natural language scene descriptions for the task of disentangling the latent entities visible in the image."
187,1,"""The authors has addressed my concerns, so I raised my rating."
188,1,\n\nThe contributions of the paper are in parts surprising and overall interesting.
189,3,"  After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation."
190,3,"""The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves."
191,3," I don\u2019t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data."
192,3,\n\n5.1: What dataset/model was this experiment done on?
193,3,  It simply applies hierarchical clustering on the learned similarities and use cross-validation to pick a stopping condition for deciding the number of clusters.
194,1,  Moreover the paper is clearly written.
195,3,"\n \nThe 2 methods compared in Fig 2 reach the same objective values at convergence, but is there any particular difference in the solutions found?"
196,3, The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class.
197,1," It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate."
198,1," While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms),"
199,3, The authors propose to introduce a set of latent variables to represent the fertility of each source words.
200,3," I now understand more the claims of the paper, and their experiments towards them."
201,2,"In a revised form, it would not look out of place in a scholarly journal."
202,3, Gradient descent can converge to a first order optimal solution. 
203,1,"""This paper is well written and easy to follow."
204,2,The article could benefit from a good linguistic editing in order for it to be better sound and...
205,2,The presentation resembles a fishing expedition with claims made beyond the performance of the technology
206,3, All these factors should play a very significant role in the experimental validation of their hypothesis.
207,3," It is trained with supervised learning, by minimising the cross-entropy error between labels and the softmax output.\n\nThe paper's claim of combining unsupervised (self-organising) with supervised training is misleading and confusing."
208,3,"\"" \n\nNot true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct.(The term \""deep learning\"" was introduced to ML in 1986 by Dechter for something else.)"
209,3,"  If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?"
210,3," After estimating this operator, it is straightforward to use this to generate the estimated optimal control input."
211,3," The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not)."
212,3,". Since the authors are extending their work and since these issues might cause training difficulties,"
213,1," The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss."
214,3, In order to populate the matrix that will eventually give rise to the PVFs the agent must collect transitions.
215,3," Another two sources of variation are: the uncertainty over how likely each path is to be taken, and the uncertainty in immediate rewards accumulated as part of some n-step return."
216,3," For example, the authors defer many technical details."
217,3,"""The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable."
218,1, I find the argumentation the authors give in Figure 6 much sharper.
219,3,\n\nComments on the details of the paper:
220,1," (2) the proposed CNN architecture, combining images and text (using word embedding."
221,3," For instance, since the reward has been designed arbitrarily, it could have been defined as giving a penalty for those missing customers that are at some distance of an agent."
222,3, Are you sure that you've run enough iterations to fully converge?(Fig 4 was still trending up for b1=64).
223,3,"\n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods?"
224,1, This seems similar to the visual domain where edge detectors emerge easily when trained on natural images with sparseness constraints as in Olshausen&Field and later reproduced with many other models that incorporate sparseness constraints.
225,3, How does different initializations affect results?
226,3," Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N."
227,3," Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed."
228,3, \n\nChapter 3 constructs a simple example with synthetic data to demonstrate the effect of Bayes factors.
229,3, The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.
230,1," The sparse coding models see something similar: without sparsity constraints, the image representations do not resemble those seen in V1, but with sparsity, the learned representations match V1 quite well."
231,1,"\n\nNonetheless, I believe that the paper represents an interesting and worthy submission to the conference."
232,3," However, I would expect there to be an optimum."
233,1," The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well)."
234,2,My first concern is that I dont get it. - (via shitmyreviewerssay
235,3,"""The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size;"
236,3,\n\n2. Why is the crash car dataset used in this scenario?
237,3,"\nMoreover, the fact that there are huge oscillations makes me think that the\nauthors are measuring the function value during the line search rather than\nthat at the end of each iteration."
238,3, \nFor example the authors state in various parts that DSC does not work on\nnon-Euclidean data.
239,3,\n\nIn terms of experimentation it would be interesting to see the reciprocal of the results between two datasets.
240,3," In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules."
241,3," However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters."
242,3, See for example the first paragraph of the Related Work.
243,2,Your discipline doesnt exist
244,3," In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better."
245,3," In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12).[[CNT], [CNT], [DIS], [MIN]] It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples."""
246,3,"\n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful."
247,3,"\n\n* \""aim to learn what parameter values of the base-level learner are useful\n  across a family of related tasks\""\n\nIf this is essentially multi-task learning, why not calling it so?  \""Learning\nwhat to learn\"" does not mean anything."
248,3, The paper discusses the influence of the label redundancy both theoretically and empirically.
249,3, The proposed method does not assume that the source sentences have only one style and allows them to have unknown styles.
250,3, How good are the trust region updates based on q_t given the huge variability associated with the mini-batch operation? 
251,3, \nWould the learner then just prefer to go back towards those states that it can approximate and endlessly loop?
252,1,"\\n\nConclusion:\n- I feel that the motivation is good,"
253,3,"  In particular, in Sec 2.5 the feedback pass for weight updating is computed."
254,3,\n\n- What architecture did you use for the prior generator GAN?
255,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
256,3,"""The paper proposes training neural networks using a trust region method, in which at each iteration a (non-convex) quadratic approximation of the objective function is found, and the minimizer of this quadratic within a fixed radius is chosen as the next iterate, with the radius of the trust region growing or shrinking at each iteration based on how closely the gains of the quadratic approximation matched those observed on the objective function."
257,3,\n\n4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST).
258,3," The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3). "
259,3,"""This paper presents a so-called cross-view training for semi-supervised deep models."
260,3," Most importantly, in the FL setup, communication is the bottleneck."
261,3,\n\nPros/cons\nPros\n-Adresses an important problem in representation learning
262,3," For one-shot learning, in addition to a given input image, the following data augmentation is proposed: a) perturbed input image (Gaussian noise added to input image features);"
263,3, Spherical interpolation as recommended by White (2016) may\n  improve qualitative results.
264,1,"\n\nI have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted."
265,3,"\n\nSPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network."
266,3," However, it's still more convinced if the paper method is demonstrated in more domains."
267,2,"If this was taken from a successfully defended thesis, as it appears to have been, then he should not have been awarded a PhD"
268,3,\n\nSecureML: A System for Scalable Privacy-Preserving Machine Learning\nPayman Mohassel and Yupeng Zhang.
269,3," However, it seems that hyper-parameters for RNN haven\u2019t been tuned enough."
270,3, The approach is evaluated in a multitask grid world domain.
271,1, \nThe paper provides for a good read.
272,1,"\n\nThe work is described in sufficient detail including the experimental setups, data set, neural networks, and results."
273,3, That could help readers to better understand the effect of TR.
274,2,"The author does not exhibit adequate acquaintance with the subject, the scholarship on it, the structure of logical argument, or English."
275,1," \n\nSTRENGTHS: The paper in general is well written and easy to ready. I appreciate the idea of the Turing test and qualitative results presented are quite impressive. Also, the use of  diverse state-of-the-art generative models is also a strong point."
276,3," The reason this might be interesting is that when the parameter space is lower dimensional (not an issue for neural nets, but could be problematic for linear FA) the constraint might be too much leading to significantly poorer performance. "
277,3,"""This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution."
278,3,\n\nThere have been a number of MTL methods based on task clustering.
279,3,"""This paper introduces a new model to perform image classification with limited computational resources at test time. "
280,3, \n\nPlease explain the reason why STB is better than the proposed method with 100k positive samples.
281,1," The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input)."
282,3, One due to the function family of variational posterior distributions used in inference and the other due to choosing to amortize inference rather than doing per-data-point inference as in SVI.
283,1," The testing framework will be made public too, which adds to the value of this paper."
284,3,"\n- there are multiple distinct metrics that could be used on the x-axis of plots, namely: wallclock time, sample complexity, number of updates."
285,3," You have very few sequeces/subjects.[[CNT], [SUB-NEG], [DFT], [MIN]] Did  you split by *subject*? I think this is CRUCIAL, and a lot of the results hinge on this answer."
286,3, The analysis suggests a practical (heuristic) algorithm incorporating two features which emerge from the theory: L2 regularization and keeping a history of past models.
287,3,".\n\n-In a sense, I find that the approach suggested by algorithm 1 has some connections with structural risk minimization: by increasing k1 and k2 - when looking for the mapping - you increase the complexity of the model searched while trying to optimize the risk which is measured by the discrepancies and loss"
288,3,  It'd\nbe stronger if the approach could also be demonstrated in another domain.
289,3, It presents experiments during training and with varying network sizes.
290,3," Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied."
291,3, It proposes a notion of low complexity networks in this context used for identifying  minimal complexity mappings which is assumed to be central for recovering the best cross domain mapping.
292,3," With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm."
293,3,\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors.
294,3," \n\nFor chemical molecule generation, a direct comparison to some more recent work (e.g. the generator of the grammar VAE [3]) would be insightful."
295,3,The authors have listed a few related algorithms in the last paragraph on page 1.
296,3,\n\nI have two specific concerns:\n* Did you perform any checks to verify whether the variance in the estimators use to bound log p(x) is controlled (for the specific # samples you use)?
297,1,\n==========================\n\nThe authors present an interesting new method for generating adversarial examples. 
298,3,"\nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this?"
299,3, A hard constraint is imposed during decoding to avoid trigram repetition.
300,3,\n\u2013 Steady state assumption: How can this be relaxed to further generalize to non-static scenes?
301,1,"\""\n- Targets an important problem\n\nCons:\n- Related work seems inadequately referenced"
302,3, There are also many papers on this topic using Gaussian process state-space (GP-SSM) models where an explicit prior is assumed over the underlying dynamical systems.
303,3,".\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach."
304,3,\nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\n
305,3, I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network.
306,3, Their method claims increased stability in contrast to the existing one.
307,1,"""[After author feedback]\nI think the approach is interesting and warrants publication."
308,1, This is a very well-accepted method actually used in real-world autonomous cars.
309,3," This is especially true when we note that only a subset of the options are shown: given enough random options, it should be possible to select a subset of them that are reasonably spread across the state space as well."
310,1," The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound."
311,3,"  Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications."
312,3," For 1) they examine a low-rank version of distributed SGD where instead of communicating full-rank model updates, the updates are factored into two low rank components, and only one of them is optimized at each iteration, while the other can be randomly sampled."
313,3," Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000."
314,3,"  However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data."
315,1,.\n\u2022\tVery impressive results
316,3," For example, it is not described anywhere what loss is used for learning the model. "
317,3,The idea of amortizing inference is perhaps more general. 
318,3,  \n- I would introduce the do-notation much earlier.
319,3," From a human standpoint, in those cases, the smoother meshes would in fact be preferable."
320,3, The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks.
321,3,"\n\nDetailed comments:\n* Authors should at very least cite (Vinyals et al, 2017) and explain why the environment and the dataset released for Starcraft 2 is less suited than the one provided by Lin et al."
322,1,". For instance, averaging the node embeddings is something that has shown promising results in previous work"
323,3," The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data."
324,3, \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1].
325,3,"""The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft."
326,3,"""This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors."
327,3, The activation function keeps the activation roughly zero-centered.
328,1,"\n\nOverall, I believe this paper to be a useful contribution to the literature."
329,3,"\n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise."
330,1,. It is clearly written (there are some typos / grammar errors).
331,3,"""This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases."
332,3, In Section 4.2.1 statements on resemblance and closeness to mean faces could be tested.
333,3, This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data.
334,3," Also, in the adding problem, it would be cleaner if you down-sampled a bit the curves (as they are super noisy)."
335,3,"  is it better to have a small probability of fulfilling all goals, or to prefer a high probability of fulfilling half the goals?"
336,3,"\n\nMoreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold."
337,3," As a consequence, it uses:\n-\tOne temporary memory storage (inspired by hippocampus) and a long term memory\n-\tA notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks."
338,3," Thus, each individual neuron seeks to maximize the sum of payments it receives from other neurons minus the cost for observing the activations of other neurons (plus an external reward for success at the task)."
339,3, For example GCN has a representation\nspace (latent) much smaller than DSCG.
340,1,"""The authors did extensive tuning of the parameters for several recurrent neural architectures"
341,3,\n\nThe shift variables seem only useful when they are not shared for different pixels.
342,3,"""In this paper, the authors propose a recurrent GAN architecture that generates continuous domain sequences."
343,3,"The same holds for statements like \""... our method is a first step ...\"", which is very hard to justify. "
344,2,"I urge the authors to not publish this article anywhere, as it will impede the progress of scientific understanding"
345,3," However, as pointed out by the authors, there is a lot of similar prior work in software testing."
346,1, \n\nClarity:  The paper is clearly presented and easy to follow.
347,3, The paper develops two models for the decision boundary:\n\n(a) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspace.
348,3, The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop.
349,1,"""I enjoyed reading the paper."
350,1, The intuition of the proposed approach is clearly explained and it seems very reasonable to me.
351,1," The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017),"
352,3,\n-Which epsilon did you use for evaluation of DDQN in the experiments?
353,1," \n\nI find the topic of universal perturbations interesting, because it potentially tells us something structural (class-independent) about the decision boundaries constructed by artificial neural networks. "
354,3,"\n8. What does \""iteration\"" mean in experimental results such as table 2?"
355,1," These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity. "
356,3," In addition, the technical term for this is finite horizon MDPs (in many cases the horizon is taken to be a constant, H)."
357,3," To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks."
358,1,"\nBut this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1."
359,1," They do constitute a good starting place to test a model,"
360,3,"""This paper proposes several client-server neural network gradient update strategies aimed at reducing uplink usage while maintaining prediction performance."
361,3,"""The authors proposed a supervised learning algorithm for modeling label and worker quality. "
362,3,"  It seems that this training process follows a projected gradient descent procedure, where the filter weights of the network are iteratively updated using regular (stochastic) gradient descent and then they are projected onto the set of rank-R tensors."
363,1, so the only real contribution would be in the experiments.
364,3, I am looking forward to a clarification in the rebuttal period.
365,3,"\n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \""jumps\"" in the training curves as signs of learning rate anneal)."
366,1,"\n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost."
367,1,.\n\n2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem.
368,3,"\n\nIn particular, the authors take an already existing dataset, design a trivial convolutional neural network, and report results on it."
369,1," In particular, this could be a first step towards better understanding the optimization landscape of memory-augmented neural networks (Memory Networks, Neural Turing Machines, etc) which try to learn reasoning tasks or algorithms."
370,3,\n\nWhile the categorization is reasonable
371,3,"""In this paper, a model is built for reading comprehension with multiple choices."
372,2,It is worth saying that I am not convinced that you contribute to the evidence base in this paper.
373,3, What is the benefit of joint-training?
374,3,\n\nThe general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.
375,3, Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains
376,2,"The paper brings to mind the Mark Twain quote: 'I didnt have time to write a short letter, so I wrote a long one instead.'"
377,3," The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below)."
378,3, \n- The worse performance compared to backprop and CNNs underlines the open question how to yield biologically plausible AND efficient algorithms and network architectures.
379,3, The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system.
380,3," Still, I think the contribution in that part is a: sentiment-psychologically inspired analysis of the Thumbrl data set."
381,1,\n\nI thought the paper was clear and well-motivated.
382,3,"\nThe h( \\Phi( x ), v^{k-1} ) sort of makes sense \u2026  value is only calculated with respect to only the observation of the maze obstacles but the policy \\pi is calculated with respect to the joint  observation and agent state."
383,3,   A language model could probably be used to generate a compelling distractor.
384,2,I find myself disagreeing with most of this papers conclusions
385,1,"\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. "
386,3,"""This paper builds on Zhang et al. (2016) (Understanding deep learning requires rethinking generalization)."
387,3, yet Table 2 shows that batch size can be predicted with much higher accuracy than chance
388,3,"\n\nThe proposed regularization is evaluated with positive results on a speech recognition task and on an  image captioning task, and with negative results (no improvement, but also no deterioration) on a language modeling and sequential MNIST digit generation tasks."
389,3," They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best)."
390,3," Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST."
391,3," However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values."
392,3,\n\nUpdate:\nThank you for the rebuttal.
393,1, An innovative training criterion based on that certificate is proposed.
394,3," As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term."
395,3,  I have a few recommendations here:\n* It would be stronger to evaluate results on a larger dataset like ILSVRC.
396,3,\n   experiment?
397,3,"""This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer."
398,3,"""The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar."
399,3," In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae)."
400,3, There is no comparison to any such simple baselines.
401,3,"""The paper presents a model titled the \""unsupervised tree-LSTM,\"" in which the authors mash up a dynamic-programming chart and a recurrent neural network."
402,3, The former is the de-facto choice of variational approximation used in VAEs and the latter is capable of expressing complex multi-modal distributions.
403,3," Maybe so, but we won\u2019t know unless the experimental protocol prescribes a sufficient range of LRs for each architecture."
404,1," However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all."
405,3," Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections."
406,3," This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation."
407,2,"Even if other readers found it comprehensible, there is not even a proposed path to make [this model] into a modeling system."
408,3, It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.
409,3," If we wanted to reduce the size of the objective according to the local linear approximation, why wouldn't we choose infinitely large step sizes?"
410,3,\n\n4) What happens with ongoing training?
411,3," Readibility could be slightly increased by putting the figures on the respective pages.\n"""
412,3,  Observation 3 states some property of non non-deterministic coupling but the concept itself seems somehow to appear out of the blue.
413,3, Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable.
414,3, \nIt could be also interesting to (geometrically) interpret the coupling proposed.
415,1," The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions."
416,2,To my mind the paper is similar to the medieval debate concerning angels dancing on pinheads. I do not think the paper should be published
417,1,\n\nThe novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance.
418,2,"I invite the authors to provide compelling, coherent, and quantitative data."
419,3,"""This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy."
420,1,"\n\nClarity\nThe paper is mostly clear,"
421,3,"\n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator."
422,1, The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring.
423,3,"\n\n    Q( S,a ) = g(S) Wa S + Ba \n\nSo this allows the Q-function more flexibility to capture each subgoal in a different linear space?"
424,3," The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique."
425,1, Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here.
426,3, \n\n***************\nUpdates: \n***************\nThe authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper.
427,3,\n\nThe paper is rather preliminary in its examination.
428,3,)\n\n- Minor: in equation 2- is the correct exponent not t'?
429,3,"\"" Why do the features human cognition uses give an optimal predictive accuracy?"
430,3,"  \n\n(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2)"
431,3,"  \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n"""
432,2,This study has the same problem with focusing on a garbage-can group that is not uniformly sampled and has no evolutionary cohesion.
433,3," The \u201creverse validation\u201d method described in Ganin et al., Domain-adversarial training of neural networks, JMLR, 2016 might be helpful."
434,3," But I feel a lot can still be done to justify them, even just one of them."
435,3, And I would quantify speed tuning via something like the slope of the firing rate vs speed curves.
436,3," They also consider the JCP-S approach, where the ALS (alternating least squares) objective is represented as the joint objective of the matrix and order-3 tensor ALS objectives."
437,3,".\nI am not sure if these are complete baselines or if the baselines need to cover other methods (again, not fully familiar with all literature here).\n"""
438,2,"Unfortunately, for the reader (and it almost seems that for the writers as well), there is no insight or conclusions coming out of their comparison."
439,3," Gradient steps that optimize the encoder,\ndecoder and generator are interleaved."
440,3, The actual observations are a weighted linear combination of the emissions from each latent HMM.
441,3,\n2) I would have liked to have seen an ablative study where the detectors are trained on pixels alone and directly compared with the detectors trained with saliency as an input channel.
442,2,I also thing that the English of the manuscript need further polishing.
443,1,\n\n\n - The paper proposes a (I believe) novel method to obtain visual explanations.
444,3, It means that we multiplied by K the number of parameters in our model (K is the number of classes).
445,1,\n\nPros\n-Several interesting ideas for evaluating evaluation metrics are proposed
446,3,"""Summary:\nThis work is about model evaluation for molecule generation and design."
447,3," \n\nI don\u2019t immediately know of work that suggests bootstrapping if an episode is terminated early artificially during training but it seems a very reasonable and straightforward thing to do. \n\n"""
448,3, How do we know if generalizing saturated STE is more worthwhile than generalizing STE?
449,3,"\n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \""noisy\"" training set and adding that to clean data?"
450,3," Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer?"
451,2,"This was a well-written study that upon my first reading appeared flawless. - R1, who recommended to rejec"
452,3,"  Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these."
453,3," The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework."
454,1,Problem statement well defined mathematically and understandable for a broad audience\n\t\u2022\t
455,3,"""This paper introduces a new architecture for end to end neural machine translation."
456,3,"\n\nIn its current form, it's not clear how the proposed approach tackles the shortcomings mentioned in the introduction."
457,3, \n \nb) why the authors use the softsign instead the tanh:  $tahnh \\in C^2 $! Meanwhile the derivative id softsign is discontinuous.
458,2,N/A. (Full review text)
459,3," Are the\nproperties that are being verified different properties, or the same property on\ndifferent networks?"
460,3,"\n\nHowever, the paper shows only preliminary results in which the generator trained to maximize the PIR score (computed based on VGG features to simulate human aesthetics evaluation) indeed is able to do so."
461,3," The authors could have expressed their network using a clean recursion, following the parse chart, but opted not to, and, instead,  provided a round-about explanation in English."
462,3,"?\n8. In the Monte-Carlo sampling, how many pairs are sampled? "
463,3, This would then be captured by the critic gradient for the CAR method that appears towards the end of page 5.
464,3,\n\nTwo local minima are observed: 1) the network ignores stucture and guesses if the task is solvable by aggregate statistics
465,3,"\na. Sigma-Delta model of spiking neurons has a long history in neuroscience starting with the work of Shin. Please note that these papers are much older than the ones you cite: \nShin, J., Adaptive noise shaping neural spike encoding and decoding. Neurocomputing, 2001. 38-40: p. 369-381. \nShin, J.,The noise shaping neural coding hypothesis: a brief history and physiological implications. Neurocomputing, 2002. 44: p. 167-175."
466,3,\n\nIs this work extending the applicability of baselines to new types of problems?
467,3," However, the presented results focus on the performance on held-out data instead of improvements in training speed."
468,1, This work to my knowledge is the first to use a DSL closer to a full language.
469,1," Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly."
470,1, \nThe paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it.
471,1,"\nUsing PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model."
472,3,\n\n- Here's another good paper to cite for the end of 2.2.1:\n  https://arxiv.org/pdf/1707.00683.pdf.
473,3,",  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density."""
474,1,"\n\nThis also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,"
475,3," Performing such an analysis with this model is challenging (i.e. retraining a GAN) and it is not clear if a given image generated by a GAN will always achieve a given epsilon perturbation/\n\nOn a more minor note, the authors suggest that generating a *diversity* of adversarial images is of practical import. "
476,3," To be accepted to ICLR, either outstanding performance or truly novel design principles are required."""
477,2,"Ugh. Read a book. This is grossly oversimplified, and not an appropriate statement for a scientific..."
478,3,    I don't think it can just be assumed a priori that humans would be super good this form of generalization.
479,3, This reward is dispersed to the the individual agents who receive individual external rewards (R_{it}^{ex}).
480,1," The appendix are very useful, and tutorial paper material (especially A)."
481,3,"\n\n3. The proposed two kernels introduce sparsity in the spatial and channel dimension, respectively. "
482,1,\n\nBreaking down the inference gap into its components is an interesting idea and could potentially provide insights when analyzing VAE performance and for further improving VAEs.
483,1,\n(5) Somewhat surprising: MC methods seems to be on-par with TD methods when the reward is sparse and even longer than the rollout horizon.
484,3, The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture.
485,3,"\n5. I did not understand the paragraph beginning with \""This poor estimation\""."
486,3," The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. "
487,3,\n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.
488,3," The approach is evaluated in a tabular domain (i.e., rooms) and Atari games."
489,3," \n\nHowever, when paired with non-cooperative players in the risky PPD game, CCC players lead to an improvement of pay-offs by around 50 percent (see Figure 2, (e)), compared to payoff received between non-cooperative players (-28.4 vs. -18, relative to -5 for defection)."
490,3,"""This paper introduces a method for learning representations for prepositions."
491,3, The authors should report statistics over\nmultiple runs.
492,3, That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta.
493,3,"\n\n\""or in the case of heart failure, predicted BNP level\"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level?"
494,3, The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.
495,1,"n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays."
496,3, It is probably unlikely that the discretisation method can be generalised to high-dimensional setting?
497,3,\n\n- I don't understand how speedup is being computed in Figure 4.
498,3,"  But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure."
499,3,\n2. The notation D for a dataset in Section 3.3 is confusing with D in system D.
500,1," Non of these areas, with the exception of semantic parsing, are addressed by the author."
501,3, And PCA as a non-random projection would a nice baseline to compare against.
502,2,Studies undertaken in such a manner as presented here degrade all science by giving the semblance of legitimacy to illegitimate work.
503,3,"\n\n[8] A Graves, A Mohamed, G Hinton, Speech recognition with deep recurrent neural networks, 2013"
504,1,"""The authors proposed a novel RNN model where both the input and the state update of the recurrent cells are skipped adaptively for some time steps."
505,1, The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima.
506,3," I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e. first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate)."
507,2,"This paper reads like a womans diary, not like a scientific piece of work"
508,3, The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step.
509,3,".\n  Please see the paper:\n  -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)\n  This paper shows that the performance gets even better for higher dimensional data if each feature is relevant."
510,3, Experiments are conducted on both Omniglot and miniImagenet datasets.
511,3, The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network).
512,3, but I think it would be good to also report WERs on the WSJ set in either case.
513,3," (That is, are there counter examples for relaxations of this assumption?)"
514,3," But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging."
515,3," It's ok not to provide the answer if it's hard to analyze, but if that's the case the paper should provide some numerical case studies to show this bound either holds or the gap is negligible in the toy example."
516,3, Were the baselines with concatenated features optimized independently?
517,3, This is applied to the relatively simple domain of Atari games video input (compared to natural images).
518,3,"\nAs far as I remember, there exists also some paper from the nineties that\nlearn the parameters of RBF networks but unfortunately I have not been able to\ngoogle some of them."
519,2,"This paper reads like a womans diary, not like a scientific piece of work"
520,3," \n\nIntroduction:\n- \""Indeed, Zagoruyko & Komodakis (2016) demonstrate that it is better to increase the width of ResNets than the depth, suggesting that perhaps only a few layers are learning useful representations.\"""
521,1,  The empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distortions.
522,1, However the application to program repair is novel (as far as I know).
523,2,No respectable biochemist would be seen in the same county as these data.
524,3," \n\nAs a final general comment, I would have appreciated a paper more self explanative, without referring to the paper [Vilnis & McCallum, 2014] which makes appear the paper a minor improvement of what it is actually. """
525,3, The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not.
526,1,  I commend the authors for making their code available already via DropBox.
527,3," \n\n\nGenerally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations."
528,3,"\n* acronyms should be expansed at their first use"""
529,1," The arguments for skipping this experiments are respectful,"
530,3," The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs."""
531,1, I have not checked the proofs in detail but the general strategy seems sound.
532,3,"\n\nUnfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1."
533,3," In fact, the number of layers is linear in the depth of the network."
534,3,"\n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b)"
535,2,This reads more like a shopping list than an academic paper.
536,3,"\n\nFurther remarks:\nIt would be interesting to see the size and position of the center of mass attacks in the appendix.[[CNT], [CNT], [DIS], [MIN]] The reviewer closely follows and is experienced with various explanation methods, their application and the quality of the expected explanations."
537,3,"\n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}"""
538,3, The resulting model is illustrated on a few goal-oriented dialog tasks.
539,3,"  If this does not happen, what is being shared across tasks?"
540,1, The paper also raises good questions for future works.
541,3," When reading the manuscript the first time, I was expecting experiments on images that have regions that are visible and regions that are masked out."
542,2,Despite all my efforts I failed to understand what the actual focus of this paper is
543,3, A hatch of how to learn the overall process is presented.
544,3," Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units."
545,1, A more convincing argument for a slow server should be provided
546,1,\n\nPros:\n- Good literature review.
547,2,It [the paper] has a kind of self-help quality to it
548,3,\n\nPrevious method used at the distribution of softmax scores as the measure.
549,1," For me, it is not easy to judge the novelty of the approach, but the authors list related works, none of which seems to solve the same task."
550,2,"It appears the authors has generated reports in a hurry, compiled, and presented as an article"
551,3," If not, could you please elaborate on what is different (in the case of 3.2 only, I mean)?"
552,3,". Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN."
553,3,"\n\n\n\n\n\n[1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf\n[2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf\n[3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification. """
554,3," Examples include , \""Attention-Based Convolutional Neural Network for Machine Comprehension\"", \""A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data\"", and \""Coarse-to-fine question answering for long documents\"""
555,1,".\n\nComments\n-------------\n- The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems."
556,3," \n2. The policy network would have to output a probability for each datapoint in the dataset U,"
557,3,  Many are/were Gaussian.\n*
558,3, How long does the encoding time take with 10 million sentences?
559,3," \nFrom these 100 evaluations (with different hyperparameters / architectures), the final performance y_T is collected."
560,1, Figure 3 illustrates the idea nicely.
561,1, The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area.
562,3,"  Moreover, the evolved networks are woefully inefficient in terms of parameter count."
563,3,  How would they respond to similar reductions?
564,3,  I suggest that all these are fully clarified as parts of Algorithm 1 itself.
565,1, and the proposed algorithm is supported by experiments.
566,3,"\n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from."
567,1, \n\nPros:\n1. This paper tackles an important research question. 
568,3,"\n\nFundamentally, a convolutional filter stands for a operation within a small neighborhood on the image."
569,3, Do the authors mean\nexp(o_t(x;w)) = 0 ?
570,3," The message is unclear and the experiments to prove it are of very limited scope,;"
571,3,\n\n\nThe work of representing emotions had been an field in psychology for over a hundred years and it is still continuing.
572,3,"""The idea of the paper is to use a GAN-like training to learn a novelty detection approach."
573,3,"\n6. In section 4, the authors talk about estimating the model uncertainty in the stopping point and propose a way to estimate it."
574,1, The main motivation seems to be that it is easier to optimize.
575,2,The statistical analyses were not correct. Actually they were so confused that I lost all confidence in the analyses and data presentation
576,3," Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin."
577,3, This would give a much more convincing account of the value of saliency in this context.
578,3," By introducing new branching architecture, coupled ensembles, they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget."
579,3, Why is this necessary?
580,3, Does that mean that larger updates are always better?
581,3," The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7)."
582,3,"\n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control)."
583,3," Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on. IEEE, 2014."
584,3,")  Also the dynamic programming for Levenshtein is foundational, not required to present that algorithm in detail, unless there is something specific that you need to point out there (which your section 3.3 modification really doesn't require to make that point)."
585,1,. The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet.
586,3,"It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs)."
587,1, Proposed method to visualize the loss function sounds too incremental from existing works.
588,3," It would be better to show how much \u201celimination\u201d and \u201csubtraction\u201d effect the final performance, besides the effect of subtraction gate.\n\n2)"
589,2,"I recommend to drop these assertions and predictions, and just characterize the effects. Unfortunately this revised scope is more limited."
590,1,\n\nThe originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper.
591,3,"\n\n2) In the paragraph above Section 4.1, the paper made two arguments. I might be wrong, but I do not agree with either of them in general. "
592,3,"\n- How were the network architecture and network size chosen, especially for the multitasker?"
593,3, Why not try on multiple different word embeddings?
594,3,\nIt seems like the derivations presents several equations that score a given span.
595,1, Joining SRM with MetaQNN is interesting as the method is a computation hog that can benefit from such refinement.
596,2,The description of sampling technique is too long and boring!
597,3, \n\nI would additionally like to see a few examples of the time series data at both the 5 minute granularity and the 15 minute granularity.
598,3,"\n\nPost-rebuttal revision:\nAfter reading the authors' response to my review, I decided to leave the score as is."""
599,2,"I hesitated to suggest this manuscript to be rejected, but I handled to the (naive?) idea that this actual pumpkin can turn into a fair-looking princess."
600,3," For example, see \""Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\"" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)"""
601,1," While they provide some good references to support this statement it would have been interesting to show whether they actually improve TTA under image parameter variations\n"""
602,1," Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas."
603,3," This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training \""fast gradient sign\"" method."
604,1,"As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest."
605,3," \n\nPage 5: \u201cnote that we do not suggest a specific neural network architecture for the middle layers, one should select whichever architecture that is appropriate for the domain at hand\u201d - such as?"
606,3,"""The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states."
607,3, Was sharpening used in the image caption generation task?
608,3," The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs."
609,3,  I think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the paper.
610,3, The pairwise brain maps would support the interpretation of the generated data.
611,3," This might give a better sense of: (1) how difficult the task is, (2) how much variation there is in the real data from patient to patient, and (3) how much variation we see in the synthetic time series."
612,3,  Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before?
613,3," \n\npage 15:\n- \""we wish to compute\""-> we aim at showing?"
614,3," (or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483)."
615,3,"""[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner."
616,3,"\n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l}"
617,1,"\n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets."
618,3,  This would assist in understanding tradeoffs in the design space.
619,3," \n\nSussillo, D., Churchland, M. M., Kaufman, M. T., & Shenoy, K. V. (2015)."
620,3, can it be a trainable parameter?
621,3," With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof."
622,1,\n\nPros:\n\nThe paper tackles what seems to be both an important and challenging problem.
623,3," By employing the cluster, this work propose a joint source/target modeling by varying how sampling is performed, e.g., draw independently or conditionally, and how the cluster are constructed, e.g., model-wise or non-model."
624,3,"""[Apologies for short review, I got called in late."
625,3, The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.
626,3, \n(1) The topic of this paper seems to have minimal connection with ICRL.
627,3, Is there one or are just making a conjecture?
628,2,SURELY THE AUTHORS CAN COME UP WITH A BETTER REFERENCE THAN WIKIPEDIA TO SUPPORT THEIR POINT HERE!
629,3,"  Basically, what's the difference between the bottom two plots in Fig 3 (other than scaling the x-axis by time/epoch), and why does ProxProp achieve better accuracy only in the right figure?"""
630,3," This by itself is not enough to boost the performance universally (e.g., if \\Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures."
631,3, The proposed regularization penalize the critic function only when its gradient has a norm larger than one using some kind of squared hinge loss.
632,2,The way the study is framed here and in the main body comes off as straw-mannish.
633,3," Suppose that TreeQN estimated \""up-right-right\"" as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing \""up-left-left\"" (this is possible because the agent re-plans at every step and follows epsilon-greedy policy)."
634,3,\n(c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation.
635,2,This paper is baffling
636,3," \n\nIf we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero."
637,1," More importantly, The heuristic proposed in the paper is interesting and promising in some respects"
638,1," \n\nI think the fact that the authors demonstrate the viability of training VDFFNWSCs that could have, in principle, arbitrary nonlinearities and normalization layers, is somewhat valuable and as such I would generally be inclined towards acceptance,"
639,3, Could the authors also indicate the CER of the model used for initialization in addition to the final CER after stage-2 training in Figure 5.
640,3, Authors should clarify how they use the inference network and what the two arrows from this inference network represent.
641,3," First, the definition of parameters should not include the word parameters."
642,3,  which has been accepted for publication in NIPS.
643,3, Furthermore the authors could consider applying their framework to other datasets and offering a benchmark experiment and code for the community to establish this as a means of evaluation to maximize the impact of a paper aimed at reproducibility and good science.
644,3,These values are finally used as features in classifying adversarial examples from normal and noisy ones.
645,3, Most of my comments focuses on this.
646,3," If so, how would one play with just a forward model?"
647,3,  How do we treat the output of Tau as an action?
648,1,  This paper defines and examines an interesting cooperative problem: Assignment and control of agents to move to certain squares under \u201cphysical\u201d constraints.
649,2,Your proposed method should be compared with another method that introduced in a prestigious paper...
650,3,. Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all
651,2,The conclusion drawn by the authors seems self explanatory and does not require any validation through the presenter work
652,1,"\n\nOverall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental."
653,3,"""In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\"
654,3,"  2) how the tune related weight of the different objective functions.  """
655,3," \n\nThis problem is not specific to the proposed approach, though: in fact, any method to build options will have to resolve the same issue."
656,3,"""The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis."
657,3, This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches.
658,2,The authors are amateurs
659,3,\n5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)?
660,1," Unlike the generative distribution sampling of GANs, the method provides an interesting compositional scheme, where the low frequencies are regressed and the high frequencies are obtained by \""copying\"" patches from the training set."
661,3,"""The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming."
662,3,"  They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL."
663,2,"I recommend acceptance, provided the editors are willing to stretch the standards for publication a..."
664,1," The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context. "
665,1,".\n(2) the proposed method is nicely designed to solve the specific real problem. For example, the edit distance is modified to be more consistent with the task."
666,1,\nSignificance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.
667,3," However, any critique needs to be done properly, by 1) attending to the actual claims that were made in the first place,"
668,3, Do we need to give the name of the column the Attention-Column-Query attention should focus on?
669,1, \n+ The paper is fairly clea
670,3, Figure 4 shows the only experimental results to \u201csupport\u201d the motivation.
671,3,"  The algorithm essentially alternates over two steps: a step that trains the DNN to predict random targets, and another step that reassigns the targets based on the overall matching with the DNN outputs."
672,1,\n\n(quality) The experiments are interesting and seem well executed.
673,1,\n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. 
674,1," \n\nInterestingly, one aspect of the proposed approach that seems to indeed be an improvement over random options is made visible by a strategy used by the authors to circumvent computational constraints. As explained in the second paragraph of page 8, instead of learning policies to maximize the pseudo-rewards associated with eigenoptions the authors used a myopic policy that only looks one step ahead (which is the same as having a policy learned with a discount factor of zero). [[CNT], [EMP-NEU], [DIS], [MIN]]The fact that these myopic policies are able to navigate to specific locations and stay there suggests that the proposed approach gives rise to dense pseudo-rewards that are very informative."
675,1,\n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams
676,3,\n- on page nine in the last paragraph there is the word 'flow' missing: '.. estimating the optical [!] between 2 [!] images.'
677,3," \n- If reported results are single runs, please replace with averages over several runs, e.g. a few random seeds."
678,3, I'm not quite sure how the authors could externally validate the synthetic data as this would also require generating synthetic outcome measures.
679,2,The reviewer's comments read 'fuck you'  (handwritten on a strip of paper scotch-taped to the editors letter
680,3," On Page 18, how is the squared removed for difference between U and Upi?"
681,3,"""The paper presents the word embedding technique which consists of: (a) construction of a positive (i.e. with truncated negative values) pointwise mutual information order-3 tensor for triples of words in a sentence"
682,3,\n\nThe model is applied to artificially generated data and to high-frequency equity data showing promising results.
683,3, The approach is then experimented on various image and text tasks.
684,3," For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion."
685,3," Inference networks, in general, have two sources of approximation errors."
686,3,"""Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion. "
687,3, Only the basic LeNet and another network are considered on Cifar-100.
688,3,  The authors limit the problem by specifying that all simulated cars must operate between a preset minimum and maximum and follow a target (random) speed within these limits.
689,1,2014\n\n+ Clarity: The paper is easy to read.
690,1,n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail. 
691,2,The abstract is intriguing but confusing
692,3,\n\nAnother motivation of the paper is that targets are given as 1s or 0s while soft targets should work better
693,3,\n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models.
694,3, Perhaps the authors can make a table listing the hyperparameters or a diagram describing the whole training procedures.
695,3,\n\nCons/Questions\n- They mention not quantizing the first and last layer of every network.
696,3,"The approach is empirically tested on the following data: MNIST, CIFAR, and SVHN."
697,3,\nThis makes me wonder what would be the performance of GCN when the k-th power\nof the adjacency is used.
698,3," Interestingly, the way they do so is through the successor representation (SR)."
699,3, Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward?
700,3," The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance."""
701,3,\n\u2028Conclusion: I would suggest that the authors address the concerns mentioned above.
702,3, Is this f(x) the original function (non quadratic) or just the local quadratic approximation?
703,3,"\n\n8. In Section 4, the authors mention that quantization could be used to compress models further although this is usually degrades WER by 2--4% relative. I think the authors should consider citing previous works which have examined quantization for embedded speech recognition [1], [2]. In particular, note that [2] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after training."
704,3,"\nBidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs."
705,3," \n\nExperiments don't vary the attack much to understand how robust the method is."""
706,3," The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top."
707,3,"\n\nThis paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the \""gates\"" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2))."
708,3," Please, please discuss and cite some papers if required."
709,3," \n\nSince a sequence of similar linear systems have to be solved could a preconditioner be gradually be solved and updated from previous iterations, using for example a BFGS approximation of the Hessian or other similar technique."
710,3," The wasserstein distance is also called the \""earth mover distance\"" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another."
711,3, It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge.
712,1," As shown in\nFigure 7 (a)(b), by only allowed an independently thinking master agent and communication among\nagents, our model already outperforms the plain CommNet model which only supports broadcast-\ning communication of the sum of the signals."
713,3," I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU."
714,3,"\n\nThe authors train a neural network to solve the defogging task, define several evaluation metrics, and argue that the neural network beats several naive baseline models."
715,3, This could probably more or less easily be added as a subsection using the current classification.
716,3,"""3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc."
717,3,"""Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \""luckiness\"" of the license plate number."
718,3,"""This paper proposes a simplified LSTM variants by removing the non-linearity of content item and output gate."
719,3, Might this be a typo in 4.1?
720,3,"and when some are left undefined, creating a more abstract generation task."
721,3,\n- Are numerical instabilities making this completely unfeasible?
722,3, How different it will be compared to a perturbation in an input space? 
723,3, It should be supposed L is at least locally convex.
724,3, The novelty here how the problem is formulated.
725,3,"""The authors train an RNN to perform deduced reckoning (ded reckoning) for spatial navigation, and then study the responses of the model neurons in the RNN."
726,1," These tasks are clearly different, as nicely shown by the authors' example of \""do(mustache = 1)\"" versus \""given mustache = 1\"" (a sample from the latter distribution contains only men)."
727,1,\n\nPros:\n- Addresses an issue of RWAs.
728,3,"""The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML."
729,3," Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images."
730,3,"\n- many realizations = one sample (not samples), I think."
731,3," Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ? "
732,3,"\n\nMy most important piece of feedback is that I think it would be useful to include a few examples of the eICU time series data, both real and synthetic."
733,3,"\n\n* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017."
734,3,\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum.
735,3," Yet, the authors demonstrate their approach in environments where the controlable dynamics is mainly deterministic (if one decides to turn right, the agents indeed turns right)."
736,3,"\n- Even through the submodular objective is only approximately solvable, it still translates into a convergence result."
737,3, The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles.
738,3," The paper assumes the tasks are \u201crelated\u201d or \u201csimilar\u201d and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution."
739,3,"\n\nDetailed comments:\n\""For all tasks, the number of batch per training epoch is set to 100."
740,3,"""The paper studies the theoretical properties of the two-layer neural networks."
741,3,"\n-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.\"
742,3, Is it the case that the defenses fall flat against samples generated by different architectures? 
743,1," \n\nThis is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition."
744,1, The certificate is derived with rigorous and sound math.
745,3,"\n- Same for 3.4, which seems like the biattention (Seo 2017) or coattention (Xiong 2017) from previous squad work."
746,3, Concretely: Is there any known theory for such objectives?
747,3," The authors propose to use a model similar to that of Vinyals et al., 2015 and change the image features it is conditioned on."
748,3,"\nThis reach is NOT inherent in integer programming, per se."
749,1," However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative."
750,3,  The proposed algorithm is tested on 3 data sets and compared with several baselines.
751,3, \n\nDimension of c_i / o ?
752,3, \n- What corpus did you use to pre-train word vectors?
753,3, Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A?
754,3,"\n\nDetailed comments\n  \u2022 [p4, basic health gathering task] \""The goal is to survive and maintain as much health\nas possible by collecting health kits..."
755,3," Interestingly, the\nfilters defined in the Hilbert space  have parameters that are learnable."
756,3, (iii) \u201cLearning Scalable Deep Kernels with Recurrent Structure\u201d (JMLR 2017).
757,3,"\n- There are techniques for incremental adaptation and a constrained MLLR (feature adaptation) approaches that are very eficient, if one wnats to get into this"
758,1,and only the 6th is really good.
759,1,\n\nI like the experiment presented in figure 5 in particular.
760,3,"? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another"
761,3, This paper instead designed a new boosting method which puts large weights on the category with large error in this round.
762,3," Animals presumably continue to learn throughout their lives.[[CNT], [null], [DIS], [MIN]] With on-going (continous) training, do the RNN neurons' spatial tuning remain stable, or do they continue to \""drift\"" (so that border cells turn into grid cells turn into irregular cells, or some such)? "
763,3, The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance.
764,1, Explanations are clear.
765,3, This is especially critical if a deep neural network is used since overfitting is a real issue.
766,3," During operation it then switches its strategy depending on a dynamically-calculated threshold reward value (considering variation in agent-specific policies, initial game states and stochasticity of rewards) relative to the total reward of the played game instance."
767,3,"\n\n[1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016."""
768,3, \n-- It is not easy to put together the conclusions in Section 6.1 and 6.2.
769,3, \n\nUpdate: the revised version of the paper addresses all my concerns about experiments.
770,1,"""+ Quality:\nThe paper discusses an interesting direction of incorporating humans in the training of a generative adversarial networks in the hope of improving generated samples."
771,3, Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach
772,3,"\n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods."
773,3,"""This paper studies the critical points of shallow and deep linear networks."
774,3,"""The paper analyzed the composition abilities of Recurrent Neural Networks."
775,1,"\n\nI'm on the fence about this work: I like the ideas and they are explained well,"
776,3,". \n\n * \""The gradient decent approach required roughly 150 iterations to converge where as the simulated annealing approach needed at least 800.\""\nThis is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach."
777,3," The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline."
778,3,This forces the different model instances to become more complementary.
779,3,\n\n-----------\n\nThis paper proposes a version of IWAE-style training that uses SMC instead of classical importance sampling.
780,3," \n\nConsulting the differences between the paper revisions, I was initially intrigued with the volume of differences that shown in the summary bar."
781,3,  Experiments include both artificial data and real data.
782,3," I also think that the authors should not discuss the general framework and rather focus on \""data teaching\"", which is the only focus of the current paper."
783,3," They propose to introduce the noise process into the generation pipeline such that the GAN generates a clean image, corrupts its own output and feeds that into the discriminator."
784,1, \n\n# Clarity\n- The paper is overall easy to follow and the description of the proposed method is clear.
785,1," I think the paper is clearly written, and has some interesting insights."""
786,3,\n2. The batch size for MNIST classification is unusually low (8) .
787,3,  The right figure would be nice to see with time on the x-axis as well.
788,3," it would be good to show this as an experimental result if indeed the quality of the learned representations is comparable."""
789,1, The authors have definitely found an interesting untapped source of interesting images.
790,3, Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues.
791,3, Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014).
792,3, But in that case much of the theoretical story goes out the window.
793,1, They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried.
794,3, Did you cap the max episode time to 30mins?
795,3,what are the image sizes for the CelebA dataset\n\n- page 5: double the\n\n
796,3,"\n\nThe answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities."
797,3,\n* Simple but effective design to achieve a better result in testing time with same total parameter budget.
798,1, The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible.
799,1,\n\nClarity:\n- The paper is well written and clarity is good.
800,3," While there have been numerous GAN-like approaches for language understanding, very few, if any, have shown worthy results."
801,3,"\"" In International Conference on Machine Learning, pp. 2034-2042. 2016."
802,3,"\n\n2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration."
803,2,This section gives the impression that you'll throw a handful of darts at a target and see what you happen to hit
804,3," I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods,"
805,3,The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. 
806,1,"\n\nThe paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general:"
807,3, the main concern I have with this paper is novelty.
808,3,"\n3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space."
809,3,"\n\nMinor comments:\n- Section 1, first paragraph, last sentence, \""that\"" -> \""than\""?[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Section 3.2, \""... using which...\"" formulation in two places in the firsth and second paragraph was a bit confusing[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Page 7, second line, just \""IS\""?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Perhaps you can clarify the last sentence in the second paragraph of Section 5.1 about computational graph not influencing gradient updates?[[CNT], [CLA-NEG], [QSN], [MIN]]\n- Section 5.2, stochastic variational inference Hoffman et al. (2013) uses natural gradients and exact variational solution for local latents so I don't think K=1 reduces to this?"""
810,3,\n\n- Most of the experiments in the main paper are on toy tasks with small LSTMs.
811,3,"\n\nThis submission claims that:\n[a] \u201c[based on the critiqued paper] one might assume that DRL-based algorithms are able to 'learn to navigate' and are thus ready to replace classical mapping and path-planning algorithms\u201d,\n[b] \u201cfollowing training and testing on constant map structures, when trained and tested on the same maps, [the NavA3C+D1D2L algorithm] is able to choose the shorter paths to the goal\u201d,"
812,3," Also in the appendix, please restate the lemma that is being proven."
813,3, \n===========================================================\n\nThis paper presents an algorithm for few shot learning.
814,3, Do you use tweaks in the generation process.
815,3," It receives instances in an online setting, where both the prediction model and the relationship between the tasks are learnt using a online kernel based approach."
816,1,"  The current 82.1% accuracy is nice to see,"
817,3,"""This paper proposes a model using hidden neurons with self-organising activation function, whose outputs feed to classifier with softmax output function."
818,3, The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network.
819,3," I did not see support in this paper for the claim in the abstract that special architectures make complex networks work better, or that they are well suited to particular data sets"
820,3,\n- An extensive study of methods for dimensionality reduction is performed for a task with sparse rewards.
821,3," \n\n1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387\u201391, 2005.\n2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91\u2013107, 2010.\n3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013."""
822,3,"  In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs."
823,1,"""1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection\n2."
824,2,Ah - now I see a glimpse of promise in this paper - Five pages into the documen
825,3," \n\nAs for future work, I think an interesting direction would also be to investigate the composition abilities for RNNs with latent (stochastic) variables."
826,3,"  The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning?"
827,1," Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge."
828,3,"""This paper try to analyze the intrinsic structure of VGG19 and give a new insight of deep neural networks."
829,3," This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract)."
830,3,\n\n- Generating high resolution images with GANs even on faces for which\nthere is almost infinite data is still a challenge.
831,3," \n\n- This is a minor point and did not have any impact on the evaluation but VAE --> VHE, reparameterization trick --> resampling trick."
832,3,"\nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better."
833,3," The authors propose a differentiable counting component, which explicitly counts the number of objects."
834,1," --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly."
835,3, Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder.
836,3, These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets.
837,3, The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM).
838,3," When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning."
839,3," Results show that with a fixed budget, it\u2019s better to label many examples once rather than fewer examples multiple times."
840,3, This means that a liner combination with the real-valued learned gate parameters is suboptimal.
841,3," According to the visualizations, the interpreter could generate meaningful attention map given a textual query."
842,1," \nWhile the idea of using mask is interesting and important, I think if this\nidea could be implemented in another way, because it resembles Gibbs sampling\nwhere each token is sampled from its sorrounding context, while its objective\nis still global, sentence-wise."
843,3,"""The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization."
844,1, Results are promising.
845,3," References to classic weighted sampling are[[CNT], [CNT], [CNT], [CNT]] \n\n  The application is limited to certain loss functions for which we can compute LSH structures."
846,3," Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected)."
847,3, The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm.
848,1,"\n\nOverall, this is an interesting study on SQuAD dataset."
849,1,\n\nThe motivation is clear and proposed methods are very sound.
850,3," I would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resources."""
851,3," The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution."
852,3,"  Besides \u201cfooling\u201d the discriminator, the generator objective is to maximise user interaction with the generated batch of images."
853,3,"\n- In table 3, what if we use the right-branching tree-lstm with attention?"
854,1,"It is a well-written, very clear paper, and it has a good understanding of the literature, and does not overstate the results."
855,3,"\n-- in the computational section: \""Training size is 9924 and testing is 6695. "
856,3," For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a \""reward\"" function, but I don't know what it means and the authors should perhaps explain further."
857,1,"\n- Nice engineering achievement, reaching the top of the leaderboard (in early October)."
858,2,Why chase a gene in this ridiculous organism?
859,3,"\n4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively. "
860,3,\n\nI am curious how the story would look if one tried to push beyond two levels...?
861,3, Even the control task is very similar to the current proposed task in this paper.
862,1,"\nEntropy-regularized formulation leads to nice relationships between the value function and the policy, and has been explored recently by many, including [Ziebart, 2010], [Schulman, 2017], [Nachum, 2017], and [Haarnoja, 2017]."
863,3, The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks.
864,3, How much does that impact the overall compute?
865,3,. They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format.
866,3,"""The paper proposes an approach to learning a distribution over filters of a CNN."
867,3,"\n\nThere are some concerns the authors should consider:\n- Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper."
868,3," Although this criticism also applies to most previous work using \u2018cluttered\u2019 variants of MNIST, I still think it needs to be considered."
869,1,"\n\nIn the model section, the paragraphs \""notation\"" and \""objective function and discussion\"" are clear."
870,3,"\n\n- \""... is more computationally and ...\""\n\n- \""... our results for performing final ...\""\n"
871,2,Since you submitted the paper to a scientific journal: where is the science?
872,1," While the idea makes sense,"
873,3, it would be clear that BDQN is outperforming DDQN..
874,3," Statistics has for decades successfully used criteria for model selection, so what is this example supposed to proof (to whom?).[[CNT], [null], [QSN], [GEN]]\n\nChapter 4 takes the work of Mandt et al as a starting point to understand how SGD with constant step size effectively can be thought of as gradient descent with noise, the amplitude of which is controlled by the step size and the mini-batch size."
875,1," The proposed algorithm is described in excellent detail, which is essential to reproducibility\n3)"
876,3," It builds on the previous work of [Finn et al. 2016] that uses a neural network to predict transformation parameters of an affine image transformation for future frame prediction, an idea akin to the Spatial Transformer Network paper of [Jaderberg et al., 2015]."
877,3, The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t.
878,1,"\n\n6. Although the results on the WSJ set are interesting,"
879,3, The approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is proposed.
880,3,  The method for quickly combining known skills (the zero-shot skill composition in the title) is switching between the two policies based on rho.
881,1,"\n\nThis paper has some very good ideas, and asks questions that are very much worth asking."
882,3, Does it also change the location of the Nash equilibria?
883,3," The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. "
884,3,"\n\nIf my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and model-based scoring."
885,3,\n\nSection 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }.
886,3,\n- The algorithm requires tuning of quite a few hyperparameters (sec. 3).
887,1, \n\n[Pros]\n- The overall direction toward more flexible/scalable memory is an important research direction in RL.
888,1,\n\nPROS:\n+ nice tensor factorization model for learning word embeddings specific to discrete covariates.
889,3, \n4. Why is the affine transform assumption valid in biology?  
890,1,\n\nThis paper was quite interesting and clearly written for the most part.
891,3,"  However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span. "
892,3," Meanwhile, the results are performed on some specific model configurations (like ResNet) and settings. It is difficult to justify whether it can generalize to other cases."
893,3,\n\nAnother comment is related to the overall content of this paper.
894,1,"\n\nIn summary, I like the idea, the application and the result of this paper."
895,3, How do the authors\nimplement the other graph-related approaches in this problem featuring\ntime series?)
896,2,"Nothing changes in our collective research programs as a result of this work. Still, the work needs to appear somewhere in the literature"
897,1,"\n\nOverall I think this paper addresses an important problem in an interesting way,"
898,1," In light of such results, one might change the policy space to enforce such structure."
899,3,"   I believe the latter, but this implies that the vocabulary dimension (V) is the same as the number of aspects, since A is apparently shared."
900,3,\n\n4. It would be useful to discuss the implementation of the method as well. 
901,3," As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100."
902,1," However, given how little we know about the behavior of modern generative models, it is a good step in the right direction."
903,3,  The authors should release the dataset to prompt the research in this area.
904,3,"\n\nHow the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?"""
905,3,"\n\n5. For each weight w, we add K learning rates u_w^j."
906,3, In the last paragraph of Section 3 ``To the best of our knowledge''.
907,3,"""In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources. "
908,1,\n- Novel task of generating UI code from UI screenshots
909,3, A quick search reveals [1] (probabilistic modeling of dependency parses to create Bayesian topic models directly) and [2] (creating a semantic vector space from a dependency parse) I suspect there are others
910,1,\n\nPros:\n1. Well written paper with clear presentation of the method. 
911,3,"""SUMMARY: This work is about prototype networks for image classification."
912,1," This experiment shows improvement over such baselines,"
913,1," In both circumstances, they demonstrate the efficacy of their technique and that it performs better than other reasonable baseline techniques: self-paced learning, no teaching, and a filter created by randomly reordering the data items filtered out from a teaching model."
914,3, In particular looking at Riverraid-new is the advantage you have there significant?
915,3,"  E is defined (in Section 2.2) to be \""all the inter-subgraph (community) edges identified by the Louvain method for each hierarchy."
916,3,"\n\nFor all the experiments, the same set of parameters are used, and it is claimed that \u201cthe method is robust in our experiment and simply works without fine tuning\u201d."
917,3,"\n\nMisc:\n\nTables 3 and 4 would be easier to parse if resources were simply reported in terms of total GPU hours."""
918,1," There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works."
919,1, Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so!
920,3," The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example. "
921,3, \n\nIn conclusion:\nThis paper presents a method for creating features from a (pre-trained) ConvNet.
922,1,"""In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\"
923,3," \nThere are missing links and references in the paper and un-explained notations, and non-informative captions."
924,3,\n\nExperiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent. 
925,3," Please cite the conference version if one is available, many arxiv references have conference versions."
926,2,This ridiculous comment that a powerful computer algebra systems was required on page 6 is absurd.
927,1,\n\nThis paper is quite original and clearly written.
928,3, It would be of particular interest to highlight connections to algorithm regularly applied to neural network training.
929,3,\n****\n\nSummary:\nThe authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.
930,3,"\n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter."
931,1,"\n\nOverall, I quite like this line of work,"
932,3, The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words.
933,3,  That is to say that I still have some reservations (though less than I had before).
934,1,"\n\nOverall, the proposed approach is novel and achieves good results on a range of tasks."
935,3, I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms.
936,1, \n\nThe main novelty of this work are 1-balancing mechanism for the replay memory.
937,3," If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short."
938,3,"\n\nThe paper is well written, but can use some proof-reading."
939,3,"\n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity?"
940,3, Corresponding to these two parts are two generators.
941,3,\n- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.
942,3,"\u201d  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across."
943,1,"""This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al."
944,1,"  I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area."""
945,2,This was one of the least interesting papers that I have read in quite some time.
946,3, This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results.
947,3,\n\n   Learner: S = \\phi(s) \n   Expert\u2019s i^th state visit:  Ei = \\phi( \\hat{s}_i }  where Ei\u2019 is the successor state to Ei
948,1," Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997)."
949,2,This literature review is nothing more than a merry dance around the books
950,3," Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution?"
951,3, I would suggest to:\n1. Compare more clearly setups where you fix the hidden size.
952,1," As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah."
953,3, This idea would have more impact if it generalized to arbitrary kernel dimensions.
954,3," Either it is referring to the fitted model, then it's a bad name, or it's an empirical distribution, then there is no pdf, but a pmf."
955,1, My understanding is that the main novelty is the proposed tensor factorization algorithm and extensive experimental evaluation.
956,3," Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling."
957,3," The idea is to jointly embed an image and a \""confidence measure\"" into a latent space, and to use these embeddings to define prototypes together with confidence estimates."
958,1,"\n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory."
959,3,\n\nIt would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict
960,1,"\n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims."
961,3, What is the variance in the numbers for Table 1?
962,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
963,3," \n\nFinally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images). "
964,2,The authors appear to be blissfully unaware ... [reviewer 6. Out of 7. Seven.
965,3,"\n\nFinally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow."
966,3, I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation
967,1,"\n\nOn the whole, I think this paper does paper does a good job of motivating the\nproposed modeling decisions."
968,3," \n\nFrom my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea."
969,3, \n\nSection 2 presents a taxonomy for the different neural network clustering methods.
970,3, Please mention the numbers with unit normalization to give a better picture.
971,3," The objects learned as shown in Appendix A are quite unconvincing, e.g. on p 9.[[CNT], [EMP-NEG], [CRT], [MAJ]] For example for Boxing why are the black and white objects broken up into 3 pieces, and why do they appear coloured in col 4?"
972,3,"\n\nMy main concerns are:\n\nControls\nTo generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced."
973,1," However, I found their empirical evaluation and experimental observations to be very interesting."
974,3, Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation.
975,3,"\n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant? "
976,3,"\nThey use a variational bound to deal with the relevance term, I(Z_l,Y), and  Monte Carlo sampling to deal with the layer-by-layer compression term, I(Z_l,Z_{l+1})."
977,3, It would really help if the paper gave some expository examples.
978,1,The SCAN data-set has the potential to become an interesting test-case for future research in this direction.
979,1," Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication."
980,3,\n\n\n2. High level paper\n\n- I believe the writing is a bit sloppy.
981,2,"I know you want to use hormones to study physiological changes, but humans are more than hormones"
982,3," In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals."
983,3," However, the previous settings can be reinterpreted in the authors setting."
984,3,"""This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling."
985,3,"\""\nWhat does this mean?"
986,2,"Line 156-160; this is the only correct, sensible and interesting finding of the paper."
987,1," The problem formulation is mostly reasonable, and the evaluation seems quite convincing."
988,1,\n\nOne of the motivations behing the model is to force label representations to be in a semantic space (where two labels with similar meanings would be nearby).
989,3,"\n\nIt combines some mechanisms known from previous work for summarization (intra-temporal attention; pointing mechanism with a switch) with novel architecture design components (intra-decoder attention), as well as a new training objective drawn from work from reinforcement learning, which directly optimizes ROUGE-L."
990,3, The only technical contributions are (4) and the way to construct the co-occurrence information A.
991,2,The supportive tone of this reviewâ€¦ took some effort. 
992,3," however, this paper needs a revision in various aspects, which I simply list in the following:;"
993,3," They indicate that since Lipton et al. \""do not\ninvestigate the Atari games, we are not able to have their method as an\nadditional baseline\""."
994,3,  but I don\u2019t know the area well enough to make specific suggestions 
995,3," If so, this further means that this is not a true gradient."
996,3, How do the authors explain this?
997,3,.\n\nPros:\n- The proposed function has similar behavior as ELU but 4x cheaper.
998,3, A projected sub-gradient descent algorithm is used.
999,3,"""This paper presents an image-to-image cross domain translation framework based on generative adversarial networks."
1000,3," The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods."
1001,3," Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training."
1002,3,\n\nFigure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process.
1003,3, Humans and animal are not given task ID and it's always clear distinction between task in real world.
1004,3, Why is there no multi-channel baseline for the GRID results?
1005,1,"\n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization)."
1006,3," If this is correct, how many Progs are consistent with a typical Y?"
1007,2,"I disagree with many aspects of this paper, but sometimes it is best to give the authors as much rope as they want."
1008,1," The authors provide a rigorous end-to-end analysis for the LDS setting, which is a mathematically clean yet highly non-trivial setup that has a long history in the controls field."
1009,3,"\"" In an ideal world I would like the assumptions required for this to hold true to be a fleshed out a little here."
1010,3, They learn the dropout distribution by variational inference with concrete relaxation.
1011,1,\n\nExperiments a are satisfying and show good performance when compared to other methods.
1012,2,This paper adds nothing to the existing knowledge of the subject 
1013,3," I am confident that it is easy to identify many precursors in the optimization literature, but I am not an expert on this."
1014,3,"  \n\n- Page 3- 2.2. Using a crowd-sourcing technique, developing a similarly small dataset (1000 images with 100 annotations) would normally cost less than 1k$."
1015,3," In some sense, however, Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task. """
1016,2,You know nothing about [general topic of the paper]. Cite [five irrelevant citations from same scholar].
1017,3," The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now."
1018,1," Intuitively, the diagram shown in Figure 4 works well for 3 classes in dimension 2."
1019,2,Perhaps what I have written is deflating
1020,3, \n\nOriginality/Significance: Kronecker factorization was introduced for Convolutional networks (citation is in the paper).
1021,2,"If participants were recruited from a university, I imagine they would usually be 18-22 years old. Why does your sample range from 18 to 63? Im a bit lost here."
1022,3,\n- How the budget in terms of Mul-Adds is actually estimated?\
1023,3,\n\nThe other evaluation decision that is confusing is the paraphrase evaluation of the phrasal verbs.
1024,1,"\n\n\nNumerical experiments\n====================\n\nGlobally, the numerical experiments are in favor of the presented method."
1025,3,\n\n1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions. 
1026,1," It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep."
1027,1, It combines several insights into a nice narrative about infinite Bayesian deep networks.
1028,3,\n\n3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM.
1029,3," Journal of Memory and Language, 59(4), 434-446.)."
1030,3,"\n\nDuring learning SR and the features, what would be the impact if the gradient for SR estimation were also propagated?"
1031,3, What is the coordinate system used\nby the authors in this case?
1032,3," To learn the mapping from image to high-level representations, an auxiliary encoder was introduced."
1033,3,"""This paper introduces two new loss functions which can be used along with the existing reconstruction and adversarial losses for language style transfer."
1034,3,". Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures."
1035,3,"\n\nThe paper claims that \""discriminative approaches\"" need to iterate over all possible entity pairs to make predictions. "
1036,3,\n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution.
1037,3," The results seem to be worse than existing networks, e.g., DenseNet (Note that SVHN is no longer a good benchmark dataset for evaluating state-of-the-art CNNs)."
1038,2,The writing is often arrestingly pedestrian
1039,3,"\n6. I do understand the notation used in equation (8) on page 4. Are <, > meant to denote less than/greater than or something else?"
1040,3, \n2. Using a new loss function for pointer / copy mechanism.
1041,3, I think these experiments would strengthen the paper.
1042,3, Do redundancy cues which work for multi-document news summarization still work for this task?
1043,3," This method is not learning-based, doesn't need training data in a simulator, generalizes to **any** exit and lane configuration and variants of this basic technique continue to be used on real-world autonomous cars."
1044,3," Further, the taxi agents have \u201cbatteries\u201d, which starts at a positive number, ticks down by one on each time step and a large negative reward is given if this number reaches zero."
1045,3, This is a very strong assumption.
1046,3, These are 9% and 6% higher than the reported numbers for adversarial GMemN2N. 
1047,1," However, it is more interesting and important to test on more advanced networks."
1048,3,\n\nMy first concern is the motivation.
1049,3, \n    Also it is really hard to understand how could they obtain such impressive result by doing an unsupervised training on a dataset containing 3353 samples taking into account the high capacity of the models they are using.
1050,3, One could imagine that computing the posterior\napproximation in equation 6 has some additional cost.
1051,1, The experiments are OK 
1052,2,"the correlations are not that strong (e.g., p = .022)"
1053,3,\n\nThe results are in general only marginally improved by the baseline corrected non-negativity constrained approach.
1054,3, What happens for deeper and narrower baselines that have a similar number of parameters?
1055,3,"  There is a PIR per level and filter (as defined in C4) but in the setup the L_{PIR} was mentioned to be a scalar function, how are the values then summarized?"
1056,3,"  With real data, acquired from humans, the training is likely to end up in a very different minima."
1057,3," EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014."""
1058,3," If the theorem remains unchanged after this modification, you should clarify the proof."
1059,3," Thus, the authors need to refine BM's result for comparison."""
1060,3," On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below)."
1061,3, This is a rather simple idea that is shown to be effective in Figure 3.
1062,3," The sentence that \""the influence of the prior diminishes as the size of the training data increases\"" is debatable for something as over-parametrized as a DNN."
1063,3,"In addition, attention context is computed for each layer, then, combined together as a single context."
1064,3,\n- what is the relationship of the presented ASG criterion to MMI?
1065,3," According to the paper, this is the first time such kind of performance are demonstrated for limited precision training."
1066,3,\n\nThe paper largely follows the work of Nachum et al 2017.
1067,1," It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees."
1068,3, I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.
1069,1,"""The paper presents a novel representation of graphs as multi-channel image-like structures"
1070,3," \n(viii) \""P denotes the number of model parameters\"" (I guess it should be a small p? hard to decipher)"
1071,3,\n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates?
1072,3, Given that the proposed model is transductive (when there is significant edge overlap) it should do far better than DC-SBM which is inductive.
1073,3,"\n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances."
1074,1," Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field."
1075,3, Here the embedding of word i is combined with the elements of the context units of words in the context.
1076,3, It is important to clarify this point.
1077,1,"\n\nPros:\n\n-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time\n\n-Good empirical evaluation"
1078,3," \n[2] A. Brunton, T. Bolkart, and S. Wuhrer.  Multilinear wavelets: A statistical shape space for human faces."
1079,3," Could you please provide more explanations and intuitions?"""
1080,3,  \n\nTrust region subproblem (TRS) has been analyzed and developed so much in the optimization literature.
1081,3," Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram."
1082,3,\n\n== Novelty/Significance ==\nControllable image generation is an important task in representation learning and computer vision.
1083,3," e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations."
1084,1, This means that they bring\nall necessary information for rebuilding their continuous counterpart.
1085,2,"These theoretical results have not been verified experimentally. Therefore, they are wrong"
1086,3,\n5. The comparison against previous work is missing some assurances I'd like to see.
1087,3," Similarly, parts of Section 4.2 seem to follow directly from the previous discussion."
1088,3," \n(2) The authors state \u201cIn back propagation, the gradient from z2 is kept from propagating to h\u201d.  This makes the learning process quite arbitrary under the objective in eq.(14). "
1089,3," The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned. "
1090,2,"I have read this MS twice, which given the grammatical howlers in the Abstract would appear to be more times than it has been read by the authors"
1091,3," There needs to be some sort of error analysis to show why this idea improves, rather than simply stating metrics."
1092,2,"If the author is comfortable having his/her name on this paper, then I won't stand in the way of its publication"
1093,3," Besides, the idea of using a rotation operation in recurrent networks has been explored before [3]."
1094,3," However, confirmation of this intuition is needed since this is a central claim of the paper."
1095,3," Moreover, to justify the effect of the randomness, the paper should have empirical experiments."
1096,3, The manuscript motivates the necessity of such technique and presents the basic intuition.
1097,3," (In fact, because of the presentation of the paper, I was hesitating whether I should suggest acceptance.)"
1098,1," Although the overall presentation is clean,"
1099,3, How large can this be expected to scale (a few thousand)?
1100,3,"\n\nIn the last paragraph of section 3.2, why is there a huge difference in real-time factors between the clean and other set?"
1101,3," In this paper, bipolar activations are used to train very deep stacked RNN. "
1102,3," \n- Unless I am mistaken, Gated Orthogonal Recurrent Units: On Learning to Forget from Jing et al. also reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively?"
1103,3,"""The authors study the effect of label noise on classification tasks."
1104,3,\n- Notation: I believe the space U is never described in the main text.
1105,3, An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. 
1106,1," I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice)."
1107,3,"""This paper proposes a model for learning to generate data conditional on attributes."
1108,3,"n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.\n"""
1109,1," Although, the authors use powerful PixelCNN priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive components."""
1110,3, Authors perform experiments on two datasets: Asian Faces Dataset and ImageNet Large Scale Recognition Challenge dataset.
1111,3, The main limitation is that the best architectures as currently described are less about discovery and more about human input;
1112,1,"\n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings."
1113,3,"Variances should be added, and preferably more than 3 initialisations used."
1114,3," Since there is some related work, it may be also worth to compare with it, or use the same datasets."
1115,3," \n\nAdditional comments:\n\nWhy is the variational approximation called \""sharpened\""?\n\nAt test time, normal VI just uses the fixed q(\\theta) after training."
1116,3,"\n-The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for \u201cdiscriminativeness\u201d and seems like something that can be gamed."
1117,3, The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.
1118,3,"\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations"
1119,3,This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results.
1120,1, The experiments show that the architecture outperforms a couple of related models on an associative retrieval problem.
1121,1,\n\n- The paper is mostly clearly organized and presented.
1122,3,How much longer does it takes to train the model with the ISTA based constraint
1123,3,\n\nThere have existed several works which also provide surveys of attribute-aware collaborative filtering
1124,1,\n\nThe paper is well written and easy to follow.
1125,3, A discussion of the links is necessary and will clearly bring more theoretical ground to the method.
1126,3,\n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).
1127,1,\n- The proposed method improves the baseline by 5% on counting questions.
1128,1," The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling."
1129,2,I doubt that many readers will read anything beyond the abstract if the article remains in its present form
1130,3,"\n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation."
1131,3," First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset."
1132,3, \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A).
1133,3,  This interpretation plays a role in the proposed training method by informing how the \u201cstep sizes\u201d in the Euler discretization should change when doubling the depth of the network.
1134,3,"\n\nThere is no notion of class invariance, so the GAN can find the space of filters that transform layer inputs into other classes, which may not be desirable. Have you tried conditioning the GAN on class?"
1135,3,"""* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label."
1136,3, Has this any connection with the final performance of the network?
1137,3,"  Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952)."""
1138,3, Results are reported on about 50 UCI datasets with different topologies.
1139,3,"\n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased."
1140,3,"""This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution."
1141,1, \n- The authors did a great job summarizing prior work and motivating their approach.
1142,3, Are long-range correlations irrelevant to the text style?
1143,3,"  Thus, Bayesian evidence prefers minima that are both deep and broad."
1144,3," Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly."
1145,1,\n\nThe paper is clearly written.
1146,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
1147,3,"""In this paper, the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs, in particular when they are conditionally independent ('factored')."
1148,1, This raises my confidence that what is reported is better than mere distillation of an ensemble of rnns.
1149,3,"  More sophisticated should have been employed."""
1150,3," If so, how is this method specific to batch normalization?"
1151,3,"\n[2] Suzuki, Masahiro, Kotaro Nakayama, and Yutaka Matsuo. \""Joint Multimodal Learning with Deep Generative Models.\"" arXiv preprint arXiv:1611.01891 (2016).\n[3] Tucker, Ledyard R. \""An inter-battery method of factor analysis."
1152,3," In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset."
1153,3," If yes, is it given during evaluation as well?"
1154,3," A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences."
1155,3, Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network.
1156,3,"""The problem of numerical instability in applying SGD to soft-max minimization is the motivation."
1157,3," The approach is compared to Isola et al\u2019s conditional adversarial networks, and unlike the conditional GAN, is able to produce a diverse set of outputs."
1158,3," This will use significant resources and is much more difficult,"
1159,1,"  It shows better generation errors by trust region methods than SGD in different tasks, despite slower running time, and the authors speculate that trust-region method can escape sharp minima and converge to wide minima and they illustrated that through some hybrid experiment."
1160,1,  This is very interesting work. 
1161,3,"\n- In the abstract, the authors should emphasize that the PIR model used in this paper is based on VGG features."
1162,3,"""This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE."
1163,3,\n- it may also be interesting to discuss the role of the language model to see which factors influence system performance
1164,3," The selection of the inputs happens through gate weights, which are sampled at train time."
1165,3,"  Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value."
1166,3,"""The paper makes some bold claims."
1167,3, \nFirst relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking.
1168,3, The authors also present an analysis of the data by applying one of the SOTA techniques on SQuAD to this data. 
1169,3,. Right now the authors explain separation rank first and then discuss tensors / matricization).
1170,1,\n\nQuality:\nThe quality is very good.
1171,1,"  I also liked the idea of having the section \""A geometric view of embeddings and tensor decomposition\"", but that section needs to be improved."
1172,1," The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed)."
1173,1," While I appreciate the technical contributions,"
1174,3," If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)"
1175,3,"\n\nMy main concerns with this paper are novelty, reproducibility and evaluation."
1176,1, The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.
1177,2,Sprinkled here and there are some things that are more or less correct. But it is all very confused.
1178,3,"\n     - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2=5?"
1179,3, The proposed method can jointly learn latent features and the cluster assignments.
1180,2,"Indeed, the article as a whole shows a lack of sophistication and nuance that is diagnostic of the particular genre of thought pieces"
1181,3," Two proposals in the paper are:\n\n(1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training,"
1182,3, \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs.
1183,3," Similar remarks for the many more examples in the supplementary materials.[[CNT], [null], [DIS], [MIN]]\n\nThe most intriguing question is the one raised in the first paragraph of the conclusion: While prepositions are natural for modeling via word triples and indeed their high frequency and small number of types makes this quite practical, the kind of concerns raised here are also applicable to a whole bunch of word types, and it would be natural to want to extend the method to them."
1184,3," To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs."
1185,3,"\nThe authors claim that \u201cBy relying on a controller for low-level decisions we are also able to completely eliminate collisions during training or testing, which makes it a possibility to perform training directly on real systems."
1186,3, It generates images in a layer-wise manner.
1187,3," \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work."
1188,3,y.\n\nMy main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017). 
1189,3,"""This paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained CNN on images within a same category."
1190,3,"\n\nWhen m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary."
1191,2,I wish I could explain what is the purpose of the manuscript.
1192,3, How is the sequence length determined for reinforcement learning tasks?
1193,3, Can they elaborate more on how they with this?
1194,3,"""This paper is an application paper on detecting when a face is disguised,"
1195,1, Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout.
1196,3, It will be informative to see how the model holds in high-dimensional settings.
1197,3," \n\n3. Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained?\n\nThanks."""
1198,3,"  Real vehicles also decouple wayfinding with local vehicle control, similar to the strategy employed here. "
1199,3,This paper would be much stronger if the authors can include these experiments and analyses.
1200,3,"  In the AP, HP and\n  AP+HP cases of Table 2, it is essentially the same predictive setup"
1201,3,"""Paper summary:\nAuthors extend [1] to form an auto-encoder CNN network for face mesh representation."
1202,3,  This paper shows some results to the contrary when applying RL to complex perceptual observation space.
1203,3,\n\n- Affect in language seems to me to be a very contextual phenomenon.
1204,3,"""This paper is concerned with both security and machine learning."
1205,1,"\n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction."
1206,3," The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible)."
1207,1,\n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community.
1208,1,"\n\nThe empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning."
1209,3," What is the citation of this in the kernel approximation context?"""
1210,1,\n+ good experimental evaluation
1211,3," Spatio-temporal video autoencoder with differentiable memory, arxiv 2017\n\nSince this is prior state-of-the-art and directly applicable to the problem, a comparison is a must. "
1212,1,"\n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN."
1213,1,\n\nPros:\n- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs
1214,3, \n\n* Please clarify what are the hyper-parameters of your meta-training algorithm\n  and how you chose them.
1215,3," It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and \""confirmation biases\"" in the model."
1216,3,"\n\n\n- Other comments:\n\nIn Fig. 5, use a consistent naming for the axes (bias and variances)."
1217,3, The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing.
1218,3, Similar remarks perhaps apply to the NLI results.
1219,2,"Ultimately, the results are just a set of observations."
1220,3, How long should be the training to ensure a good and stable convergence of the method?
1221,3,"\n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data."
1222,1,\n\nClarity\n\nThe rationale in the paper is straightforward.
1223,3,\n\nI have some small qualms with the presentation of the method.
1224,3,"""The paper proposes a technique for training quantized neural networks, where the precision (number of bits) varies per layer and is learned in an end-to-end fashion."
1225,3,"\n-- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated?"
1226,3,. The main object of study is to quantify the benefits of overlap in convolutional architectures.
1227,3,  I am looking forward to the reply to my questions above; my final score will depend on these.
1228,1,\n\nThe proposed model and method are reasonably original and novel.
1229,3," This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue."
1230,3,"\n\nClarity: The clarity of the text is fine, though errors make things difficult sometimes."
1231,3, This should at least be verified and compared appropriately.
1232,3,\n6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs?
1233,3, So you train in (a) on the steps task until 350k steps?
1234,3,"  \"" I'm not sure I follow though, because the alpha terms do indeed depend on the word $t$, as per equation (1), which includes v_t, a vector representation of the target aspect."
1235,3," For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult."
1236,3," Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training."
1237,3, Would NATAC-k need a different number of clusters than the one from NATAC?
1238,3,".\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max"
1239,3,"""In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization."
1240,3,"\n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one."
1241,3," The new model \""EEN\"" is compared to a deterministic model and conditional GAN."
1242,3,"  In those experiments, our model shows to be significantly less prone to overfitting than a traditional feed-forward network of same size, despite having more parameters.\"" ===>  Well all this needs to be included in the same paper. "
1243,1, This seems like a good algorithm in many ways.
1244,3,\nWhat is the reconstruction error in case 1?
1245,2,They show they can account for 20% of the variance. No wonder. The usually accepted level is 50% to be useful
1246,3," On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance."
1247,3," The impact of this choice is S_T is no longer a termination state, and there is a direct fully discounted transition to the start states."
1248,3," The definition of intervention in the Background applies only to do-interventions (Pearl 2009) and not to general interventions (e.g. consider soft, uncertain or fat-hand interventions)."
1249,3, The generative model defines the process that produces the dataset.
1250,3, It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited.
1251,1,\n\n# Novelty and Significance\n- The problem considered in this paper is interesting.
1252,1,"\n\nGenerally, I think that the paper is written well (except some issues listed at the end)."
1253,1," What most impressed me, however, was the literature review."
1254,3, Table 1 has two sub-tables: left and right. The sub-tables have the AP column in different places.
1255,3,"  \n\nIf the authors stated goal is \""different than the standard sentiment analysis goal of predicting whether a sentence expresses positive or negative sentiment\"" they should be aware that this is exactly what PANAS is designed to do - not to infer the latent emotional state of a person, except to the extent that their affect is positive or negative."
1256,3,"n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm."
1257,3, They also claim that their method is reaching a new milestone in evolutionary search strategies performance.
1258,3," \n\nSome minor comments:\nWhen applied to the rejected examples, wouldn't the ground truth # of clusters no longer be 4 or 10 because there are some known-class examples mixed in?"
1259,3,  Although I don\u2019t think this is the responsibility of this paper (although something that should be considered).
1260,3,"\n\n(C) Do the authors expect that it will be straightforward to remove the assumption that A is symmetric, or is this an inherent limitation of the approach?"
1261,3," The border pixels are probably sufficient to learn the\nprogram perfectly, and in fact this may be exactly what the neural net is\nlearning."
1262,3, Could you please provide more justification for such assumption?
1263,1, Below are some less important comments.\n\nSec 5.1: great results!
1264,3,"\n\nComments:\n1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other."
1265,3," This step is the real meat of the paper, yet I struggle to find a concrete definition in the text.[[CNT], [null], [DIS], [GEN]] Is this really just an average over a few recent weights during optimization?"
1266,3,What would the results be with 45k?
1267,3,"\n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')"""
1268,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
1269,3, This approach amounts to a change of basis - and therefore the resolution invariance is not surprising.
1270,1,\n\nPros:\nUses a simple parametrization of the rotation matrices.
1271,3,I believe this type of issue has to be examinated \nfor this type of approach to be widely use in inverse physical problems.
1272,3," \n\nThe primary claims of the paper are as follows:\ni) The proposed approach is a generative model of graphs, specifically producing \""sibling\"" graphs"
1273,3,"\n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture."
1274,3," Unlike deep neural networks, since RFF is such a simple model, I think providing precise theoretical understanding is crucial."
1275,3,The two methods are used separately.
1276,3, The final optimization problem that is used for training of the propose VAE should be formally defined.
1277,1,  \n\nClarity \n- The main idea of the proposed method is clear.
1278,3,\n\n1. Why don\u2019t you report your model performance without data augmentation in Table 1?
1279,3, \n\nI thought this was an impressive paper that looked at theoretical properties of CNNs.
1280,3,"\n\nSecondly, the motivation of adding the rotation operation is not properly justified."
1281,3,"""This is a mostly experimental paper which evaluates the capabilities of neural networks with weight matrices that are block diagonal."
1282,3, \n- the the -> the\n\npage 6:\n- deterministic coupling could be discussed/motivated when introduced.
1283,3, This will clearly not be the case when the continuous weights coincide with the quantized values for the current bit setting.
1284,3," Shouldn't the computational complexity then be at least O(n^2), which makes it actually much slower than, say, SQuAD models that do greedy decoding O(2n + nm)?"
1285,3, Multi-mention reasoning or more document context?
1286,3,"""This paper studies the issue of truncated backpropagation for meta-optimization."
1287,3,". Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). "
1288,3, The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.
1289,3," Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector,"
1290,3, Questions which I'd like to seen answered: how good is the OCN representation when used for clustering compared to the PCN representation?
1291,1,. I think the paper's comparisons are valid
1292,3,\n\nSpecific comments: I would remove the significance daggers from table 2 as the standard deviations are already reported and the null hypothesis for which significance is measured seems unclear.
1293,3," However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven."
1294,3," Overall, the proposed method is a relatively simple tweak to softmax."
1295,1,"  This observation is neat in my opinion, and does suggest a different use of the Jacobian in deep learning."
1296,3, The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples
1297,3,"\nThe heuristic reward is proportional to the cosine distance between the learner and expert \u201csubgoals\""\n\n   Rh = B  <   Wv LearnerDirectionInStateS,  "
1298,3," For example, we can use +1, 0, -1 to approximate the weights."
1299,3,"\n\nFinally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1. I believe this is just a typo."
1300,1, The fact that the language generated is non-trivial (Java-like) is a substantial plus\n5) Good discussion of limitations\n\n
1301,3," Unsupervised learning for physical interaction through video prediction. In NIPS, 2016."
1302,3,\n\nCons:\n- The main algorithm MCL is only a hueristic.
1303,1, The results are convincing to me.
1304,3,"\nSection 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used."
1305,3,"and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception"
1306,3," From the initial part of the paper, it seems that the proposed PrivyNet is supposed to be a meta-learning framework to split a DNN in order to improve privacy while maintaining a certain accuracy level\n3."
1307,2,Several books that are cited but there is no evidence that they have ever been studied and...
1308,2,"I have 3 main objections to this paper: it is self-contradictory, it's functionally obsolete, and it's been submitted to the wrong journal"
1309,2,The paper is - and I mean this with no disrespect to the author- a sort of echidna or platypus of a paper.
1310,1," Given, that the authors were able to improve the results in the sequential MNIST and improve the average baselines, my rating improves one point."
1311,3, They propose three training schemes to train a low precision student network from a teacher network.
1312,3," for example, the DiscoGAN results have some artifacts but capture the texture better in row 3."
1313,3," By \""do not anchor a specific meaning into the disentanglement\""?"
1314,3,"""The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update."
1315,3," Also Section 3 could be improved and simplified.[[CNT], [EMP-NEU], [SUG], [MIN]] It would be also good to add some more related work. "
1316,3," Especially in Figure 6 (right), the original Hyperband algorithm ends up higher than f-Hyperband."
1317,3," c) based on these insights, the paper proposes a variant of MALM using a Laplace approximation (with additional approximations for the covariance matrix."
1318,3, The conclusion is that reading background knowledge from concept net boost performance using some architecture.
1319,1, The introduction and related work part are clear with strong motivations to me.
1320,3, Experimental results show that the simplified versions work as well as the full LSTM.
1321,3, How long is a typical game? 
1322,3, The discriminator LSTM takes a sequence (and conditional information) as input and classifies each element of the sequence as real or synthetic -- the entire sequence is then classified by vote.
1323,3, In Section 2 the authors review conceptors.
1324,3,"\nFor evaluation data was split randomly in 80% train, 10% test and 10% validation."
1325,3,\n\nOne additional question: Skip connections have been shown to be very useful in ConvNets.
1326,3, The reason is that the label already appears in the loss of the nodes  in 5.1. Isn't using the label also as input redundant?
1327,3," Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information."
1328,1,"  The paper presents significant, novel work in a straightforward, clear and engaging way."
1329,2,"I nearly said reject, but then I recalled that I have a hangover and am feeling grumpy"
1330,3," \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? "
1331,3," \n\nThe authors demonstrate novel approaches for generating real-valued sequences using adversarial training, a train on synthetic, test of real and vice versa method for evaluating GANS, generating synthetic medical time series data, and an empirical privacy analysis."
1332,2,Pity about the main thesis. - First Sentence of the Revie
1333,3,"\n\nA second part of the paper looks at whether explanations are global or local.[[CNT], [CNT], [CNT], [MIN]] The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case."
1334,3," Does it have some support?\n"""
1335,3, The regret of concave player can be bounded using existing result for FTRL.
1336,1," They show that by bootstrapping from the final state of the time-limited domain, they are able to learn better policies for the time-unlimited case."
1337,3," Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input?"
1338,3,"""Summary: This paper explores how to handle two practical issues in reinforcement learning."
1339,3,  In this case the authors look at finding interpretable teaching strategies.
1340,1,.\nI raised my score to 7.
1341,3, Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs.
1342,3," \n\nIn summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small."
1343,3,. The detailed comments are as follows:\n\n- The main contribution of this paper is to apply word pairs instead of words to RBM models
1344,3, What is the actual contribution of the paper w.r.t. to this body of work?
1345,1," Experiments show improved generalization over mini-batch SGD, which is the main positive aspect of this paper. "
1346,1,"\n\n- No typos at all, which I find very unusual. Nice job!"
1347,3, the so called overlapping convolutional architectures
1348,2,An alternative to counting sheep
1349,3, We can then compute a posterior distribution over programs
1350,1,\n\nCOMMENTS:\n\nThe paper presents a simple observation that seems very relevant especially as computing resources are becoming increasingly available for rent on short time scales.
1351,3,"""Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100."
1352,3," As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level."
1353,3, How this current work compares with the existing such literature?
1354,3," After characterizing the kernel function, this allows us to use the GP framework for prediction, model selection, uncertainty estimation, etc."
1355,3, though they don't provide much insight into the rate at which epsilon should be decreased with n or similar properties.
1356,3, i.e. one small dataset with the only experiment purportedly showing generalization to red cars.
1357,3," That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014)."
1358,3,"\n\n5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this."
1359,2,"Not now, not ever."
1360,3,"  How\u2019s the score of the proposed model compared with the above paper as well as [Tang et al. 2016]?\n"""
1361,3,"n\nNote that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes."
1362,3," Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have."
1363,3," \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. """
1364,3," In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution."
1365,3, The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference.
1366,1,  \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization.
1367,1,"  Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n"""
1368,3," If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems."
1369,3,"\n\n\nNitpicks:\nthe gradient descent -> gradient descent or the gradient descent algorithm\nseeming -> seemingly\narbitrary flexible -> arbitrarily flexible\ncan name \""gradient descent that maximizes\"": gradient ascent.\nThe mini- max or the maximin solution is defined -> are defined\nis the follow -> is the follower\n"""
1370,3,"\n3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization."
1371,3,\n\n\u201cWe can quantitatively determine how close the posterior is to a FFG distribution by comparing the Optimal FFG bound and the Optimal Flow bound.
1372,3,\n\nI personally miss a more technical and detailed exposition of the ideas.
1373,3,"\n\nThe key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization. "
1374,3, I find that recent paper by Tamar et al 2016.
1375,3," I think a good paper should investigate this fact more."""
1376,3," For example, you could evaluate the deep-net-predicted drag ratio vs. the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shape. "
1377,3, Please elaborate on this.
1378,3, \n\n1) The total budget / number of training samples is fixed.
1379,2,"I would suggest activating the spellchecker on Word, or keeping the cat from walking on your keyboard"
1380,3,n\n6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?\n\n7
1381,3,". Finally, I would add a colorbar to indicate numerical values for the different grayscale values."
1382,3,"""The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions."
1383,3,"\n\nProx tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, \""Proximal splitting methods in signal processing,\"" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf"
1384,3, Figure 2 is sufficient to illustrate the model to readers familiar with the literature.
1385,1," For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters."
1386,3,"\n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical?"
1387,1," \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy."
1388,1,The fix is very specific and restricted.
1389,3,"\n\n1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples)."
1390,1," It is a good incremental research,"
1391,1, but the initial results are quite promising.\n\n
1392,3,"""The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates."
1393,3,"\n\nThe lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds."
1394,3," Finally, the mapping function state->subgoal is learned by performing a gradient descent on the expected total cost (based on the surrogate reward function, which also has free parameters that need to be learned)."
1395,3,"\n\n\nExperimental evaluation\nThe experimental evaluation uses 2 datasets, MNIST and EMNIST, both are very specific for character recognition."
1396,3,"""This paper investigates the Predictive State Recurrent Neural Networks (PSRNN) model that embed the predictive states in a Reproducible Hilbert Kernel Space and then update the predictive states given new observation in this space."
1397,3,"""This paper satisfies the following necessary conditions for\nacceptance."
1398,3, It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment.
1399,3,"  Granted, most world model also try to predict the reward."
1400,3,. The node embeddings are then projected into a 2-dimensional space by PCA.
1401,3," In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents."
1402,1," Its core benefit is its adaptiveness towards diverse opposing player strategies (e.g. selfish, prosocial, CCC) while maintaining maximum reward."
1403,3,"""Summary: \nThe authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix."
1404,1," Although the motivation is well grounded,"
1405,3,"  Thus, the model also have a vector representation for pair of word and position in the region."
1406,3," Is it possible to transfer student-teacher training practices to other tasks?"""
1407,1," We see that CCC is a successful, robust, and simple strategy in this\ngame."""
1408,3," Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well."
1409,3, The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms.
1410,1,"\n\nThe first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights."
1411,3,"\nDuring testing, the robot is presented with a sequence of goals in a related but different task."
1412,3,  Or are they different (maybe arbitrary) groupings over the feature maps?
1413,3, \n\nThis is either a very important paper or the analysis is incorrect but it's not my area of expertise.
1414,3, Isn't that a good way to help avoid local optima?
1415,3, Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments.
1416,3, The number of target words can be then derived and they're all predicted in parallel.
1417,3,\n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without.
1418,3,"Overall, even though the architecture is not very novel,;"
1419,3,"\n\nSHOKRI, R., AND SHMATIKOV, V. Privacy-preserving deep learning."
1420,3," They also showed how to extend this idea to the linear case: one replaces the Laplacian normally used to build PVFs with a matrix formed by sampling differences phi(s') - phi(s), where phi are features."
1421,3,"\n\n[R1] \u201cVideostory: A new multimedia embedding for few-example recognition and translation of events,\u201d in ACM MM, 2014\n\n[R2] \u201cTransductive Multi-View Zero-Shot Learning\u201d, IEEE TPAMI 2015\n\n[R3] \u201cVideo2vec embeddings recognize events when examples are scarce,\u201d IEEE TPAMI 2014\n"""
1422,3,"\n\n- About eq (12), even if softmax cross-entropy doesn't have a closed-form prox (and check the tables of Combettes and Pesquet), because it is separable (if I understand correctly) then it ought to be amenable to solving with a handful of Newton iterations which would be quite cheap."
1423,2,This work is not an easy topic. 
1424,2,"eviewer : 'The project can hardly be described as high risk/high gain' 
Reviewer : 'The project is clearly high risk/high gain"
1425,3," Do other deep neural networks, such as Resnet, Googlenet, can  have the same phenomenon?"
1426,1,"\n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs."
1427,3,"Their main theoretical result from this is that deeper networks take exponentially longer to train (under certain conditions), which they test out with (linear?) networks of depth 50 and width 100."
1428,3, I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly.
1429,3," I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats."""
1430,3,\n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer.
1431,3,"\n5) Learning rates and their drops should be rescaled taking into account the schedule of the batch size and the rules to adapt learning rates in large-scale settings as by Goyal. """
1432,3, It would be great to have more intuitions.
1433,3,\]n-The paper proposes interesting assumptions and results for measuring the complexity of semantic mappings
1434,3, \n\nThen the authors present the results for machine translation task and also analysis of their proposed method.
1435,3,"""The paper presents a Depthwise Separable Graph Convolution network that aims\nat generalizing Depthwise convolutions, that exhibit a nice performance in image\nrelated tasks, to the graph domain."
1436,3,\n\nRe: High resolution.
1437,3,"\n\nArticle is based on recent works such as Wasserstein GANs and AC-GANs\nby (Odena et al., 2016)."
1438,2,"The paper is overlong, very verbose and contains unnecessary repetition. - (via shitmyreviewerssay)"
1439,3,"""This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis."
1440,3," The focus is on the key aspect, which is generalisation across heteregeneous data."
1441,1,"\n\nI think the paper does a fairly good job at doing what it does,"
1442,3, Each HMM emits an unobserved output with an explicit duration period
1443,3, Answering this question may help understand what influence variational inference has on this model.
1444,1," The algorithms are shown to be consistent, and demonstrated to be more efficient than an existing semi-dual algorithm."
1445,1,\nThe experimental results make more sense now.
1446,3," Unfortunately, the paper requires significant improvements, both in terms of substance and in terms of presentation."
1447,3, \u201cDeep neural networks without training deep networks\u201d.
1448,3," e.g., \np.2:\nT: s \\to a \\to s', \\pi : s \\to a\nA denotes a set and its cardinal\nIn (1), shouldn't it be \\Phi(o)?"
1449,2,This whole paper is wildly speculative and needlessly convoluted.
1450,3," Their innovations are to use reinforcement learning to guide sequential generation of tokes towards a high reward output, incorporate syntax checking into the synthesis procedure to prune syntactically invalid programs."
1451,1, \nThe paper has extensive experiments in a variety of domains.
1452,3,"\n\nHowever, the method described is restricted in the following aspects."
1453,3,"""Paper summary:\nExisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks."
1454,3,  (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?
1455,3,\n\n- How were the tasks selected?
1456,3,"\nLet || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*.\nIn that case, Lipschitz continuity writes\nf(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||*\nIn the case where || . || is an \\ell-p norm, then || . ||* is an \\ell-q norm; with 1/p+1/q = 1."
1457,3, The recurrent model takes the form of a graph neural network.
1458,3,"\""  Do the authors mean that the negative log-likelihood will be improved in this case?"
1459,3," Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E)"
1460,3, I've heard of heuristics saying that a fixed stepsize is best and then you just make sure to stop the algorithm a bit early before it diverges or behaves wildly -- is that true here?
1461,3,"""The paper proposes to extend the usual PPMI matrix factorization (Levy and Goldberg, 2014) to a (3rd-order) PPMI tensor factorization. The paper chooses symmetric CP decomposition so that word representations are tied across all three views."
1462,3, One would expect Q_MC to work well in Grid Map domain if the conjecture put forth by the authors was to hold universally.
1463,3,"\nEven the footnote has been copied & pasted: \u00ab\u00a0For convenience we assume that the system is k-observable: that is, the distribution of all future observations\nis determined by the distribution of the next k observations. (Note: not by the next k observations\nthemselves.)"
1464,3," Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders."
1465,3," \nFurthermore, the manuscript seems to suggest, that the simulation results are somehow related to human vision as it is stated:\n\u201cThe model provides apparently realistic saccades, for they cover the full range of the image and tend to point over regions that contain class-characteristic pixels.\u201d\nbut no actual comparisons or evaluations are provided. """
1466,1,\n\nPaper Strengths:\n- An incremental yet interesting advance in geometric CNNs.
1467,3, The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs.
1468,1," On the TriviaQA dataset, the proposed model achieves state of the art results on both domains (wikipedia and web)."
1469,3," It's possible some of the issues arise from the particular architectures they choose to investigate and demonstrate on (eg I have mostly seen ResNets in the context of CNNs but they analyze on FC topologies, the form of the loss, etc) but that's a guess and there are some further analysis in the supp material for these networks which I haven't looked at in detail."
1470,3,"\n\nGiven that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons."
1471,3,"\n\nRegarding the experimental results, is there any insight on why the dense networks are falling short?"
1472,3," This should be explicitly shown, at least in the appendix."
1473,3, Are the differences actually significant?
1474,3," \n2) Batch size is scheduled to change between B_min and B_max\n3) Different setting of B_min and B_max>=B_min are considered, e.g., among [64, 128, 256, 512, ...] or [64, 256, 1024, ...] if it is too expensive."
1475,3,"(2) the latent variable parameters are functions of a CNN, "
1476,3, This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data.
1477,3,  \n- What does the PIR with the model in Section 3 stand for?
1478,3, (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this).
1479,3," \n\nOverall this feels like an cute hack, supported by plausible intuition but without deep theory or compelling results on real tasks (yet)."
1480,3,did you study the effect of the architectures in terms of striding and pooling how it affects the results?
1481,3,"""This paper examines the problem of optimizing deep networks of hard-threshold units."
1482,3,\n\n- Learning translation without parallel corpora is a useful task and leveraging a pragmatic reference game to induce additional semantically valid samples of a source language is an interesting approach to do so.
1483,3,\n6. Page 7: The following sentence is confusing and should be clarified:
1484,3, I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.
1485,3," \nHow is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model?"
1486,2,"I also do not feel that the lead PI is qualified to undertake this work, [..]  she needs to be academically successful first"
1487,3,\n\nI do have some serious questions/concerns about this method:
1488,3," First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.)."
1489,3,". If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention? "
1490,1,"""The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it."
1491,1,"\n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental."
1492,2,Is this really a discovery or just the confirmation of math?
1493,1,\n\nThe paper is well written and easy to understand.
1494,1,I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow.
1495,3,"\n\nY. Bengio, N. Leonard, and A. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv:1308.3432, 2013.\n\n"""
1496,1,"\n\nOverall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking."
1497,2,The paper raises the suspicion that the author has not been trained as a historian
1498,3," Second, the paper offers a simplification of LSTMs that compute the value by which the memory cell at each time step in terms of a deterministic function of the input rather than a function of the input and the current context."
1499,3,\n2. How is the forward model / value function used in MCTS?
1500,3, Are there theorems to be had?
1501,3," Also, in the experiments, the authors mention multiple attempt with the same settings -- are these experiments differentiated only by their initialization?"
1502,3, Unclear if this would work at all in higher-dimensional time series.
1503,3,"\n\nTheorem 1 is only valid for entropy-based regularizations, what is the difficulty for having a similar result with L2 regularization?"
1504,1, The paper is well-written and easy to follow.
1505,3," To reduce the memory required for training, the authors also propose a path-wise training procedure based on the independent convolution paths of CrescendoNet."
1506,3," The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters."
1507,3,  It would also be nice to observe failure cases of the model.
1508,2,The writing is not at school level
1509,3," The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule."
1510,1," As a final point, the authors state, \""as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L."
1511,3, The experimental results are similar to previously proposed methods.
1512,2,I have difficulties identifying the aim and added value of the present study to this topic.
1513,3, What does it do that a usual LSTM cell could not learn?
1514,3,"\n\nSentence before Equation (5): I believe there is a typo here, \u201cf takes z_i\u201d should be \u201cf takes u_t\u201d."
1515,1," but this seems like an effective technique for people who want to build effective systems with whatever data they\u2019ve got. \n"""
1516,3,. Would the approach work as well using a more standard encoder-decoder model with determinstic Z?
1517,3," Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017."
1518,1,"\n\nIn particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation."
1519,3,\n\nSmaller nitpicks:\n\n> \u201cNew state of the art for evolutionary strategies on this task
1520,3," It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests."
1521,3, but I have some concerns regarding this work.
1522,3," They also introduce a \""raw point cloud GAN\"" (r-GAN) that, instead of generating a latent code, directly produces a point cloud.\"
1523,2,It would be wholly inappropriate to randomize living people to an intervention.
1524,3,"""This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input."
1525,3, This is a very active area of research and the paper needs to justify their approach.
1526,3,\n\nThe causality assumption does not seem to apply to the few-shot classification case.
1527,3, How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only?
1528,3,"""The paper addresses the problem of learning the form of the activation functions in neural networks."
1529,3,"""Summary:\nThis paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention."
1530,1,"\n\n\nReview update after reply:\n\nThe authors have responded to most of my concerns and I think the paper is much stronger now and discuss the relation with regularized OT. I change the rating to Accept. \n"" 001 101"
1531,3," Here, the image is considered to be an effect of all the labels."
1532,3,\n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice.
1533,3, The SVM is trained for the final classification task at hand using the last layer features of the deep network.
1534,1, The paper's results also suggest that we should be thinking more broadly about how to using complicated structured distributions as teachers for model compression.
1535,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
1536,1," and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn."
1537,2,"This is nonsense, It is disingenuous to say the least &amp; The authors should stop pretending their method is useful h/"
1538,3,"  All methods work on \u201cfc7 features from the RGB stream of a two-stream CNN provided by the organizers of the [Charades] challenge\u201d, and the best-performing method (Skip GRU) achieves 9.02 mAP."
1539,2,Asterisk rather than Asterix
1540,3,  Which parameters are fit using the fine-tuning loss described on page 3?
1541,3, The model is trained by a policy gradient algorithm.
1542,3,"""This paper proposes a feature augmentation method for one-shot learning."
1543,3," On the other hand, \ngiven the expression of the proposed regulatization,\nit seems to lead to non-convex optimization problems which are hard to solve. Any comment on that?"
1544,3," But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work."
1545,3," Especially with natural images,  the spacial location and the scale should be critical. "
1546,3, This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter
1547,3, Did the author experiment with a comparable architecture?
1548,1,"\n\nAlthough the manuscript has many positive aspects to it,"
1549,3,"  For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?"
1550,3, the paper falls short in terms of providing experimental validation that would demonstrate the latter point.
1551,1,\nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN.
1552,3, The teacher's strategy should also be  learned from data.
1553,1, The authors show that UA results in gains on several of the games.
1554,3," The results are neat, but \nI couldn't tell why this approach was better than others."
1555,3,"  While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy."
1556,1, and the proofs are clearly written.
1557,3, All of the reported results are what you would expect.
1558,3," \n? p.6: Do you have one model for all the relations or does every relation has its own LSTM, CNN, feed-forward network?"
1559,3,\nThe training procedure follows an alternative minimization in EM style.
1560,1," Per se, the model is incrementally new,"
1561,2,This is a confusing paper
1562,1,"\u201d These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3."
1563,3, Where is this restriction coming from?
1564,3," Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms."
1565,3," A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent."
1566,1,"\n\nOverall, the paper presents several incremental improvement over existing theories."
1567,1, \n\n\n## Clarity\n\nOverall the paper reads reasonably well.
1568,1, The paper claims superior results using the described method.
1569,1,". They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work. "
1570,1," And for (2), it is a relatively standard approach in utilizing CNN features."
1571,3," The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection."
1572,1, Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence.
1573,3,\n\nSignificance/Conclusion: The idea of meta-learning or learning to learn is fairly common now
1574,3,\n\nI am not familiar with the task at hand so I cannot properly judge the quality/accuracy of the results obtained but it seems ok.
1575,3, Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results.
1576,3,. The authors filtered the number of words and word-pairs to very small
1577,3, \nd)\tHow is the RW statistically addressing the generation of high-order (subgraph) features?
1578,1,"""Congratulations on a very interesting and clear paper."
1579,1," \n\nThus I think that the paper should be published."""
1580,3,"\n\nWonja et al, The Devil is in the Decoder, In BMVC, 2017\n"""
1581,1,\n\nI really enjoyed reading this paper.
1582,3,"\n\nPage 2\n\""convolutional neural networks(CNN)\"" -> \""convolutional neural networks (CNN)[[CNT], [CLA-NEG], [CRT], [MIN]] \""\n\""related works\"" -> \""related work \"""
1583,3,\nOnly a single synthetic task is reported.\n\n
1584,2,The authors criticize the approach of [citation of X] before introducing their own. Seems somewhat hypocritical to me.
1585,3,  \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic!
1586,1,  but the method of analysing the failure modes is interesting.
1587,3, Focusing on discrete targets gains the benefits of quantized networks.
1588,1," Differently, it can memorize the training data and performs as good as the baseline model. "
1589,3," It may help the readers better understand the values of this work.\n    """
1590,1," The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non-trivial, and is a contribution of this paper."
1591,3," Conversely, if the diffusion time were measured under another policy, it also seems likely that options built with a random policy would not perform so well (assuming that the state space is reasonably large to make an exhaustive exploration infeasible)."
1592,3,".\u201d This experiment only shows that this is true in aggregate, not for specific neurons?"
1593,3, In this case it seems likely that the decrease in the diffusion time would not be as apparent as in the experiments of the paper.
1594,1,"\n\nThe proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs)."
1595,3," In fact, Vinyals et al. note this very point in their paper."
1596,2,This is an interesting paper but it is not relevant
1597,1,"\nThe paper is overall well-written, and the proposed idea seems interesting."
1598,1,\n\nThe paper is well organized and well written.
1599,3, I get that tau is going to be constrained by whatever representation PATH function was trained on and that this representation might affect the overall performance - performance.
1600,3,"\nIf rejected from here, it could perhaps be submitted as an ICASSP or Interspeech paper."""
1601,3, \n\nHave you considered the widely-used NCE method to handle the large vocabulary?
1602,3,"\""consistent\"" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2.[[CNT], [CLA-NEG], [CRT], [MAJ]] You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel ."
1603,3," These topics seem sufficiently related to the proposed approach that the authors should include them in their related work section, and explain the similarities and differences."
1604,3," I agree with authors that this is an attempt to combine system identification with generating control inputs together, but I am not sure how to remove the restriction on A."
1605,3," Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention"
1606,2,"Concerning the discussion, again the merits of the work are downplayed to a point that its almost..."
1607,3," Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case."
1608,3," see :\n\""Think Globally, Embed Locally\u2014Locally Linear Meta-embedding of Words\"", Bollegala et al., 2017\nhttps://arxiv.org/pdf/1709.06671.pdf\n\nThat paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission."
1609,3," By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework."
1610,1,"n\n1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers."
1611,3,What is the objective for the images in Fig. 4? 
1612,1, The final dataset is also a good size (36M search queries).
1613,1,"\n\nI think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs."
1614,2,Farcical
1615,3, Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.
1616,3," Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1)."
1617,1," But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.\n\n"""
1618,3,"\n\n\n*This relates also to this: \n\n\""Later we empirically verify that, even when the overall in-\nformation revealed does not increase per se, an independent master agent tend to absorb the same\ninformation within a big picture and effectively helps to make decision in a global manner."
1619,3,  The authors test their method on various adversarially constructed inputs (with varying degrees of noise).
1620,3,"  It is very similar to \""Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies\"" and is effectively an indirect form of tile coding (each could be seen as a fixed voronoi cell)."
1621,3," \nThe implicit are potentially one order of magnitude more costly than an explicit step since they require\nto solve a linear system, but can be solved (exactly or partially) using conjugate gradient steps."
1622,1,"\n\nOn the whole I appreciate the novelty of the task and dataset,"
1623,1,\n(2) The introduction and related work are well written.
1624,3,"\n- Below eq. (1), I am not sure what the V in P_V refers to."
1625,3,"""Summary:\nThis paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep networks. "
1626,1, \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN. 
1627,3,"\nThus its Lipschitz constant is different, potentially equal to\nL_q = max_{k} \\| L_q^k \\|, \nwhere L_q^k is the Lipschitz constant of f_c-f_k."
1628,3,"\nExample 1 from abstract: \n\u201cWe show that for a wide class of differentiable activation functions (this class involved \u201calmost\u201d all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.\u201d\n\nThis is certainly not proven and in fact not formally stated anywhere in the paper."
1629,3," Intuitively it seems that L is redefined, and for, say, n = 4, the model is M(i,j,k,n) = \\sum_1^R u_ir u_jr u_kr u_nr."
1630,3,  Investigating some of these questions would help us understand how well the approach works and in which settings.
1631,3,"""This paper concerns open-world classification."
1632,3,"and (b) although the tensor is non-negative, its symmetric factorization is not guaranteed to be non-negative and further elaboration of this issue seem to be important to me."
1633,3, Why is this the case?
1634,3," Maybe add a sentence regarding the large/Small eigenvalues?"""
1635,3," From the security perspective, the scenarios are too simplistic."
1636,1,\n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.
1637,3," As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting."
1638,3, In the first paragraph of Sec. 3 the competing deep networks are introduced.
1639,1," Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity."
1640,3,"\n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN."
1641,3,"\n\n- The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable."
1642,3, What is the problem the authors are trying to solve with AESMC (over existing methods)? 
1643,1," This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n"""
1644,3,"\nThe emphasis on the computational demand of 1-3 minutes for LCE seems like a red herring: MetaQNN trained 2700 networks in 100 GPU days, i.e., about 1 network per GPU hour."
1645,2,These snotty kids in quantum information
1646,3, \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings.
1647,3, The authors empirically demonstrated the gated fast weights outperforms other baseline methods on the associative retrieval task.
1648,3,"\"" In Information Theory Workshop (ITW), 2015 IEEE, pp. 1-5. IEEE, 2015."
1649,1, I am raising the score.
1650,1,"\n\nThe experimental results presented in this paper are quite good,"
1651,3,"""This paper proposes to use neural network and gradient descent to automatically design for engineering tasks."
1652,3,"""This paper tackles the object counting problem in visual question answering."
1653,3, What exactly is the purpose of this paper? 
1654,3,  The proposed method is focused on discovering sparse neural networks.
1655,3,"""The authors propose to use synthetic data generated by GANs as a replacement for personally identifiable data in training ML models for privacy-sensitive applications such as medicine."
1656,3,"and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3]."
1657,3,"\n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016."
1658,1,"\n\n\n## Significance\n\nMany recent previous efforts have looked at the importance of batch sizes\nduring training, so topic is relevant to the community."
1659,3,"""This paper proposes a variant of hierarchical hidden Markov Models (HMMs) where the chains operate at different time-scales with an associate d spectral estimation procedure that is computationally efficient."
1660,3,"  Some explicit discussion of convolutional layers may be\nhelpful.  """
1661,3," Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums."
1662,3,"\n\n* Sec. 3. 4 mentions that the \u201conly image\u201d encoder is used to obtain the representation for the image, but the \u201conly image\u201d encoder is expected to capture the \u201cindescribable component\u201d from the image, then how is the attribute information from the image captured in this framework?"
1663,3,"\nThere are T different models, one for each prefix y_{1:t} of length t."
1664,3," Smith and Le (2017)\npresent a differential equation model for the scale of gradients in SGD,\nfinding a linear scaling rule proportional to eps N/B, where eps = learning\nrate, N = training set size, and B = batch size."
1665,3, It\u2019s not immediately clear what the semantics of this posterior are then.
1666,3, Second - Is your approach applicable to these frameworks?
1667,1, \n\nExperimental results seem promising but I wasn\u2019t fully convinced of its conclusions.
1668,2,The authors need to add a level of puzzlement to their interpretations
1669,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
1670,3,\n\nThe paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF.
1671,3," It can consider different classifiers and loss functions, and a sampling strategy for making the optimization problem scalable is proposed."
1672,3," These rejected images could be clustered to identify the number of unseen classes; either for revealing the underlying structure of the unseen classes, or to reduce annotation costs."
1673,3," For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions."
1674,1,"\n\nOverall, the paper is clearly written and easy to understand the main motivation and methods."
1675,3,"""The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules. "
1676,3, At the very least a mention of how the convergence proof would follow other common proofs in RL.
1677,3,"""\n1) Summary\nThis paper proposes a flow-based neural network architecture and adversarial training for multi-step video prediction."
1678,1,"""The paper is well motivated and written."
1679,3, Is it because it reduces the amplitude of the updates (and thus simply slows down the training)?
1680,1, \n\n() Pros / Cons:\n+ simple yet powerful method for text classification
1681,3,"It combines the advanced attention mechanism, pointer networks and REINFORCE learning signal to train a sequence-to-sequence model for text summarization."
1682,3, \n1. I am not very familiar with the literature of few shot learning.
1683,3,"""It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing"
1684,3, What are the memory requirements and computational complexity of the proposed method?
1685,3," In addition, how should \\tau be chosen in these experiments?\n"""
1686,3, Does this theorem imply that the global optima can be reached from a random initialization?
1687,2,"Not a well prepared application, full of mistakes &amp; lacking some necessary preliminary data. Not at all fundable."
1688,3," \n\nI would have liked some analysis of the convergence results for TD learning with this constraint, or some better intuition in how this effects learning."
1689,3," It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm."
1690,3, \nThe proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network.
1691,3,"\n\nOverall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score."""
1692,3," On Page 3 we assume that K = 1, but Theorem 6 still maintains a dependence on K"
1693,3,"""This paper applies the boosting trick to deep learning."
1694,1,  So I recommend acceptance.
1695,3, An adversarially trained model learns on two different distributions.
1696,3,"""The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space."
1697,2,The results of the study are adequately justified.
1698,1," \n\nPros:\nOne of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions."
1699,3,"  When the mean of input is zero, there is no angle bias in the first layer."
1700,1,\n2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches.
1701,3, and 2) predicting a variable's name by consider its semantic context.
1702,3,"""The authors are motivated by two problems: Inputting non-Euclidean data (such as graphs) into deep CNNs, and analyzing optimization properties of deep networks."
1703,3," Authors are then testing the model using test sets that do not follow the same distribution than training data, example,  longer sequences."
1704,3,"   \nFirst, there is quite a bit of recent work on learning to teach and curriculum learning."
1705,3, This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint.
1706,3," Let\u2019s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, \u2026]."
1707,3,"""Summary\nThe paper presents an interesting view on the recently proposed MAML formulation of meta-learning (Finn et al)."
1708,3,"  Second, why not simply propose learning exponential family models where the parameters of these models are (deep nets) conditioned on the input?"
1709,3," At least, it should show the generation texts were affected about DAs in a systemic way."
1710,3,"\n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \""In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S\u00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1)."
1711,1," I would foresee more and more works will be devoted to this area, considering its close connection to our daily life."
1712,1,"\n\nFor document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this."
1713,3," The group-wise posteriors allow amortization of the information cost KL(group posterior || prior) across all examples in the group, which the authors liken to the \""KL annealing\"" tricks that are sometimes used to avoid posterior collapse when training models with strong decoders p(x|z) using current techniques for approximate variational inference in deep nets."
1714,3, are the results significantly different?
1715,2,The approach taken is fundamentally inadequate and flawed for almost all use cases
1716,1,\n\nPros:\n- Important starting question
1717,3, Could the authors provide insight into why they did not use the ResNet structure from the  tcml paper in their L-MLMA scheme ?
1718,3," See, for example, uses of Langevin dynamics as a non-convex optimization method."
1719,3, \n\nThe method is evaluated in simulation with comparisons to a simple baseline that tries to get over to the right lane as well as human performance.
1720,1," For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component."
1721,3," On Page 8, P(B) should be a degree-4 polynomial of B.[[CNT], [CLA-NEG], [SUG], [MIN]]\n\nThe paper does not contains any experimental results on real data."""
1722,3," Why spend so much time on a step-by-step derivation anyway, as this is all \""classic\"" and has been carried out many times before (in a cleaner write-up)?"
1723,1,"n\nSpecific comments/questions:\n- The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this."
1724,3," It is argued that the ML approach has some \""discrepancy\"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity."
1725,3," While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline."
1726,3, Most of the techniques used are well-known in the tensor community (outside of machine learning).
1727,3,"  Below I list some of the key problems.[[CNT], [null], [DIS], [GEN]]  First of all the authors claim in the introduction that their algorithm is very fast and with provable theoretical guarantees."
1728,3," That is because almost all tasks require good representations for all words, not just prepositions."
1729,3, The dimensions of the base embeddings are some kind of latent attributes and each individual dimension could be used by the model to capture a variety of attributes.
1730,3,"\n(3) detailed information are provided about the experiments, such as data, model and inference."
1731,3, A bi-directional LSTM is used to encode latent space in the training stage.
1732,3, The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network.
1733,1,"  The most novel contribution of this ICLR paper seems to be equation (4), where the authors set up an optimization problem to solve for optimal inputs; much of that optimization set-up relies on Hazan's work, though."
1734,3,"\n\n2) \"" Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work."
1735,1," The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly."
1736,3,  Or is it designed to still classify adversarial examples in a robust way?
1737,1," 19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design."
1738,3,  So I would have liked a comparison to that simple method (using similar regression technique to generalize over states with similar features).
1739,3,"\n\n* p 3 bottom -- give size of dataset\n\n* p 5 AUC curve -> ROC curve\n\n* p 6 Fig 4 use text over each image to better specify the details given in the caption.\n\n\n\n"""
1740,3,"""The authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperature."
1741,3,"  Furthermore, even if suitable datasets are not available, the authors could have chosen to train different architectures."
1742,1,"\n\n\nPart 3 is the most interesting part of the paper,"
1743,3,"""This paper introduces a graph neural net approach to few-shot learning."
1744,3, The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes. 
1745,3, The idea of using rank-1 tensor decomposition for training low-rank filtering operations in DNNs has already been proposed and used in several other work.
1746,3,"  After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks."""
1747,3,"?\n        * comparison to GloVe on the entire corpus (not covariate specific)\n        * no reference for the metrics used (AP, BLESS, etc.?)"
1748,3,"\n2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5."
1749,3," Since the efficiency is one of the main contributions, I suggest authors add this comparison.\n\n"
1750,3,  I think further work is required (perhaps expanding the search space) to resolve the current limitations of automated architecture search.
1751,1,"   The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there."
1752,3,"  I think just counting number of layers as a complexity is not reasonable when we deal with images, with respect to  what preserves contents and what matches texture or style"
1753,1," Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted."
1754,1,"\n\nOriginality\n\nThis is one of the first uses of GANs in the context of neuroimaging."""
1755,1, The experimental results supports the claim.
1756,1," While the approach (as has been shown in the past) is very reasonable,"
1757,3,\n\nComments on the details of the paper:
1758,3,"\n\nIn table 3, it would be very helpful to display the English source."
1759,1, \n\npros:\n--I liked the posterior sharpening idea.
1760,3, To me it looks like the probability of it being larger than zero is something like 2/3.
1761,3, One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks.
1762,3," I would be curious if the same trends and conclusions can be drawn from a larger dataset -- e.g., the internal dataset that results are reported on later in the paper, or on a set like Switchboard."
1763,3,The goal is to learn sparse structures across layers of fully connected networks.
1764,3, The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task
1765,3, Were these experiments run until completion?
1766,2,I dont believe in simulations
1767,3,"\n\n(2) In Figure 1, I suggest adding an \""N \\times\"" symbol to the left of the Q-P Attention Layer and remove the current list of such layers, in order to be consistent to the other parts of the figure."
1768,2,'It is not even wrong' - Wolfgang Pauli commenting on the manuscript of a junior colleagu
1769,3,"\n\nBottom line: The paper may contribute to the current discussion of the Zhang et al 2016 paper, but I feel  it does not make a significant contribution to the state of knowledge in machine learning."
1770,1,. The proposed model is novel but incremental comparing to existing frameworks.
1771,3, I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract.
1772,1,"\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent"
1773,1,\n\nThe idea seems promising
1774,1,\n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel.
1775,3," A possible improvement is to try other means for the embedding instead of the Euclidean one.\n\n"""
1776,1,\n- Suggested modifications from DTP to STDP increase its biological plausibility without making its performance worse.
1777,1,\n\nI think the paper could do a better job differentiating from those other papers.
1778,1, This conclusion seems to be intuitive and expected.
1779,1,"\n\nOverall, this paper appears very interesting."
1780,3," Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition."
1781,3," Now let's assume we insert a scaling constant after each residual block, say c = 0.5."
1782,1, Experiments are performed on a new robot arm dataset proposed in the paper where they outperform the used baselines.
1783,3," The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al."
1784,3,"\nThe analysis is also performed by varying the network architectures, considering data augmentation and/or fine tuning."
1785,3, What was the width of each layer? 
1786,3," The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation."
1787,3,  This would require a phi(s) function that is trained in a way that doesn\u2019t depend on the action a.
1788,3," Extending control laws through self-exploration under random disturbances has been studied in character control (e.g. \""Domain of Attraction Expansion for Physics-based Character Control\"" by Borno et al.), but the dimensionality of the problem makes this exploration very expensive (even for short time frames, and even in simulation)."
1789,3,\n- I am not sure if the attention overt chart is a highlight of the paper or not.
1790,1," \n\nThe results are interesting, but more explanation is needed for the main message to be conveyed more clearly."
1791,3," It can be written as x + r_1 + r_2 + .. + r_B, where r_b is the value computed by residual block b."
1792,3," Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid."
1793,3," \n\n[3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015)."
1794,3," With both theoretical and experimental analysis, it suggests the optimal batch-size given learning rate and training data size."
1795,1,\n\nI think that defining metrics for evaluating the degree of disentanglement in representations is  great problem to look at.
1796,3,\n\n\nMy Comments:\n\nThis paper is a direct application of adversarial learning to the task of reading comprehension.
1797,3," So I suggest the authors do not claim that this method is a \""general-purpose\"" sentence embedding model."
1798,3," From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods."
1799,1,\n\nThe model and algorithm in this paper are simple and straightforward.
1800,3,"\n+The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical?"
1801,3," However, freezing the layers and continue to train the last layer is of a minor novelty."
1802,2,The manuscript presents the resits [sic] of a meta-analysis which seems to be based on very abundant but very poor primary data
1803,3," However, a ReLU can also be approximated by a smooth function and a Taylor series."
1804,2,They claim on page 9 a significant improvement of their method. This is stupid!
1805,1,"\nIn some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy."
1806,3," Beyond this, the paper does not provide any futher original idea."
1807,3," I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math."
1808,3,"\nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016)."
1809,3,"\n\n- The notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology, e.g., clustering."
1810,3," It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. """
1811,3," Base your definition on F=(V,E) or F=(V,A)."
1812,3,"  For example PANAS-X includes words like:\u201cstrong\u201d ,\u201cactive\u201d, \u201chealthy\u201d, \u201csleepy\u201d which are not considered emotion words by psychology."
1813,2,[The methods section] reads more as if these explanations are put in to guide the authors themselves
1814,3, Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR.
1815,1, It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good).
1816,3,"""\nSUMMARY\n\nThis paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection."
1817,3, That method could also be considered as a possible approach to compare against here.
1818,3,\n \nContributions:\n\n1 The authors proposed Crescendo block that consists of convolution paths with increasing depth.
1819,3," Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time."
1820,1,\n\nClarity: This paper clearly written;
1821,1,"\n\n\n## Quality\n\nOverall, only single training runs from a random initialization are used."
1822,3," At training time, the generator's encoder computes a context representation using the masked sequence."
1823,3," For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents?"
1824,3, Is there a way to avoid this?
1825,1,"\n \n7) I haven\u2019t had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done!"
1826,1,The task itself is interesting and novel.
1827,3, Batch-norm?
1828,1, Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features.
1829,3, The paper uses this view to improve the MAML algorithm.
1830,1, The probabilistic framework itself is quite straight-forward.
1831,3,"\n\nDepending on how the authors respond to the reviews, I would consider upgrading the score of my review."""
1832,3,\n\nPros:\n  1. Experimental study on retrofitting existing word vectors for ESL and TOEFL lexical similarity datasets
1833,1, The derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potential.
1834,3, The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator.
1835,3," Now, if we think of a mini-batch as being a batched version of single pattern updates, then clearly the effective step length should scale with the batch size, which - because of the batch size normalization with N/B - means \\epsilon needs to scale with B."
1836,3," More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label."
1837,3," \n\nIn conclusion, because the novelty is also not that big (CP decomposition for word embeddings is a very natural idea) I believe the evaluation and analysis must be significantly strengthened for acceptance. """
1838,1,"\n\nCons:\n- Overall, the approach seems to be an incremental improvement over the previous work ResNeXt."
1839,3," \n\n2. As well know, VGG16 with well training strategy (learning rate decay) could achieve at least 92 percent accuracy."
1840,3," Even some speculation on how this aspect\n  could be applied would be appreciated (admittedly, many GAN papers could use\n  some reflection of this sort)."""
1841,2,"But fundamentally, why did you bother?"
1842,3,"\n* In practice, how much more GPU memory is required to train the model with parallel branches (with same parameter budgets) because memory consumption is one of the main problems of networks with multiple branches."
1843,3," e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n"""
1844,2,"The figure is really not needed. Were I to be shown this by a psychologist, I suspect I would hire a replacement"
1845,2,Someone has been foraging in theory and has managed to learn how to mangle simple concepts and hide them behind pretentious empty prose
1846,3,  The authors have made their synthetic dataset publicly available.\
1847,1," While performing worse than the amTFT approach and only working well for larger number of iterations, the outcome-based evaluation shows benefits."
1848,3," However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used."
1849,3, \n\nThe unimodal claim for distribution without randomness is weak.
1850,1," \n\n\nThere is a nice variety of authors and words, though I question if even with all those books, the corpus is big enough to produce meaningful vectors."
1851,3," The authors should consider a test where the robot remains stationary with a fixed goal, but obstacles are move around it to  see how it affects the selected action distributions."
1852,3," \n\n[Original review]\nThe authors propose auto-encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC."
1853,3,\n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].
1854,3,"\n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here)."
1855,3," Then, this graph is processed with an graph neural network framework that relies on modelling the differences between features maps, \\propto \\phi(abs(x_i-x_j))."
1856,1, \n\nOriginality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new.
1857,3,"""The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers."
1858,3,\n\n3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to.
1859,3,"\n\nThe distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes."
1860,3,  The contribution of this paper is to add to this a method which uses 2 different ELBOs for updating different sets of parameters.
1861,3, And such representation is generally regarded as essential for the success of deep learning.
1862,3,\n- How does the fixed horizon interact with conjoining goals?
1863,3,"\n - what is a \""pushforward measure\""?"
1864,3, This one is a more recent development.
1865,3, Where does this ground truth come from?
1866,2,This work is antithetical to the spirit of [XX research] and will impede potentially important developments.
1867,1,\n- Useful possible applications identified
1868,3,"\n\nTheir experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun)."
1869,3," Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines."
1870,1,\n \nQuality/clarity:\n\nThe paper is well written and easy to follow.
1871,3,\n1. Why use CNN for the style representation layer?
1872,3,"  I am wondering how D_S is trained, how the GT labels are obtained and whether it is trained jointly."
1873,3,"\n\nMy comments are the following:\n\n1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks.[[CNT], [null], [QSN], [MIN]]\n\n2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method."
1874,3,  This is a greatly restricted setting compared to real images.
1875,3, It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method.
1876,3,"  In the experiments, the authors compared with several previous attention methods to show the effectiveness of the proposed concept detection module and demonstrated its superiority on several configurations, including in-domain and out-of-domain cases."
1877,3,"""The paper addresses the problem of devising a quantitative benchmark to evaluate the capability of algorithms to disentangle factors of variation in the data."
1878,3," The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context."
1879,1,"\n\n\nPOST REBUTTAL RESPONSE:\nThe authors have addressed the comments on the MNIST experiments and show better results,"
1880,3, A better citation would be Jordan et\tal 1999.
1881,3,"""The paper shows that interpretations for DNN decisions, e.g. computed by methods such as sensitivity analysis or DeepLift, are fragile: Visually (to a human) inperceptibly different image cause greatly different explanations (and also to an extent different classifier outputs)."
1882,3,   The paper then proposes an inference framework (to approximately marginalize out all GP functions)  based on sparse GP methods that use inducing points and variational inference.
1883,3," I think the following one is also relevant:\n-- Law et al. Deep spectral clustering learning. ICML 2015.\n\n"""
1884,2,The abstract is ok in the context of a weak manuscript
1885,3,", it is especially important to compare your method more thoroughly to simpler methods."
1886,3, What would happen if shapes different than random squared patterns were used at test time?
1887,3, \n\nReview:\n\nUsing an implicit step leads to a descent step in a direction which is different than the gradient step.
1888,3,"\n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \""reading\"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%)."
1889,3," The top 50 entries have METEOR scores >= 0.25, while the maximum METEOR score reported by the authors is 0.22."
1890,3, It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those.
1891,1,\n\nThe robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem.
1892,2,It would be charitable to call this a comparison of apples and oranges. Its more like steak and...
1893,1,"\n\nWeaknesses\n - Although the proposed model is helpful to model counting information in VQA,"
1894,3,"""Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems."
1895,3," The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach."
1896,1, While the authors use a set of very novel generative models
1897,3,"""In the context of multitask reinforcement learning, this paper considers the problem of learning behaviours when given specifications of subtasks and the relationship between them, in the form of a task graph."
1898,3," Although the focus is apparently different, these methods are clearly closely related"
1899,3, There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3).
1900,3," For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context."
1901,3," \"" It had more difficulty optimizing for the three-color result\"" why?"
1902,1,\n\nThe paper is clear and very well written.
1903,1, The angle the authors took is interesting (essentially replacing one level of the bi-level optimization problem in machine teaching works with a reinforcement learning setup).
1904,1," I have listed this review as a good for publication due to the novelty of ideas presented,"
1905,3,\n\n- Motivation in section 4.1 was a bit iffy.
1906,3,. Is there any limit on how many examples each worker has to label?
1907,3, Why is this a reasonable starting point to study the emergence of grid cells?
1908,3," \n\nSignificance\n- The paper could compare against other relevant baselines that combine model-based and model-free RL methods, such as SVG (stochastic value gradient)."
1909,3, The proposed loss has been compared extensively against a number of closely related approaches in methodology.
1910,3,\nUsually the size of a covariate would be the dimensionality.
1911,3, As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed.
1912,3," Could you explain it in more details?\n\n"""
1913,3,nThe authors provided detailed and convincing answers to my questions.
1914,3,"\n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it\u2019s not clear from the text and experiments whether it actually was necessary."
1915,3,\n\nSignificance: The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail.
1916,3,"While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community."
1917,3,"""This paper proposes a computationally fast method to train neural networks with normalized weights."
1918,3,"\n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models."
1919,3,"""This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory). "
1920,2,"I am not aware of a published citation that justifies these calculations, but I have really not felt the need for such a citation given the straightforward computation"
1921,3,"""# Summary\nThis paper presents a new external-memory-based neural network (Neural Map) for handling partial observability in reinforcement learning."
1922,1,"\n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner."
1923,1," Intuitively, (1) is an easy result."
1924,3, This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner.
1925,3,"\n-\tPag7 there are two \u201cso that\u201d in 3.1; capital letter \u201cIt used 32x10^12..\u201d; beside, here, why do not report the difference in computation w.r.t. not-spiking nets?"
1926,3,"\n\nThe paper frequently refers to \""embedding\"" \""imaginary trajectories\"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me)."
1927,2,"There is no research methodology, no data, no model, no significant analysis and no conclusions which arise from the study"
1928,2,The authors have not bothered to learn the first thing about the theories they are hoping to refute with ill-designed experiments and muddled rationale.
1929,3, This happens a lot in Sec. 5.2.
1930,3," If it were novel, it would be an incremental development."
1931,3,"\n\n- I think the authors should make a claim on whether their proposed attack works only for defenses that are agnostic to the attack (such as PGD or region based), or for defenses that know this is a likely attack (see the following comment as well)."
1932,3,\n\n2. I'm not sure how much to trust the Fourier-spectra.
1933,3,"  SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum."
1934,3, 2) Uniform convergence of the empirical risk to population risk.
1935,3," This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models."
1936,3,"I didn\u2019t see any mention to those in the definition for I(x,t). You only mention initial conditions."
1937,3,"""The paper describes a technique to incorporate dialog acts into neural conversational agents."
1938,2,eview of a PhD project paper submitted one year after successful defense: Student should get a new PhD project
1939,3,"  In a settig of model verification, the explanation via heatmaps is exactly what one wants to have: If tiny change to the image causes lots of change to the prediction (and explanation) we can visualize the instability of the model not the explanation method."
1940,1,\n2. Strong experimental setup that analyses in details the proposed extensions.
1941,3, It will be interesting to compare with some existing second-order optimization algorithms for deep learning.
1942,3,After a revision I would consider to increase the score.
1943,3," With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference."
1944,3," Since L is NON Convex, it could not be automatically considered as bounded."
1945,3, 3) by avoiding introducing false claims based on a misunderstanding of terminology 
1946,3, It conducts an extensive assessment of the methods in the critiqued paper but does not introduce any alternative method.
1947,3, Is it due to the well designed mutation operations?
1948,1,\n\nPros:\n\u2022\tWell written and clear
1949,2,The first sentence is unfortunate
1950,3,\n\nThey demonstrate the usefulness of the algorithm against a DQN baseline on Doom game problems.
1951,3,  This would allow to disentangle the impact of the learning mechanism from the impact of the learning objective.
1952,3, Are the assumptions in Theorem 2 reasonable?
1953,3," Somewhat related to this point, it would be insightful to show the pattern of local connections learned in the RNN to see how it differs from the aforementioned pattern of connectivity"
1954,3, How about A and A2 here?
1955,3," Navigation is commonly defined as the goal driven control of an agent, following localization, and is a broad skill that involves the determination of position and direction, with or without a map of the environment (Fox 1998, \u201d Markov Localization: A Probabilistic Framework for Mobile Robot Localization and Navigation\u201d)."
1956,3, \n- For the binary setting you mentioned that you had to reduce the entropy thus added a \u201cbeta density regulariser\u201d. Did you add R(p) or log R(p) to the objective function?
1957,3,"  The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1], "
1958,3,\n(b) Searching in the latent space z could be strongly dependent on the matching inverter $I_\\gamma(.)$.
1959,3, how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%).
1960,3,"\n\nOriginality:\nIt would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer."
1961,3,"The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information."
1962,3, G2 takes all the duty  can still obtain a lower L_ld.
1963,3, It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these.
1964,3,\n\nOther comments:\n- Paper would benefit from writing improvements to make it read better.
1965,3," On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work!"
1966,3,"The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that)."
1967,3," Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?   \n\n\n"" "
1968,1,"  Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results."
1969,1," These scaling rules provide guidance on how to increase the batch size, which is desirable for increasing the parralelism of SGD training."
1970,3,"""The authors propose a new sampling based approach for inference in latent variable models."
1971,3,"\nICLR Workshops, 2016\n\n* It would be good to clarify that the paper is focusing on the image caption agreement task from Kuhnle and Copestake, as opposed to generic visual question answering."
1972,3," To clarify: I think the proposed method is genuinely novel, but a bit of context would help the reader understand which aspects are and which aspects aren\u2019t."
1973,3, The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.
1974,3," They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating."
1975,3," In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn\u2019t clear to me that this is the right baseline."
1976,3,\n\n1) Sec 2.2 introduces the C&W attack.
1977,3," Furthermore, the use of these replacement tables means that even when the noise is natural, it\u2019s still kind of artificial."
1978,3,"""The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one."
1979,1, The method is simple and shows to be extremely effective/accurate in practice.
1980,1," In particular, with respect to the highlighted points 1 and 2, point 1 has been thoroughly answered and the novelty with respect previous work is now clearly stated in the paper."
1981,1,"  The\nresulting simple prediction framework is then used for early stopping,\nin particular within the Hyperband hyperparameter search algorithm."
1982,3, Sensitivtiy-n seems to be an extension of the region perturbation idea to me.
1983,3,"There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable."
1984,3," \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance."
1985,3," If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication."
1986,3,n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\
1987,3,\n(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes.
1988,3,"Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix)."""
1989,3," Under the setup of FL, it is assumed that training takes place over edge-device like compute nodes that have access to subsets of data (potentially of different size), and each node can potentially be of different computational power."
1990,2,"Please respond to the Reviewer 2's comments, who suggested Rejection of the paper'. Reviewer 2 Comments to the authors: 'None."
1991,3," From a practical computational perspective, the algorithm will be implemented on a machine which processes on finite representations of data."
1992,2,"There are also other points that have to be addressed, but I do not think it is makes sense to go into further detail."
1993,2,"It is difficult to see the merits of this proposal, and it is doubtful whether the author can contribute anything to this area of research"
1994,1, I especially liked the general idea of using multiple modalities to improve embeddings of relational data.
1995,3,"\n\n(b) A locally positively curved model, in which there is a positively curved outer bound for the collection of points which are assigned a given label."
1996,3, \n\nI think the paper could also discuss a bit more in detail the results provided.
1997,3," The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards? "
1998,3," However, oftentimes a source of variation can be a variable that is hard or impossible to express in a simple vector z (for instance the sentiment of a scene) even when these factors are known."
1999,2,"I am concerned that the survey data for this report were collected in 2005, 15 years ago."
2000,3,"\n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function?"
2001,3,"\n(ii) Given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages."
2002,3, Experiments on MNIST using the proposed deep function machines (DFM) are provided.
2003,3," Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper."
2004,3,"\n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen."
2005,3, \n\n3. The final paragraph of the paper discusses extending the approach to architectures with skip-connections.
2006,3, If so they should clarify this otherwise it confuses the reader a bit.
2007,3," Also, it would be good to evaluate the AIR model from Eslami, 2016 on the same simple shapes dataset and show unconditional samples."
2008,3," To this end, slighly novel."
2009,3," The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations."
2010,3,"""my main concern is the relevance of this paper to ICLR."
2011,3, The invariance introduced here does not seem to be related to any real world phenomenon. 
2012,2,My major concern to accept this work-in-progress paper is that these findings are not super interesting to readers in my opinion.
2013,3, it is just hard to get excited by it.
2014,3,\nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that.
2015,3,"""In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle."
2016,3," Because this paper is not comparing to the state-of-the-art, no specification of the leaderboard version may confuse the other reviewers and readers. "
2017,3, How do the features learned at different layers compare to the ones of the original network trained for image classification?
2018,2,"Indeed, by the end of the paper, the reader is left with a feeling of so what now?"
2019,1," \n\nThe datasets used by the authors are balanced, so they artificially transform them into long-tailed,"
2020,1,"\n\nThe method involves a nice, albeit minor, trick, where the chi-squared distribution of the sum of the z_{i}^{2} has its dependence on the dimensionality removed"
2021,3,"The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).\n\nThe paper explores multiple loss functions and techniques to train these models."
2022,3,"  In Advances in Neural Information Processing Systems, pp. 3844\u20133852, 2016."
2023,3,"\n\nThe present paper extends the above work to include the learning of a Mahalanobis matrix, S, for each instance, in addition to learning its projection."
2024,3, The objective to be maximized is a lower bound to 1/alpha * the likelihood.
2025,3,"""This paper wants to probe the non-linear invariances learnt by CNNs. This is attempted by selecting a particular layer, and modelling the space of filters that result in activations that are indistinguishable from activations generated by the real filters (using a GAN). "
2026,1,Authors should show these results for SRM.
2027,3," More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z)."
2028,3, \n\nQuestions:\nHave you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study)
2029,3,"""This paper proposed an approach for detecting adversarial examples using saliency maps."
2030,3,"""The paper presents a method for improving the diversity of Generative Adversarial Network (GAN) by promoting the Gnet's weights to be as informative as possible."
2031,3,"\n\nIf I am not mistaken, the Vendrov WordNet test set is a set of positive pairs."
2032,1, This is an interesting property and it seems to be the major strength of TR over TT. 
2033,3,"  It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve."
2034,3,"\n\nIt would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error. "
2035,3, They propose directly optimizing the time-dependent discrimination index using a siamese survival network.
2036,2,This article reads like the work of a reasonably competent undergraduate.
2037,1,"  They observe better performance for their approach in comparison to ADDA, improved WGAN and MMD, when restricting the discriminators to be a linear classifier."
2038,3, I was not very sure as to why the proposed method is more general than existing approaches.
2039,3, More information on the experimental design would help.
2040,3,"  Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings."
2041,1," The paper is aiming to solve a practical problem, and has done some solid research work to validate that."
2042,3,"\n- cramer->Cramer\n- wasserstein->Wasserstein (2x)\n- gans-> GANs\n- Salimans et al. is provided twice, and the second is wrong anyway."
2043,3,"\n\n5. For each weight w, we add K learning rates u_w^j."
2044,3,"\nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD."
2045,3,"\n\nIn the experiments section, can you provide a citation for ADAM and explain how the parameters were selected?"
2046,3, It uses this attention as a\ncommon bottleneck for downstream predictors that select actions and answers to\nquestions.
2047,3,"  As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations)."
2048,3," The reason is that the gradient of proximal is evaluated at the future point, and different functions will have different future points."
2049,3,\n\n- Another aspect of the previous points is that it\u2019s not clear if stacking KRU layers will work well.
2050,3," The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings."
2051,2,I find the title and the main premise of the abstract confusing and illogical. [key concept X] has the logic of a Monty Python sketch
2052,3,\n\nI don\u2019t understand why the authors say the PATH function can be viewed as an inverse?
2053,3,"  Under such an approach, the unedited string receives probability one, which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive normalization.  "
2054,3,\n\n\nA few comments: \n\nI find that there are several ways the paper could make a stronger contribution:
2055,3,\n \nHow should one choose tau_theta?
2056,3," I did not check the proof of this result in detail, but it appears to be correct."
2057,3,"""The main contribution of this work is just a combination of LSH schemes and SGD updates."
2058,3, \n4.\tOne of the observations made in the paper is that \u201ctraining on one dataset and evaluating on the other results in a drop in the performance.
2059,1,\n\n- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations).
2060,3,\n10. I would have liked to see a plot of how the value of lambda changes throughout optimization.
2061,2,"Right now, there is zero rationale for the study and zero reason to read the study."
2062,3," In addition, it seems that all computations are done\ninto a \\ell^2 space instead of in the RKHS (equations 5 and 11)."
2063,3," In particular, in section 3, several sentences are taken as they are from the Downey et al.\u2019s paper."
2064,1," but it's also really valuable, because it's much more close to real world usage of language models."
2065,3, Is any qualification task used?
2066,3, The authors never mention mini-batch when Algorithm 1 is introduced.
2067,1,\n\nPros:\n-Interesting problem and interesting direction.
2068,1," Thus, despite the strong results,"
2069,1,\n\nSome of the results are quite entertaining indeed.
2070,1, The work is clearly presented and the evaluations seem convincing.
2071,3,"  It can be traced back to [3], used in PGM setting in [4] and used in VAE setting in [1]."
2072,2,is bisexuals referring to the entire sample or those who identify solely as bisexual?
2073,3,Operator learning has been already studied in FDA. See for e.g. the problem of functional regression with functional responses.
2074,3,"\n\n3. From the experiments, it is shown that the proposed model outperforms several baseline methods in both normal tasks and out-of-domain ones."
2075,3,"\nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \""at a large venue\"" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example. """
2076,3,"http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf"""
2077,3,\n\nThe authors present techniques that are of similar flavor to quantized+sparsified updates.
2078,3, \n\nThe authors argue that ASG is better than CTC in section 2.3.1 because it does not use the blank symbol and can be faster during decoding.
2079,1,". To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables."
2080,3,. The word-pair was generated by the Standford dependency parser
2081,1, I think this is a key experiment that should be run as this result would be much easier to compare with the other methods.
2082,3, How about t_j in equation (5)?
2083,3," It argues that because there should be an optimal noise scale (which maximizes test performance), the batch size should be taken proportional to the learning rate, as well as the training set size, and proportional to 1/(1-m)."
2084,1,"\n\nThe results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6)."
2085,3, The authors may want to consider the following comments:\n\n1. I did not really understand the analogy with STDP in neuroscience because it relies on the assumption that spiking of the post-synaptic neuron encodes the backpropagating error signal.
2086,3," \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table."
2087,3, Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms.
2088,3," I was not able to understand whether syntactic corrected is enforce by way of the reward introduced in section 4, or by way of the conditioning introduced in section 5.1. Discussion of the experimental results coould similarly be clearer."
2089,3," Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration."
2090,3, This model shows the rather strong performance of spelling models \u2013 at least on this task \u2013 which he again benefit from training in the context of the end objective.
2091,1,"it is clear how this capability can help practical applications, especially no such examples are shown in the paper."
2092,3,"  If not, why?\n\nWhen contrasting this work with existing approaches, can you explain how existing work builds toward the same solution that you are focusing on?"
2093,3,"\n* The growing body of work on deep kernel learning, which \u201ccombines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes\u201d. E.g.: (i) \u201cDeep Kernel Learning\u201d (AISTATS 2016);"
2094,3," So, the comparison should be to any other work that can deal with \""similar environment but different details\""."
2095,1," \n\nIn conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time."
2096,2,I really wanted to like this study
2097,1,"This is an interesting idea with linguistic validity, and practically possible because of the commonness and promiscuity of prepositions, reflecting their primary grammatical and relational roles (as function words not content words)."
2098,3, \n\nDoes selecting a projection based on compactness remove the randomness?
2099,1, The MNIST example is compelling.
2100,1," On the other hand, I think the ICLR community would benefit from about the opportunities to work on problems of this nature."
2101,3,"\n\nI don't understand why (1) differs from other approaches, in the sense that one cannot simply reduce the number of epochs without hurting performance."
2102,3,"""In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train."
2103,3," Works such as [2,3] have used this objective function for single step and multi-step frame prediction training, respectively."
2104,3,"\n\nFig. 5, bad readability of axes labels."
2105,2,"lthough the benefits of unprofessional reviews include gifs, laughts &amp; shared gasps of horror, this important new paper by  and  lays out the drawbacks:  In the interest of scholarly rigour we will quote from the 'highlights"
2106,3,\n\nWhat is the computational complexity of BBB with posterior sharpening?
2107,2,"Overally speaking, the manuscript is well written."
2108,3," However, the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges."
2109,3," AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S."
2110,3," \n\nA further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not."
2111,3,\n\n- Please add citations for Figures 1a-1b.
2112,3, This would cut down the text a bit to make space for more experiments.
2113,1,\nThis would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution.
2114,1,\npros:\n 1. The paper is well written and easy to follow.
2115,3, and less content on problem definition and formulation.
2116,3," The work can be made stronger by addressing some of the questions above such as what role is played by the neural architecture and whether the results hold up under evaluation on a larger dataset."""
2117,3," \nSignificance: medium-high\n\nReferences:\n[1] https://arxiv.org/pdf/1602.02867.pdf\n[2] https://arxiv.org/pdf/1612.08810.pdf\n[3] https://arxiv.org/pdf/1707.03497.pdf"""
2118,3, I would suggest the authors also run the experiments on CIFAR-10.
2119,3," The practicability of the method can be controversial, the number of attempts require to build the (meta-)training set of runs can be huge and lead to something that would be much more costful that letting the runs going on for more iterations."
2120,1,\n* The experimental evaluation appears to be sound.
2121,1," In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience."
2122,1,"""Quality: Although the research problem is an interesting direction"
2123,3, Some analysis on the conditions that under which the continuation equilibria e.g. cooperation in the social dilemma is expected to arise would also be beneficial.
2124,3,"  In that work a \""distractor sentence\"" is manually added to a passage to superficially, but not logically, support an incorrect answer."
2125,3, I am not aware of any evidence for this.
2126,3,. This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings.
2127,3," Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout."
2128,3,"""This paper provides a new method for learning representations of prepositions."
2129,3,"\n3. train a supervised classifier on the private data\nso that the GAN training-sampling loop basically functions as an anonymization procedure. For this to pan out, we'd need to see that the GAN samples are a) useful for a range of supervised tasks, and b) do not leak private information."
2130,2,This strikes me as the worst kind of postmodern legerdemain
2131,3,\n\nAnother question that I had is why use a L1 loss when in the evaluation you're using L2?
2132,3," For example, how far does using the expected context vector deviate from marginalizing the monotonic attention? "
2133,1,  It may inspire more useful research in the future.
2134,1," \n\n+ The authors perform ablation experiments, which are always nice to see."
2135,3, The reasoning here is that the image feature space may not be semantically organized so that we are not guaranteed that a small perturbation of an image vector will yield image vectors that correspond to semantically similar images (belonging to the same class).
2136,3," My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1]."
2137,3," \nIn particular, it is possible that the number of examples needed for achieving a given approximation is at least exponential."
2138,3,"""\nThis paper explores the use of simple models for predicting the final\nvalidation performance of a neural network, from intermediate values\nduring training."
2139,3," Thus, the author's response is still not convincing to me."
2140,3, \n\n3. Adding n^2 linear layers for image classification essentially makes the model much larger.
2141,2,You have two many misprints
2142,1, They extend the scheme to allow the use of different scaling parameters and to m-bit quantization.
2143,3," Does the learnt model performs well on other dataset (for instance,\nacquired on a different region or at a distant time). "
2144,3, Besides L_1 is not well defined.
2145,3,"\n\n> Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data"
2146,3," Additionally, standard RNNs (non-gated versions) have an ill-conditioned recurrent weight matrix, leading to vanishing/exploding gradients during training."
2147,3,"5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size? "
2148,2,You need to learn how to think inside the box and stop smoking whatever it is you're smoking
2149,2,"Thats not possible to do without mind-reading, and theres nothing in the Method section about mind-reading methods"
2150,3,"\n\nThe central claim of the paper, that a trust region method will be better at avoiding narrow basins, seems plausible, since if the trust region is sufficiently large then it will simply pass straight over them."
2151,1, The experimental setups are explained in detail.
2152,3,"""The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator."
2153,3," However, I have three main concerns:\n1) Presentation. The organization of the paper could be improved. "
2154,3,"  Heuristics for distributing connections among windows/groups and a measure called \""scatter\"" are introduced to construct the connectivity masks, and evaluated experimentally on CIFAR-10 and -100, MNIST and Morse code symbols."
2155,3," However, in this paper, the way it is added is simply by updating word representations based on this extra text."
2156,3, Is this update rule\nguaranteed to produce convergence of \\theta?
2157,3, There were a few things that jumped out to me that I was surprised by.
2158,2,Have you no command of the English language?
2159,3,"\n\nThough experiments, they show that there are two kinds of minima, depending on whether we allow negative initializations in the convolution kernels."
2160,3," Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?)"
2161,1,\n3) Validation on real-world software data
2162,2,"The fact that something has not been studies is not, in itself, a reason why it should be studied."
2163,3,"\n\nQuestions and comments:\n\u2013 While an 85% compression rate is significant, 88% accuracy on MNIST seems poor."
2164,3,"\n\n[3] Huang, Gao, et al. \""Deep networks with stochastic depth.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n"""
2165,1,\n\nExperiments are performed on the usual reference benchmarks for the task and show\nsensible improvements with respect to the state-of-the-art.
2166,1,\n\nI think the paper is well-written.
2167,3,"\n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way."
2168,3,\n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality.
2169,3,\n\nI would like to see the model ablation w.r.t. repetition avoidance trick by muting the second trigram at test time.
2170,3,"""The paper proposes a model for prediction under uncertainty where the separate out deterministic component prediction and uncertain component prediction."
2171,3," \n5. It will be interesting to see the impacts of physics based knowledge on choice of network architecture, hyper-parameters and other training considerations."
2172,1," Specifically in the PPD game, the use of CCC produces interesting results; when paired with cooperate agents in the PPD game, CCC-based players produce higher overall reward than pairing cooperative players (see Figure 2, (d) & (e))."
2173,3," The proposed approach is evaluated on few shot learning tasks, on omniglot and timit."
2174,1, A loss function based on the Wasserstein distance is used. \nThe paper is interesting
2175,2,The conclusions are still not consistent with the (lack of) findings.
2176,3,\n\nComments and questions:\n\n1) How computationally expensive is FTP?
2177,1,\n\n- The DSL is richer than that of previous related work like Balog et al. (2016).
2178,3,  It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader.
2179,3," The paper would be a much stronger contribution, if the experiments could be improved."
2180,3," Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution). "
2181,3,\n\nThe paper is moderately well written and structured.
2182,3,"\n\n- Given that the paper seeks to use uncertainty in estimates and the\n  entire regression setup could be trivially made Bayesian with no\n  significant computational cost over a kernelized SVM or OLS,\n  especially if you're doing LOOCV to estimate uncertainty in the\n  frequentist models."
2183,1, \n\nThe results presented in the paper are convincing
2184,3, Does the average iteration error behaves differently in case of a tanh-RNN ?
2185,3, The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting.
2186,3,"\n- In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not 0"
2187,1," Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model.\n"""
2188,3," From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder $f_\\beta(x)$ which gaussianizes the input (thus emulating the explicit transform) ?"
2189,3, It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.
2190,2,"Rev 1: The paper is generally well written (the English is good)
Rev 2: A proof reading by a mother tongue would improve readability."
2191,1,"\n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked."
2192,3,"  For instance, please show what is the actual contribution of the proposed reconstruction loss to the classification accuracy with the other losses existing or not?"
2193,3," Hence, I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper, for example. "
2194,3,n- figure 2: do these images correspond to each other?
2195,1,"""The paper presents an interesting spectral algorithm for multiscale hmm"
2196,3," Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?"
2197,2,This [sentence] construction should be reserved for police procedurals and bad Mafia movies.
2198,3," \n\nSecondly, many existing works on multiple passage reading comprehension (or open-domain QA as often named in the papers) found that dealing with sentence-level passages could result in better (or on par) results compared with working on the whole documents."
2199,1, \n\n\nPROS AND CONS\n\nImportant problem generally.
2200,3,\n\nStrengths:\n1. The authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolution. 
2201,3,"""The authors consider the metrics for evaluating disentangled representations."
2202,1,"\n\nWhile it is clearly written, my main concern is whether this model is significant enough."
2203,3,\n\nThe method is evaluated in one experiment with many different settings.
2204,3, Do you observe a similar performance vs FNNs in existing methods?
2205,3," \n\nIn the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions."
2206,3," You can, therefore, conclude that the authors are\ntuning the amount of exploration that they perform on each specific problem."
2207,3,"""This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width."
2208,2,"It is difficult to imagine any paper overtaking this one for lack of imagination, logic, or dataâ€”it is beyond redemption"
2209,3," however in order to put the \""clear accept\"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).\n\n"""
2210,1, Including videos of failure cases is also appreciated.
2211,3, The generator is trained with a WGAN optimization.
2212,3," I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct?"
2213,3,"\nAlso, \""K\"" is already used to denote the mini-batch size, so it's a slight abuse to reuse \""k\"" to denote the \""kth marginal\""."
2214,3,. Given that in many applications such parent-class supervised information is not available
2215,3, \n\n-BiCNet and CommNet are both aiming to learn communication protocols which allow decentralized execution.
2216,1, \n+ Informative analysis for clearly identifying the strengths of the proposed method and where it is failing to perform as expected.
2217,2,I have received one highly negative review and wondered whether it could be overcome by other reviews. I conclude that the answer is no
2218,3,The resulting log-polar image is analyzed by a conventional CNN.
2219,2,"When the reader is finished struggling through all the methods and results, he/she is left wondering whether it was worth the time."
2220,1,\n\n- The idealized case of quadratic function the analytical solution offers a good way to understand how T-step look ahead can benefit the meta-algorithm.
2221,3,"  Style transfer has two key components, the first is how well it is transferred to the target style; second is how well it preserves the original contents."
2222,3, and (b) symmetric tensor CP factorization of this tensor.
2223,3," \n3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations."
2224,1,\n\nQuality:\nI found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simple.
2225,2,"Despite the apparently impeccable arrangement of the essay, with headings and sub-headings, the progression of argument is not always transparent, often hindered by otiose wording. 

"
2226,3, The proposed method is a useful extension of existing methods but needs to evaluated more rigorously.
2227,2,default settings?? huh???
2228,3,"\n[6] Gomez, F., Schmidhuber, J., & Miikkulainen, R. (2006). Efficient nonlinear control through neuroevolution."
2229,2,"It is a bit strange for me that authors have used Python for statistical analysis instead of SPSS or MATLAB as usual. Please, explain"
2230,3, These priors are indirectly induced from the data - the example discussed is via an empirical diagonal covariance assumption for a multivariate Gaussian. 
2231,3, The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors.
2232,3,\n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).
2233,1," \n\nGenerally speaking, I like the overall idea, which, as far as I know, is a novel approach for dealing with discrete inputs."
2234,3," I have a few, mostly cosmetic, complaints but this can easily be addressed in a revision."
2235,3, Multi-layer representations are mostly interesting because each layer shares hidden basis functions.
2236,3, In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant). 
2237,1,\n\n\nConclusion:\nI think the paper presents an interesting idea which should be exposed to the community.
2238,2,"You aimed for the bare minimum, and missed!"
2239,1," Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice."
2240,3,"""Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes."
2241,3," but I am not sure that the contribution is significant enough for acceptance.\n"""
2242,2,The work that this group does is a disgrace to science
2243,3,"""This paper shows a simple method for predicting the performance that neural networks will achieve with a given architecture, hyperparameters, and based on an initial part of the learning curve."
2244,3,  It would be good if it were made clearer which part of the mathematical analysis in sections 2 and 5 are original.
2245,3,"""The authors propose an approach for zero-shot visual learning. "
2246,1," On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further."""
2247,3,"""This paper proposes a new approach for multi-task learning."
2248,3,"""The authors proposed a graph neural network based architecture for learning generative models of graphs."
2249,3," Or is it joint learning and you learn all LSTMs and CNNs yourself? (Besides the reuse of VGG, I could not find this information explicitly stated within the paper.)."
2250,3,"Indeed, there is a long history of motion planning research that specifically addresses the problem of planning in the face of dynamic obstacles, as well as work that plans using predictive models of vehicle behavior (e.g., see the work by Jon How's group)."
2251,3,\nScheme A consists of training a high precision teacher jointly with a low precision student.
2252,3,"\n\nAmong these results (1), (2), (4) are sort of known in the literature."
2253,3," Instead of GMMs and dictionary learning in PCL,  MagNet trains autoencoders for defense and provides sufficient experiments to claim its defense capability."
2254,3, I think it might be of interest to some audiences in ICLR.
2255,3, To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units).
2256,3,"Where the Hazan paper concerns itself with the system id portion of the control problem, this paper seems to be the controls extension of that same approach."
2257,1,\n\n+ The numerical experiments are encouraging
2258,1,\n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice
2259,1,\n\nThe paper is well written and provides some new insights on incorporating kernels in CNN.
2260,3, My comment is mainly about its importance for large-scale computer vision applications.
2261,3,\n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function.
2262,3,\n\nThe main contribution appears to be the optimisation procedure for the binary masking tensor g.
2263,3,"\n- Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST."
2264,3,"\nThey also examine random masking, eg a sparsification of the updates, that retains a random subset of the entries of the gradient update (eg by zero-ing out a random subset of elements)."
2265,3,"""The manuscript introduces the sensor transformation attention networks, a generic neural architecture able to learn the attention that must be payed to different input channels (sensors) depending on the relative quality of each sensor with respect to the others."
2266,3, It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks.
2267,1, the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable. 
2268,1,\n- The paper is clearly written and key contributions are well present.
2269,3," In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins)."
2270,3,"\n\n3. More importantly, the paper tested only on small size datasets (UCI), where the inputs are relatively well-structured."
2271,3," They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data."
2272,3,"\n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs."
2273,3,"  \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent."
2274,3," For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from \""Attention is All You Need\"")."
2275,3,\n- Figures 3 and 4 illustrate some oscillations of the proposed approach.
2276,3, Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.
2277,3, First learns a deterministic mapping from x to y.phases.
2278,1," Importantly, the model achieves state-of-the-art performance of the SQuAD dataset."
2279,3,\n  Is beam search performed in the case of sequences?
2280,3, This technique generalizes existing work under full relaxation of assumptions.
2281,3,"  However, once the transition scores are introduced in ASG, the search space becomes quadratic in the number of characters, while CTC is still linear in the number characters."
2282,3,"""(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems."
2283,3," The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results."
2284,1," In this vein, the experiments with DP-\u000fSGD are more interesting,"
2285,3," \n\nOne question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?"""
2286,3, \n\n- This work also assumes that other cars in the vicinity can be simply observed without any perception uncertainty or even through occlusions.
2287,1," It seems the novelty here is mainly the empirical exploration (will return to this later), and numerical integration for various activation functions."
2288,1," My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \""only\"" 6 billion words \u2013 2 orders of magnitude less data."
2289,3,\nIs that correct?
2290,3,". How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?"""
2291,3, They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements.
2292,3," It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix."
2293,3, The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty.
2294,3, Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)?
2295,3, The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output.
2296,1," This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable."""
2297,3," The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015)."
2298,3,"\n\nComments: I have my concerns about the effectiveness of the notion of privacy introduced in this paper.[[CNT], [null], [SMY], [GEN]] The definition of privacy loss in Equation 5 is an average notion, where the averaging is performed over all the sensitive training data samples."
2299,3,\n(b) Using a novel in-filling procedure to overcome the complexities in GAN training.
2300,3,"\n\nOverall, the paper describes a computer visions system based on synthesizing images, and not necessarily a new theoretical framework to compete with GANs."
2301,3,\n\nThis is a short paper that contains five pages.
2302,3, \n? p.4: The word embeddings for the CNN are pre-trained word2vec/Glove/xyz embeddings?
2303,3, Can you give more details or refer to where in the paper it is discussed/tested?
2304,3,\n\nComments:\n1. I recommend the authors to tone down their claims.
2305,3," Here, the generalization error is measured instead, which is heavily influenced by regularization."
2306,3,\n\nGTI uses the Louvain hierarchical community detection method to identify the hierarchy in the graph and METIS to partition the communities.
2307,3," Basically, these all seem like interesting and important empirical questions that would be nice to explore in a bit more detail."
2308,1," The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability. "
2309,1,"\n\nSoundness: As far as I can tell, the work is sound."
2310,3, More advanced attacks need to be considered.
2311,1,"\It shows that in this very simple setting, the \""evidence\"" of a model correlates well with the test accuracy, and thus could explain this phenomena (evidence is low for model trained on random data, but high for model trained on real data)."
2312,1,n\n- Originality:\nLooking at RNN from an iterative refinement point of view seems novel.
2313,1," \n\nThe paper is well written, clear, organized and easy to follow."
2314,3,\n\n3. Why do you motivate the learning method using self-play?
2315,1, The proposed model combines some of the strengths of factorization machines and of polynomial regression.
2316,2,There are so many things wrong with this manuscript that I do not know where to begin
2317,3, Is there good reason to think RL agents will need to contend with time-limited domains in the future?
2318,1,\n\n\n\nStrength:\n- The experimental results on the simple skip RNNs have shown a good improvement over the previous results.
2319,1," \n- Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization"
2320,3,"""This paper presents a model to encode and decode trees in distributed representations."
2321,3, The method is also appealing for its use of some kind of emergence between two levels of hierarchy.
2322,2,"The data as presented is not very convincing, even to a believer."
2323,3," In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD."
2324,3,"""The paper proposes a method for generating images from attributes."
2325,3,\n 2. Manifold analysis of the intrinsic structure of DNN is a important direction for further study.
2326,3, I thank them for that.
2327,3,  Would it make more sense to show that on large RNNs with thousands of hidden units?
2328,3,  The authors propose a first modeling called ASC.
2329,3," I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained."
2330,3,. The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice. 
2331,1,"\n\nThe paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing."
2332,1, The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well. 
2333,3,\n\n4. How and which hyper-parameters were optimized?
2334,3," GCN looks at the nearest neighbors and the paper\nshows that using also the 2-ring improves performance.\n"""
2335,1,"\n- The plots in Section 6 are interesting, it would be nice if the authors had an explanation of why the loss surface changes the way it does when skip connections are added."
2336,3, The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1.
2337,1,\n\nThe experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms.
2338,1," \n\nThe paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers."
2339,2,This is depressing. So much work with so little science
2340,3,"\n\nIn fact, this is already appearent both from the model architectures and the\ngenerated examples: because the model aims to fill-in blanks from the text\naround (up to that time), generated texts are generally locally valid but not\nalways valid globally. This issue is also pointed out by authors in Appendix\nA.2."
2341,3,. The idea is to introduce the notion of pseudo labelling.
2342,3,  It should help make it clearer in the paper as well.
2343,3,"\n\nStrengths:\n- The complementary kernels come at no loss compare to standard ones\n- The resulting wider networks can achieve better accuracies than the original ones\n\nWeaknesses:\n- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions\n- The improvement over the baseline is not very impressive\n- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)\n\nDetailed comments:\n- The separation into + and x patterns is quite clear for 3x3 kernels."
2344,3," To provide an analysis of why it works and quantitative results, is part of the same contribution I would say."""
2345,3,"""This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU)."
2346,3,"""This paper proposed to use affect lexica to improve word embeddings."
2347,1,"\n\nThrough a set of experiments, the paper shows the effectiveness of the method."
2348,3, The authors must nonetheless have some intuition about this.
2349,1," Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems."
2350,1,The approach is thoroughly explained for a large audience.
2351,3," Description of the model architecture is largely done in the appendix, this puts the focus of the paper on the experimental section."
2352,3,\n\nI disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA.
2353,3, The proposed approach uses a syntax-checker to limit the next-token distribution to syntactically-valid tokens.
2354,3,"\n\n5. Figure 4: For the models in Figure 4, it would be useful to indicate the starting CER of the stage-1 model before stage-2 training to get a sense of how stage-2 training impacts performance."
2355,1,"  Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well. "
2356,3," Specifically, the authors propose to extend deep neural networks to the case where hidden layers can be infinite-dimensional."
2357,3," The choice of using noise-free data only is a limiting constraint (in [Chen et al. 2016], Info-GAN is applied to real-world data)."
2358,3,"\n\nRebuttal Response: I am still not confident about the significance of contribution 1, so keeping the score the same."""
2359,2,"Not now, not ever. - Review of an NSF grant submission. This was the entire review"
2360,3," Unless these two are done, we cannot assertively say that the proposal seems to do interesting things."
2361,3, The authors discuss Pareto optimality without ever formally describing what multi-objective function defines this supposed Pareto boundary.
2362,3," Therefore, SRM has huge preparing training dataset time complexity that is not mentioned in the paper."
2363,3,\n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ())
2364,1,. There are a number of interesting and intriguing side-notes and pieces of future work mentioned.
2365,1, The main contribution of the paper is the introduce such techniques to the ML community and presents experimental results for support.
2366,3,"""This paper applies several NN architectures to classify url\u2019s between benign and malware related URLs."
2367,3,"\np8. Figure 7. How did you decide which data points to include in the plots?"""
2368,3,"\"" arXiv preprint arXiv:1707.05589 (2017)."
2369,3, The paper is a experimental paper as it has more content on the experimentation
2370,3,"""In this paper, the authors propose a method of compressing network by means of weight ternarization."
2371,3,"\n- Re. the formulation in Thm 2: is it clear that there is a unique global optimum (my intuition would say there could be several), thus: better write \""_a_ global minimum\""?"
2372,3,\n\nI went over the math.
2373,1," \n\nI find the problem of defogging quite interesting, even though it is a bit too Starcraft-specific some findings could perhaps be translated to other partially observed environments."
2374,2,This inclusion criteria is not needed because to be in a master program they would be legal adults. Please remove and adjust everywhere
2375,1,\n\nPros: The paper is proposing a simple yet effective method to predict accuracy.
2376,1, Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.
2377,2,"Given the pedigree of the senior author, I would have expected better"
2378,1,"\n\n\n4. Low level technical\n\n- The end of Section 2 has an extra 'p' character[[CNT], [CLA-NEG], [DFT], [MIN]]\n- Section 3.1: \""Here, X and y define a set of samples and ideal output distributions we use for training\"" this sentence is a bit confusing.[[CNT], [CLA-NEG], [CRT], [MIN]] Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3.[[CNT], [CLA-NEG], [CRT], [MIN]]\n- Section 3.1: \""W is the learnt model...\\hat{W} is the final, trained model\"" This is unclear: W and \\hat{W} seem to describe the same thing.[[CNT], [EMP-NEG], [CRT], [MIN]] I would just remove \""is the learnt model and\""[[CNT], [CLA-NEG], [SUG], [MIN]]\n\n\n5. Review summary\n\nWhile the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read."
2379,3, This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension.
2380,2,These snotty kids in quantum information
2381,3, \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained?
2382,3," The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed."
2383,3," Similarly for CIFAR, where only up to depth 3 is used."
2384,3,"\n\nAnyway, some other related work:\nLample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes."
2385,3," The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks."
2386,3, The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it.
2387,2,"The authors spelling of coordinate, while technically correct, is arcane and annoying."
2388,3,"\nIs it possible to perform the CP decomposition by minimizing the activation reconstruction loss (like proposed by Zhang et al. 2016), and not the tensor reconstruction loss (as usual)?"
2389,3," However, the authors show this per-step evaluation in the Amazon Mechanical Turk, and predicted object position evaluations."
2390,1," Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea."
2391,3,"\n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features."
2392,3, Also some curves in the appendix stop abruptly without visible explosions.
2393,3,  Your method seems to be connected with Deep GPs although there appear to be important differences as well. E.g. you place GPs on the scalar activation functions in an otherwise  heavily parametrized neural network (having interconnection weights between layers) while deep GPs model the full hidden layer mapping as a single GP (which does not require interconnection weights).
2394,3,"""This paper proposes a method for parameter space noise in exploration."
2395,3, I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch?
2396,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
2397,1,  Experimental results are decent \u2014 there are clear speedups to be had based on the authors' experiments.
2398,3,"""This work exploits the causality principle to quantify how the weights of successive layers adapt to each other."
2399,3," However, such contributions are quite minor, and technically heuristic."
2400,3,"3) when exact matches exist, simpler methods may be sufficient, such as matching edges."
2401,3,\n-- the wealth of literature on combining autoencoders (or autoencoder-like\nstructures such as ALI/BiGAN) and GANs merits at least passing mention.
2402,2,"Most of the work is methodological rather than scientific, making it somewhat boring."
2403,3,"""# Summary\nThis paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state."
2404,3,.\nPreliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU
2405,1, Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled.
2406,3,"\n2. Does the result implies that we should make the decision boundary more flat, or curved but on different directions? And how to achieve that?"
2407,3,"\n\n\""Both deep learning and FHE are relatively recent paradigms\"". Deep learning is certainly not recent, while Gentry's paper is now 7 years old."
2408,3," It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function. "
2409,3,\n\n2) High spatial resolution images. 
2410,1,\n\nI believe this is a updated version of https://arxiv.org/abs/1705.07393 (Recurrent Additive Networks) with stronger experimental results.
2411,3," Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?"
2412,3,"""This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations."
2413,3, This network is trained through supervised learning for the output and reinforcement learning for discrete operations.
2414,3,"\n\n# Minor\n\nmissing . after \""hypergradient exactly\"".[[CNT], [CLA-NEG], [DFT], [MIN]]\n\n\""we could optimization the hyperparam-\"" (typo).[[CNT], [CLA-NEG], [DFT], [MIN]]\n\nReferences:\n Justin  Domke.    Generic  methods  for  optimization-based modeling.  In\nInternational Conference on Artificial Intelligence and Statistics, 2012."
2415,1, These works can benefit from the uncertainty analysis scheme introduced in this paper.
2416,3," In fact, I suspect that the vanishing of the contribution of lower blocks bears more responsibility for the declining performance of VAN at higher depths than the removal of skip connections."
2417,3,"\n\nSome other smaller points:\n- \""zero-shot skill composition\"" sounds a lot like what used to be called \""planning\"" or \""reasoning"
2418,3,"  Can you provide visualizations of the communities of the interpolated graphs in Fig 7? """
2419,3," The underlying technique that is used to operate on the irregular graph is spectral decomposition, which enables convolutions in the spectral domain."
2420,2,Your model is a black hole from which no light escapes
2421,3,\n\n4. I'm not sure whether it makes good sense to apply an SVD decomposition to the \\hat{z} vectors.
2422,1,\n- The paper is mostly clear and well written.
2423,3,"\""\nThis claim is highly biased by who is giving the \""intuitive sense\""."
2424,1," The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN."
2425,3,"""The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position."
2426,3,"\nIs that also being done for the baseline DDQN, for example, by tuning epsilon in\neach problem?"
2427,3, Might this also suffer from non-convergence issues like you argue SVAE does?
2428,3,"By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT."
2429,3, \n\n3) I have some questions on the details.
2430,1," So I increased my score. \n"""
2431,3, ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9)
2432,3,"""This paper introduces a comparison between several approaches for evaluating GANs."
2433,3," First, the paper proposes a dual algorithm to estimate Kantorovich plans, i.e. a coupling between two input distributions minimizing a given cost function, using dual functions parameterized as neural networks."
2434,1," \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible."
2435,3,"\n  \u2022 [p4, Delayed rewards] It might be interesting to have a delay sampled from a distribution with some known mean."
2436,3,"""Quality\n\nThis paper demonstrates that human category representations can be inferred by sampling deep feature spaces."
2437,3," It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function)."
2438,3," \n\nThe dual problem of regularized logistic regression is an entropy-regularized concave quadratic objective problem where the Hessian is y_i y_j <x_i, x_j>, thus highlighting the pairwise similarities between the points x_i & x_j; here the labels represent whether the point x comes from the samples A from the target distribution or B from the proposal distribution."
2439,3,"\n\nORIGINAL REVIEW: In this paper, the authors present a new training strategy, VAN, for training very deep feed-forward networks without skip connections (henceforth called VDFFNWSC) by introducing skip connections early in training and then gradually removing them."
2440,2,This research is not worth a hill of beans
2441,3,\n  The last task is real world -- 40 attribute classification on the CelebA face dataset of 200K\n  images.
2442,1,\n\n\nPaper Strengths:\n- The proposed technique seems simple yet effective for multi-task learning.
2443,1,"\n- Uses same hyperparameters as original training, making the process of using this much simpler."
2444,3,  Is it because the method can be understood as some form of block-coordinate Newton with momentum?
2445,2,As a service to the authors I have decided to try to convey a sense of the extreme nature of the problems encumbering this submission
2446,3," If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication."
2447,3," There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting."
2448,3," \n\n- Second, it appears that the proposed methods may rely on running the dynamical system several times before attempting to control it."
2449,2,None of the following comments on the original manuscript has been correctly reflected or answered [1st round review
2450,3, \n\nThis is a significant and timely topic.
2451,3,"  Gated Graph Neural Networks (GGNNs, trained by maximum likelihood objective) are then run for 8 iterations at test time."
2452,3," \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n"""
2453,1,"\n\nOverall, the paper is well-written."
2454,3, They describe their training procedure and their sampling approach for the gate weights.
2455,3," Experimental evidence of the relevance of the method is provided on a toy task involving a NIW prior of Gaussians, and the (benchmark) MiniImageNet task."
2456,3," In the used model, there are two types of latent features, \""core\"" features and \""style\"" features, and the goal is to achieved by avoiding using the changing style features."
2457,3, Goyal et al (2017) show how\nto train deep models on ImageNet effectively with large (but fixed) batch\nsizes by using a linear scaling rule.
2458,3,"\n\n\nReferences:\n[1] Chelsea Finn, Ian Goodfellow, and Sergey Levine."
2459,3,"\n\n    Loss = E[    R + MAXa\u2019 Qp(S\u2019,a\u2019)    -   Qp(S,a)   ]  \n\nThe authors suggest we can augment the environment reward R \nwith a heuristic reward Rh proportional to the similarity between \nthe learner \u201csubgoal\"" and the expert \u201csubgoal\"" in similar states."
2460,3," It is a still kind of soft-attention, except that is performed for each word in a sentence."
2461,3,"""The paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consistent."
2462,1,"  In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach."
2463,1," I think the idea is intuitive and reasonable, the result is nice."
2464,1,n\n\nPros:\n\n\nThe paper is clear and the proposed problem is novel and well-defined.
2465,3,Is such improvement really important?
2466,3,\n- a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFs
2467,3,c. There are several (existing) ideas of how to combine node representations into a representation for the entire graph
2468,1,\n\nReview:\n\nThe paper is clearly written.
2469,3, I would like to see the results on more datasets.
2470,3, Doesn't this also prevent optimal results ?
2471,1,"  Moderate novelty,"
2472,1,\u201d \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework.
2473,3,"The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other \""governing\"" sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc."
2474,3," In contrast, it is less effective to protect against singular perturbations."
2475,3," For every neuron out of two, authors propose to preserve the negative inputs."
2476,3, But what exactly should the distribution be like to be learnable and how to quantify such \u201crelated\u201d or \u201csimilar\u201d relationship across tasks? 
2477,3," I can't however, think of other baselines other than \""ignore\"" so I guess that is acceptable."
2478,3,"""In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations."
2479,2,Add statistical support! - Review of NIH grant with biostatistician as a co-investigator
2480,3, And less tuning is needed for these larger datasets.
2481,3,". Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency"
2482,3,\n\n** original review **\n\n\nThe paper presents a way to regularize a sequence generator by making the hidden states also predict the hidden states of an RNN working backward.
2483,2,The original study was published in PsycScience. This [failed replication] is just some work by a grad student. Reject.
2484,1," I think the paper is borderline, leaning towards accept."
2485,3, I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts.
2486,3, But I still wonder the effect of permuting the layers.
2487,3, Correctly modeling the future is very important for tasks that require making decisions of what to do in the future based on what we predict from the past.
2488,1,". I therefore recommend that the paper be accepted.\n\n"""
2489,3," \n[4] Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies."
2490,3," I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful."
2491,3," Would policies generalize to later tasks better with larger, or smaller networks?"
2492,3,"""Thank you for the submission. "
2493,3,"  One non-language task (e.g., some RL agent with an LSTM, or learning to execute or something) and one synthetic task (copying or something) might be sensible."
2494,3," Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary."
2495,3," The method is shown to be superior in tasks of 3-way outlier detection, supervised analogy recovery, and sentiment analysis."
2496,1," While these results are enlightening,"
2497,3, What is the graph \nin the time series or among the multiple time series?
2498,1, \n\nMajor comments:\n\nNo major flaws.
2499,3,\n2. What is the intuition in adding target cluster entropy in Eq. 3?
2500,3, The new construction builds upon graph polynomial filters and utilizes them on each successive layer of the neural network with ReLU activation functions.
2501,3, It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works.
2502,3," \n\nAlthough it is interesting, the above case case also calls into question the additional value brought by the use of constrained optimization, a main contribution of the paper."
2503,3, The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN.
2504,3," \n6. In some of your experiments, every training method converges to the same accuracy after enough training (Fig.2b), while in others, not quite (Fig. 2a and 2c)."
2505,1, All proofs are provided and easy to follow.
2506,3, Does it lead to better stability to choose one or the other?
2507,2,"presumptuous, ignorant and downright dangerous"
2508,3," The loss ratios depend on initial loss, which is not important for the final performance of the system."
2509,3,"""The authors proposed a generative model of random walks on graphs."
2510,3," \n\nOTHER COMMENTS:\n- p3: both images in Figure 1 are labeled Figure 1.a\n- p3: typo \""theis\"" --> \""this\"" \n\nAbe & Mamitsuksa (ICML-1998)"
2511,3,"  \n\n4. In section 4.3, the authors claimed that numerical diffentiation only hurts 1 percent error for second derivative."
2512,3, The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task.
2513,3, Is this important to achieving better results as well or is the guidance reward enough? 
2514,1, since it sometimes improves performance it appears to have some regularizing effect on the model in addition to controlling the trade-off between speed and accuracy.
2515,3,  How sensitive is the network to errors in this model?
2516,3," \n\nThis paper then investigates the behavior of this adversarial logistic distance in the context of aligning distributions for domain adaptation, with experiments on a visual adaptation task."
2517,3, This makes a lot of the questions asked in this paper extremely relevant to the field.
2518,3," If I were a medical expert, I would not have a clue about how these results and models could be applied in practice, or about what medical insight I could achieve."
2519,3," This is pervasive throughout the paper, and is detrimental to the reader\u2019s understanding."
2520,3, This would explain the massive gains in Yago.
2521,1,---------------\n\nI want to love this paper.
2522,3,"""The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g. simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output."
2523,3," While the algorithm does not have a theory for general non-quadratic functions,"
2524,3," However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck?"
2525,3,\n- Knapsack problem has been added
2526,3, It would be nice to see how the behavior and boundaries look in these cases. 
2527,3,"\n- 8.1. contains copy-paste from the main text.[[CNT], [null], [DIS], [MIN]]\n- \""proposition from Goodfellow\"" -> please be more precise\n- What is Fig 8 used for?"
2528,3,"\nHence, several policies which are better than random are likely to be required for sampling this data, in general. "
2529,3,\n\nI am also concerned about the hyper-parameter tuning for the baselines.
2530,1,"""This is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural networks."
2531,3," In fact, the limit of training with a fixed dataset is that the model \u2018sees\u2019 the data multiple times across epochs with the risk of memorizing"
2532,3, There is reasonable novelty in the proposed method compared to the existing literature.
2533,1,\n2) The inclusion of ablation studies to strengthen the analysis of the proposed technique
2534,3," For instance, is designing alternative loss function useful in practice? "
2535,3," Additionally, this paper proposes a new quantitative evaluation criteria based on the observed flow in the prediction in comparison to the groundtruth."
2536,3, The encoder is then used as a feature extractor for classification and regression of targets (e.g. heard disease).
2537,1,\n- simple merge or RL and SL in an end-to-end trainable model\n- improvements over previous solutions
2538,3, \n\nDetailed Comments:\n(*) Pros\n-The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST.
2539,3,"  As far as I know, symmetric LDS models are not common in the controls community."
2540,3,"""The authors present confidence-based autodidactic returns, a Deep learning RL method to adjust the weights of an eligibility vector in TD(lambda)-like value estimation to favour more stable estimates of the state."
2541,3, The core idea is to perform a random projection of the data (which is supposed to decrease the impact from adversarial distortions). 
2542,3, CTC is a sequence training criterion
2543,3," In the latter case, a stronger parameter is applied, followed by reduced regularization parameter."
2544,3," If yes, can you please report the error bars."
2545,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
2546,3," In my opinion, this is one of the crucial contributions of this paper."
2547,3,  The authors analyzed the the generalization for the following scenarios\n\n- the generalization ability of RNNs on random subset of SCAN commands
2548,3," From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information."
2549,3," It is because the toy task author demonstrates is actually quite similar to copy tasks, that previous state should be remembered."
2550,3, They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction.
2551,3,"Even under the assumption that a normalized edit probability is not particularly critical (an issue that was not raised at all in the paper, let alone assessed), the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural language."
2552,3," Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures?"
2553,3," This is described in Veit et al, 2016."
2554,3,"  The first sentence on page 6 gives this intuition; this should come much earlier.[[CNT], [PNF-NEG], [CRT], [MIN]] \n\nPage 5: \u201ca feed-forward auto-encoder with N input neurons\u2026\u201d Previously, N was defined as the size of the input domain."
2555,3," \n\n*** Update after author response ***\n\nThanks to the authors for their responses. My score is unchanged."""
2556,3," \n\nTo analyze this specific property, the authors propose a concept called \""start-end rank\"" that essentially models the richness of the dependency between two disjoint subsets of inputs"
2557,2,This paper is written.
2558,3," None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject."
2559,3," On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation."
2560,1," The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \""style\"" and \""content\"", is an interesting and long-standing problem."
2561,3, This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier.
2562,2,The writer of the manuscript is utterly ridiculous and appears to believe they will solve poverty through radio astronomy
2563,3,"\n\n-\tAuthors reason about biological inspired approaches, using Attention and Memory, based on existing literature."
2564,1," \n\nSo while the performance of the overall system is impressive,"
2565,3,"\n- In section 2, \""This [pre-training or co-training with maximum likelihood]\n  precludes there being a latent encoding of the sentence."
2566,3," In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets."
2567,1,\n- The methods are not well motivated. 
2568,3,It would also be interesting to understand more fully how performance scales to larger networks.
2569,2,The rest of the Introduction is just as badly done as the first paragraph so I will not continue
2570,1, The presentation is clear 
2571,3," Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training)."
2572,3," Also, since Genevay et al. propose using SAG for their algorithm, it seems strange to use plain SGD; how would the results compare if you used SAG (or SAGA/etc) for both algorithms?"
2573,3,\n* Is the assumption that \\sigma has Taylor expansion to order d tight?
2574,3,"\n\nQ: The spatial grouping that is happening in the compositional stage, is it solely due to the multi-scale hypercolumns?"
2575,3, I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1.
2576,3,\n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).
2577,3," For FGSM, the iterative version proposed by (Kurakin, ICLR 2017) should be used."
2578,3,  \nIn addition a relaxation is performed allowing each constituent to deviate a bit from unitarity (\u201csoft unitary constraint\u201d).
2579,3,"\nAdditionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on?"
2580,3, How can such a limited technique be generalizing?
2581,1,\n\n2) The theoretical results justify the optimization procedures presented in\nsection 5.
2582,3," I think you simply mean the gradient of the value function, for the given s_t, but its not clear. "
2583,1," The research itself seems fine,"
2584,3,\n\nSignificance: Moderate.
2585,2,"Your piece is very well written and researched. I think the is interesting, but not particularly..."
2586,3, The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties.
2587,3, \n\n2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN?
2588,3," Furthermore, simulation and real data examples to explore the properties and utility of the method are required. "
2589,3," As an additional (somewhat unimportant) point, the TD-error is usually defined as the negative of what you have."
2590,3," Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81?"
2591,3, A section devoted to showing what practical problems could be potentially solved by this method would be useful.
2592,3,\n- Can't see why SEARNN can help with the vanishing gradient problem.
2593,3,"\n\n5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015), \""n\"" abruptly appears without proper introduction / context."
2594,3," Excluding a class from the\nsoftmax is equivalent to having a logit of -\\infty, not zero."
2595,3,\n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy.
2596,1," Overall,  the study is interesting and contains some new idea."
2597,3," For example, the policy gradient in (8), which is further decomposed in Appendix A.1 as (15) and (16) and simplified, appears to be Eq. (50) of [Schulman et al., 2017] (https://arxiv.org/abs/1704.06440)."
2598,3, What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win?
2599,3," Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO."
2600,3," \n\nA non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.\n"""
2601,3," If the paper were substantially cleaned-up, I would be willing to increase my rating. """
2602,3, These are just two complementary lines of work.
2603,3,"\n\nAll of this is somewhat straightforward; a penalty is paid by representing numbers via fixed point arithmetic, which is used to deal with ReLU mostly."
2604,3,  Please refer to this paper \u201cInteractive Attention Networks for Aspect-Level Sentiment Classification\u201d.
2605,3,"  In Bayesian Dropout, there is an explicit variational objective."
2606,3, The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-
2607,3," In this setting, they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names. "
2608,3," \n- The authors implicitly contradict the argument of Theis et al against monolithic evaluation metrics for generative models, but this is not strongly supported."
2609,3, The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.
2610,3,"If the noise vector is perturbed and a new plausible filter set is created, the input data can be optimised to find the input that produces the same set of activations."
2611,3,"\n\nMajor comments\n-------------------------\nI was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature? "
2612,3," Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss."
2613,3, The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).
2614,1,"  \n\nWhile the idea of integrating the spelling error probability into the search for completions is a sound one, the specific details of the model being pursued feel very ad hoc, which diminishes the ultimate impact of these results."
2615,3,  Region embeddings are then composed (summed) and fed through a standard model.
2616,3,\n1. Benchmarking Deep Reinforcement Learning for Continuous Control\n2. Deep Reinforcement Learning that Matters
2617,3," \n\nAlso, the authors should provide more detailed description regarding the scheduling of the alpha and lambda values during training, and how sensitive it is to the final clustering performance."
2618,2,"I have read this paper several times through, and I have nothing to say in its defense."
2619,3,"""This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs)."
2620,3," There are two such baselines: random fixed weightings of the n-step returns, and persisting with the usual weighting but changing lambda on each time step (either randomly or according to some decay schedule)."
2621,3," If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable? "
2622,3, At which layer do the features become speaker invariant?
2623,2,This reads like a pretty good MA level seminar paper but comes nowhere near the intellectual status required for publication in journal X
2624,1, This idea is novel and interesting.
2625,3,"  \n- Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others)."
2626,3, It seems to rely on some mixing RW conditions to model the distinct graph communities.
2627,2,This paper does not contain information that could make a scientific proposal.
2628,3,"""This paper presents an embedding algorithm for text corpora that allows known\ncovariates, e.g. author information, to modify a shared embedding to take context\ninto account."
2629,3," From the context, I guess the authors mean \""empirical training distribution\""?"
2630,3,"""This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences."
2631,3," It might help to emphasize the speed-up in compute more in the contributions.  """
2632,3,\nEq. (23) uses Q(F|A) to mean the same as P(F|A) as far as I understand. Then why use Q?
2633,2,here is no point in [..] the statistical overkill in the submission. It simply should not have happened. It should never happen again
2634,3," This model stands in contrast to theories of basic emotions, which posit that a discrete and independent neural system subserves every emotion."
2635,3," For certain transfer functions, the authors perform a numerical integration to compute the resulting kernels."
2636,3, Is there a real reason for that?
2637,1, This is surely an important step towards more general deep learning models.
2638,1,"\n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain."
2639,3,\n4. I would like to see more explanation for the figure in Appendix A.
2640,3," Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method."
2641,3," For each output token, they aggregate scores for all positions that the output token can be copied from."
2642,3, Stachenfeld et al. (2014) have showed that PVFs can be obtained as a linear transformation of the eigenvectors of the matrix formed by stacking all SRs of an MDP.
2643,3,"""This paper presents an alternative approach to constructing variational lower bounds on data log likelihood in deep, directed generative models with latent variables."
2644,1," I\u2019m especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance."
2645,1,Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier.
2646,3, \n\n4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.
2647,3,\n\nMost parts of the paper provide a detailed review of the literature.
2648,3,"Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art."
2649,3,"\n\n3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer."
2650,3," \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones."
2651,3,"\n* It could also be welcome to use a more grounded vocabulary, e.g. on p.2 \u201cFigure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources\u201d could be much more simply said as \u201cFigure 1 shows the ellipses corresponding to three sets of R^3 points\u201d."
2652,3, The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers
2653,3," To apply BFGS, one might have to replace the RELU function by a smooth counterpart.."
2654,3, Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic).
2655,3," Again, the short walk may be capturing the communities but the high-dimensional random walk sample path seems like a high price to pay to learn community structure."
2656,3," The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines."
2657,3, This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework.
2658,1,\n\nThe author\u2019s articulate a specific solution that provides heuristic guidance rewards that cause the \nlearner to favor actions that achieve subgoals calculated from expert behavior\nand refactors the representation of the Q function so that it \nhas a component that is a function of the subgoal extracted from the expert.
2659,3, \n\nHow do you execute a trajectory?
2660,3," Also, the number of hidden units in the 2 layer MLP from para 5 in Sec 2."
2661,3, It would be interesting to see these results.
2662,3,"\n\nRegarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help?"
2663,1," They slowly worked through a simple set of ideas trying to convey a better understanding to the reader, with a focus on performance of RL in practice."
2664,3,\n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper.
2665,3, In the experiments they only consider training the model with the context selection variable and the data variables observed.
2666,3," In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed ."
2667,1,I believe that using cross-entropy with softmax should also push semantically similar labels to be nearby in the weight space entering the softmax.
2668,3,"\n\nIn the ImageNet classifier family prediction, how different are the various families from each other?"
2669,3,"Distinguishability in high dimensions is an easy problem (as any GAN experiment confirms, see for example Arjovsky & Bottou, ICLR 2017), so it's not surprising or particularly insightful that one can train a classifier to easily recognize the boundaries."
2670,3,\nA toy model with a single unit is used to illustrate the basic ideas behind the method.
2671,1, The idea seems interesting.
2672,3, The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention. 
2673,1," \n\nClarity: The paper is well structured and written, with a nice and well-founded literature review."
2674,3,"\n3. Design, solving inverse problem using Deep Learning are not quite novel, see\nStoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017)."
2675,3,"\n8. Although in terms of \""effective training data points\"" the proposed method outperforms the other methods, in terms of time (Fig.5) the difference between it and say, NoTeach, is not that significant (especially at very high desired accuracy)."
2676,3,\n* Where is Table 1?
2677,3,"\n- \""large training error on wrongly labeled examples\"" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels?"
2678,3," The authors state that WAE generates \""samples\nof better quality\"" (than VAE) without any condition being put on when\nit does this."
2679,3,"\n\n2). In Algorithm 1, how do you deal with vocabulary items in the new domain that do not exist in the previous domains i.e. when the intersection of V_i and V_{n+1} is the null set."
2680,3,"\n\nFor the structured QA task, there are 400 training examples, and 100 named entities. This means that the number of training examples per named entity is very small."
2681,3,I think that this general approach deserves further attention from the community.
2682,3," Remove the variational component, and phrasing it simply as an auto-encoder."
2683,1,"  While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions."
2684,1," In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks."
2685,1," Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related \""sentences\"" could be encoded, including document title, section title, footnotes, hyperlinked sentences.This is a valid good idea and indeed improves results."
2686,3," Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives. "
2687,2,A classic instance of reinvention of the square wheel.
2688,3, Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features.
2689,3," The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence."
2690,3,"""The main contribution of the paper seems to be the application to this problem, plus minor algorithmic/problem-setting contributions that consist in considering partial observability and to balance multiple objectives."
2691,1," I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?"""
2692,3,"\n\nOriginality: Low-rank tensors have been used to derive features in many prior works in NLP (e.g., Lei et al., 2014)."
2693,3, This method is algebraic method closely related to spanning sub spaces and SVD.
2694,3," Prior work along this line includes [3, 4, 5, 6, 7]."
2695,1," However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel."
2696,1, In general I found this paper clearly written and technically sound.
2697,3," Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy"
2698,3,"\n\n3. \u201cThe average recent iterate\u201c described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose \u201c3\u201d, and the effectiveness of different choices should be discussed, as well as the \u201c24\u201d used in state features."
2699,3, Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one).
2700,3, For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN.
2701,3," For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2?"
2702,3,"Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply."
2703,3,"""The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space."
2704,3,"""This paper introduces a neural network architecture for continual learning."
2705,3, It does not have skip connections yet performs quite well.
2706,3," As far I understand, all of the original work policy gradients involved stochastic policies."
2707,1, The work also propose a neat model motivated by the environment and outperform various baselines.
2708,3, 2014 (they used this method to perform data imputation).
2709,1,The experiments are clear and the three different schemes provide good analytical insights.
2710,3," The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency."
2711,3,"\n\n*Remarks on methodology*\n\nBy initializing a decoding by \u201cguessing\u201d a value, the decoder will focus on high-probability starting regions of the space of possible structures."
2712,3,"""This paper proposes a model of \""structured alignments\"" between sentences as a means of comparing two sentences by matching their latent structures."
2713,3,"  \n\nIn Fig 3. The full batch loss of Adam+ProxProp is higher than Adam+BackProp regarding time, which is different from Fig. 2."
2714,3," Could the authors perhaps comment on:\na) whether it is even meaningful to talk about the approximations as probabilistic distribution during the optimization (given the psd approximation to the Hessian), or does it only make sense after convergence? "
2715,3,\n- Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent. 
2716,3,  Answers to these questions can automatically determine suitable experiments to run as well. 
2717,3," Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against \""shared\"" adversarial perturbation, in particular against universal perturbation."
2718,3,"\n\n(side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act."
2719,3, This algorithm is a form of RLSVI (2014) where you only consider uncertainty over the last (linear) layer - I think you should present it like this.
2720,1, The test not only distinguishes real from synthesized faces but also evaluates the observer ability by determining whether the observer is a human.
2721,3, Perhaps not that much?
2722,1," Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible."
2723,3,"\n\nIs there a specific use scenario for the prefix splitting, other than for the evaluation of unseen prefixes?"
2724,3,"""This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics."
2725,1, The clarity of the paper has improved;
2726,1,  The paper is well presented and organized.
2727,3,  Section 2 should be clearer and used to better explain\nrelated approaches to motivate the proposed one.
2728,3," I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper."
2729,3," The authors of this paper simply write \""Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty."
2730,3,    Thus the authors claim that one has to be careful about using feature importance maps.
2731,2,One wonders whether the analysis was an exercise in using a cannon to open an unlocked door
2732,3,"  In the real world, however, it is unlikely that any low level controller would be able to do this perfectly."
2733,3, The attack model involves a standard l_inf norm constraint.
2734,2,he authors report results from pages 16-26. This section reflects what I would brutally call 'death by figures
2735,3," This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory."
2736,3," If the focus is transfer, one could argue that another way of training the PATH net could be by training jointly the PATH net and goal net, with the intend of then transferring to another reward scheme."
2737,3,"  \n\n\nReferences\n\n[1] Le et al, \u201cA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u201d, arXiv 2015\n[2] Arjovsky et al, \u201cUnitary Evolution Recurrent Neural Networks\u201d, ICML 2016\n[3] Cooijmans et al, \u201cRecurrent Batch Normalization\u201d, ICLR 2017\n[4] Zhang et al, \u201cArchitectural Complexity Measures of Recurrent Neural Networks\u201d, NIPS 2016\n[5] Sigurdsson et al, \u201cHollywood in homes: Crowdsourcing data collection for activity understanding\u201d, ECCV 2016\n[6] Sigurdsson et al, \u201cAsynchronous temporal fields for action recognition\u201d, CVPR 2017"""
2738,3,"\n\n----------------------\n\n\n\nIn this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch-size, activation function, no. layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully."
2739,1," However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy."
2740,3, This fact weakens the scope of the online hyper-parameter optimization approach.
2741,3,  How important are these two methods to the success of GTI?
2742,2,To me the question is uninteresting
2743,3,"""This paper proposes a tensor factorization-type method for learning one hidden-layer neural network."
2744,3, Are there any other potential biases brought because the data collection tools?
2745,3, Are you perhaps using natural logarithms to estimate and plot I(Z;Y)?
2746,3," The paper shows that by appropriately defining the neuron level reward function, the model can learn a better policy in different tasks."
2747,3,"Rather than jointly optimize the student and teacher (as done previously), they have form a coupled relation between the student and teacher where each is providing a best response to the other. "
2748,3,"""This paper is concerned with video prediction, for use in robotic motion planning."
2749,3,\n\nSuggested References:\nBahdanau et al. (2016) An Actor-critic Algorithm for Sequence Prediction.
2750,3,  But these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already available.
2751,3,"  If so, is this done jointly with the training of the auto-encoder?"
2752,3," How does this compare to the near-identity constraints in resnets in Shaham et al. ?\n\n"""
2753,3,  Why not learning the representation using an unsupervised learning method (unsupervised pre training)?
2754,1, The proposed method out-performs all other graph convolutions on all the tasks (except image classification) though having comparable or less number of parameters.
2755,3,"\n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts."
2756,3,"\n\nRegarding repeatability, the claim [d] is contradicted in section 5 when the authors demonstrate that the NavA3C+D1D2L algorithm does achieve a reduction in latency to goal in 8 out of 10 experiments on random goal, static map and random or static spawns."
2757,3," For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that?"
2758,1," Therefore, one could argue that the proposed approach not only generate useful options, it also gives rise to dense pseudo-rewards that make it easier to build the policies associated with them."""
2759,3, Is it Word2vec or something else?
2760,3,"""The paper reformulates the model-agnostic meta-learning algorithm (MAML) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model. "
2761,3," Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers)."
2762,3," In particular, mini-ImageNet is a commonly-used benchmark for this task that this approach can be applied to for comparison with recent methods that do not use data augmentation."
2763,3,"""In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games."
2764,3," \n\nv) During NLP pre-processing (section 4), how do you prune the irrelevant documents?\n"""
2765,1,".\n\n(3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing"
2766,2,Among the topics to be subject to the integrity of attention needs to be work related to the subject of innovation does not
2767,3,"\n\n\n[Collins et al. JMLR 2008] Michael Collins, Amir Globerson, Terry Koo , Xavier Carreras, Peter L. Bartlett, Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks, , JMLR 2008.\n\n [Dziugaite et al. UAI 2015] Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In UAI, 2015"""
2768,3, As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model.
2769,3," 6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method."
2770,1,"\n2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct."
2771,3," \n\nIn addition, as the encoder-decoder structure gradually becomes the standard choice of sequence prediction, I would suggest the authors to add the sum of parameters into model ablation for reference."
2772,3,"""The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks."
2773,3,  E-greedy approaches will always struggle to choose the same random action repeatedly.
2774,3," From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form."
2775,2,The interchangeably use of evaluate and validate is a concern as it is not clear if authors know the difference between these 'verbs' 
2776,3,"\n\nTo this reviewer\u2019s understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work."
2777,2,The presentation of the paper is difficult to follow for a hard scientist and it sometimes reads as if it were machine generated
2778,3, Is this protocol derived from some other source?
2779,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
2780,3,  This should also work for (5).
2781,2,I started to review this but could not get much past the abstract.
2782,3, At inference time y can be optimized by gradient descent steps.
2783,3, I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates.
2784,3,"\nThey should also present early their model and their mathematical motivation: in what sense is their new penalty \""preferable\""?"
2785,1, There is some originality in the proof which differs from recent related papers.
2786,3,"""This work proposes to replace the gradient step for updating the network parameters to a proximal step (implicit gradient) so that a large stepsize can be taken."
2787,3,"  \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search."
2788,2,"Many of the most serious errors are more or less just copied out from what he has read, so it is hard to know how to deal with such cases"
2789,3,. It could be beneficial to do an intermediate experiment (a handful of attributes on a middling task)
2790,1,"\n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster."
2791,1, It is well-known that these are sensitive to initialization and often require running the optimizer with multiple random seeds and picking the best one.
2792,1,"\n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference."
2793,3, Could the authors also report these?
2794,3,"  It is also unclear when a model, e.g., BiLSTM or CRF is the energy function (discriminator) or inference network (generator)."
2795,1,"""This is a nice paper."
2796,3,"""The paper proposes to use the start-end rank to measure the long-term dependency in RNNs."
2797,3," I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \""wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples."
2798,3, This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors?
2799,3,\n\n- Will the authors release code to reproduce all their experiments and methods?
2800,3, \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets.
2801,3," In terms of numerical stability, though experimental results were reported, there is no theoretical analysis."
2802,3,. There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.
2803,3, Their experiments show that this improve technique can produce complete training sets for three programs.
2804,3, Are there any obvious modes?
2805,3, I will take this point to be moot.
2806,3," Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales."
2807,3,"\nAuthors evaluate their RIN on the adding, sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models."
2808,3, \n\nOne question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches.
2809,1,.\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model.
2810,2,It is essentially an opinion piece that editorializes shamelessly about the superior methods of a recent paper in the first person
2811,3,"   Some previous work is cited, but I would point the authors to much older work of Parr and Russell on HAMs (hierarchies of abstract machines) and later work by Andre and Russell, which did something very similar (though, indeed, not in hybrid domains)."
2812,3," Differently from (Yogatama et al, 2017), this paper doesn\u2019t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way."
2813,1,"\n\n  - Finally, although the empirical evaluation is quite extensive and outperforms the state-of the art, I think it would be important to compare the proposed algorithm to other tensor factorization approaches mentioned above."
2814,3, What could explain this difference in the performances?
2815,1," however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination."
2816,3, Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it. 
2817,3," The central thesis is that\ninstead of the \""conventional wisdom\"" to fix the batch size during training and\ndecay the learning rate, it is equally effective (in terms of training/test\nerror reached) to gradually increase batch size during training while fixing\nthe learning rate."
2818,3," Also, quantitative figures would be useful to get the big picture."
2819,3,"   Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same."
2820,3, It may be useful to compare to the uncertainty produced by a GP with suitable kernels.
2821,3," \""On the state of the art of evaluation in neural language models."
2822,3," Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1."
2823,3,"  However, in section 4, in (7), the authors claimed the target vector v_t will affect the context shifting their representation to c\u2019_i."
2824,3, It seems that no reward shaping is used.
2825,1," The second advantage is its potentially better representation, proved by better results compared to models using recurrent networks on the TriviaQA dataset."
2826,3, The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks.
2827,3, This is further conflated by fig7 which attempts to illustrate the quality of the learned value functions.
2828,1,"\n\n(4) Moreover, neural networks are mostly interesting because they learn the representation."
2829,1,\n\nI would say the current results indicate the conventional approach to TD is working well if not better than the new one.
2830,3,"\n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices."
2831,3," However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words."
2832,3, \n\n\n\n\nWhat follows are comments on specific parts of the paper:
2833,3,"  \n\nOne can imagine there might be scenarios where the local guidance rewards of this \nform could be problematic, particularly in scenarios where the expert and learner are not identical\nand it is possible to return to previous states, such as the grid worlds the authors discuss:"
2834,1,\n\nThe paper is well-written overall.
2835,3," \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically. "
2836,3,\n\nThe node classification experiment could use a bit more refinement.
2837,3,"\n\nThe paper concludes with \""Overall the complex-valued neural networks do not perform as well as expected"
2838,3,\n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).
2839,3," Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified."
2840,3," The classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode. "
2841,3,"It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based. "
2842,3," Finally, a few of the experimental settings differ from their baselines in nontrivial ways."
2843,3," However, it is well-known that spectral algorithm is not robust to model mis-specification."
2844,1,\n\n+ves:\nExplaining the power of depth in NNs is fundamental to an understanding of deep learning.
2845,1, It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem.
2846,3," Then: \""[...] the action a_0 should be in a local vicinity of a."
2847,3,  I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization.
2848,1," This choice seems to be appropriate, since standard methods are used."
2849,3,"  Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers."
2850,3," My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer."
2851,3,"  This  somehow allows to non-parametrically infer from the data the \""shape\"" of the activation functions needed for a specific problem."
2852,3,". They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style."
2853,1,"  That would make sense, given that then the dimensions of the vector f_p is a scalar (activation value) per image for that image, in layer k, around pixel p."
2854,2,Not earth shatteringly original but it is hard to be so in this field.
2855,1, The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets 
2856,3,"\n\nIn the related work section, the IB problem can also be solved efficiently for meta-Gaussian distribution as explained in Rey et al. 2012 (Meta-gaussian information bottleneck)."
2857,1,\n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks.
2858,3," \nFor one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability."
2859,3," However, still we need to calculate F."
2860,3,\np7-8. I had trouble following the left/right & front/back notation.
2861,1,"""The quality of this paper is good."
2862,1,"  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise."
2863,3, A good example of this is a supervised actor-critic by Barto (2004).
2864,1," SeaRnn improves the results obtained by MLE training in three different problems, including a large-vocabulary machine translation."
2865,3,  More discussion is needed about the role of edges in E.
2866,3," Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions."
2867,1,"""Thanks for an interesting paper."
2868,2,"presumptuous, ignorant, ill-conceived and downright dangerous"
2869,1," \n\nFor the CIFAR experiments, the experiment design is reasonable for a general comparison."
2870,3,"\n\nThe approach is tested on two artificially generated datasets and on two real-world datasets, and compared with standard approaches such as the autoregressive model, the Kalman filter, and a regression LSTM.\"
2871,1, The presentations of the ideas are pretty clear.
2872,3,"\n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017)."
2873,3,"  The proposed dataset, called the SCAN dataset, is a selected subset of the CommonAI navigation tasks data set."
2874,3, This would require the combination of two Hessian matrices.
2875,3,\n\nThe authors call their findings theory.
2876,3,"""\nSummary of the paper\n-------------------------------\n\nThis paper proposes to factorize the hidden-to-hidden matrix of RNNs into a Kronecker product of small matrices, thus reducing the number of parameters, without reducing the size of the hidden vector."
2877,3, This paper extends the existing results in some subtle ways.
2878,3," For example, how is the performance of a model containing SW-SC or CW-SC without deepening or widening the networks?"
2879,3," The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments."
2880,3, \n3. In the function approximation case the value function and q functions parameterized by \\theta are only approximations of the expected return.
2881,3," Given the paper title, I would have expected some experiments in a generative context."
2882,3,"\n\n    PATH(  s,  Tau( s, th^g ),  a ; th^p )\n\n    d / { d th^g }  PATH(  s,  Tau( s, th^g ),  a ; th^p )"
2883,3,"""This paper proposes a new space for reasoning about human identity."
2884,3," \n(c) If the paper length is limited, a supplementary material about those details would be preferred."
2885,3," If this is base-2 logarithms I would expect a value close to 1. """
2886,1,\n\nThis is a nice paper which I would like to see accepted.
2887,3,"\n\n- The claim that \""implicit SGD never overshoots the optimum\"" needs more supports. Is it proved in some previous papers? "
2888,3, We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label.
2889,2,"The experiments are reasonable, but they fail the fundamental test of good science"
2890,3," The main results are two-fold: if the decision boundary are flat (such as linear classifiers), then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correlated."
2891,3,"\n\n2. Since the model is aimed at grounding the language on the vision based on interactions, it is worth to show how well the final model could ground the text words to each of the visual objects."
2892,3," However, there are some questions (as mentioned in the Weaknesses section) which need to be clarified before I can recommend acceptance for the paper."
2893,3,"\n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)?"
2894,3, Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks.
2895,3, I believe the paper would have been much stronger if either of the two above are further investigated.
2896,1, The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect.
2897,3, My list of concerns are the following: \n\n    The authors state as their first contribution the presentation of a novel dataset.
2898,3,"\n\nOn the top of page 3: \""M is the matrix size\""."
2899,2,"I'm really sorry about this reviewer. If you'd like, I can get you a new one. - Edito"
2900,3, TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values.
2901,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
2902,3,"""This paper proposes a self-normalizing bipolar extension for the ReLU activation family."
2903,1," \n\nNegative points:\nAlthough proposed idea is interesting,"
2904,3," At test time, the gates with the highest values are kept on, while the other ones are shut."
2905,3,"To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters)."
2906,3," \n\n[1]\n@article{tatarchenko2017octree,\n  title={Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs},\n  author={Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},\n  journal={arXiv preprint arXiv:1703.09438},\n  year={2017}\n}\n\nIn light of the authors' octree updates score is updated."
2907,3," The paper should explain how/if one can compute representations given just the image, for instance, say by not using amortized inference. "
2908,3," In the literature, specially in machine learning, there is ``fever\u2019\u2019 about HMC, in my opinion, partially unjustified."
2909,2,"In fact, your hypotheses are not all that complex are they? Nor is Figure 1; nest-ce pas? ,moi"
2910,3, The resulting image can be subtracted from the original encoding to highlight problematic areas.
2911,1,\n\nPros\n\n1. It is new to apply quadrature rules to improve kernel approximation.
2912,3,. What conclusions should I make from it?\n\n* Why not use KL divergence as your \\Delta function?\n\n* Why are the results in Table 5 on the dev data?\n\n* I was confused by Table 4
2913,3, RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017).
2914,3,"""As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed."
2915,3,\n\nAdditional experimental results would make it a stronger paper.
2916,3,  Finally the authors stack these warp layers to create a \u201cwarped resnet\u201d which they show does about as well as an ordinary ResNet but has better parallelization properties.
2917,3," Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model."
2918,3,\n\nSection 2 motivates the suggested linear scaling using previous SGD analysis\nfrom Smith and Le (2017).
2919,3,\n- An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method.
2920,3," The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates."
2921,3,\n\nThe proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order.
2922,3, The regularizer rewards high entropy in the signs of discriminator activations.
2923,3,\n\n* You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs.
2924,3,\n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.
2925,3, For text processing the authors use a standard LSTM taking as input GLOVE vectors of words in a sentence.
2926,1,"\n\nThe results obtained from the analytical forms of the critical points are interesting,"
2927,3,"""This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images."
2928,2,This paper does not leave me satisfied
2929,3,\n\nI found the study of the effect of regularization to be potentially the most informative for neuroscience but it is only superficially treated.
2930,1, \n\nPros:\n+ The paper is generally very clearly written.
2931,1, The first step of building a supervised initial model looks straight forward.
2932,2,"I believe that there are important questions in this area, questions that have intellectual merit, but the PI has not found anyâ€¦"
2933,3," Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial."
2934,3, Experiments are conducted on the Mini-ImageNet dataset and the PASCAL3D+ dataset for few-shot learning.
2935,3, \n\nComments on the Assumptions:\n- Please explain the motivation behind the standard Gaussian assumption of the input vector x.
2936,3,"\n\npage 8, line 7 in section 4.3: \""the the\"" (unintended repetition)[[CNT], [PNF-NEG], [SUG], [MIN]]\n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something?"
2937,3,"\n\n The paper proposes \""FastNorm\"", which is a way to implicitly maintain the normalized weight matrix using much less computation."
2938,3,"""Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS)."
2939,3, I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further.
2940,3," However, it\nwould also be useful to know how much is the extra computational costs of\nthe proposed method."
2941,1," The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations."
2942,3," A larger stepsize is not always better, and smaller is not worse. "
2943,3,\n\nThis paper addresses an important issue in scaling up reinforcement learning to large domains with complex interdependencies in subtasks.
2944,3," If the combination of supervised learning with RL is better, than this should be clearly stated."
2945,3,\n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?
2946,1," The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits,"
2947,3,"  \n(1) By using the objective in eq.(14), how to learn the embeddings E?"
2948,1,"\n\n+ Significance:\nWhile the results are interesting,"
2949,3," Our rule of thumb is, when the training accuracy raises slowly, run SGD for 10 epochs (because it\u2019s already close to minimum)."
2950,2,I dont believe in simulations
2951,3,  Was it due to the convolutional net structure (you could test this)?
2952,3,\n\n3) Equation (3) -- put the missing 2 subscript for the l2 norm of |f_(w+u)(x) - f_w(x)|_2 on the LHS (for clarity).
2953,3, They found it improved the accuracy.
2954,3, In the case of Mirowski et al. this means that the LSTM has somehow learned to do general SLAM in a meta-learning sense.
2955,2,This manuscript is not publishable in a reasonable sense
2956,3," \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy)."
2957,3,"\n2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000)."
2958,3,"""This work proposes an LSTM based model for time-evolving probability densities."
2959,3," Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason?"
2960,3,"\n\n  - Minor comment. In the shifted PMI section, the authors mention the parameter alpha and set specific values of this parameter based on experiments."
2961,3, which also attempts at unifying VAEs and GANs.
2962,1,\n\npros:\n(a) An interesting problem to evaluate the robustness of black-box classifier systems
2963,3,"""The paper is mostly a survey about clustering methods with neural networks."
2964,3," (1) What is an \""informal\"" theorem?"
2965,3,\n- Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors?
2966,1,\n* The experimental results are promising on multiple datasets.
2967,1, The experiments are very clearly presented and solidly designed.
2968,3, Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does?
2969,1, The analysis of the approaches in terms of their ability to decode is also sound and interesting.
2970,3," These two strategies are thus \""equivalent\""."
2971,1,"\n\nOverall, while I like the and think the goal is good,"
2972,3,"""This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization."
2973,3, Would RINs readily learn to reset parts of the hidden state?
2974,2,"This is a pretty trivial study, sample size is suspiciously high, and a tiny effect of 5% percent (who cares if it's significant)"
2975,2,"The question in the first paragraph is not functional in my opinion, as are the words inner workings"
2976,2,"Since the paper is mathematically empty &amp; provides no new ideas or findings, I see no reason to publish it"
2977,3,"\n\nReferences:\n[1] Gomez, F. J., & Miikkulainen, R. (1999). Solving non-Markovian control tasks with neuroevolution."
2978,3,"\n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \""Improved deep reinforcement learning for robotics through distribution-based experience retention,\"" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952."
2979,3, Presumably the strong diagonal attention is attending to the previous word when computing the representation to generate the subsequent word?
2980,3," Accordingly, I did not change my scores."""
2981,3, And then train another net to predict future frames given the input and residual error from the first network.
2982,1,\n- The paper is clearly written.
2983,2,This looks like a very early draft
2984,1,"\n- NS-GANs + GPs seems to be best sample-generating combination, and faster than\n  WGAN-GP"
2985,3, (Figure 4 provides qualitative examples from ImageNet but no quantitative assessment.
2986,2,This paper has the same relevance as a paper in astronomy which places the earth as the centre of the universe
2987,3,"[3] Patterson, S. and Teh, Y.W., 2013."
2988,2,"Im disappointed to say, given the outstanding pedigree of the authors of this paper, that it adds nothing to earlier research."
2989,3, \nc)\tWould this approach be able to generate a lattice?
2990,3," Even more puzzling, why does this error go up again for the blue curve (no interaction)? Shouldn\u2019t at least this curve be smooth?\n"""
2991,3,"\n\nThe paper builds upon approximate n-step discrete-action Q-learning \nwhere the Q value for an action is a linear function of the state features:\n\n    Qp(S,a) = Wa S + Ba\n\nwhere parameters p = ( Wa, Ba )."
2992,3,"""This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history."
2993,3,"\n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum."
2994,3,;\n - Assimilation methods are usually independent of the type of data at hand.
2995,3," In particular, a tree structured LSTM is taken and modified."
2996,3,\n- Maybe provide a reference for HMM/GMM and EM (forward backward training)
2997,3," If this allows the removing of the target network for instance, in those more difficult tasks, then this would be a huge deal."
2998,2,"I shall not comment beyond the end of the methods section, and shall comment selectively rather than exhaustively (which would indeed be exhausting)"
2999,3," The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search."
3000,1,"\n\nExperiments\n- Many of the previous comments still hold, please proofread"
3001,3, Here it would be good to be more specific about the domain.
3002,3," Also, some plots/visualizations of the loss surface given in Equations 4 and 5 would be very helpful."
3003,3,"""ORIGINALITY & SIGNIFICANCE\n\nThe authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network. "
3004,3,"\n\nEq 11 might be clearer if you show that Eq 8 can also be rewritten in the same wheat, provided, you make h_{t-1} an argument to output and content."
3005,1, It is commendable that the authors also discuss the memory requirements and increased wall clock time of the model.
3006,3,"\nThen, I think that the claim of page 6 saying that Domain Adaptation can be performed \""nearly optimally\"" has then to be rephrased."
3007,3," However, many of dynamic systems are inherently discontinuous (collision/contact dynamics) or chaotic (turbulent flow)."
3008,1," \n\nThe extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable."
3009,3,"\n\n\n[1] QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks https://arxiv.org/abs/1610.02132"
3010,3," There, a simple deterministic condition (the null space property) for successful recovery is proved."
3011,3," For example, verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence."
3012,3,\n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one.
3013,3, It would be great that the paper could be more explicit about its limitations.
3014,3," \n\nI am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. \n"""
3015,1,\n\nCasting MAML as HB seems a good idea.
3016,1, Novelties are clearly identified by the authors.
3017,1," the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al., "
3018,1, \n\nPros and positive remarks: \n--I liked the idea behind this paper.
3019,3, It is not clear why the the optimized full-rank tensor is more easy to decompose if it was initialized with a low-rank tensor.
3020,3,"""This paper utilizes ACOL algorithm for unsupervised learning."
3021,3, This makes me\nwonder if the comparison in table 2 is fair.
3022,3," During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa."
3023,3,\n6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments?
3024,1, This paper had made a good survey.
3025,3,"  Without this, it is actually really hard to talk about general mechanism of learning adaptive Fourier features for kernel algorithms (which is how the authors present their contribution); instead we have a method heavily customized and well-tailored to the (not particularly exciting) SVM scenario (with optimization performed by the standard annealing method;"
3026,2,The final section illustrates the papers poverty. We are given the sort of banality that civil servants write for their masters speeches
3027,2,My greatest criticism of the paper is the tendency of the authors to make an argument and then immediately contradict themselves
3028,3,  LL wins by a bit more in Table 2.
3029,3," \"" Based on my knowledge, there are two main families of semi-supervised deep learning methods, classified by depending on deep generative models or not. "
3030,3," \"" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet."
3031,3,"""\nSummary:\n A method for creation of semantical adversary examples in suggested."
3032,3,\n\nThe algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability.
3033,1, The main idea is to design complementary kernels that cover the same receptive field as the regular convolution.
3034,1,"\n\n(2) The paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding, which help a lot for understanding how the proposed methods contribute to the improvement."
3035,3," During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold."
3036,3,"""The paper proposes a new method for detecting out of distribution samples."
3037,3," Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work."
3038,3, My impression is hence that the only possible outcome is\n\nrejection.
3039,3,"  The proposed pipeline converts each character to an embedding with the only sentence of description being \""Each character is converted by a lookup table to a vector representation, known as character embedding.\"""
3040,3," The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2)."
3041,3, The essential problems here are how to identify which states should be stored and how to retrieve memory during action prediction.
3042,3,\n7. Why is average accuracy the right thing?
3043,1," I lean on accept side.  \n\n\n"""
3044,3,"\n\nIn the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities."
3045,3, \n\nI was expecting the authors to mention \u201cgoal emulation\u201d and \u201cdistal teacher learning\u201d in their related work.
3046,3,"  I.e., with enough throws, I can always hit the bullseye with a dart even when blindfolded."
3047,1,"\n\nThe paper is easy to read and organized very well, and has adequate literature review."
3048,1," This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup."
3049,3," The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation."
3050,3," \n\n3. In the first paragraph of Section 3.3: \""[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation."
3051,1, Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well.
3052,3,  May this helped Q_MC have better perceptual capabilities?
3053,3," It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation."
3054,1, The\nadded stochasticity and the model ensembling interpretation is probably novel.
3055,3,"""The authors propose a new algorithm for exploration in Deep RL."
3056,3," I'm thinking in particular of:\n\nProximal Policy Optimization Algorithms (Schulman et. al., 2017)"
3057,2,"The paper is overlong, very verbose and contains unnecessary repetition."
3058,1,"\n\nNevertheless, the parameterisation of the conditional variational distribution q(\\theta | \\phi, (x, y)) using recognition model is interesting and could be useful in other models."
3059,3," Unless I misunderstand the definition of CRPS and PLL, that width should matter, no?"
3060,2,It is difficult from this reviewers perspective to even call these studies.
3061,3, It is not clear to me why this types of theoretical invariance is tested on such as specific dataset.
3062,3," I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit."
3063,3, It\u2019s a bit suspicious that it doesn\u2019t achieve 20+ in Pong.
3064,2,"Overall, I dont quite get what the authors think theyve accomplished."
3065,3,\n[4] Convex multitask learning with flexible task clusters (ICML)
3066,3, More details are needed to understand this.
3067,1," I think this is a valuable engineering contribution,;"
3068,1," I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues."
3069,3," Additionally, in order to avoid vanishing/exploding gradients in standard RNNs, a soft unitary constraint is used."
3070,3," Contributions include a new dual-based algorithm for the fundamental task of computing an optimal transport coupling, the ability to deal with continuous distributions tractably by using a neural net to parameterize the functions which occur in the dual formulation, learning a Monge map parameterized by a neural net allowing extremely tractable mapping of samples from one distribution to another, and a plethora of supporting theoretical results."
3071,3, How is the 2d subspace chosen?
3072,3," \n\nWhile the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016)"
3073,3," However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis. "
3074,3," Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious."
3075,1,"  Further, the work is clearly presented."
3076,3," With enough data, both might produce similar results."
3077,3,"\n\nIn the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture)"
3078,3," A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works."
3079,2,One gets the feeling that the references were included for the sake of having something to refer to.
3080,1," \n\n[Strengths]\n\n1. I think this paper proposed interesting tasks to combine the vision, language, and actions."
3081,1,"\n- Despite all the suggestions and questions below, the method is clearly on par with standard A3C across a wide range of tasks, which makes it an attractive architecture to explore further."
3082,3," While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset."
3083,3," \n\nDespite mentioning matrix completion in the introduction and comparing to work of Ge et. al., the paper does not seem to have strong implications for matrix completion."
3084,3,"- although of course, since all results should be reported on cross-validated testing subsets anyhow,"
3085,3, \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain.
3086,3," Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO."
3087,3," Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization?"
3088,3," I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?"
3089,3,. Novel combinations of GAN formulations (non-saturating with\ngradient penalties) are evaluated to disentangle the effects of formulation\nchanges.
3090,3,"""The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation."
3091,3,"   The objects discovered could be discovered with mosaicing (since the background is static) and background subtraction. \n"""
3092,3,"\n6. For the MS-COCO, examples can you provide more detailed results as shown for synthetic datasets? Majority vote is a very weak baseline."
3093,3, Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.
3094,3,"   I'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings, but still catches some errors."
3095,3,"\n[2] TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep Learning\nhttps://arxiv.org/abs/1705.07878\n[3] Parallel SGD: When does averaging help? \nhttps://arxiv.org/abs/1606.07365\n\n"""
3096,3," The main point is that the intervention distributions are correct (this fact seems to be there, but is \""hidden\"" in the CIGN notation in the corollary)."
3097,1,"  While the intuition is nice and interesting,"
3098,1," The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al. 2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1-norm of the layers instead of the Frobenius norm."
3099,1,"\n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \""deep reinforcement learning\""."
3100,1,"\n\nThe proposed model has several interesting novelties (mainly in terms of new applications/experiments, and being fully auto-regressive), yet also shares many similarities with the generative component of the model introduced in [1] (not cited): Both models make use of (recurrent) graph neural networks to learn intermediate node representations, from which they predict whether new nodes/edges should be added or not."
3101,3, The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis. 
3102,2,The author should abandon the premise that his work can be considered research.
3103,1,"\n\nOverall, while this is an interesting and important problem, and the engineering details are interesting and reasonably well-motivated, the main contribution of the paper is based on a pretty flawed approach to modeling correction probability, which would limit the ultimate applicability of the methods."
3104,3,"""This paper considers the problem of autonomous lane changing for self-driving cars in multi-lane multi-agent slot car setting."
3105,1,  I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness.
3106,1,"  They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom."
3107,1,"\n\nThese algorithms seem to be an improvement over the current state of the art for this problem setting,"
3108,3,"\n\n(2) Moreover, as the authors rightly point out, covariance functions of the form used in (4) have already been proposed."
3109,3,"\nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart?"
3110,3,"\n\n5. As mentioned in the paper, there are many methods which introduce sparsity in the convolution layer, such as \u201crandom kernels\u201d, \u201clow-rank approximated kernels\u201d and \u201cmixed-shape kernels\u201d."
3111,2,"The correlations are not that strong (e.g., p = .022)"
3112,3,"\u00a0\u00bb\n- p6: $C^{\\epsilon_0}_{A,B}$ is used (after Def. 2) before being defined. \n- p7: build->built\n\nSection II :\nA diagram explaining  the different mappings (h_A, h_B, h_AB, etc.) and their spaces (D_A, D_B, D_Z) would greatly help the understanding."
3113,3,"  If curves are already averages over several experiment repeats, some form of error bars or variance plot would also be informative."
3114,3," If the authors release their code as promised, the off-the-shelf tool would be a very valuable contribution to the GAN community."
3115,1, The analyses are interesting and done well.
3116,3," This objection also concerns the right hand figure, and the semantics of the meaning of the upper vs lower triangles should be clarified in the caption (rather than just in the text)."""
3117,3,"\"" PLoS computational biology 11.8 (2015): e1004315."
3118,3," They might be true for a narrow field of application. But in general, I think they are not quite correct."
3119,3,"""The authors propose a new exploration algorithm for Deep RL"
3120,3,"\n\n- Page 4. 3.3) \""Note that.. outdoor images\"" this is implicitly adding the designers' bias to the results."
3121,1,\nMakes a strong case that random noise injection inside conditional GANs does not produce enough diversity
3122,3,"\n\nResults show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model. "
3123,3,  Suggest adding a forward reference to (5).
3124,3, The other proposed components contribute less significant.
3125,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
3126,2,This left me somewhere between scratching my head and pulling my hair out
3127,1,"  \n\n2. The proposed idea is very well motivated, and the proposed model seems correct."
3128,2,Neither research nor science
3129,1," Interestingly, these include representations that have been observed in mammals and that have attracted considerable attention, even honored with a Nobel prize. "
3130,3, Are there any\n  comparison with these previous efforts?
3131,3, Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach.
3132,3," While I think that this experiment is well done, it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of tasks."
3133,3," This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are"
3134,1,".\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up."
3135,3," This assumption seems reasonable if a central station broadcasts all agents' positions and customers are only allowed to stop vehicles in the street, without ever contacting the central station; otherwise if agents order vehicles in advance (e.g., by calling or using an app) the central station should be able to communicate customers locations too."
3136,3, Follows a section of experiments on variants of MNIST commonly used for continual learning.\n\n
3137,1,"""This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL."
3138,3,". It might be cleaner to either treat the general K case throughout, or state the theorem for K = 1.\n\n11."
3139,3," They define three criteria: Disentanglement, Informativeness, and Completeness."
3140,3," They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting."
3141,3,  I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.
3142,1,  So the PATH function helps and longer paths are better.
3143,3," In particular, if we seek to minimize f(W) such that W belongs to asset that can be easily projected on, then projected gradient descent would apply traditional gradient descent on the current iterate, followed by a projection step onto this set."
3144,3, Table 1 gives a hint of that but it is not clear whether much smaller models with standard beam search are possibly as good and fast as NAT -- losing 2-5 BLEU points on WMT14 is significant.
3145,1,\n\nPositive aspects:\nThe paper is well written and clear to understand.
3146,3,\nThe proposed methodology is evaluated on some standard benchmarks in vision.
3147,3, This is one of the things experimenters would really care about -- the price to pay for regularisation compared to the unknown unregularized optimum.
3148,1," The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally."
3149,3," \nIn Neal\u2019s case, the method was explored for single output regression, where the fact that we lose this sharing of basis functions may not be so restrictive."
3150,1, The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously.
3151,3,"""This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation. "
3152,3,"\n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved."
3153,1,"""The authors define a novel method for creating a pair of models, a student and a teacher model, that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to people."
3154,3," Likely what you meant is the q function, at state s_t?"
3155,3,I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods.
3156,3," The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS."
3157,1,"""Deep neural networks have found great success in various applications."
3158,3,"   At best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are."
3159,3, It would be good to justify (empirically) the proposed reward function.
3160,3,\n\nATARI 2600 games: I am not sure what state restoration is.
3161,3,   Two minor concerns are 1) what is the relationship between the anti-labeler and and discriminator?
3162,1, The resulting model is general-purpose and experiments demonstrate efficacy on few-shot image classification and a range of reinforcement learning tasks.
3163,3,Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove.
3164,3," I am wondering how this will be used and evaluated in medical imaging setting. \n"""
3165,3," I would like to see more details on how the evaluation is done here: presumably, the lower I set the threshold, the higher my score?"
3166,3,\n- Approximating a cluster with a single sample (sec. 2.3) seems rather crude.
3167,3," They show that their idea is effective in reducing private information leakage,"
3168,3,Semi-supervised means that one combines\nlabeled and unlabeled data.
3169,3," Afterward, the authors argue that deterministic network cannot adequately several modalities."
3170,3,"\n\n[1] R Collobert, C Puhrsch, G Synnaeve, Wav2letter: an end-to-end convnet-based speech recognition system, 2016\n\n[2] Y Dauphin, A Fan, M Auli, D Grangier, Language modeling with gated convolutional nets, 2017\n\n[3] A Graves and N Jaitly, Towards End-to-End Speech Recognition with Recurrent Neural Networks, 2014"
3171,2,Please use something in parenthesis so your reader doesnt have to take a break from reviewing your work to Google it
3172,2,"eviewer 2 method acting You will notice that my comments are a bit jumbled, this is somewhat a reflection of the need for improved organisation of the MS in general"
3173,3,"""This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network. "
3174,3," Why not IBM 4, for instance ?"
3175,1, Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation.
3176,3,\nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus!
3177,3,"\n- covariate specific analogies presented confusingly and similar but simpler analysis might be possible by looking at variance in neighbours v_b and v_d without involving v_a and v_c (i.e. don't talk about analogies but about similarities)"""
3178,3, They only compare it to a method named CORAL and to Typical\nVariation Normalization (TVN).
3179,2,A classic instance of reinvention of the square wheel.
3180,3, The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory.
3181,3," E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words."
3182,1,"\n\nAs a purely empirical study, it poses more new and open questions on GAN\noptimization than it is able to answer; providing theoretical answers is\ndeferred to future studies."
3183,3," Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively."
3184,3," Specifically, the idea is to use the structure of the PGM to perform efficient inference."
3185,3, The motivation for this choice would be helpful.
3186,3," The paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations, or even what it means geometrically that positive curvature obtains at every data point."
3187,3,. This is achieved by forcing the output of some channels being constant during training.
3188,3," Is this just a restatement of the previous paragraph, which concluded convergence will be slow if \\eta is too small?"
3189,3,"   Either human training data showing very effective generalization (if one could somehow make \""novel\"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN."
3190,3," In particular the inner (task) training is initially cast as point-wise max likelihood estimation, and then (sec4) improved upon by making use of the Laplace approximation."
3191,1,\n\nI thought the little 2-mode MOG was a nice example of the premise of the model.
3192,3," I think the paper could be improved by spending more time on a detailed description of the model for the Omniglot experiments (as illustrated in Figure 3). E.g., explicitly describing how group-wise and per-example posteriors are composed in this model, using Equations and pseudo-code for the main training loop, would have saved me some time."
3193,2,Figure 6. This figure is silly.
3194,3, Sufficient implementation detail and analysis on results.
3195,3,. The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task
3196,3, How is the ensemble of (1-T) training models trained to predict the f(T)?
3197,2,This is a pointless paper. It offers neither interesting new data nor cogent explanation
3198,3, They investigate pros and cons in detail adding more valuable analysis in the appendix.
3199,3," These are all novel contributions, but each one seems incremental in the context of previous work on this and similar algorithms (E.G. Nokland, Direct Feedback Alignment Provides Learning in Deep Neural Networks, 2016; Baldi et al, Learning in the Machine: The Symmetries of the Deep Learning Channel, 2017). \n\n"""
3200,3," The whole network is trained on the \""fill-in-the-blank\"" task using the sequence-to-sequence architecture for both the generator and the discriminator."
3201,3, The authors are well aware of this. They still manage to provide added value.
3202,3," \n\nVisually, these perturbations seem to have strong, oriented local high-frequency content \u2014 perhaps they cause very large responses in specific filters in the lower layers of a network, and conventional architectures are not robust to this? "
3203,3," From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question)."
3204,3,"  However, under one-shot learning, won\u2019t this  make each class still have only one instance for training?"
3205,3," If the focus of the  paper is on  obtaining good entailment results, maybe an NLP conference can be a more suitable venue.\n"""
3206,3,"""Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper."
3207,3,"Traditionally, for determining optimal execution plans, one may resort to cost-based optimization (e.g., database management systems)."
3208,2,The scientific contribution of this paper - if there is any at all - is at best hopelessly insignificant.
3209,1," \n - Thanks for the clarification and adding this citation. """
3210,3," \n- It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters?"
3211,3,\n\nPost-rebuttal comments:\n\nI have revised my score after considering comments from other reviewers and the revised paper.
3212,3,"""The paper presents a novel adversarial training setup, based on distance based loss of the feature embedding."
3213,1,\n\nSignificance\nThis paper makes incremental improvements and would be of moderate interest to\nthe machine learning community.
3214,3,  This leads to the question: How much CCC perform compared to random policy selection?
3215,1," CCNNs do not outperform the prior CNN results listed in Table 2,3,4."
3216,3, but it is not clear at all how this particular connection is more than a curiosity.
3217,3,"""The main result specifies a (trigger) strategy (CCC) and corresponding algorithm that leads to an efficient outcome in social dilemmas, the theoretical basis of which is provided by theorem 1."
3218,1, The idea proposed (learning a selection strategy for choosing a subset of synthesis examples) is good.
3219,2,"I nearly said reject, but then I recalled that I have a hangover and am feeling grumpy"
3220,1, The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis.
3221,3," Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours."
3222,3,"""The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin)."
3223,3," the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random."
3224,3,"\nThey conduct experiments in 3 datasets where they experiment with augmentation in the image feature space by random noise, as well as the two aforementioned types of augmentation in the semantic space."
3225,3,\n\n3. Increasing k also comes at a computational cost.
3226,3,.  \n\nOriginality and significance:
3227,2,the sthanthard of writing is impercable
3228,3,\n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency.
3229,3,"\nBased on the explanation of how these experiments are performed, the authors show individual images to mechanical turkers."
3230,3," Though limited to binary classification, the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk."
3231,3, \n\nOrdering problem: A solution for the ordering problem was proposed in [2]: learning a matching function between the orderings of model output and ground truth.
3232,3,"""This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185)."
3233,3,\n\nMinor:\n\nGTI was not introduced before it is first mentioned in the into.
3234,3," The paper proposes a \""Q-masking\"" strategy that reduces the action space according to constraints or prior knowledge."
3235,2,"Overall, the superheated sense of justification and self-advertising on display here are simply..."
3236,2,"First, unless my statistics is failing me, a less than 1.0 SD is not significant."
3237,3, Which versions of the algorithm (explicit/implicit) were used for which experiments ?
3238,3," For example, Entnet is able to reason as the input is fed in and the decoding costs are low. "
3239,3, The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.
3240,3, i.e |\\partial L / \\partial z \\partial w|<= B.
3241,3,"""The authors provide a method for learning from demonstrations where several modalities of the same task are given."
3242,2,"Limited scholarship, flawed design, and faulty logic engenders no enthusiasm whatsoever."
3243,3," How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does \u2018F\u2019 refer to? There is dependency of \u2018F\u2019 on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update?"
3244,3, Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).
3245,3,\n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.
3246,3, Reporting that would help interpret the numbers.
3247,1, \n\nPros:\n + simple model\n + strong quantitative results
3248,1," \nIn conclusion, I think the research direction is worth pursuing, but the proposed modification from VPN is not well-justifie"
3249,2,I felt like I was reading a horror movie
3250,3," I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations."""
3251,3,"\n2. Scoring disentanglement against known sources of variation is sensible and studied well here, but how would the authors evaluate or propose to evaluate in datasets with unknown sources of variation?"
3252,3, Is it to save the additional (small) overhead of using skip connections?
3253,1,\n- A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.
3254,1,"\n\nThe main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed."
3255,3," The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed."
3256,3,"\n\nIt would be useful to describe exactly the extent to which supervision is used - the method only needs positive and negative links, and does not require any additional order information (i.e., WordNet strictly contains more information than what is being used)."
3257,3," The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced."
3258,1,\n\n(3) The main results of this paper seem technical sound.
3259,3,"\n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines."
3260,3, (This is totally consistent with Krishnan et al.\u2019s finding that eliminating amortization error gave a bigger improvement for more complex datasets than for MNIST.)
3261,1, This is true for mathematical aspects as well as program generating specific terms
3262,3,"""The paper describes a way of combining a causal graph describing the dependency structure of labels with two conditional GAN architectures (causalGAN and causalBEGAN) that generate  images conditioning on the binary labels."
3263,2,Intermediary steps and the apologetics for [topic x] derived from an ahistorical cult and its author.
3264,1,.\n\nPros:\n- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas.
3265,3," Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures."
3266,3," Furthermore, the authors also provide the probabilistic interpretation of the models."
3267,1," \n\nAlthough this work and its results are very useful for practitioners,"
3268,1," The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results."
3269,3, What is the reason that we need to run adversarial attacks 'even faster'?
3270,1,\n\nThe proposed approach leverages recent work that gives a novel parametrization of control problems in the LDS setting.
3271,3,\n\nThis article compares their proposed architecture with RNN (GRU with 10 hidden unit) in few toy tasks.
3272,3," Thus, I would like to see a comparison between SN with vanilla DNN. """
3273,3, Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning.
3274,2,The stimuli are impossible to compute.
3275,3, I would put it at the front.
3276,3,\nThe encoder maps the input to a probabilistic latent space.
3277,3,"   The idea is to characterize the latent feature space (a function of inputs) as observed vs unobserved, and use a module to fit a 'cluster-aware' loss that aims to cluster similar classes tighter in the latent space."
3278,3, Is it due to a well crafted search space that is potentially easier?
3279,3, Better discussing the strong theoretical assumption should be incorporated.
3280,1,\n\nComments for the author:\n\nThe paper is well-written and easy to follow.
3281,3,\n\nIn the abstract the authors mention that the Depthwise Separable Graph Convolution\nthat they propose is the key to understand the connections between geometric\nconvolution methods and traditional 2D ones.
3282,1," (p.2)\n - the related work section is well-written and interesting, but it's a bit odd to have it at the end."
3283,3,"""This paper presents a new way of training stochastic neural network following an information relevance/compression framework similar to the Information Bottleneck."
3284,3, Does it come from a high variance?
3285,3," The idea of \u201csoft ordering\u201d enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular."
3286,3, The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction.
3287,3," The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets."
3288,2,"Like Dr Whos Tardis, the inside of the paper is bigger than the outside. - (H/"
3289,3," As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different."
3290,3, \n\nClarity: The paper is written well and the issues raised are well motivated and proper background is given.
3291,3,"""# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP)."
3292,3,\n- Regarding to the human experiments with AMT: how do the authors deal with noise on the workers performance?
3293,1,  The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.
3294,2,This paper is absolutely ridiculous. It shouldn't be published anywhere and the author should not be encouraged to revise
3295,3," Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved."
3296,3," \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality."
3297,3, So first they are separately learnt and then fine-tuned together.
3298,2,It is shocking to read how statistics are being misused just for the sake of being able to write something.
3299,3,\n2) What can be said about rate of convergence in terms of network parameters?
3300,1,\n\nOriginality:\n\nThe proposed contribution is original. 
3301,1," It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically-invalid programs, thus pruning the search space."
3302,3," Again, use [*].\n\n[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.\n"""
3303,3,"  Two problems are considered:  \""any-time\"" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images."
3304,3,\nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n
3305,2,The derivation is correct but too simple! The paper is therefore not suitable for a general readership.
3306,1,. Examples are given where appropriate in a clear and coherent manner\n\t\u2022\t
3307,3,"\n\nMinor Weaknesses:\n- Since this paper is closely related to Monti et al., it would be good if authors used one or two same benchmarks as in Monti et al. for the comparisons."
3308,2,The authors should opt for a less pretentious title
3309,3," For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier"
3310,3," Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data)."
3311,1,"\n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.)."
3312,3,"""Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention."
3313,3," Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?"
3314,3," Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster?"
3315,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
3316,3,"""This paper mainly focuses on the square loss function of linear networks."
3317,3,"""This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases."
3318,3, \u2019 That happens to be (as you\u2019ve proven) a good measure by which to select examples for the synthesizer.
3319,3," I think ideally, I would want to see this on Atari or some of the continuous control domains often used."
3320,1," \n\nThe predictions in Section 3 appear to be very good, and it is nice to see the ablation study."
3321,3,\n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme?
3322,3, For a class project this could get an A in a ML class!
3323,3, It may also be interesting to consider class-specific representations that are more general than just the class label.
3324,3,"""The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language."
3325,3," Would be interesting to see the time/accuracy frontier."""
3326,3,"\n\n5) the experiment in section 6.1, figure 5 is just slightly different from that in figure 3."
3327,3,"\n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations."
3328,3,  There is a PIR per level and feature defined in C4.
3329,2,"While the problem is a very important one for modern society, the topic and lessons are not of broad interest"
3330,3,"  Instead of using Monte Carlo approximation as in the traditional random features literature, the main point of the paper is to learn these Fourier features in a min-max sense."
3331,3, How does the timing compare to Isola et al\u2019s Conditional GAN?
3332,3, It would be also useful to see a comparison of different CNNs and see how the observation holds more or less depending on the performance of the network.
3333,1," The answers to the critiques referenced in the \n paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience."
3334,3,  Just because increasing width may be better than increasing depth does not mean that deep layers don't learn useful representations.
3335,3," However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic."
3336,3," The main result here, is to show that using orthogonal random features approximates well the original kernel comparing to random fourrier features as considered in PSRNN."
3337,3," Please explain the difference."""
3338,1,"""Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay."
3339,3," This suggests that those constraints (lower overall connectivity strengths, and lower metabolic costs) might play a role in the EC's navigation function."
3340,3,"""The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of \u201chard\u201d functions that are not easily representable by shallower networks."
3341,1," It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses."
3342,1,\n--The maths is very rigorous.
3343,3,"""An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated."
3344,1, The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster.
3345,3," In particular it demonstrates adversarial training of a recurrent generator for an ICU monitoring multidimensional time series, proposes to evaluate such models by the performance (on real data) of supervised classifiers trained on the synthetic data (\""TSTR\""), and empirically analyzes the privacy implications of training and using such a model."
3346,3," It is straightforward if two policies are to be mixed. Although the mixing method is more reasonable than the genetic crossover operator, it is strange to compare with that operator in a method far away from the genetic algorithm."
3347,3,\n\n4) One extra line of derivation would be helpful for the reader to rederive the bound|w|^2/2sigma^2  <= O(...) just above equation (4).
3348,3, but where are the scores for each span?
3349,2,"This paper reads like a womans diary, not like a scientific piece of work"
3350,3, The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned.
3351,3,"""The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension."
3352,3,"""Summary:\n\nThis paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations."
3353,3," At if we only care about running the net, then reducing the time from 0.4s to 0.2s doesn't seem to be that useful (maybe it is for real-time predictions?"
3354,3,  The open-world related tasks have been defined in many previous works.
3355,1,".\n\nThe results a are convincing, even if they are not state of the art in all the trials."
3356,3," The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant."""
3357,3,  I think another way of teasing apart such results would be recommended.
3358,3,"\n\n- Depth versus width: Another measure that is missing is also the number of feature maps how wide is the network , how does this interplays with the depth?"
3359,3,"\nc.\tFor BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)?"
3360,3,\nHow is this actually done in practice?
3361,2,"A great deal of effort has been expended here, but to what end?"
3362,3,Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model.
3363,1,"\n\nIn summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation."
3364,3," Altogether, it seems like this paper contains a significant amount of additional text beyond what other submissions enjoyed."
3365,3,". I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent."
3366,3,"\n\nThis makes me wonder whether the \""complex numbers\"" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in \""Compressing neural networks with the hashing trick\"" by Chen et al.)."
3367,3, How does performance change with different amounts of training data
3368,3, The paper first relies on convolutional neural networks to extract image features.
3369,3, \n\nChapter 2 provides a sort of a mini-tutorial to (Bayesian) model selection based on standard Bayes factors.
3370,1,n- good problem for FA algorithm / well motivated
3371,3, The matrix U is learned from the data as well.
3372,1," The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task"
3373,3,"""In recent years there have been many notable successes in deep reinforcement learning."
3374,3,\n\nYou never explicitly mention what your training loss is in section 5.1.
3375,3," \nTo avoid the intractable marginalization over latent variables, the paper applies variational inference to approximate the posterior within the context of given training data."
3376,3," \n\nI have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection."
3377,3, Could the authors prioritize clarification to that point !
3378,1,"\n\nSignificance\nThe work could be potentially significant,"
3379,1,\n\nPros:\n1. Very well written paper with good theoretical and experimental analysis.
3380,3," 1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16."
3381,3,"""The paper considers a problem of adversarial examples applied to the deep neural networks."
3382,1,"\n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table."
3383,1," The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task."
3384,2,"As I understand it, you already have good evidence (youve said as much on Facebook). There's no reason to leave this as an open question."
3385,3,\n-The evaluations rely on using a pre-trained imagenet model as a representation.
3386,3, \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits
3387,1,\n\nThe paper is well-written and easy to follow
3388,1," Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance."
3389,3,"Unfortunately, there isn't much insight to be gained from them."
3390,1,\n\n\nComments:\n\nThe paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed.
3391,1, but in some settings it seems very useful.
3392,3,What is the difference between the left and right plots?
3393,3,"\n\nIn addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training."
3394,2,DO NOT have your heroes set the villain on FIRE. Its actually a war crime andâ€¦makes the heroes look like major arses.
3395,3," Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works."
3396,3, \n\nThe presentation could also be improved with some language edits.
3397,3," Moreover, only some architectures appear to employ batch normalization, which is a potent regularizer."
3398,3,\n\n3) Regularizing deeper networks: in the experiments of varying the length did you see if the results can be stabilized using dropout with deep networks and small feature maps?\n\n4) between training g and h ?
3399,3," At which point would it break?"""
3400,1," But, the LTMN significantly outperforms the baseline solver even in the training set."
3401,3," As far as I know, this has been done in several previous works, such as: (a). Hierarchical question-image co-attention for visual question answering (https://scholar.google.com/scholar?oi=bibs&cluster=15146345852176060026&btnI=1&hl=en). Lu et al. NIPS 2016."
3402,3,"\n\nI only have a few small questions/comments:\n* A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size. "
3403,1,"\n\nMy main concern about this paper is that although the presented techniques work well in practice,"
3404,1," \u201d\n\nOverall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well-written, well-presented and addresses an important problem."
3405,3,\n\nThe experiments compare with the recent relevant literature.
3406,2,"I am concerned that the survey data for this report were collected in 2005, 15 years ago."
3407,3,"  E.g., we would also like to learn synonymy with light verb like \""take note\"" or \""pay attention\"" means roughly \""notice\"" or \""observe\""; or the widely studied SVO triples like <rock,sank,ship> would also seem to cry out for a tensor decomposition."
3408,3,"\n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning."
3409,3,\n\nAlmost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes.
3410,3," It is a pity that not also more general image classification has been considered (CIFAR100, ImageNet, Places365, etc), that would provide insights to the more general behaviour of the proposed ideas."
3411,3,"\n\nThe attention score is then a function of these concatenated vectors i.e. \\alpha_{ij} = \\exp(S(HoW_i^C, HoW_j^Q))"
3412,2,"The authors merely used somewhat
bigger guns than previous studies and generated nothing but more smoke."
3413,3," \n\nTwo ways I could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build \""defoggers\"" for other domains (and spelling out more explicitly what domains the authors expect their insights to generalize to), or doubling down on the StarCraft application specifically and showing that the defogger helps to win games."
3414,3," In the last paragraph of page 7 the authors mention that eigenoptions are more effective in reducing the diffusion time than \u201crandom options\u201d built based on randomly selected sub-goals. However, looking at Figure 4, the terminal states of the eigenoptions look a bit like randomly-selected  sub-goals."
3415,3,"  In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch."
3416,3,"""The paper takes a recent paper of Zhang et al 2016 as the starting point to investigate the generalization capabilities of models trained by stochastic gradient descent."
3417,1, Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.
3418,3," For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside).\"
3419,3," While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry."
3420,3,"\n\n- For measuring performance, authors employ keystroke saving rate."
3421,1, \n\nPros: \n- The problem is relevant and also appears in similar form in domain adaptation and transfer learning.
3422,2,The sthanthard of writing is impercable
3423,3," Even tough not mandatory, it would also be a clear good addition to also demonstrate more convincing experiments in a different setting."
3424,3, It would be interesting to compare these algorithms as well.
3425,3,"\n6.\tTypo: Dataset section, phrases --> phases."
3426,3," Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers."
3427,3, I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods.
3428,3, Neal (1994) and Williams (1997) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions.
3429,3," However, differentiating TreeQN also amounts to back-propagating through a \""single\"" trajectory in the tree that gives the maximum Q-value. "
3430,1, A novelty of this work seems to be transforming a graph into an image.
3431,1,\n\nUsing the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monotone.
3432,2,I felt like I was reading a horror movie
3433,1," Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop."
3434,3, This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor.
3435,2,However the paper has several fundamental flaws that give it little or no value as a thoughtful piece of research
3436,3,"\nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals."
3437,3,"  It points out that the adversarial logistic distance can be viewed as an iteratively reweighted empirical estimator of the MMD distance, an interesting analogy (but also showing the limited power of the adversarial logistic distance for getting good generating distributions, given e.g. that the MMD has been observed in the past to perform poorly for face generation [Dziugaite et al. UAI 2015]). "
3438,2,The map on the article is entirely unscientific. The data itself is also very dubious.
3439,3, Was the model architecture tuned based on the proposed representations? 
3440,3, Did you do a grid search to find the optimal level of sparsity at each level?
3441,3," Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support."
3442,3,"""The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn."
3443,1,"  The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well. "
3444,3,"\n\n*Based on the rebuttals and thorough experimental results, I modified the global rating."
3445,1," Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation."
3446,3,"\n\n- I don't think the content on pages 12, 13, and 14 adds much to the\n  paper---consider moving these to an appendix."""
3447,2,I would rather read a meta-analysis
3448,3, I suspect this lower number of particles might be model-dependent.
3449,1,"\n\nSIGNIFICANCE: I think the paper addresses very interesting problem and significant amount of work is done towards the evaluation, but there are some further important questions that should be answered before the paper can be published."
3450,3,- sec4: the acronym L2HMC is not expanded anywhere in the paper\n
3451,3,"\n- MMI was also widely used in HMM/GMM systems, not just NN systems"
3452,1,"\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified."
3453,3,\n\nWhy is hard attention (sec 3.3) necessary?
3454,1,"\n\nThe first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge."
3455,3,"\n\nThe experimental part establishes a baseline using standard seq2seq models on the new dataset, by exploring large variations of model architectures and a large part of the hyper-parameter space."
3456,1," In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel."
3457,3,"\n- in the introduction (page two) the authors refer to SST prediction as a 'relatively complex physical modeling problem', whereas in the conclusion (page ten) it is referred to as 'a problem of intermediate complexity'. This seems to be inconsistent."""
3458,1," This is good work: it is well written, the experiments are thorough and the proposed method is original and works well."
3459,2,"I was originally very excited to review this paper, since such a bridge would span uncharted lands â€“ here be dragons!"
3460,3, Have the authors considered this issue?
3461,3, Experiments on MNIST are provided to analyse what this approach learns.
3462,3,If that are you reusing the frames you used during training to distill?
3463,3, Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM)
3464,3," In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow."
3465,1,\n- The paper points out situations when the methods are equivalent
3466,3,but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations
3467,2,"The authors spelling of coordinate, while technically correct, is arcane and annoying."
3468,2,The length of this review is occasioned by the density of error and misconceived arguments in this manuscript.
3469,2,"I urge the authors to not publish this article anywhere, as it will impede the progress of scientific understanding."
3470,1,\n- Most of the results are qualitative
3471,1, The observation on multiplicative compositionality is the main strength of the paper.
3472,1,\n\nThis is an interesting work. 
3473,2,"If I never see another piece of writing started with these lines, it will still be too soon."
3474,3, The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved).
3475,1,"\n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods. "
3476,2,They present an irresponsibly unbalanced literature review of the issues at hand and design an experiment that is ill suited to address the stated aims of the paper' 1/
3477,3, 2. Robotic knot-tying with a loose rope where visual input of the initial and final rope states are given as input.
3478,3,"""This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence."
3479,3, Proc R Soc Lond B Biol Sci 216: 427\u2013459. pmid:6129637
3480,2,"entire review:] 'Research method is very important; however, the reviewer cannot accept a paper without hypothesis, validity, reliability"
3481,3," \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background."
3482,3,? How much translation
3483,3, I am not familiar with the state-of-the-art methods in this field.
3484,3," It is stated that at the beginning of the training, or after increasing the batch size, the magnitude of parameter updates is suppressed because $A$ has to accumulate gradient signals over a time scale $B/(N(1-m))$."
3485,3,"""A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory."
3486,1, It seems the implicit SGD approach is better in the experimental comparisons. 
3487,3,  \n\nThe authors perform numerous empirical experiments on several types of problems.
3488,3," I suspect that the results will look different when plotted in different ways, and would enjoy some extra plots in the appendix."
3489,1," everything is very clearly explained,"
3490,2,This would seem to constitute the very minimum basic scientific requirement for attempting to publish...
3491,1, Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods.
3492,3,Pipeline: -Data are augmented with domain-specific transformations
3493,3,n- We already have good algorithms that take subtask graphs and execute them in the right order from the planning litterature
3494,3,\n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved).
3495,3,"\n\n2. One of the conclusions of the paper is: \u201cAlthough CNNs do not intrinsically classify objects based on their shapes, they can learn to do so when trained with enough number of images with the same shape and different colors."
3496,3," b) insight into the connection between MAML and MAP estimation in non-linear HB models with implicit priors,"
3497,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
3498,3," In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels)."
3499,3," In particular about commonly accepted intuition for avoiding exploding/vanishing gradients and why all the recent bag of tricks (BN, Adam) do not actually address the problems they set out to alleviate."
3500,2,Arrestingly Pedestrian is both insulting and a great band name
3501,3,  Further I found it not surprising that the PIR changes when a highly parameterised model is trained for this task.
3502,3,"""This very well written paper covers the span between W-GAN and VAE."
3503,3, I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before.
3504,3,". The network proposed here, FusionHet, fixes problem."
3505,3, \n\nThe proposal is practical.
3506,3, The author's choice of tasks seem somewhat artificial in that they impose time limits on otherwise unlimited domains in order to demonstrate experimental improvement.
3507,3,"  For example, the probability p_1 of 'pkoe' versus p_2 of 'zoze' as likely versions of 'poke' (as, say, the prefix of pokemon, as in your example) should be such that p_1 >>> p_2, not equal as they are in your model."
3508,3,"\n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides."
3509,3, Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.
3510,3,"""The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations."
3511,3," For the constraint structure you have, projection is trivial (just clip the values)"
3512,3, Learning the Speech Front-end With Raw Waveform CLDNNs.
3513,3,"""The paper proposes a piecewise linear activation function that is build on ELU."
3514,3,I am wondering how a variety of multi-task settings can be handled by the proposed approach.
3515,3,"""The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding."
3516,3, \n\n2. Can the author provide analysis on scalability the proposed method?
3517,3," In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet)."
3518,2,The results look like a smorgasbord of data
3519,3,"""The paper presents a means of evaluating a neural network securely using homomorphic encryption"
3520,1,\n\n- Quality/Clarity:\nThe paper is well written and pleasant to read\
3521,3,"""This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule."
3522,2,"The 'Theoretical Analysis' section is trivial and requires no analysis, as any sensible schoolkid can identify its solution"
3523,1,\n- The paper is easy to follow and clearly describes the implementation details needed to reach the results. 
3524,3,"""The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms"
3525,3, i.e |\\partial L / \\partial z \\partial w|<= B.
3526,2,"Yes measurements were made, but why, besides a teaching exercise, remains obscure."
3527,3, \n\nIt looks that this paper denotes the style of the target domain as y^* and assume that it is shared by all samples in X_2.
3528,3," When the game is symmetric, this might be \""the natural\"" solution but in general it is far from clear why all players would want to maximize the total payoff."
3529,1, I think such generalization is interesting but the innovation seems to be very limited.
3530,2,The team is very experienced. It [the paper] felt a bit less self referential than they often are
3531,3," This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective."
3532,1,"\n\n- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model."
3533,3,"\n\nFurther, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001)"
3534,3,"\nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin."
3535,3," The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).\n\n---\n\nQuality: The experiments compare the three proposed neural network architectures with two syntax-based architectures."
3536,1," Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea."
3537,3,"It would have been great had the authors mentioned that u_j \\in {0,1}. "
3538,3, but it is significant given the lack of theory and understanding of deep learning.
3539,3," This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. """
3540,2,This paper is very weak
3541,3, (why should they be different?)
3542,3, but isn\u2019t this just a deeper model with shared weights? And a max operation?
3543,2,"Given the dodgy sampling technique, one could and should wonder how useful it is to include a variable with so little explained variance"
3544,3," \n- typo, find the \u201cmost orthogonal\u201d representation if the inputs -> of the inputs "
3545,3, While I agree that a robust and fine-tuning-free model is ideal 1) this has to be justified by experiment. 2) showing the experiment with different parameters will help us understand the role each component plays.
3546,2,Future work: The authors personal research agenda is irrelevant here.
3547,3,  I think this should have been one of the baselines to compare to for that reason.
3548,2,The Discussion section of the paper is neither informative nor enlightening and is certainly theoretically questionable
3549,3,"\nIn fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see \""Decoding Distributed Tree Structures\"" and \""Distributed tree kernels\""."
3550,3," Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.\n"""
3551,1, Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines.
3552,1," This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time."
3553,1, The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective.
3554,1,"\n\nI recommend acceptance, despite some reservations."
3555,2,Nobody in their right mind would ever suggest such a model
3556,3, Is that correct?
3557,3, The authors\nshould clearly include a description of why Lipton's approach cannot be applied\nto the Atari games or include it as a baseline.
3558,2,The PI is an excellent networker but that he does not have sufficient required scientific expertise and capacity to successfully execute the project
3559,3,"\n- Many papers get rid of the HMM pipeline, I would add https://arxiv.org/abs/1408.2873, which predates Deep Speech"
3560,3,"  Also, what does \""step size\"" mean in the TR method?"
3561,3, So my rating based on the message of the paper would be 6/10.
3562,1," The overall framework could be relevant to multiple areas in graph analytics, including graph comparison, graph sampling, graph embedding and relational feature selection."
3563,2,I recommend the publication even if I am not impressed
3564,3,"\n\nAppendix A isn't referred to from the main text as far as I could tell.[[CNT], [null], [DIS], [GEN]] Just merge it into the main text?\n\n\n\n"""
3565,1,\nExperimental results show that this outperforms an epsilon-greedy baseline.
3566,1, \n\nPros: \n\n- The network compression problem is of general interest to ICLR audience.
3567,3," In section 3.3 part Ablation Study on Features Sets, line 5, the sentence should be \u201cAp are more important than HP\u201d.\n"""
3568,3,"""The main idea is to use the accuracy of a classifier trained on synthetic training examples produced by a generative model to define an evaluation metric for the generative model. "
3569,3,\n\n2. Label embedding learning has been investigated in many previous works.
3570,3, The teacher model learns via reinforcement learning which items to include in each minibatch of the data set.
3571,1,"""The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified, "
3572,3,"  These two sections would benefit from a more careful layout of the process, what is going on in a forward pass, a backward pass, how does this interact."
3573,3,\n\n- Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated.
3574,3, Why is the complexity O(r^3) independent of the parameter d?
3575,3,\n2. The generalization over TT makes sense.
3576,1," It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today."
3577,1,"\n\nOriginality: The works seems to be relatively original combination of ideas from Bayesian evidence, to deep neural network research. "
3578,3, Usually it would be defined as $\\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$
3579,2,I apologize for this but frankly some parts read like a report of a high-school student on a scientific experiment.
3580,3,"\n\n- The derivations in pages 4-6 are somewhat disconnected from the rest of the paper: the optimal baseline derivation is very standard (even if adapted to the slightly different situation situated here), and for reasons highlighted by the authors in this paper, they are not often used; the 'marginalized' baseline is more common, and indeed, the authors adopt this one as well."
3581,3,"""The paper proposes to generate embedding of named-entities on the fly during dialogue sessions."
3582,3," One can see the merits in employing a hierarchical action space, whereby decision making operates over high-level actions, each associated with low-level controllers, but that the adopted formulation is not fundamental to this abstraction."
3583,3,"""The work claims a measure of robustness of networks that is attack-agnostic."
3584,3," Also, I see the advantage of referring only to the \""good\"" quantiles when needed."
3585,3, The use of learned representations needs more rigorous justification
3586,3, This model does not use any recurrent operation but it is not per se simpler than a recurrent model.
3587,3, As it is the closet method to this paper it is essential to be compared against.
3588,3," If the text is from the user, a named entity recognizer is used."
3589,3," On the other hand, the usefulness of the learned representation for planning is unclear."
3590,3,"""This paper studies active learning for convolutional neural networks"
3591,2,"For a section on thought, very little seems to have gone into it."
3592,3, I believe the paper could justify this approach better by providing a bit more insights as to why it is required. 
3593,1," Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc."
3594,3, Do all objectives happen to yield their best performance under the same LR?
3595,3,"\n\nI would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.)."
3596,3," \n\nFour simple, shallow neural nets are built as query, write, value, and policy networks."
3597,3,"\n\n[1] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. \""Sparse deep belief net model for visual area V2.\"" Advances in neural information processing systems. 2008."
3598,3,".\nAlso Astrid and Lee 2017 do not seem to report the instabilities during fine-tuning of the decomposed layers, and argue that these layers should not be freezed."
3599,3, How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)?
3600,1," One aspect does does strike me as novel is the \""gated composition module\"", which allows differentiation of messages to other agents based on the receivers internal state."
3601,2,"It is at best of little value and, in the worst case, irrelevant and offensive"
3602,3,"\n\n- Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct?\"
3603,1, It is also impressive how much faster their model performs on tasks without sacrificing much performance.
3604,3,\n\n* Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info.
3605,3, This should be made more\nclear in the paper.
3606,3,"\n\nThe approach is based on `profile coefficients\u2019 which are learned for every channel in a convolution layer, or for every column in the fully connected layer."
3607,1, I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset.
3608,3, It would enhance readability of the paper if the results were more self-contained.
3609,3, They are train of the same machine or cluster in a controlled manner by the person that would use the system.
3610,1," Overall I am weakly inclined to accept this paper."""
3611,3,"  gan is an interesting idea to apply to solve many problems; it'll be helpful to get the intuition of which properties of gan solves the problem in this particular application to discrete autoencoders."""
3612,3, The paper describes the use additional mechanisms for synchronization and memory loading.
3613,3," To be clear, I think that even if the proposed approach were to be slower than the state of the art it would still be very interesting."
3614,1,"\n\nFinally, I should say that TAGCN idea is interesting."
3615,1,"\n\nOverall, I really enjoy reading this paper and recommend for acceptance!"
3616,3,"""This paper includes several controlled empirical studies comparing MC and TD methods in predicting of value function with complex DNN function approximators."
3617,2,This sort of presentation issue continued through the first few sections (after which I was reading with decreasing attention
3618,3, (a) why are non-overlapping architectures so common?
3619,3,\nRotation and scale from the polar origin result in translation of the log-polar representation.
3620,3, I would have liked to have seen results on ImageNet.
3621,3,\n\nThe conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective
3622,3," \n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation."
3623,3,"\n\n-dealing with partial observability (work on decentralized POMDPs by Peshkin et al, 2000, Bernstein, Amato, etc.)"
3624,3,"However, it is very hard to generalize from these toy problems."
3625,3,"\nThe proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people."
3626,3, In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion.
3627,3," \n\nThe images have been modified wrt. to some noise, such that they deviate from the natural statistics for images of that kind."
3628,3," In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner's dilemma game."
3629,1," It is interesting that the mixed objective (which targets F1) also brings improvement on EM. \n"""
3630,1, Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.
3631,3," The reduction of performance could mainly cause by seeing fewer raw images, but not the labels."
3632,3,"\n\nIn Tamar's work, a mapping from observation to reward is learned."
3633,3," In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network."
3634,3,\u201d: Why not just compare the optimal with the AIS evaluation?
3635,3," \n\nOther than that, I have some other comments"
3636,1,"""This paper presents a novel application of machine learning using Graph NN's on ASTs to identify incorrect variable usage and predict variable names in context."
3637,1," To me, this approach is the right way of constructing control variates for estimating policy gradient."
3638,3," There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \""evolutionary\"" computing."
3639,3," The authors specifically examine random subsampling (which is the same as random masking, with different weights) and probabilistic quantization, where each element of a gradient update is randomly quantized to b bits."
3640,3,"""The paper introduces Topology Adaptive GCN (TAGCN) to generalize convolutional\nnetworks to graph-structured data."
3641,3,\n4. evaluates different f within MCTS for MiniRTS.
3642,3,"  To\"": Where is Table 4.1??"
3643,3," In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned."
3644,3, \n(ii) What is a one-hot label?
3645,3,  The weights are based on the confidence of the value function of the n-step return.
3646,1," \n\nPros: \n\n- The paper is clearly written, self-contained and a pleasure to read."
3647,3,"""The paper represents an empirical validation of the well-known idea (it was published several times before) \nto increase the batch size over time."
3648,3,"\n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$."
3649,3, Should $\\phi^*$  be $\\hat \\phi$ ?
3650,1, The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.
3651,3, It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs.
3652,3,"  However, there are a number of statistical issues that should be addressed."
3653,1,"""Quality\nThe paper is well-written and clear, and includes relevant comparisons to previous work (NPI and recursive NPI)."
3654,3," and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems. """
3655,3,"\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. "
3656,1,"\n\nWhile this paper is as far as I can tell novel in how it does what it does,"
3657,1," Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models."
3658,2,"Moreover, it is unclear whether the effect is sufficiently important to warrant replication."
3659,3, and the experiments also need to be improved
3660,3," This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations."
3661,2,"I think this article is carefully argued, but at no moment is it stated that Heidegger was a Nazi."
3662,3, This is worked around by updating the parameter using the sign of its gradient.
3663,3,"\n\nThis is true for the CIFAR net but the opposite is true for ResNet, right?"
3664,3,"\n\nI like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model."
3665,1," \n\nCooperative multi-agent problem solving is an important problem in machine learning, artificial intelligence, and cognitive science."
3666,3, The paper states that it builds upon the formulation in [Defferrard et al. 2016] as explained in section 3.
3667,3," In their experimental results, the phase transition is not observed anymore with their protocol."
3668,3, It tries to provide an explanation for the phenomenon and a procedure to test when it happens.
3669,1, I found the yelp transfer results particularly impressive.
3670,3," The relevance of perceptual image realism to the intended task (control) is not substantiated, as discussed earlier."
3671,1,"""1. This is a good application paper, can be quite interesting in a workshop related to Deep Learning applications to physical sciences and engineering."
3672,3," The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms."
3673,3,"""This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples."
3674,3,\n\nThere are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy?
3675,3,"\n  \u2022 [p4, Sparse rewards] I am not sure it is fair to say that the general difficulty is kept fixed."
3676,1,"The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance."
3677,3, Please add the Swiss roll example.
3678,3, A rudimentary  experimental evaluation with small networks is provided.
3679,1," \n\nOn the positive side, the task is a nice example of reasoning about a complex hidden state space, which is an important problem moving forwards in deep learning."
3680,1," In principle, the idea seems to be clear,"
3681,3,"""Quality\n\nThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data."
3682,3,\nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions).
3683,3,\n\nCould you also run experiments on the real-world datasets used by the 3BE paper?
3684,3," The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data."
3685,3,"  However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words."
3686,2,"While the authors do pick a good problem, thats where the quality of the paper ends for me."
3687,3, They demonstrate that proposed model could work better and rational of write network could be observed.
3688,2,The author is tilting at windmills.
3689,3,"""The authors try to combine the power of GANs with hierarchical community structure detections."
3690,1,"\n- This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results."
3691,3," In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning."""
3692,3, The \u201czero-shot\u201d refers to the fact that all learning is performed before the human defines the task.
3693,1,\nThe paper benefits from such a relationship and derives an actor-critic algorithm.
3694,3,"\n\nThe paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge."
3695,3," What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation."
3696,1," While the idea is sound,"
3697,3, In principle IS and SMC can achieve arbitrarily high accuracy by making K astronomically large.
3698,3," To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (\""x-space\"" of the GAN), or varied the magnitude of filters or filter planes."
3699,3,"  If so, the training time amortizes to some extent\u2014can you quantify this?"
3700,3,  The method is demonstrated to the generate better results than the baseline on a variety of datasets and noise processes.
3701,3," In this paper, the authors proposed a conditional way to generate images compositionally."
3702,1,\nClarity: The paper is clearly written.
3703,3," The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention."
3704,3," \n\nPath-wise training is not original enough or indeed different enough from drop-path to count as a major contribution.\"" Veit et al only claimed that ensembling is a feature of ResNet, but they did not claim that this was the cause of the performance of ResNet."
3705,1,\n\nThe choice of early stopping is a very interesting problem especially for the EO-creitenrion.
3706,3,\n(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words?
3707,3, This is of course always possible in the MDP formalism.
3708,3," Maybe there is something deeper going on here, but it is not obvious to me."
3709,3,"""This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (\""n-grams\"") which can be queried efficiently."
3710,3,"\n\np 1: authors write: \""Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012)."
3711,3,\n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated.
3712,1,\n\nI like this paper.
3713,3,"""This paper considers distributed synchronous SGD, and proposes to use \""partial pulling\"" to alleviate the problem with slow servers."
3714,3,"\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017)."
3715,3, Were hyperparamters fixed for training across different feature sizes of VGG19 and ResNet-152?
3716,3,  We want to infer a posterior density given some data.
3717,3, The overall effect would just be to raise the\nmagnitude of logits across the entire softmax.
3718,3,  The improvement is in the noise.
3719,3,"\n\nSection 4. 1 \u201c Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle."
3720,1,  Bold everywhere or nowhere
3721,3," However they advocate that a better candidate is what they refer to as \""semantic space\"" formed by embedding the (word) labels of the images according to pre-trained language models like word2vec."
3722,3,but I find (as a non-meta-learner expert) that certain fundamental aspects could have been explained better or in more detail (see below for details).
3723,1,"\n\nOverall, I think this is an interesting paper,"
3724,3," ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011)."
3725,3, Theorem 5 should be phrased as \n\nall critical points of the population risk that is non-singular are global minima.
3726,3,\n\nThe authors also report in table 2 the scores obtained for DDQN by Osband et\nal. 2016.
3727,3,"""This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation."
3728,3, This idea has been around for a long time (although I\u2019m having a hard time coming up with a reference).
3729,3," There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy."
3730,2,The following paragraph will strike many of your readers as shrill. They will stop reading the article and throw it into the fire
3731,3," \n\nOnce we learn the similarity function, the rest of the approach is straightforward, without any particular technical ingenuity."
3732,3,"\n\nA few suggestions for experiments:\nA. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric."
3733,3," Rather use something more standard, with well-known baselines, such as the taxi domain."
3734,3,"\n\n- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper."
3735,3, They conclude with empirical observations about the performance of this algorithm.
3736,3, The related work section mentions Generative Adversarial Nets.
3737,3, And the parameter sharing in the parametric one might actually be beneficial.
3738,3, They suggest that training on such signal can be beneficial when training deep models on complex perceptual input spaces.
3739,3,"""I thank the authors for the thoughtful response and rebuttal. "
3740,3,"\nThe contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models)."
3741,3,"""The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \""strong\"" network)."
3742,3,"""This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the \u201ccontent update\u201d values computed at each time step."
3743,3," This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments."
3744,3," In some cases it is good to outline a powerful and generic framework (like the authors did here with defining \""teaching\"" in a very broad sense, including selecting good loss functions and hypothesis spaces) and then explain that the current work focuses on one aspect (selecting training data points)."
3745,3," This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision."
3746,3," \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way."
3747,3," \n\nAs stated in the introduction, the authors think there are several drawbacks of existing methods including \""training instability, lack of topology generalization and computational complexity."
3748,1, The architecture achieves impressive results on two tasks: SNLI and the reverse dictionary of Hill et al. (2016).
3749,2,This work amounts to a form of methodological perfectionism. Perfectionism can be the enemy of the possible in science.
3750,1,\n\nPros\n\nWell-written and original contribution demonstrating the use of GANs in the context of neuroimaging.
3751,1, This is a novel use of existing methods.
3752,3," Do the results reported in e.g. Table 1 change as a function of beam width, and if so, how? \n"""
3753,3," Regarding the technical details, the reviewer has the following comments: \n\n- What's the limitation of this attack method?"
3754,1," I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods."
3755,1," \n3, The experimental results look convincing."
3756,1, The topic is important. Experiments on real data show improvements compared to several traditional approaches.
3757,1,"  On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships."
3758,3,  Both picking up the passenger (reachability) and dropping them off somewhere are essentially the same task: moving to a point.
3759,1,\n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee.
3760,2,The biggest value of this paper lies in the fact that its pages are bound together.
3761,1,"\n\nOverall the paper is written quite well, and the authors do a good job of explaining their thesis."
3762,3, The proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the imposter.
3763,1,"\n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop."
3764,3,"n\nSince the min-max problem is intractable in general,"
3765,1," \n\nThe method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training. "
3766,3,\n\nThe reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed.
3767,3," Say, show the affinity matrix between the words and the objects to indicate the correlations."
3768,3,\np3. Table 1. Where do the choice of CL Junction densities come from?
3769,3,"\n\n- While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem.\"
3770,1,\n\nStrengths:\n- The paper is well-written and clear.
3771,3,  The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator.
3772,3,"""This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process."
3773,3,"That result could make some predictions for experiment, that would be testable with chronic methods (like Ca2+ imaging) that can record from the same neurons over multiple experimental sessions."
3774,3,.\u201d Are these zero partial derivatives of the post-relu or pre-relu?
3775,3,"""This is a fine paper that generally reads as a new episode in a series on motion-based video prediction with an eye towards robotic manipulation [Finn et al. 2016, Finn and Levine 2017, Ebert et al. 2017]."
3776,3," In fact, an interesting experiment here would have been to do ablation studies e.g. by replacing the LSTM with a feed forward fully connected network."
3777,3, and c) when to stop performing actions.
3778,3, Is this really worth doing? 
3779,3," These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error)."
3780,1," This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks."
3781,1," A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1."
3782,3, \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions.
3783,3, Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.
3784,3,"""The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric."
3785,3, Again I would ask the authors to make these plots for FSGM.
3786,3,"\n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper."
3787,3," Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations."
3788,3,"""This paper combines a simple PAC-Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm."
3789,1, The experimental results are presented well for a range of Mujoco tasks. 
3790,3,  The related work include auto-encoders where the weights of symmetric layers are tied.
3791,3, This can be done by evaluating elementary symmetric polynomials at well-chosen values.
3792,3,  What does it represent?
3793,1,"\n\nIt is a well-written paper,"
3794,3," \n\u201cFor future work, a possible study is to investigate what classes of problems DRN can solve."
3795,3, The authors propose a new learning strategy called Q-masking which couples well a defined low level controller with a high level tactical decision making policy.
3796,3," For example, are there results which prove anything regarding the convergence of a stochastic process under different amounts of noise?"
3797,3,"\n\nAlso, the claim that \""we show efficient designs\"" is very thin to me since there are no experimental comparisons between the proposed method and existing works."
3798,3,"\n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics."
3799,1,\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.
3800,2,This is not a paper. This is part of â€¦ something. It cannot be reviewed and should be rejected right...
3801,1," The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels."
3802,3,"""This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally."
3803,3,\n\nHow much can change between the goal images and the environment before the system fails?
3804,1, \n+ strong experimental results
3805,3,\n\nWith my evaluation I do not want to be discouraging about the general approach.
3806,3,"\n\nIn the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified."
3807,3, Does the discretization into strokes matter?
3808,3, One should note that the birthday theorem assumes uniform sampling. 
3809,2,"Do we need to clarify the meaning of, all?"
3810,3, There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work. 
3811,1, I think the approach is good and fruitful.
3812,3,"\n\nFor generative modeling: you do have guarantees that, *if* your optimization and function parameterization can reach the global optimum, you will obtain the best map relative to the cost function."
3813,1,"""The paper is interesting,"
3814,3,"""The paper presents an extension of the Xception network of (Chollet et al. 2016) 2D grids to generic graphs."
3815,1,\n\nWhat I like about the approach is the investigation of the interplay between unsupervised and hierarchical supervised learning in a biological context.
3816,3," Intuitively, the randomness in target may achieve certain regularization effect."
3817,3," During research, we have multiple executable oracles and need to produce good training data from them."
3818,3, The authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset.
3819,3, There is some empirical testing\nwhich show the presented method in a good light.
3820,3,"""The authors investigate a modified input layer that results in color invariant networks."
3821,3," E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc."
3822,1,"\n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected."""
3823,3,"The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution."
3824,3, How statistically valid are the results?
3825,3,"""Summary:\nThis paper proposes an approach to generate images which are more aesthetically pleasing, considering the feedback of users via user interaction."
3826,3, Other AnonReviewer also point out some similar work.
3827,1,\n\npro:\n- Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work\n- Easy to read and follow
3828,3," However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks."
3829,3,\n\nThe UIs that are represented in the dataset seem quite simple; it\u2019s not clear that this will transfer to arbitrarily complex and multi-page UIs.
3830,3, Could there be a way to measure the invariances?
3831,3, The architecture includes a differentiable warping scheme which allows back propagation of the error and is inspired by the fundamental solution of the PDE model. 
3832,3," On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment."
3833,3,"\n* The work on multimodal embeddings like \""Multimodal Distributional Semantics\"" by Bruni et al. or \""Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception.\"" by Kiela et al. could be discussed/cited."
3834,3,"\n3. Fix notation on page 3.[[CNT], [PNF-NEG], [SUG], [MIN]] Dot is used on the right hand side to indicate an argument but not left hand side for equation after \""with respect to \\lambda\""."
3835,3,"\n - According to \""SPICE: Semantic Propositional Image Caption Evaluation\"" current metrics used in image captioning don't correlate with human judgement."
3836,3, but it could be more fluent.
3837,3," The method includes two independent components: an \u2018input defender\u2019 which tried to inspect the input, and a \u2018latent defender\u2019 trying to inspect a hidden representation."
3838,1, Rest of presented work is more or less standard.
3839,3,"  \n\n- In Eq 3, please be explicit as to whether the '+' is here a concatentation or an actual (element-wise) sum?"
3840,3," Second, it is unclear how close the proposed method is to finding the optimal regularization parameter \\lambda."
3841,3,". They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined."
3842,1,"\n\nThe paper is written well, good to understand, and technically sound."
3843,3,\n3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task.
3844,1," Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more."
3845,3, Will it converge faster?
3846,3,"n\nIt would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.\"
3847,3," For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled?"
3848,3, Or it's trained jointly with PCN and OCN? 
3849,3," The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network."
3850,1, I recommend the paper for acceptance and expect it will garner interest from the community.
3851,3,"\n\nFor future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable AI in the first place."
3852,3,\n\nThe baseline methods result in rewards much lower than those in previous experimental papers.
3853,3,"\n\n\nComments on prior work:\n\np 1: authors write: \""vanilla backpropagation (VBP)\"" \""was proposed around 1987 Rumelhart et al. (1985)."
3854,3,\n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from.
3855,3," Although \\tau goes to 0, there is still a gap between Q and Q'."
3856,2,eviewer 2: 'THOU SHALL AWAIT MY JUDGEMENT... FOR SIX  EARTHLY YEARS
3857,1,"\n\nThe idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales."
3858,1,"\n\nOriginality\n\nWhile others have pointed out limitations before, this paper considers relational networks for the first time."
3859,3,"""This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences."
3860,3," However, then what is learned though the proposed formulation?"
3861,3, I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information).
3862,1,"\n\n----\nAfter reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper."
3863,3,  The authors argue for the advantages of a generative VAE approach (but without being convincing).
3864,3,\n\n3. The fact that partial observability helped to alleviate the credit-assignment noise caused by the missing customer penalty might be an artefact of the setting.
3865,3,"  Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)\n\n"
3866,3, The paper describes a classifier that predicts the dialog act of the next utterance.
3867,1,\n\nPros:\n - clear definitions of terms\n - overall outline of paper is good\n - novel metric.
3868,3," \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training."
3869,3," \nTo learn the function that maps states into subgoals, the authors propose a surrogate reward model that corresponds to the angle between: the difference between two consecutive states (which captures velocity or direction) and a given subgoal."
3870,1,"\n\nThese results are interesting,"
3871,3,"  In this section, why are you not reporting the results from the original Show&Tell paper?"
3872,1," We find this \u201chybrid\u201d method is both fast and accurate, for both small batch and large batch."
3873,3, The\nonly difference is that Lipton et al. use variational inference with a\nfactorized Gaussian distribution to approximate the posterior on all the\nnetwork weights.
3874,3," Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE-style bounds (w.r.t. the inference parameters) grows with their accuracy."
3875,2,"The authors use a log transformation, which is statistical machination, intended to deceive -.."
3876,3,  I have a few suggestions along the text but nothing major.
3877,1, the authors propose a simple and robust approach for doing it by using the value function estimation network of A3C.
3878,3," A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations.\n\nDetailed:\n-\n"""
3879,3," In this light, the performance relative to the baselines is particularly important."
3880,3,\n\n* Figure 1: What is the x-axis here?
3881,3," Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label)."
3882,3, A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards.
3883,3,"""Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks."
3884,3, I would also be quite interested to see the performance of the proposed algorithm for different values of parameters (such as the butch size).
3885,1," But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR."""
3886,3,"""Science is about reproducible results and it is very commendable from scientists to hold their peers accountable for their work by verifying their results."
3887,3, Classifying digits? Recreating digits?
3888,3,"\n- Figure 4 shows \""most interpretable mixture components\""."
3889,1, THe additional experiments are a good addition.
3890,3, RL results are reported with only the best-performing attention setup for each dataset.
3891,3,"  If it had better (e.g. ROC curve) evaluation and some modeling novelty, I would rate it higher still."
3892,1,\n\nSome discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate.
3893,1, Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10.
3894,3,"  For example, for Conv4, how many channels at each layer?"
3895,2,I really want to like this study. ? Crap. It's me. I'm stopping you
3896,3,"\""\n- In general, tables that report averages would do well to report error bars as\n  well."
3897,3, Especially the work builds upon the recent work by Haarnoja et al (2017) and Schulman et al (2017) (both unpublished Arxiv papers).
3898,3,"  Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases."
3899,3," The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size."
3900,3,"\n\nOverall, this paper feels like a small improvement over RN."
3901,3,"""The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning."
3902,3," This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations."
3903,3, The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally.
3904,3,"""This work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster, which represents a set of similar data point, e.g., sentence, according to a metric."
3905,3," Also, the detailed specification of the VAE should be detailed."
3906,3,\nEq. (10) uses subscript d where it should be using subscript n\nEq.
3907,1," In particular, the latter problem is well posed and has good properties (see, e.g., Lim, Comon. Nonengative approximations of nonnegative tensors (2009))."
3908,3,"""In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network."
3909,3, I suspect it might be speaker identification.
3910,3,"\n\nFinally, what are the savings from reducing this time complexity?"
3911,1,  Glad to see that the code will be released.
3912,3, \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5).
3913,1,"\n\nBased on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization."
3914,2,"There may be a good paper crying to be let out of this manuscript but if so, it is hard to know."
3915,1,n\n3. Quite good results on several relevant tasks.
3916,3," If so, how?"
3917,3,"  Finally, the authors notice that \""In the rotation-invariant case, where \u03a9 is a discrete set, heuristics are available\""."
3918,2,Note that you failed to provide to provide the contribution number in the acknowledgements. The paper is NOT COMPLETE!
3919,3," The theoretical analysis shows that if the loss is strongly convex, then the algorithm returns a solution which is close to the optimal solution."
3920,3, The key innovation here is to use pixel saliency as a channel for the adversarial example detector.
3921,3,\n\nTable 4 is confusing.
3922,3,"More generally, the paper is riddled with non sequiturs."
3923,3,\n\n* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks.
3924,1, This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps.
3925,3, but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used).
3926,3," In addition, the point made by the TCML authors is fair (\""nothing prevented you from...\"") and I would also recommend mentioning the reviewed paper's authors' decision (not to test deeper architectures) in the footnote."
3927,3, The authors only experiment with MNIST dataset.
3928,3," However, it would have been great to have an experiment that actually makes use of the learned features to make predictions."
3929,3, I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications).
3930,1,\n\nPros:\n1. A simple approach to encourage better representations learned from unlabeled examples.
3931,3, The new results are quite informative and addressed some of the concerns raised by me and other reviewers.
3932,3,"\n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well."
3933,3,\n\nThe approach raises the natural questions of where the tasks and the task graphs come from.
3934,3," Unlike in this previous work, the approach proposed here induces this behavior though a multi-agent reference game. "
3935,1,"\n\nOverall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.\n"""
3936,3," SDP while tractable, would still require very expensive computation to solve exactly."
3937,3, It shows comparable results with standard LSTM.
3938,3," In particular, they use a CNN encoder-decoder to learn a motion field, and a warping function from the last component to provide forecasting."
3939,3, Why not adopt it in CrescendoNet? 
3940,1,\n\nPROS: \nThe problem faced by the authors is interesting.
3941,2,"Bad, very bad."
3942,3," A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm."
3943,1, Nonetheless I find the results interesting to the RL community and a starting point to further analysis of the MC methods (or adaptations of TD methods) that work better with image observation spaces.
3944,3,"""This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision."
3945,3,\n\n- The improvement over the original models are of the order of less than 1 percent.
3946,3," If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1."
3947,3,"\n\nSpace was clearly not an issue with the paper, it still have available space to add further explanations"
3948,3,\n\n* I'd like to hear more about the effects of different parametrizations of the airfoil surface.
3949,3, Initial applications to domain adaptation and generative modeling are also shown.
3950,1,\n\nPros:\n(+) The paper is well written and the method is well explained
3951,3," These are the crucial questions related with fair comparisons, which I would like to know specific answers to make my final decisions."
3952,3," what are the effects of regulariazations in this regard? """
3953,1," The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making."
3954,1,"\n- Page 3: \""more computationally and\"" -> \""more computationally efficient and[[CNT], [CLA-NEG], [CRT], [MIN]]\""\n- Page 3: \""for performing final\"" -> \""for predicting final[[CNT], [CLA-NEG], [CRT], [MIN]]\""\n\n\nPoints in favor of the paper:\n- Simple method"
3955,3,\n\n\n6 The model size of CrescendoNet is larger than residual networks with similar performance.
3956,3," The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.\n\nI think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.\n\nFirst, Assumption 4 seems a bit too abstract."
3957,3,Is it possible that a different choice of hyperparameters can change the model ranking
3958,2,"If you arent going to make to the effort to understand, then you should just give up"
3959,3," Without such experiment, it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernels."
3960,3,Is it possible to use all the binary attributes in the dataset.
3961,3,"  \n* The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs, but it\u2019s not clear if they do this for the (vanilla) ResNet as well."
3962,3,"\nIt would also be nice to see the full learning curves for all experiments, where different stages (decompose->optimize->decompose->finetune->...) are explicitly marked."
3963,1, And I love the results. 
3964,3, \n\nSeveral figure captions should be updated to clarify which model and dataset\nare studied.
3965,3,"\n \nAll that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable."
3966,1,"""Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output."
3967,3,\n\n## Paper Summary\n\nThe paper examines the influence of batch size on the behavior of stochastic\ngradient descent to minimize cost functions.
3968,3,"\n\nFurthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs)."
3969,3," That\u2019s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair."
3970,1, It would be nice to see the behavior for different values of lambda.
3971,1,\n\nOverall the paper has very good motivation and significance.
3972,3," To this, they add channels based on low base quality, low mapping quality."
3973,3," As argued in Section 1, the ability of \nobtaining signals token-wise looks beneficial at first, but it will actually\nbreak a global validity of syntax and other sentence-wise phenoma."
3974,1,"  It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss."
3975,1, but it is a good idea to speed up the RC models.
3976,3, The authors then experiment with a variety of TD and MC based deep learners to explore which methods are most robust to increases in difficulty along these dimensions.
3977,3," Some related works are mentioned in the paper, but those are spread in different sections."
3978,3,"\n - more experiments showing a comparison of the proposed metric to others (e.g. inception score, Mturk assessments of sample quality, etc.) would be necessary to find the metric convincing"
3979,3,"  They also propose a new metric, motion accuracy, which uses the accuracy of the predicted position of the object instead of conventional metrics like PSNR, which is more relevant for robotic motion planning."
3980,3," To start, why not plot the empirical histogram of p(z|x) (for some fixed x's) to get a sense of how well-behaved it is as a distribution."
3981,3,"n\nI did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one?"
3982,3," The numerical experiments are motivated as a way to \""understand the capacity of the network with regards to modeling the external environment\"" (abstract)."
3983,3, Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time.
3984,1,\n\nPros:\n* Theoretically well motivated\n* Promising results on synthetic task\n* Potential for impact\n
3985,1,\n\n+ effort to compare to previous experimental setups
3986,1,\n\n----------\n\nOVERALL JUDGMENT\nThe paper presents a clever use of VAEs for generating entity pairs conditioning on relations.
3987,3," This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions)."
3988,3,"\u3000Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims."
3989,3,\n\\pi^* is defined as a distribution over state-action space and then \\pi is defined as a function; 
3990,3,There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers.
3991,3,"""The paper proposes to make the inner layers in a neural network be block diagonal, mainly as an alternative to pruning."
3992,1," Also, as the authors note the method seems to be limited to conditional sequence generators."
3993,3,"""Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax."
3994,3, We are talking about sample variance?
3995,3," \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?"""
3996,3," If not, please update the experiments to be consistent with the baselines."
3997,3, Some examples :\n\n\u00ab\u00a0provide a compact representation of a dynamical system\nby representing state as a set of predictions of features of future observations.
3998,1, Visual analysis of the attention map is convincing.
3999,3," More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. \n\n2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)? "
4000,1,"\n\nDetailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks."
4001,3,  It would certainly seem strange if this were not the case.
4002,3," However, the results are more focused on compute cost."
4003,3," Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear."
4004,1, The paper uses the right composition tools like moments accountant to get strong privacy guarantees.
4005,1," On one hand, fleet management is an interesting and important problem."
4006,3,"\n\n\n[1] Thoma et al: \""Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics\"", 2017."
4007,3," The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example."
4008,3,. Citations in [2] could be a good place to start.
4009,1, \n\nIt will be better if the authors could provide more details in the methodology or framework section.
4010,1, The experiments clearly show the viability of the approach and give interesting insights.
4011,3,"\n\nThe paper proposes the Skip RNN model which allows a recurrent network to selectively skip updating its hidden state for some inputs, leading to reduced computation at test-time."
4012,3,\n- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers?
4013,1,. Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\
4014,3,"\"".... Device Placement\"" seems to suggest that one is placing devices when in fact, the operators are being placed."""
4015,3," \n\n(3) Reproducibility. There are a lot of details missing; the setup is quite complex, but only partially described."
4016,3," The position {x,y} is a binned representation of discrete or continuous coordinates."
4017,3," I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  """
4018,1, \n\nSignificance \n\nThe approach outlined in this paper may spawn a new research direction.
4019,3,". The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features."
4020,3,\n\nSummary\nThe authors present methods for generating synthetic sequences.
4021,3,"\n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix.[[CNT], [SUB-NEG], [DIS], [MIN]] And do try to copy a bit more of them in the main text where reasonable.[[CNT], [SUB-NEG], [DIS], [MIN]] \n\nWhat is the role of PLAID?"
4022,3," Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \""sub-human\"" games you might hope.)"
4023,1,"""The idea of using GANs for outlier detection is interesting and the problem is relevant."
4024,2,"If published, uneducated and misinformed statements like this would jeopardize the credibility of this journal"
4025,1,"\n\nI have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper."
4026,3," Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation."
4027,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
4028,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
4029,3, The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter.
4030,3, This should probably be noted.
4031,3,"\nSpecifically, we compare the performance among the CommNet model, our\nMS-MARL model without explicit master state (e.g. the occupancy map of controlled agents in this\ncase), and our full model with an explicit occupancy map as a state to the master agent."
4032,3," Therefore, the following sentence seems sensational without substance: \""Therefore, on a full meta-modeling experiment involving thousands of neural network configurations, our method could be faster by several orders of magnitude as compared to LCE based on current implementations."
4033,3,"By the way, it will be better to compare to the snapshot of Oct 2017 as well, indicating the position of this work during the submission deadline."
4034,3,  The authors should validate these claims with an ablation study that compares performance with and without masking.
4035,1,- I think this is an important conceptual paper.
4036,1,"    In other text learning task (e.g., [1]) SPL showed improved performance."
4037,3, Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above.
4038,3,  These CARs are used in the A3C algorithm.
4039,3,\nWould it be possible to empirically evaluate how the R3NN performs on this dataset?
4040,3,\nIs this a standard assumption in the literature?
4041,1," Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice."
4042,3,"""This paper describes a method to induce source-side dependency structures in service to neural machine translation."
4043,3,"""This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly."
4044,3, So the provided proof is new?
4045,2,This study is weak. not innovation h/
4046,3, were hyper parameters tuned on a separate validation set?
4047,1,. The experiments show improvement over baselines.
4048,3,"\n-- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.Yan et al., In ECCV 2016."
4049,1,\n\nThe skill function uses a RNN as function approximator and minimizes the sum of two losses i.e. the state mismatch loss over the trajectory (using an explicitly learnt forward model) and the action mismatch loss (using a model-free action prediction module) .
4050,3," For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values."
4051,2,The theoretical section is unreadable for anyone unfamiliar with the language of relevant French theorists[paper about *German* theorists
4052,3,"""The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm."
4053,3,"\nd.\tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample?"
4054,1,"\n\nPros: Rigorous analysis, well motivated problem, generalizable results to deep learning theory"
4055,1," We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context."
4056,3,\n\n---\n\nSome questions/comments:\n- Do we need to add the print statements for any new programs that the students submit?
4057,3," The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \""strong\"" penalty parameter."
4058,3, Did you investigate this?\
4059,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
4060,1,  I think I mostly followed it.
4061,1, The good results obtained support the design decisions made.
4062,3,"""The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \""fully-aware\"" of all levels of abstraction, e.g. word-level, phrase-level, etc."
4063,3,"    But, perhaps I am misunderstanding something."
4064,3,"\n\nIn the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing."
4065,3,"  ( If the goal state s\u2019 was just the next state, then this would just be a dynamics model and this would be model-based learning?"
4066,3,"\n\nWhile increasing batch size at the proposed linear scale is simple and seems\nto be effective, a careful reader will be curious how much more could be\ngained from the backtracking line search method proposed in De et al."
4067,1," The task is interesting and relevant, especially for in low-resource language pair settings.\"
4068,3,"""The paper \u2018Deep learning for Physical Process: incorporating prior physical knowledge\u2019 proposes\nto question the use of data-intensive strategies such as deep learning in solving physical \ninverse problems that are traditionally solved through assimilation strategies."
4069,3," This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world."
4070,2,"n a review of a neuroimaging methods paper: By 'sex' you mean gender, and not sexual behavior?"
4071,3,. \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks
4072,1," As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. """
4073,3,"  In the former case, the statement \u201cx is uniformly sampled from X\u201d does not make sense because X is practically infinite."
4074,3," Intelligent tutoring systems. Science, 228. 456-462."
4075,3," Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is \u201cquite a reasonable baseline\u201d."
4076,1," The main novelty seems to be algorithm 2, which finds the minimizer of the quadratic approximation within the trust region by performing GD iterations until the boundary is hit (if it is--it might not, if the quadratic is convex), and then Riemannian GD along the boundary."
4077,3,"  In particular, they exhibit some high frequency artefacts."
4078,3,"""This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network."
4079,3,  The input to each model is a sparse sum of the outputs of modules in the previous set.
4080,3," They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. "
4081,1," The new section \""intuitive analysis\"" is very nice."
4082,3,"  As the authors show based on the Fokker-Planck equation of the underlying stochastic process, there exists a conservative current (a gradient of an underlying potential) and a non-conservative current (which might induce stationary persistent currents at long times)."
4083,3,  Is that a general suggestion or a data-set specific recommendation?
4084,3, Is the generalization hurt?
4085,3,"\n- How large are the training/test sets used in these experiments (for test sets, number of words, for training sets, amount of data in hours (is this ~10,000hrs), whether any data augmentation such as multi-style training was done, etc.)"
4086,3,"""The paper addresses the task of dealing with named entities in goal oriented dialog systems."
4087,3,\nHave you tried on other tasks?
4088,3," So I suggest the authors to better explain the connection between the told problem and their proposed solution, and how this can solve the problem."
4089,3,"""The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs."
4090,1, The theorems provide exponential gaps for very simple polynomial functions.
4091,3," For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer."
4092,3," In particular, please mathematically formulate each proposed technique in Section 4."
4093,3," By including unlabeled examples in an episode, it is already known that they belong to one of the K classes."
4094,3,"""This paper proposes a learning method (PIB) based on the information bottleneck framework."
4095,3, This seems like a pretty intense process: solving some representative subset of all possible RL problems for a particular environment \u2026 Maybe one choses s and s\u2019 so they are not too far away from each other (the experimental section later confirms this distance is >= 7.
4096,3, The paper experiments with two types of models \u2013 1) a model which only predicts the span in a document and 2) a model which generates the answer after predicting the span.
4097,3, \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison.
4098,3,  Experiments are based on the federated averaging algorithm. 
4099,3," I did not get the time to go through the proofs, but they handle the fully connected case as far as I understand."
4100,3, If this shift exists (as for the\ncase of negative images) it is possible to refer to the extensive deep domain adaptation literature.
4101,3," \n\npage 16:\n- proof of Proposition 2 : key idea here is using the positive and negative part of (f-g). This could simplify the proof."""
4102,3," \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network?"
4103,3,"""In this paper a new neural network architecture for semi-supervised graph classification is proposed."
4104,3," Intuitively, one can see why this may be advantageous as one gets some information from the past."
4105,1," On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original."
4106,3, The authors might want to comment on the relative merits between GP-SSMs and DE-RNNs.
4107,3, The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time.
4108,1," \nOverall, this is a good paper,"
4109,1,"""The main strengths of the paper are the supporting experimental results in comparison to plain feed-forward networks (FNNs)."
4110,3, though it did take two read-throughs before the equations really clicked for me.
4111,3,\nIt would be interesting to see the accuracy of the fitness approximation during the rank selection procedure.
4112,3,"\n\n- In Algo 1, it would help to be more explicit about the updates (a), (b), (c), e.g., for (a), give a reference to eq (8), and for (b), reference equations (9,10)."
4113,3,"\n- are the training and testing sets all disjoint (sec 4.3)?\n- at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg \""tested on nored-test\"")"
4114,3," For readers less familiar with amortized variational inference in deep nets, the benefit would be larger."
4115,3, I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017).
4116,1, I support its acceptance.
4117,2,"The paper is badly written, and poorly organized. In its current form, the paper cannot be accepted. The paper is poorly written or poorly thought-out. I think the paper is poorly written and most of its statements are wrongly and poorly motivated."
4118,3," Once they\nhave been calculated in Algorithm 1, how are they used?"
4119,3,\n\n2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation.
4120,3," Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this."
4121,3,"  Discriminative training is not invented to overcome the lack of manual segmentations, and is equally applicable to the case where we have manual segmentations."
4122,3, A random starting point is generated.
4123,1," The argument for motion accuracy is clear and is clearly stated: it\u2019s the measure that is actually tied to the intended application, which is using action-conditional motion prediction for control."
4124,3," If this subject is important, it would have been interesting to compare the approaches on other large scale problems and possibly with other implementations."
4125,3, One option would be to run user studies where humans judge the quality of the matches.
4126,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
4127,3," More generally, a now quite common way to handle this problem is to use \""pointing\"" or \""copying\"", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too."
4128,1,"\n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner,"
4129,3,  It would be nice to have an even informal explanation.
4130,3,"  While the longest path in FractalNet is stronger than the other members of the ensemble, it is nevertheless an ensemble."
4131,3,\n\nThe proposed scheme is applicable to the batch setting when most deep network are learned using stochastic gradient type methods.
4132,3,"  The main contribution of the paper is to combine these two\nmethods (equations 6-10) and evaluate the results in the large-scale setting of\nATARI games, showing that it works in practice."
4133,3,"""Paper Summary:\nThis work proposes a new geometric CNN model to process spatially sparse data."
4134,3," The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay."
4135,3," However, this is not the case for the former (see, e.g., Comon et al., 2008 as cited in this paper)."
4136,3,\n3. Add an experiment where you vary 'pf' and 'qf' (and keep the hidden size fixed) to show how the optimization/generalization performances can be tweaked.
4137,2,"Written in parts like an experience track paper, minus the experience"
4138,3,"This low-dim latent space is first predicted from the residual of the (deterministic prediction - groundtruth), and then the low-dim encoding goes into a network that predicts a corrected image."
4139,3,Then a lemma by Kalai and Vempala can be used.
4140,3," The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings."
4141,3,"\n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?"
4142,3,\n-- Proposed architecture is general enough to be useful for other sequence-to-sequence problems
4143,3, The first to publish the application of VBP to NNs was Werbos in 1982. Please correct. 
4144,3,"\n\n* Novelty. The network design builds heavily on the work of [Finn et al., 2106]."
4145,3," Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector."
4146,1,"\n\nI like the weak convergence results, but this is just weak convergence."
4147,3,? Should the hyperparameters be tuned separately for each generative model being evaluated?
4148,2,"I suspect it may represent the 'off-cuts' of some other scholarly project, scraped together from the workship floor to make up another publication"
4149,1, The analysis is very thorough and the methods described may find use in analyzing other tasks.
4150,3," The proposed approach leverages reinforcement learning by defining the reward as how fast the learner learns, and use policy gradient to update the teacher parameters."
4151,3," It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \""Dot Product Proportionality Property."
4152,3, \n\nQuestions/Comments:\n\n- How is the checkpointing module represented?
4153,3, Does this mean that you need to convert the raw meshes coming from the 3D camera into a particular topology before you can use this algorithm?
4154,3,?\n- How were the 5k source words for Procrustes supervised baseline selected
4155,1,"\n\nThe multiplicative relation analysis is interesting,"
4156,1,"""Pros\n- The proposed model is a nice way of multiplicatively combining two features :\n  one which determines which classes to pay attention to, and other that\nprovides useful features for discrimination."
4157,1,  They motivate their approach by repeating the previously made claim that the naive gradient approach is non-convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation.
4158,3,\n\n2) This problem might actually matter.
4159,3,"\n\nIt would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.   "
4160,1,"  The idea presented seems to have merit ,"
4161,3, Are you using it also on the vanilla RNN or the LSTM?
4162,3,"  The algoroithm of STB should be briefly explained in Section 4.2."""
4163,3, The authors also provide some visualisation of the parameters of their model.
4164,3," Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence \""up-left-left\"", while back-propagating through \""up-right-right\"" action sequence in the TreeQN's plan."
4165,1," \n\nPros:\n- Simple approach based on nearest-neighbors, likely easier to train compared to GANs."
4166,3," In particular, the\nselection probability of any particular example could be estimated through a\nheuristic, for example by simply counting the number of neighbouring examples\nthat have a different color, weighted by whether they are in the set of examples\nalready, to assess its \""borderness\"", with high values being more important to\nachieve a good program."
4167,1,\n+ better performance
4168,3, It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee.
4169,3, but not in the current form.
4170,3, Have you tried this for high-dimensional models as well?
4171,3,"  Specifically, estimating the log probability to be proportional to the number of edits in the Levenshtein distance is really not the right thing to do at all."
4172,1," which isnt very common in most meta-learning papers today, so it\u2019s encouraging that the method works in that regime.\n"
4173,3, Would that allow improving generalization to varying item or image sizes?
4174,3, Are you saying the encoder produces it using a fully-connected layer?
4175,3,"""The authors propose a model for sequence classification and sequential decision making. "
4176,3, The GAN is a WGAN trained on the train set (only to keep the generator).
4177,1," I generally liked the paper and the approach, here are some more detailed comments."
4178,3," If this is the case, some configurations could easily require more time to evaluate than the others."
4179,2,This would be an embarrassment even if submitted as an undergraduate class paper' h/
4180,1,"\n\nIn addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10"
4181,3," Also, the attack model should formally introduced."
4182,3,"\nii) The performance difference between Xception and proposed method for image classification experiments using CIFAR is incoherent with the intuitions provided Sec 3.1 as the proposed method have more parameters and is a generalized version of DSC."""
4183,3," \n\nBut I am not sure if the novelty is strong enough for an ICLR paper. \n"""
4184,3, What precisely do we learn here?
4185,3,\n3. Section 4.1: You say that you don't train the recurrent matrix in the KRU version.
4186,1,"While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets."
4187,3, -- finding a more efficient search path would be an important next step.
4188,3," For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically."
4189,3, I checked the numbers in table 2 and the numbers aren't on par with the recent methods.
4190,1," \n- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments"
4191,3,"\n* Gaussian-noised\n* approximation of the it objective\n* before eq9: \""that solves\"": well, it doesn't really \""solve\"" the minimisation, in that it is not a minimum; reformulate this?"
4192,3,\n \n \n3. Background\n \n\u201ca function from the state and the action of an agent to the real value\u201d -> a reward function
4193,3," Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient."
4194,3,"\n\n  - Further, the proposed joint approach, where the second and third order information are combined requires further analysis."
4195,3, so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be removed.
4196,3," The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax."
4197,3," For instance, what are the implications of using higher-resolution images as input to DenseNet / decreasing the number of layers?"
4198,3," \n\n(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations"
4199,3,"""This paper proposes a technique to improve the output of GANs by maximising a separate score that aims to mimic human interactions."
4200,3, Showing versus Doing. Teaching by Demonstration.
4201,2,This paper makes no contribution
4202,1,"\n\nThe underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique."
4203,3," The input defender is based on sparse coding, and the latent defender on modeling the latent activity as a mixture of Gaussians."
4204,3," The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator."
4205,3,"\n\nThe inference gap is log p(x) - L[q], the approximation gap is log p(x) - L[q^* ] and the amortization gap is L[q^* ] - L[q]. The amortization gap is easily evaluated."
4206,3, Similar trend holds for other metrics like BLEU-4.
4207,3,"\n\n2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct."
4208,3,\nI would be glad to reconsider my grade if the questions regarding the motivation of the two-pass decomposition and the comparison with Astrid and Lee 2017 are answered.
4209,2,An alternative to counting sheep.
4210,3, Such optimizations are very common in deep learning field while less known in the field of security.
4211,3, This may significantly change the conclusions drawn from\nthe experiments.
4212,3,"\nIf the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper.\n\n[1] "
4213,3,  Is there any way to extend the current results to non-symmetric systems?
4214,3, Recent successes in Deep RL--including Atari and AlphaGo all train and test on exactly the same environment (except for random starts in Atari and no two games of Go being the same).
4215,1,\n\nThe proposed algorithm is clearly described.
4216,3,"  \nShin, J.H., Adaptation in spiking neurons based on the noise shaping neural coding hypothesis."
4217,1,\n\nPros:\n-Theoretical results on the convergence of OT/Monge maps\n-Regularized formulation compatible with SGD
4218,1, Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks.
4219,3," Nature neuroscience, 18(7), 1025-1033."
4220,3," The paper presents a neural task graph solver (NTS), which encodes this as a recursive-reverse-recursive neural network."
4221,3," A strict editor would be helpful, because the underlying content is good\n - odd that your definition of generalization in GANs appears immediately preceding the section titled \""Generalisation in GANs\""\n - the paragraph at the end of the \""Generalisation in GANs\"" section is confusing."
4222,1, and indeed the results here are interesting as they favour relatively simpler structures.
4223,3," Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation."
4224,3,"\n[3] Tishby, Naftali, and Noga Zaslavsky. \""Deep learning and the information bottleneck principle."
4225,3," \"" It is my understanding that Pytorch support higher order derivative both for ReLu and Max-pooling."
4226,2,The standard of writing (including spelling and grammar) is also satisfactory.
4227,3,"  Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention."
4228,3,"  Besides, comparisons/discussions based on extensive analysis on various deconvolution architectures presented in Wonja et al., 2017 would also be interesting."
4229,1," It is presented well (modulo the above problems), and it makes some strong points."
4230,3, However I have some concerns to this paper.
4231,1,"\n\n**EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept)."""
4232,1,"\n\nPros:\nThe authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer."
4233,3,"3. In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs."
4234,1, Methods and results are clearly described.
4235,3,"  I think this paper is still pretty borderline, but I increased my rating to a 6."
4236,3,\n\n2. The authors implement the proposed sparse-complementary convolution on NVIDIA GPU and achieved competitive speed under the same computational load to regular convolution.
4237,3,\n\n2. More details on how node embedding vectors are initialized.
4238,3,"""The paper introduces a modified actor-critic algorithm where a \u201cguide actor\u201d uses approximate second order methods to aid computation."
4239,3," To solve this task, the paper also presents a new\nmodel architecture that effectively computes a low-rank attention over both\npositions and feature indices in the input image."
4240,2,The authors are treating what might be called a boutique version within a model that is already considered a boutique model' by many.
4241,3," Additionally, it is evaluated over the MEN and Mturk datasets."
4242,3,\n\nClarity. What is shared attention exactly?
4243,1, The study seems fairly thorough (both vanilla and cold-start experiments are reported).
4244,3," For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper."
4245,3," The authors propose to use SVD tools to estimate the dimension of the deep manifolds, and conduct experiments on three categories of ImageNet."
4246,3,"  Also, many decision problems for wighted automata are known to be undecidable.[[CNT], [CNT], [DIS], [MIN]]  I am not sure that the paragraph is useful for the paper.[[CNT], [null], [CRT], [MIN]]  A discussion on learning as in footnote 1 shoud me more interesting."
4247,3, The use of genetic algorithms has also been considered.
4248,1," The problem was very well-motivated, and the analysis was sharp and offered interesting insights into the problem of maze solving."
4249,2,"By now, there are over 1,000 [articles on this topic], but these authors have not read a single one."
4250,3," However, the theoretical results are not very satisfactory."
4251,3,\n\n7. Why don\u2019t you treat number of hops as a hyper-parameter and choose it based on validation set?
4252,1," The main novelties of the paper are: (1) the use of Tumblr data,"
4253,3,"  Then to make it fast, the implicit step is approximated using conjugate gradient method because the step is solving a quadratic problem."
4254,3," However,\nsome aspects of the paper can be improved by adding more explanations."
4255,1," The method relates closely to prior work on action-dependent baselines,"
4256,3, The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.
4257,3,. Why are the cost functions non-stationary?
4258,3,"\n- AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change."
4259,3," \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task."
4260,3," The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \""temperature parameter\"". "
4261,3, The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem.
4262,3," Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN."""
4263,3," More details follow.\n\n\nComments:\n1. All architectures and objectives (both classic and PIB-based) are trained using a single, fixed learning rate (LR)."
4264,3, This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).
4265,3, But I feel this sounds like a new angle and not the one that is adopted by the authors in this paper.
4266,3,  The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net.
4267,2,I can see that the manuscript has archival value
4268,3,"For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum."
4269,3,"\n\nThe proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results."
4270,1,"\nIn conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier,"
4271,1,"""Summary:\nThis paper presents a very interesting perspective on why deep neural networks may generalize well, in spite of their high capacity (Zhang et al, 2017)."
4272,2,This manuscript achieves the dubious distinction of being conceptually stillborn
4273,3, the pain point is knowing when tasks are begin and end.
4274,3," Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations."
4275,3,"\nA comparison the Graves 2013 is absolutely required, more comparisons are desired."
4276,3,"  \nE.g., the statement at the end of 2.1 is unclear at that point in the document. [[CNT], [CLA-NEG], [CRT], [MIN]] How is the PIR drawn exactly?"
4277,3," Eg a global model is to be trained by local updates that occur on mobile phones, and communication cost is high due to slow up-link."
4278,3, They extended the training objective functions of Word2vec and Glove with the affect information.
4279,3,\n\nI will be happy to revisit the rating if the experimental section is enriched.
4280,3, \nA gating mechanism is used in order to produce the contribution from the hidden state of the master to the logits of each agent.
4281,3,\n- Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term?
4282,3,\n\nThe main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot
4283,3," Thus, if we have the SR matrix we can replace the Laplacian mentioned above."
4284,2,"I guess this proposal could be interesting, if youre interested in this obscure sect of biology"
4285,1,"\n\nI don\u2019t don\u2019t feel like the clause between equations (17) and (18), \u201cwhen sharing attention weights from the decoder with the encoder\u201d is a good description of your clever \u201cshared attention\u201d idea."
4286,3, The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters.
4287,1,\n\nPros:\n(1) the paper is very well organized and easy to read
4288,3,"""This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning."
4289,3, The basic idea is that instead of geometrically weighting the n-step returns we should instead weight them according to the agent's own estimate of its confidence in it's learned value function.
4290,3, This Lipschitz property has already been proposed by recent methods and has showed some success.
4291,3," Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability"
4292,1, \n\nI like the idea of using deep learning for physical equations.
4293,1,"""On the positive side the paper is well written and the problem is interesting."
4294,3," \n\nThe paper works out a probabilistic analysis arguing that when either of these conditions obtains, there exists a fooling perturbation which affects most of the data. "
4295,3," If the duality parameter is large enough, the functions become convex given that the initial losses are smooth."
4296,3," However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides."
4297,3,\n\n* Fig 6: What does 'clean gradients' mean?
4298,3, The question is if it is worth adding so many pixel-wise parameters.
4299,3, Diagonal covariances are still very restrictive.
4300,1,"\n\nOverall the results seem interesting,"
4301,3,\n\nThe main takeaway from the entire paper is not clear very much.
4302,1,"""The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the \""fourier feature\"" corresponding to the kernel at a set of randomly sampled quadrature points."
4303,1," The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations)."
4304,3," In particular, the strategy for choosing the hyperparameters (e.g., \\alpha, \\alpha_mu, local learning rate, \\alpha_mu) need to be developed ."
4305,1," The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement."
4306,3," (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize."
4307,3," Since it works so well, maybe it could be promoted into the method section?"
4308,3, Would be good to provide its training curve.
4309,3, The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems.
4310,2,"The research adds meaningful to the literature, how?"
4311,1, Given this and the other reviews I have bumped up my score from a 5 to a 6.
4312,3,"\n\nis a hierarchical -> are hierarchical\n\nyields -> yield\n\ntwice \""otherwise\"" after Eqn. 7\n\nare can be viewed\n\nthey occurs\n\ncan can readily expanded\n\ntransfer transform\n"""
4313,3, I think it would be possible for the synthetic sequence to also generate an outcome measure (i.e. death) based on the first 4 hours of stay.
4314,3," Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster."
4315,1,"  \n\nThis approach is original and significant,"
4316,3,". In the construction presented here, the network\u2019s size is essentially in the layer of size m.."
4317,1,"  The main contributions of this paper are: \""U-max\"" idea (for numerical stability reasons) and an \""\""proposing an \""implicit SGD\"" idea.\n\nUnlike the first review, I see what the term \""exact\"" in the title is supposed to mean. I believe this was explained in the paper."
4318,3,"""The paper proposes another training objective for training neural sequence-to-sequence models."
4319,3,"""This paper provides a survey of attribute-aware collaborative filtering."
4320,3, Why do we require the correspondence between the classification confidence of tranformed and original data?
4321,1,"""This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling."
4322,1,\n* nice results and decent experiments (but see below)
4323,3,"\n\n*Clarity*\n\nThe paper is, in general, well-written. However, the presentation in 4 is hard to follow."
4324,3," According to\nLemma 1, the proposed method is unbiased for the optimal weights in the large\ndata limit."
4325,3,"""This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions."
4326,3,\n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each
4327,3,"\n\nI suggest using the same axis limits for all subplots in Figure 3."""
4328,1,"\n - a new dataset \""crashed cars"
4329,3," Among\"".\n- Please add commas/periods at the end of equations."
4330,3,\n\nCons:\n-The paper addresses a problem with an issue with RWAs. 
4331,3,\nCan the authors comment on the similarity/differences between the approaches?
4332,3," However, when x is not repeated frequently, both RAML and SQDML are biased."
4333,1,"The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization)."
4334,1,\n\nQuality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth. 
4335,2,"Not now, not ever"
4336,3, Classification is done with soft k-NN on the class prototypes.
4337,1, I think this is a nice contribution that does yield to some interesting insights.
4338,3,"""\nIn this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together. "
4339,2,The beekeeping example also fails to convince. There is no discussion of bees' actual lived experience.
4340,3," As long as those issues remain unresolved, my rating is at is but if those issues were resolved it could go up to a 6."
4341,3,"""The paper discusses learning in a neural network with three layers, where the middle layer is topographically organized."
4342,3, Please define what semantic attention is.
4343,3, But certain kinds of safety constraints like 'do not drive in the blindspot of other vehicles' sometimes require the ego car to speed up for a bit beyond the speed limit to pass the blindspot area and then slow down.
4344,3,"""The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights."
4345,3," It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration."
4346,3," However, I find this result to be out of the context with the main theme of the paper."
4347,3," The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used,"
4348,3,"\n* In Equation 2, should there be a balancing parameter for the reconstruction loss?"
4349,1, \n\n****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.
4350,1,"   The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?)."
4351,3,"\n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value."
4352,1," I recommend it be accepted."""
4353,3," However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm."
4354,3," The primary motivation comes from the use of a separate target network in DQN, which seems to be needed in Atari (though I am not aware of any clear result that demonstrates why, rather just from informal discussions)."
4355,3," This proposed paper rejects some of the claims that were made in Mirowski et al. 2016, mainly the capacity of the deep RL agent to learn to navigate in such environments."
4356,3, Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3?
4357,3," \n\nIn section 3.1, it would be helpful to cite a reference to support the form of dual problem."
4358,3,"  The methods used in Frey & Jojic are different from what is proposed in the paper, but there needs to be comparisons."
4359,3, I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.
4360,3,"  A very important type of dialog act is \""switching topic\"", often done to ensure that the conversation will continue. "
4361,1,\n\nPros\n- \u201cMonolithic\u201d policy representations can make it difficult to reuse or jointly represent policies for related tasks in the same environment; a modular architecture is hence desirable.
4362,1,\n(4) MC methods tend to degrade less when the reward signal is delayed.
4363,3, One requires the strict positivity of the densities (to properly define conditionals).
4364,1," it adds an interesting perspective to the discussion of how network optimization \""works\"", how it handles local optima and which role they play, and how the objective function landscape is \""perceived\"" by different optimizers."
4365,3, The paper also shows major speedup compared to ELU and TANH (unit-wise speedup)
4366,3, They evaluate their model on 6 small scale RNN experiments.
4367,3,"  It would be nice to have a succinct summary of how all of the pieces presented fit together, e.g. the original victim network, fine-tuning loss, per-class dictionary learning w/ OMP."
4368,3," It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation?"
4369,1," Overall, the metrics approached by the authors are reasonable, though the way pseudo-distribution are define in terms of normalized weight magnitudes is seems a little ad hoc to me."
4370,3," At least in \""standard\"" compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without).\"
4371,1," The first advantage of the proposed model is short response time due to parallelism of non-sequential output generation, proved by experiments on the SQuAD dataset."
4372,3, Why did the choices that were made in the paper yield this success?
4373,3,". This seems tt the authors propose to add a second activation function (the softsign), why not use the one is in teh layer?"
4374,3,Apologies if I am missing something obvious.
4375,1," The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013)."
4376,3,\n\nThe toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.
4377,3,"""This paper presents an algorithm for clustering using DNNs."
4378,2,In the experimental part there is a lack of scientific by taking over results from reference literature.
4379,3," I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution."
4380,1, but the different proof technique might be a good enough reason to accept this paper.
4381,3,"\n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337\u2013348, 199.\u2028"
4382,3," I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method)."
4383,3, Is it possible to further improve the accuracy by a more careful fine-tuning?
4384,3,. Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)? 
4385,3," If the error rates are different for different tasks, it is not sensible to measure raw accuracies."
4386,3," Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units."
4387,3, Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour.
4388,3, The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed.
4389,3,"\n\nIt is also known that these benchmarks, while being widely used, are small and\nresult in high variance results."
4390,3, Is the proposed layer really useful once a powerful model is used?
4391,3,"  \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models."""
4392,3," Highly peaked -> confidence, spread out -> out of distribution."
4393,3, Are there existing generative models based on walk paths?
4394,2,The title of the submission is misleading and it should be renamed 'A Personal Diary'
4395,3, \n- difficulty generalizing to longer sequence (compared to training sequences) of commands.
4396,3, The proposed method uses \n - a generator trained in a GAN setup\n - an autoencoder to obtain a latent space representation\n - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it). 
4397,3,\n- What are the \u201ctier-1\u201d and \u201ctier-2\u201d models in this section?
4398,3," \n\nUnderstanding how to make complex models interpretable is an extremely important problem in ML for a number of reasons (e.g., AI ethics, explainable AI). "
4399,3,".\n\n- Fully evaluating the \""claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\"" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization."
4400,1,"\n\nPros:\nThe network is very clean and easy to implement, and the results are OK."
4401,3," If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results."""
4402,1,"\n\nOriginality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real-world problems."
4403,3," However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$)."
4404,3,"""In conventional boosting methods, one puts a weight on each sample."
4405,3,"  \n\n- The gains here appear to be consistent, but they seem marginal."
4406,3,\n\n- Have you thought of an end to end way to learn the prior generator GAN?
4407,3,  This would help to address the contribution of Q-masking vs. simply abstracting the action space.
4408,3, To see the confusion matrixes could be useful.
4409,2,"Adequate, if not particularly appealing."
4410,3," This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word\u2019s soft head (called Shared Attention in the paper); and (3) they gate when attention is used."
4411,1, \nSignificance: This paper is somewhat significant.
4412,3,"  This means, it's \""just\"" using NNs as a model class."
4413,3, Does the proposed approach still show gains over Attend Infer Repeat?
4414,3," If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module."
4415,1,"\n\n(2) that therefore, block diagonal layers lead to more efficient networks."
4416,2,NO'  (h/
4417,3,"\n\nIn a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term."
4418,3," Hence, the learning framework of the\nauthors can be casted more as a learning with similarity function than learning\ninto a RKHS [2]."
4419,3, Can bugs and suboptimal configurations be ruled out during the experiments?
4420,1, I would say the idea is well justified in several ways.
4421,3," \n\nThe title is also misleading in using the word \""exact\""[[CNT], [null], [CRT], [MAJ]]. I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\"
4422,3," An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques). \n\n"
4423,3,"""The paper proposes to combine the recently proposed DenseNet architecture with LSTMs to tackle the problem of predicting different pathologic patterns from chest x-rays."
4424,3,"\n\nother questions / comments:\n- \""we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\"" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation?"
4425,3, What about a graph with poorly connect communities?
4426,1, The theory part of the paper is reasonable and quite well written.
4427,3," Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data."
4428,3,"\n\n4. Proof of theorem 3.3.\nTheorem 3.3 is one of the key results in this paper, yet its proof is just \""noted\""."
4429,3, The method is inspired by the recent work on max entropy reinforcement learning and links between Q-learning and policy gradient methods.
4430,2,...It's impossible to judge the claims of the authors although there is real reason to believe that the claims may pan out...
4431,1,"""The idea is clearly stated (but lacks some details) and I enjoyed reading the paper."
4432,3,    \n\n\nI thank the author for their detailed answers.
4433,3," It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3."
4434,3,". \n\n- The experimental results are not properly presented, with many overlapping figures"
4435,3," Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4."
4436,1,"If it does actual data augmentation, it should perform better"
4437,3, A proper embedding would have mapped $x$ into a function\nbelonging to $\\mH$.
4438,3," In Balestriero & Baraniuk, it is shown that any DNN can be approximated via a linear spline and hence can be inverted to produce the \""reconstruction\"" of the input, which can be naturally used to do unsupervised or semi-supervised learning. "
4439,2,The whole premise just reverted the biosensor field back 20yrs
4440,1, Following from this connection authors propose a new loss function.
4441,3, This reference should be included in the related work.
4442,3," That matrix is then mapped back to system id (A,B,C,D)."
4443,1," Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution."
4444,3, The second checks to see if your alternative weighting is simply approximating the benefits of changing lambda with time or state.
4445,3," This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol."
4446,3," Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from."
4447,3,"""The paper introduces a neural translation model that automatically discovers phrases."
4448,3, Essentially the authors propose an approach to use a node embedding to achieve graph classification.
4449,2,I gave up pointing out all the mistakes because even the author apparently doesn't consider this manuscript a good scientific presentation
4450,3,\n4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices.
4451,3,"  To estimate the ID, the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network, in which the intrinsic dimension is determined (in a more or less standard way) by the rank at which two consecutive singular values has a ratio exceeding some threshold."
4452,3,"  It is quite common for everyday people to use emotion words this way e.g. using #love to express strong approval rather than an actual feeling of love.[[CNT], [EMP-NEU], [DIS], [MIN]]   \n\nIn their analysis the authors claim:\n\u201cThe 15 emotions retained were those with high relative frequencies on Tumblr among the PANAS-X scale (Watson & Clark, 1999)\u201d."
4453,3, Each class is represented by a class prototype which is given by the average of the projections of the class instances.
4454,3, Have you compared with using random embeddings for the named entities?
4455,2,"The fact that the question of this paper has never been asked should, on balance, count against the paper."
4456,3," I think it is important to see how fast the teacher model converges, too."
4457,3, The theory part focuses on effectiveness of drawing samples from the reservoir.
4458,2,"This is a very difficult paper to review, and difficult - even painful - to read"
4459,3, Some suggestions / criticisms are given below.\n\n1) The findings seem conceptually similar to the older sparse coding ideas from the visual cortex.
4460,3," The method can be interpreted to generalize other recurrent network regularizers, such as putting an L2 loss on the hidden states."
4461,3, How does your formalism take this into account?
4462,3,"\n- There is discussion as to what i-vectors model (speaker or environment information) - I would leave out this discussion entirely here, it is enough to mention that other systems use adaptation, and maybe re-run an unadapted baselien for comparsion"
4463,3," Given the labels, the rest of the architecture are extensions of conditional GANs, a causalGAN with a Labeller and an Anti-Labeller (of which I\u2019m not completely sure I understand the necessity) and an extension of a BEGAN."
4464,2,"It was hard to bite myself through, but in the back I found some meat."
4465,3,  The contribution of this paper seems to be the specific way the decomposition is used in training the DNN.
4466,1, The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful.
4467,1, \n\nThe notion of local openness seems very well adapted to deriving these type of results in a clean manner.
4468,2,It is clear that the author has read way too much and understood way too little.
4469,3," \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified."
4470,3,. The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers
4471,1," \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy."
4472,3," Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW."
4473,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean
4474,1," the analysis seems correct,"
4475,3,"  This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.)."
4476,1," There are a few ideas the paper discusses:\n\n(1) compared to pruning weight matrices and making them sparse, block diagonal matrices are more efficient since they utilize level 3 BLAS rather than sparse operations which have significant overhead and are not \""worth it\"" until the matrix is extremely sparse. I think this case is well supported via their experiments, and I largely agree."
4477,3,"\n\nThey face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform."
4478,1,"It seems that the proposed method works well for heat sink problem and the steady flow around airfoil, both of which do not fall into the more complex physics regime."
4479,3,"\n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state."
4480,1,"""Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons."
4481,3," The above heuristic is obviously specific to the domain, but similar\nheuristics could be easily constructed for other domains."
4482,3," I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting."
4483,3, It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice.
4484,1, These two proposed neural network models seem performing well empirically.
4485,3," In spirit, these simulations are similar to those in the original paper by M. Egorov."
4486,3,\nThe basic form of SGD selects an example uniformly.
4487,1, The main idea behind the proposed test is very insightful. 
4488,3, The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels.
4489,2,I am personally offended that the authors believed that this study had a reasonable chance of being accepted to a serious journal
4490,3, The application is to predict errors made by students on programming tasks.
4491,3," \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem"
4492,1, The results are promising.
4493,1, It would be interesting to understand how the different metrics relate
4494,3,"""This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer. "
4495,1,\n\nThe paper presents interesting ideas and findings in an important challenging area.
4496,1, The only other work I found is\nGaussian Quadrature for Kernel Features NIPS 2017. \nThe work is pretty recent so the author might not know it when submitting the paper.
4497,3,"""Twin Networks: Using the Future as a Regularizer"
4498,3,. This is achieved by learning to play the game in both directions. Authors show results in a word-level translation task and also a sentence-level translation task. They also show that having more languages help the agent to learn better
4499,1," \nThe experimental results seem promising, but the presentation can be improved."
4500,3,"\n- Besides, how are D and W exactly defined?"
4501,3," The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points."
4502,3, The Xception network decouples the spatial correlations from depth channels correlations by having separate weights for each depth channel.
4503,3," The title is misleading, this may be the direction where the authors of the submission want to go, but the title  \u201c.. with human interactions\u201d is clearly misleading.[[CNT], [CNT], [CRT], [MAJ]] \u201cModel of human interactions\u201d may be more appropriate.[[CNT], [CNT], [CRT], [MAJ]] \n\nThe technical idea of this paper is to introduce a separate score in the GAN training process."
4504,3," Which of the MNIST models from Table 1 was used?\n"""
4505,3,  \n\nThe motivation section in the beginning of the paper motivates using the backbone structure to get a sparse network.
4506,3,"\n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques."
4507,1," When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000]."
4508,1," The work is valuable,"
4509,3, This work proposes to do sequential prediction of adjacent pixel features (via intermediate feature maps) resulting in more spatially smooth outputs for deconvolution layer.
4510,3," Given these clarifications in an author response, I would be willing to increase the score."
4511,3," Specifically, the authors develop a Fourier-based generalization bound."
4512,3,"""This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems."
4513,3, \n\nThe authors propose to use cosine distance between representations \nof what they call the \u201csubgoals\u201d of learner and expert.
4514,3,"\n\n## Minor\n- I challenge the claim that thousands and millions of time steps are a common issue in \u201crobotics, remote sensing, control systems, speech recognition, medicine and finance\u201d, as claimed in the first paragraph of the introduction."
4515,3,"\n- What is the trade-off when the size of the coreset increases?\n"""
4516,3, What is the range of t1 and t2?
4517,2,"My disappointment when finding flawed analyses, conclusions, and terminology [..] was therefore substantial"
4518,3, How were these loss functions are combined?
4519,3," Could you please explain why such phenomenon happens?"""
4520,3," Would AE-k beat NATAC with a different dimensionality of latent space and k?"""
4521,3,Conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketches. 
4522,1,"\n\nThus I think that although this paper is written well,"
4523,1," \n\nNext, the evaluation wrt to claim (ii) is novel and may help developers understand the model characteristics."
4524,3," So, slight to no novelty."
4525,3,"""This paper is an experimental paper."
4526,3,"\u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept."""
4527,3,Is it scalability?
4528,3,"\n\nThis paper starts by bashing the complexity of conventional HMM systems, and states the benefits of their approach."
4529,3," The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3)."
4530,3, I would assume from the description that the colors are based\n  on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR.
4531,3,"  The problem is framed as RL problem, where the state space corresponds to learning configurations, and teacher actions change the state.  Supervision is obtained by observing the learner's performance."
4532,1,\n\nThere are no fancy new methods or state-of-the-art numbers in this paper.
4533,3, \n\nBut let\u2019s assume that at some point they can be used as the authors propose.
4534,2,This is a very weak paper trying to bend things to build a relationship that does not exist
4535,3,"   It would be helpful if there are comparisons to these models, and use similar datasets. "
4536,3," Instead of listing each step in each subsection, a general introduction picture should be introduced first. More intuition is also needed for each step."
4537,3," \n\np1: authors write: \""Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015)."
4538,3,\n- How does bipolar activation compare to model train with BN on CIFAR10?
4539,3," More generally, the direction of the gradient will be highly dependent on the specific setting of the current weights."
4540,1, In particular I appreciate that the authors stress the importance\nof 'non-equilibrium physics' for understanding the SGD process.
4541,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
4542,3," It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels."
4543,3, The model first encodes the document via tuple extraction.
4544,3, Instead of performing the optimisation to find x' have you tried visualising the real data sample that gives the closest activations?
4545,3," \u201cSemi-Supervised Learning with Deep Generative Models.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1406.5298.\n\n[C]: Wang, Weiran, Xinchen Yan, Honglak Lee, and Karen Livescu. 2016. \u201cDeep Variational Canonical Correlation Analysis.\u201d arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1610.03454."
4546,3," First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle."
4547,3,\n\nHow the 4000 hypernyms have been selected?
4548,3,"\n\n1. It seems the concept of \""binarized activation patterns\"", which the proposed regularizer is designed upon, is closely coupled with rectifier nets."
4549,3, It would also make the paper more clear.
4550,2,This looks like a very early draft
4551,1,\n\nSignificance\n==========\nHaving an RL approach that can benefit from truly off-policy samples is highly relevant.
4552,1,This would give a stronger sense of the kind of wins that are possible in this framework
4553,2,"leftover is a noun, like old pizza. left over is the verb you want."
4554,1, The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces.
4555,3,"The width of the network is bounded by the two input distributions, so is this network just incredibly deep?"
4556,3, Sketching some such scenarios would help the reader understand why the issue is practically important.
4557,3,"\n\nAs a matter of fact, one of the issues of the presented quantized techniques (the fact that random rotations might be needed when the dynamic range of elements is large, or when the updates are nearly sparse) is easily resolved by algorithms like QSGD and Terngrad that respect (and promote) sparsity in the updates."
4558,3,"\n\nAs a minor comment, I advise the authors to use a different letter for \""word embeddings\"" and the \""projected word embeddings\"" (equation at the bottom of page 3)."
4559,3, They obtain state of the art results on most of the datasets.
4560,3,"\n7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections."
4561,3, Perhaps this statement could be qualified.
4562,3,"\n\nIf MI is invariant to monotone transformations and information curves are determined by MIs, why \u201ctransformations basically makes information curve arbitrary\u201d?"
4563,3,"\n\nDuring experiments, the autodidactic returns perform better only half of the time as compared to lambda returns."
4564,3,"\n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives."""
4565,3," The number of layer as a complexity is not appropriate , as we need to take in account many parameters:  the pooling or the striding for the resolution, the presence or the absence of residual connections (for content preservation), the number of feature maps. More experimentation is needed."
4566,1," As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic)."
4567,3," Given the same receptive field with multiple complementary kernels, is the kernel shape important for the training?"
4568,2,Your research paper is not motivated. There is no motivation behind your algorithm. Your results lack motivation
4569,3,\n\nCons:\nEmpirical gains might not be very large.
4570,3, In order for the Taylor approximation to be good?
4571,3, \n\n- The authors use spearman's rank order correlation and Top-k intersection as metrics for interpretation similarity.
4572,2,I dont think this study would add anything to either theory or practice
4573,3," This typically yields a non-convex non-concave optimization problem.[[CNT], [null], [DIS], [GEN]] This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm."
4574,3," It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding."""
4575,1,"   The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist. "
4576,3," If so, better move that part to the models section instead of mention it briefly in the experiments section."
4577,1," \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced."
4578,3,. Any references here
4579,3, We definitely see many very similar images in fairly small sample generated.
4580,3, Experiments are only conducted in CIFAR100.
4581,3, Doesn\u2019t this support the argument about need in stochastic prediction?
4582,3,\n\n\nMinor:\n\nThe work is first introduced as multi-layer but only the single hidden layer case is actually discussed.
4583,3, More info on this throughout the paper would be a valuable contribution. 
4584,3, But what's the regularizer that's vanishing?
4585,1,"\n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance."
4586,3," \n\nFor the latter question, the authors propose using a \""query network\"" that based on the current state, pulls out one state from the memory according to certain probability distribution."
4587,3," While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on."
4588,3, In fact this proof shows the situation is much better than that.
4589,1,"\nThe paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied."
4590,1, The idea is interesting and novel that PACT has not been applied to compressing networks in the past.
4591,3, This corresponds to a mixture of Neural Statisicians.
4592,3," Furthermore, given its simplicity, I would expect a comparison against scheduled sampling."
4593,3,"\n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported."
4594,3, What are \u201cproduction percentages\u201d?
4595,1,"""The paper seems clear enough and original enough."
4596,1,"\n\nFollowing the author's response and revisions, I have raised my grade.\n"""
4597,1,\n\nPros:\n- Many experiments which try to study the effect
4598,2,"The following discussion seems to ignore this major flaw, which turns mere arm-waving into Olympic-level calisthenics"
4599,3, The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training.
4600,1,    All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).
4601,1," To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc."
4602,3," E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax."
4603,1," Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments"
4604,3,"\nQuickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E."
4605,3,"\n\nThe proposed method seems effective, and the proposed DSAE metric is nice, though it\u2019s surprising if previous papers have not used metrics similar to normalized reduction in log-ppl"
4606,3,"""This paper examines ways of producing word embeddings for rare words on demand."
4607,3,\n - On p.3 above sec 3.1: What is u?
4608,2,I am not getting much support for your paper. I hope that privately at least you did not have great confidence in it.
4609,1,"\n\nWhile the idea may be novel and interesting,"
4610,1,. It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening.
4611,3," Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015."
4612,3,"\n - To provide a fair comparison authors, should compare their results with other paper results."
4613,3," I mean, the regularizer could push h^b to small norm, compensating with higher norm for the output word embeddings."
4614,3,"\n- It has a limited capacity so if the distribution of environments is too large, its performance will plateau (figure 5)."
4615,3,"\n2. generate new synthetic data, assume this data does not leak private information"
4616,1,"  \n\nThe paper is clear in most points, with some parts that could use further elucidation."
4617,3,"\n-- It would appear that the baselines could be improved further using standard techniques"""
4618,3,"\n[2] M. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. In ICLR, 2016."
4619,2,"Weak, poor experimental design, no analysis possible, carelessly written, poorly thought through"
4620,3,"""This paper explores a new approach to optimal transport."
4621,3,   \n\nThe Circumplex Model of Emotions (Posner et al 2005) the authors refer to actually stands in opposition to the theories of Ekman.
4622,3,"\n\n  - Third, the authors claim that one of their goals is an experimental exploration of tensor factorization approaches with provable guarantees applied to the word embedding problem."
4623,3, How can the method make sure that  simple adding generated images with each component will lead to a meaningful image in the end?
4624,2,The authors should discard the data and collect it again properly.
4625,3," According to Lemma 1 and its finite\nsample version in Theorem 1, the risk on the target domain can be upper bounded\nby the combination of 1) the re-weighted empirical risk on the source domain;"
4626,1, This is an improvement on earlier proposals (e.g. Revisiting Synchronous SGD) that allow for slow workers.
4627,3,". From what I\u2019ve seen, log(tf)-idf LSA seems to perform about as well as LDA"
4628,3," However, it is unclear that the rank of the logit matrix is the right quantity to consider."
4629,1,"\n\nThe experiments are well-presented, and appear to convincingly show a benefit."
4630,1,"\nMost important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it."
4631,3," \n\nA criticism of the paper is that it does not require Stein\u2019s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate."
4632,3,"\n\ne) En the definition of R_AC, I denoted by <k,l> the pair of nodes (k \\ne l)."
4633,3, The Bayesian learning of the last layer is then\ntractable since it consists of a linear Gaussian model.
4634,3," In experiments, results with beta=1.0 need to be presented to assess the importance of network inversion and the reconstruction loss."
4635,3,"\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). "
4636,3, Would running the problem on quadratics with different condition numbers be insightful?
4637,3, Using second order methods is not an end in itself.
4638,3,  The experiments show that sparsity is achieved and still the discovered sparse networks have comparable or better performance compared to dense networks.
4639,3," There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture."
4640,1," Finally, the authors propose a new method by using one unexplored combination of taxonomy features."
4641,3," \""\nFormalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf )."
4642,3,"\n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?"""
4643,1," In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power."
4644,3,"\n(5). In section 4.2.2, the authors write \""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\""."
4645,3," Thus, it is already reasonable to\npractitioners that the proposed linear scaling of batch sizes during training\nwould be effective."
4646,1," There is a start at an interesting idea here, and I appreciate the\nthorough treatment of the background, including CEGIS and submodularity as a motivation for doing\ngreedy active learning, although I'd also appreciate a discussion of relationships between this approach \nand what is done in the active learning literature."
4647,3,"\n\nHowever, there are also a few things to be cautious of... and some of them serious:"
4648,1,  The results is interesting and novel.
4649,1,\n\nPROS \n- A new approach to analyzing the behavior of weight matrices during learning
4650,3,"\n3. Auto encoder network\n\nThese network are jointly trained, and the joint-loss is simply the addition of a cross-entropy loss (from OCN), the binary cross-entropy loss (from PCN) and a pixel wise loss (from AE). "
4651,3," The only novelty is these \""HoW\"" inputs to the extra attention mechanism that takes a richer word representation into account."
4652,3,"""This paper proposed the new approach for feature upsampling called pixel deconvolution, which aims to resolve checkboard artifact of conventional deconvolution."
4653,1,"\n\nThe overall idea and approach being pursued here is a good one,"
4654,3,"The authors point out that the original \""value iteration network\u201d (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network."
4655,3,"  The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation."
4656,3, scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3.
4657,3, Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.
4658,1,\n(b) generating adversarial examples for image classification as well as text analysis.
4659,1," \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation)."
4660,2,"A good deal of effort has been expended here, but to what end? - Reviewer "
4661,3," For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi)."
4662,3,\n\n1) k goes to infinity\n2) alpha goes to 1\n3) g(w*) goes to 0
4663,3,"""This paper produces word embedding tensors where the third order gives covariate information, via venue or author."
4664,3," It's true that Le and Zuidema take a parse forest from an existing parser, but it still contains an exponential number of trees, as does the work in here."
4665,3,"\n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial."
4666,3, The best method finally outperforms the lea-3d baseline for summarization.
4667,3," A very similar type of ideas comes up in constrained or proximal quasi-Newton methods, and I have seen many papers (or paper submissions) on this style of method (e.g., see the 2017 SIAM Review paper on FWI by Metivier et al. at https://doi.org/10.1137/16M1093239). In every case, the answer seems to be that it can work on *some problems* and for a few well chosen parameters, so I don't yet buy that ProxProp is going to make a huge savings on a wide-range of problems."
4668,3," This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form."
4669,3, Are there metrics other than overall accuracy that could be used to measure performance? 
4670,3," Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking."
4671,3,\n[3] M.K. Titsias and N.D. Lawrence. Bayesian Gaussian process latent variable model. 
4672,1,\n\n\n\nStrengths:\n\nThe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms.
4673,3,"\n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas."
4674,3, Is it just that JCP-S also incorporates 2nd order embeddings?
4675,3,"\n\nIt should also be noted that the PANAS (Positive and Negative Affect Scale) scale and the PANAS-X (the \u201cX\u201d is for eXtended) scale are questionnaires used to elicit from participants feelings of positive and negative affect, they are not collections of \""core\"" emotion words, but rather words that are colloquially attached to either positive or negative sentiment."
4676,3," Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader."
4677,1,"""I very much appreciate the objectives of this paper:  learning compositional structures is critical for scaling and transfer."
4678,3," It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right?"
4679,3, Do you know why the trend is not consistent across datasets?
4680,3," \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables."
4681,3,"  Since only depth, width, and skip connections are considered, the end network must end up looking like a ResNet or DenseNet, but with some connections pruned."
4682,1,"\n\nThe model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before)."
4683,3," For example, does this observation that the local curvature of the loss around minima is different for ResNets and VGG allows to interpret the difference in their performances ?"
4684,3, \n- writing of the paper
4685,3, This gives an image that is similar to the original but with features that caused the classification of the disease removed.
4686,1, \n\nOn the whole this seems like a promising paper.
4687,2,Perpetuating the recognition of this in the face of irrefutable evidence is dilettantish and scientifically unacceptable
4688,1,"""Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field."
4689,3,"""This paper proposes to train recursive neural network on subtask graphs in order to execute a series of tasks in the right order, as is described by the subtask graph's dependencies."
4690,3, I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization. 
4691,3," \n[3] M.J. Kusner, B. Paige, J.M. Hernandez-Lobato, Grammar Variational Autoencoder, ICML 2017\n[4] O. Vinyals, M. Fortunato, N. Jaitly, Pointer Networks, NIPS 2015"""
4692,3,"\u201d. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf."
4693,3, The claim of competitive performance needs better justification according to the presentation of the F1 scores.
4694,3,"\n\nWeaknesses:\n\n1.\tIt would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset, other than the size."
4695,3, This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements.
4696,2,this may eventually be a cited paper.
4697,3," Unlike prior work, this is a real-valued instead of a binary quantity."
4698,1,"\n\nSignificance:\nThe proposed methodology is relevant, because disentangling representations are an active field of research and currently are not evaluated in a standardized way."
4699,1, \n3. The result of optimal batch size setting is useful to wide range of learning methods.
4700,3," However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs."
4701,1,\n- The paper proposes a new objective function to measure joint sensitivity
4702,3,"\n\n5) Experiments on synthetic datasets, where both the shift in policy and the\nshift in domain are simulated and therefore can be controlled, would better \ndemonstrate how the performance of the proposed approach (and thsoe baseline \nmethods) changes as the degree of design shift varies."
4703,3,"\n\nIn Section 3.2, the authors listed a couple of loss functions."
4704,3,"""The authors present a scalable model for questioning answering that is able to train on long documents."
4705,1,"  I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper)."
4706,3," However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d."
4707,3,"The actual word embedding method, therefore, has a big influence on performance (as you show)."
4708,3," In this work the authors consider substituting the previous penalty by \""\\lambda E_{z~\\tau}}(max( ||\\grad f (z)||-1,0)^2\""."
4709,3,"  I understand that the authors wanted to\nhave \""what\"", \""which\"" and \""how\"" sections but this is not clear at all."
4710,3,This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal.
4711,1, The clustering by weight (4.1.) is nice and convincing that the model learns something useful.
4712,3," \""Training very deep networks.\"" Advances in neural information processing systems."
4713,3, The challenges in the proposed dataset as outlined in the paper seem worth pushing for.
4714,3," Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters."
4715,3," In Figure 5, k seems to be larger than the number of entities."
4716,2,"Bad language, weird sentences, half true statements and even nonsense statements continue throughout the draft, I refuse to review more of this draft until these language issues get fixed properly."
4717,3, Any practical way of choosing the mask?
4718,2,"There is no research methodology, no data, no model, no significant analysis and no conclusions which arise from the study"
4719,3, The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation.
4720,3, The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed.
4721,3," Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME)."""
4722,3,"\u201d - what does \u201cthe accuracy improvement is smaller than 0.1%\u201d mean?"""
4723,3,"""This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE."
4724,3, The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.
4725,3,It would have been good to have a summary/conclusion/future work section\n\u00a0\nSUMMARY: ACCEPT.
4726,3,\nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference.
4727,3," In International Conference on Learning Representations (ICLR)."""
4728,3,"""This paper introduces a new design of kernels in convolutional neural networks."
4729,3," The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying."
4730,3," Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal."
4731,2,"Large parts of the manuscript read now more like a Master thesis than a scientific paper. I hope that the more experienced co-authors - if there are any - can help with this aspect of style. 
"
4732,3,"  If it is no longer a lower bound, what is the rationale behind maximizing it?"
4733,3,My understanding is that one of the main assumptions made in previous work is that of a center-surround pattern of lateral connectivity.
4734,3,\n\n8th line of 5.1 you mention Nesterov momentum method -> a precise reference and precise equation to lift ambiguities might be helpful.
4735,3,\n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches
4736,3," Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina."
4737,3," Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)?"
4738,3,It would be a good idea to put the test results directly on Fig. 4 as it does not ease reading currently (and postpone ResNet-56 in the appendix).
4739,3,". However, the main techniques such as RBM, parser to extract word pairs, tf-idf for filtering, and k-means for clustering, are all existing standard techniques."
4740,3," Additionally, would it be possible to compute this statistic for *real* images?"
4741,3," that addresses three important problems simultaneously: \n(a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT)."
4742,3," The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task."
4743,3," However, I think it would also be important to compare to other tensor factorization approaches."
4744,3," It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2. "
4745,3," Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n"""
4746,3," They also propose to use a soft unitary constraint on those small matrices (which is equivalent to a soft unitary constraint on the Kronecker product of those matrices), that is fast to compute."
4747,3," It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction. """
4748,3," \n\n1, the derivation of the update of \\alpha relies on the expectation formulation."
4749,1,", but agree with the authors that the theoretical results are non-trivial and interesting on their own merit."
4750,1,  The results are interesting
4751,3," It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value."
4752,1,"\n\nIn general, I find many of the observations in this paper interesting."
4753,1,"""pros:\nThis is a great paper - I enjoyed reading it."
4754,3," For example, this work may guide to design better CNN structure for higher accuracy and lower computation cost."
4755,1,.\n3. The writing is easy to follow.
4756,1,Results are well discussed with reasonable observations.
4757,3," Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline."
4758,3,How can we ensure that some optimal policy can still be represented using appropriate Goal function outputs? 
4759,1,"\n2. With the same number or less of parameters, this method is able to outperform previous methods, with much less time."
4760,3,\n - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward?
4761,3, \n\nOther minor points:\n- In the definition of f_nodes: What is p(y)?
4762,3, It this the case?
4763,1, I can appreciate though that this a fine line to walk.
4764,1," The numerical experiments show a significant improvement in accuracy of the approach."""
4765,3, Are these multiple rollouts of the policies?
4766,3,"Were the word representations fixed, or fine tuned during training?"
4767,3," If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited."
4768,3,"\n- In experiments, although the authors say \""lots of datasets are used\"", only two datasets are used, which is not enough to examine the performance of outlier detection methods."
4769,1, It is shown empirically that the constrained update does not diverge on Baird\u2019s counter example and improves performance in a grid world domain and cart pole over DQN.
4770,3,\n\n4. The real data experiments shows some improvements in predictive accuracy with fast inference.
4771,3, Did authors also experiment with gradient normalization in the intermediate CNN layers?
4772,3,"  In experiments, it is shown that the proposed method improves training speed in terms of number of training iterations, achieves 0.8 CIDEr points improvement over baselines using the proposed training, and also achieves improved performance for the task of speech recognition."
4773,3," The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM."
4774,2,"Theories can be meaningful when they can explain phenomenon. If theories exist for themselves, they may be play of language."
4775,3," The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks."
4776,3," It adds insights that are of value even if the method should turn out to have significant overlap with existing work (see above), and perform \""only\"" on par with these:;"
4777,3," Therefore, i found many of the discussion to be questionable."
4778,3,  The coding scheme then mimics the proporitional-integral-derivative idea from control-theory.
4779,3,"Results on MNIST, CIFAR-10 and ImageNet are very competitive"
4780,3," It would be more instructive to see the results for a better, convolutional classifier."
4781,3," This is the core claimed contribution:\nempirical evidence that these strategies are \""equivalent\""."
4782,3, I think these experiments should be elaborated on.
4783,3, It is unclear what these word features are. Are they one-hot encodings or embeddings or something else?
4784,3,"""This paper applies gated convolutional neural networks [1] to speech recognition, using the training criterion ASG [2]."
4785,2,Black-box modeling exercise using a hodge-podge of data tied together with a poorly-defined model
4786,3," Your results show 4 failed tasks, is this your reproduction of Entnet?"
4787,1," While the idea would be interesting in general,"
4788,3,\n- Could you elaborate on the last sentence of section 4.1?
4789,1,"   I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution. "
4790,1," \n\nBy focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question."
4791,3," I would suggest to either really add multi-hidden-layer results (which is not really doable in a conference revision), or state multi-layer work as outlook."
4792,3,"\n\n(3) What is the relation between the \""PhaseCond, QPAtt+\""\b in Table 2 and the \""PhaseCond\"" in Table 3?"
4793,3," The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples."
4794,3,"\n- Since this paper's main proposal is a methodological one, I would make the publication conditional on the fact that code is released. \n\n\n"""
4795,3," The main idea is to learn \""baseline\"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called \""Ranking-based Exponential Similarity Measure\"" (RESM), which is based on the recently proposed APSyn measure."
4796,1, it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter. 
4797,3, Convolutions/downsampling-convolutions/upsampling that are demonstrated in the paper basically boil down to function prediction on the same exact global graph.
4798,1, In the image captioning experiment the authors choose two networks (Show & Tell and Soft attention) that they improve using the proposed method that end up performing similar to the second best baseline (Yao et al. 2016) based on Table 3 and their response.
4799,1," \n\nThat said, the paper has several positive aspects in all areas:\n\nOriginality - the paper presents first combination of DenseNets with LSTM-based output factorization,"
4800,1," \n\nWhile the CIFAR-100 results look promising, the ImageNet-1k results are less impressive."
4801,3,"  The main approaches fall into two categories: structured, where low-rank/sparse updates are learned, and sketched, where full updates are either sub-sampled or compressed before being sent to the central server."
4802,3,I think the same method can be applied to GRUs or LSTMs
4803,3,"The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies."
4804,3,"\no\tIn general: a block diagram showing the relation between all the system\u2019s components may be useful, plus the details about the structure and optimization of the various modules."
4805,3,"""In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task-specific knowledge into different modules."
4806,3, This analysis is conducted by drawing on results from the field of critical percolation in physics.
4807,3,"  For example, the conjugate gradient-based method leading to the Steihaug-Toint point is so much used. [Note: Here, the gradient refers to the gradient of the quadratic model, and it uses only Hessian-vector products.] http://www.ii.uib.no/~trond/publications/papers/trust.pdf."
4808,3, The point of showing your algorithm not solve Baird\u2019s counter example is unclear.
4809,3, Why does CTC fail when trained without the blanks?
4810,3," Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?"
4811,1,\n\nPros:\n--The paper is very well written and easy to follow.
4812,3," Three NN architectures are explored, which leverage program semantics rather than pure syntax."
4813,3,"\n- Section 4 and Alg 1: S we do not really care about the \""labels/targets\"" of the examples."
4814,3,"Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components."
4815,3," This is then compared with random, word2vec, and NNSE, the latter of two which are matrix factorization based (or interpretable) methods."
4816,1,"\n\nSecondly, this model is just a direct combination of the recent powerful algorithms such as DOC and other simple traditional models."
4817,1, The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility.
4818,3, Some examples beyond the contrived MNIST toy examples would be welcome.
4819,1," In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network."
4820,3," Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST"
4821,3,"""\n\nThis paper proposes to use deep reinforcement learning to solve a multiagent coordination task."
4822,1, \n\nThe proposed approach is well founded and the experimental evaluations are promising. 
4823,3,"\n\n- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq. (1), as also suggested in this work."
4824,3," \nThe authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator."
4825,2,This paper is to science as astrology is to astrophysics' h/
4826,3, how update in (7) is guaranteed to be DP?
4827,3,"\nIt would be also interesting to see if the network somehow learns an implicit global state representation used for planning and how is the developed plan changed when new information from one of the slave agents arrives."""
4828,3, though I make some suggestions for the camera ready version below to improve clarity.
4829,1,"""Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic."
4830,3," The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed. "
4831,3," Showing results with multiple hops (1,2,..) would be useful here."
4832,3, This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model). 
4833,1,\n- Experiments on two different network architectures showcasing the generality of the proposed method.
4834,1, At the very least a comparison to concrete would be nice.
4835,3," Granted this results in a small difference in timing 0.06s versus 0.01s, however it would seem that avoiding a backward pass is a somewhat small speed gain."
4836,1,"\n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014)."
4837,1,\n\nThe unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks.
4838,3, The paper should at least outline the strategy.
4839,3," Therefore, contributions of lower blocks vanish exponentially."
4840,3,"Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?"
4841,1," Additionally, there is a paragraph of \u201carchitecture improvements\u201d which also are minimal changes.[[CNT], [EMP-NEG], [CRT], [MAJ]] Based on the title of section 3, it seems that there is a novelty on the \u201cprediction with flow\u201d part of this method."
4842,2,"There is essentially nothing unexpected, although the central observation of [..] is an unexpectedly large and important effect"
4843,3," Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to."
4844,3," the paper is not missing any information.\n"""
4845,3," It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).\n"""
4846,1,.   The general approach of assigning a parameter budget to ensure fairness in comparison between complex and real-valued networks seems reasonable --
4847,3,It would be better kept as a relative value and plotted on a log-scale so that positive and negative improvements can be viewed on an equal setting. 
4848,3,"\n\""The Socher et al. (2011b) propose a basic form"
4849,3,"  As the authors point out, hashtags and words may be used sarcastically or in different ways from what is understood in emotion theory."
4850,3, We refer to this game as the Pong\nPlayer\u2019s Dilemma (PPD).
4851,3,"   \n\nWith that being said, there is some work that needs to be done to make the paper clearer."
4852,1,\n\nThis is a timely and interesting topic.
4853,1,"\nThough not technically difficult, the extension of GloVe to covariate-dependent\nembeddings is very interesting and well motivated."
4854,1,"""This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks."
4855,3, Finally they learn a model that predicts correctness of syntax in absence of a syntax checker.
4856,3, The method is tested on standard sequential MNIST variants as long as a class incremental variant.
4857,3, The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension.
4858,3, Authors present a set of experiments designed to support this observation.
4859,3," The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets)."
4860,3,"  They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters."
4861,3,"\n\nUnder these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution."
4862,1," The authors make a useful comparison between these bounds in Section 3 showing that none is dominating the others, but still analyzing their properties in terms of structural properties of the weight matrices."
4863,3,"Therefore, there is an open question: are the differences in results between VAN and ResNet in your experiments caused by the removal of skip connections during training or by this scaling?"
4864,3, As a matter of fact we can clearly recognize a squared hinge loss is the proposed loss.
4865,3, How about real online learning with streaming data where the total number of data points are unknown?.
4866,1," This variant is of course very well suited to the proposed method, and a bit artificial."
4867,3,  \n\n(3) It is not clear how will the vocabulary-information be exploited?
4868,1,"""# Update after the rebuttal\nThank you for the rebuttal."
4869,3,"It may be better to relate the\nstructure of the network to other measures of the hardness of a problem, e.g.\nthe phase transition."
4870,3," Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n"""
4871,3," For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported."
4872,3," \""Recurrent orthogonal networks and long-memory tasks."
4873,3," As they use a very similar iterative fine-tuning workflow, it is not clear why the two-pass decomposition + freezing should work better than one-pass decomposition + iterative fine-tuning with no freezing."
4874,3," The idea is to learn to cooperate (think prisoner's dilemma) but in more complex domains. Generally, in repeated prisoner's dilemma, one can punish one's opponent for noncooperation."
4875,3,"""This paper proposes to use graph neural networks for the purpose of few-shot learning, as well as semi-supervised learning and active learning."
4876,3," In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. "
4877,1,"\n\nI believe the paper should be accepted.\n"""
4878,3,  \n- Figure 3 is plotted for just one random starting state.
4879,3,"    How do the authors know that humans are effectively generalizing rather than just \""interpolating\"" within their (very rich) training set?"
4880,3,"""This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism."
4881,3," However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,\u201chorse\u201d) may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse)."
4882,2,At least I say it to your face and sign my name
4883,3," \n- In the case of images, what is a dominant vs recessive pattern?"
4884,3," The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set. "
4885,3,"\n\n\nFrom the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction.\nAlso, there is the work by Hendricks et al on Generating Visual Explanations."
4886,3," However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),"
4887,3," \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections."
4888,1,\n\n-Clarity/Quality\nThe paper is well written and pleasant to read.
4889,3," The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time."
4890,3,\n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence.
4891,3, Also specific details should be reserved until a general explanation of the problem has been made.
4892,2,The examples are stale â€“ the method is not exciting. There is nothing much here.
4893,3,"""The main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of Jia and Liang, EMNLP 2017."
4894,3,\n- Why store rewards and gamma\u2019s at each time step in memory instead of just the total discounted reward?
4895,3," and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale."
4896,1,?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.
4897,3," A major rework should be considered."""
4898,3,  Or the actual optimization?
4899,1,"  That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients."
4900,3,\n\n2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d-1) <= tilde{beta}^(d-1) <= e beta^(d-1) is proven from the property |beta-tilde{beta}|<= 1/d beta (middle of p.4).
4901,3,  This should be at least one of the baselines.
4902,3, How many times is this procedure repeated?
4903,3,"""Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping."
4904,3,"""This paper introduces a generative approach for 3D point clouds."
4905,3,  The authors present their approach and evaluate it empirically.
4906,3, Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin.
4907,3, The experiments show robustness to these types of noise.
4908,3,"\n\nFrom \""CONSEQUENTIALIST CONDITIONAL COOPERATION IN SOCIAL DILEMMAS WITH IMPERFECT INFORMATION\"":\nTo demonstrate this we follow the method of Tampuu et al. (2017) to construct a version of Atari Pong \nwhich makes the game into a social dilemma."
4909,1," For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation. "
4910,3,"\n\nIn section 4, in the algorithmic steps"
4911,3,"and 4) by extensively researching the literature before trying to affirm that a general method (here, Deep RL) cannot solve certain tasks."
4912,1,\n\n2. Clearly written\
4913,2,"It would take a great deal of time to sort this out. From what little I can glean from the scientific nature of this submission, addressing the comments listed above will not yield a submission suitable for XXXX"
4914,3,"""This paper proposes to re-evaluate some of the methods presented in a previous paper with a somewhat more general evaluation method."
4915,3,\n\n* Using dictionary embeddings a la Hill et al.
4916,3," However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported."
4917,3,"\n* Section 2.3: What is meant by \""reconstruction data\""?"
4918,3, It uses back propagation (gradient descent) to improve the design. 
4919,3, 1. Visual navigation where images of start and goal states are given as input.
4920,1,\n(c) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples.
4921,3," Also, if any visualization (over the chart) can be provided, that\u2019d be helpful to understand what is going on. \n"""
4922,3,\n4. Authors stated that the last step is to build a mapping from the GPS features into the response Y.
4923,3,\n\nThis paper would be stronger if it compared with Rainforth et al.\u2019s proposed approaches.
4924,3,It would be excellent if the authors can extend this to higher dimensional time series.
4925,1,"  There are good results on the convex hull problem, which is promising."
4926,3,"""Summary: The authors observe that the current image generation models generate realistic images however as the dimensions of the latent vector is fully entangled, small changes to a single neuron can effect every output pixel in arbitrary ways."
4927,3," \n\nSome other issues regarding quantitative results:\n- In Table 1, there are 152 clusters for 10-d latent space after convergence, but there are 61 clusters for 10-d latent space in Table 2 for the same MNIST dataset."
4928,3,"""This paper examines sparse connection patterns in upper layers of convolutional image classification networks."
4929,3," What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn't merely over-optimize on ROUGE. "
4930,3," \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising."
4931,1," I found the architecture very intuitively laid out, even though this is not my area of expertise."
4932,3," Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''."
4933,3," Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints."
4934,3, This would not\nresult in excluding the unit.
4935,1,"  Specifically, the authors grounded each word in a sentence to all locations of the visual map, and then perform a simple concept detection upon it."
4936,3,\n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc.
4937,3," \n\nThe theoretical result of the ProxProp considers the full batch, and it can not be easily extended to the stochastic variant (mini-batch)."
4938,2,"This paper is conceptually unclear, and the causal argument/hypotheses are muddled. In short, it is a mess. I stopped reading after page 7"
4939,3, It yields smaller receptive field than the proposed method when the model depth is very small.
4940,3, Why not\n  just using better enough basic English and the text of the target user?
4941,3, This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach. 
4942,3," I suggest instead to say that \""|tilde{beta}- beta| <= 1/d (gamma/2B)^(1/d) is a sufficient condition to have the needed condition |tilde{beta}-beta| <= 1/d beta over this range, thus we can use a cover of size dm^(1/2d)."
4943,3,"""This paper studies the amortization gap in VAEs."
4944,3,"However, the writing still needs to be polished."
4945,3, The authors propose to learn subgoals (actually local rewards) to encourage the agent to go towards the same direction as the expert when encountering similar states.
4946,3, Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper.
4947,3, This should be at least discussed in the paper.
4948,1,"\nThe authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow."
4949,1,\n\nOn the whole this is interesting work and the results are very nice
4950,1,\n\n-- The rating has been updated.
4951,3," For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem)."
4952,3," While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning)."
4953,3, The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets.
4954,3,\n\n3. Why comparing to A3C+ which is not necessarily better than A3C in final performance?
4955,1,"More to a point. I think the search tree perspective is interesting,"
4956,3,"\n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm."
4957,1,\n\nThe contribution is clear
4958,3," The main originality seems to be captured in Algorithm 1, which computes the strength between two words."
4959,3,"(Indeed I have a typo in my previous review regarding \""w.r.t. k-th sample\"", which should be \""w.r.t. k-th update\"". )"
4960,3, The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose.
4961,1,"\nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates."
4962,3,"\n- In Sec. 4.4, why is it important that the samples are fresh?"
4963,1,"""- Good work on developing VAEs for few-shot learning."
4964,2,"I sent your MS to three referees in the hopes of finding someone who might like it a little.  Sadly, I failed"
4965,3,The motivation behind it is that \nin GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions.
4966,3,\n\nGeneral Review:\n\nMore experimentation with the latent codes will be interesting:
4967,1,\n\nI am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset.
4968,3," In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen."
4969,3," The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework."
4970,1,"\n\nI believe the work is very promising,"
4971,3, \n* Are we sure that the textual description do not explicitly contain the information of the triple to be predicted?
4972,3,"\nFirstly, only one toy dataset is used for experimental evaluations."
4973,3, I would be interested to know if they authors see ways to generalize to better classifiers.
4974,3," The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \""spectrally normalized\"" objective."
4975,3, \n- It would be interesting to see qualitative visual results on recognitions.
4976,3,"""- The paper proposes to learn a soft ordering over a set of layers for multitask learning (MTL) i.e.\n  at every step of the forward propagation, each task is free to choose its unique soft (`convex')\n  combination of the outputs from all available layers."
4977,3, Or is only the final image and description optimized.
4978,3,"\n- Citations for \u00ab Deep Reinforcement Learning with Double Q-learning \u00bb and \u00ab Dueling Network Architectures for Deep Reinforcement Learning \u00bb could refer to their conference versions\n- \u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner \u00bb: please specify the exact formula"
4979,3,"""This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric."
4980,3,"""Summary: This paper proposes a different approach to deep multi-task learning using \u201csoft ordering."
4981,3," When you train the base learners, their loss functions will become weighted."
4982,1,\n\nOverall I found the paper interesting
4983,3,"""The paper investigates the iterative estimation view on gated recurrent networks (GNN)."
4984,3," Using GAN, the architecture allows for model-agnostic learning, controllable fitting, ensemble graph generation."
4985,1," In general, the paper shows improvements on an image caption agreement task introduced in Kuhnle and Copestake, 2017."
4986,1,    \n       \nIn general I like the idea and I believe that it can lead to a very useful model.
4987,1," The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on."
4988,3, but has a couple shortcomings and some fatal (but reparable) flaws:.
4989,1,"\n\nThough the results are good,"
4990,3, \n\nThe high-level policy is learnt via fairly standard Q-learning (epsilon-greedy exploration policy and a NN function approximator.)
4991,3,"""Summary:\n\nThe paper proposes to learn new priors for latent codes z  for GAN training."
4992,1,\n\nImportance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points.
4993,3," \n\n- Is there anything special about the GAN approach, versus other generative approaches? "
4994,3,"""This paper introduces a model for learning robust discrete-space representations with autoencoders."
4995,3, They perform various experiments to analyze how these quantities depend on modeling decisions and data sets.
4996,3, there is a very saturated market of papers proposing various architectures for CIFAR-10 and related datasets.
4997,1, \n\nClarity:  The paper is clearly presented and easy to follow.
4998,3," In the revised versions, it has been clarified that the tested distribution is not assumed to be uniform but the distribution has \""effectively\"" small support size using an indistinguishability notion."
4999,1,"\n\nSignificance \n\nThis work demonstrates failures of relational networks on relational tasks, which is an important message."
5000,3," It would make a lot of sense to use the same loss as the evaluation metric (not to mention the properties of PCA)."""
5001,3,"\n\n6. In the paper, the author mentioned another sparse-complementary baseline (sc-seq), which applies sparse kernels sequentially."
5002,3," \n7. No large matrices need to be formed or inverted, however more passes needed per outer step."
5003,3,"\""\n\nIn Theorem 2, do you need to care about boundary conditions for your equation? "
5004,3,  The experiments should be reproducible given the descriptions in the paper.
5005,3,Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT).
5006,3,"  This provides a way of comparing models in a way which is independent of the model parametrization (unfortunately, however, computing the evidence is intractable for large networks)."
5007,3," Is it the \""Mesh Upsampling\"" operation defined at the end of page 4?"
5008,2,"As such, this paper is not suitable for publication in a peer reviewed scientific journal of any sort"
5009,3," Could you consider testing the result on more different datasets to verify if the results are generalizable? """
5010,3,"   Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR)."
5011,1,"\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process."
5012,3,  What is the influence of the layer on the performance?
5013,1," The authors demonstrate that this change results in significantly improved BLEU scores across a number of translation tasks. Furthermore, increasing the number of agents/languages in this setting seems to \n\nOverall I think this is an interesting paper"
5014,3," Just as you said right before presenting Eq. (1), typically the goal of learning a DGM is just to match Q_x with the true data distrubution P_x. It is **not** however to match Q(x,z) with P(x,z)."
5015,1," Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine."
5016,1,"\n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. "
5017,3,\n\nSection 5.4:\nUsing a restricted variational family causes the decoder to learn to induce posteriors that are easier to approximate with that variational family.
5018,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. - Edito"
5019,3," The kernel recursive least-squares algorithm. IEEE Transactions on Signal Processing, 52(8):2275\u20132285, 2004."
5020,1,  The excellent Figure 2 supports this point.
5021,1," Also, the presentation is quite clear and the paper well written."
5022,3,References should be added to the relevant methods.
5023,1," \n\nOverall, I believe the idea is nice, and the initial analysis is good,"
5024,1,"  The assumptions that large Fourier peaks happen close to origin is probably well-justified from the empirical point of view,"
5025,3,"""The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution."
5026,3, For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice.
5027,3,"n\n* Regarding your footnote discussing using a positive vs. negative sign on the entropy regularization term, I recommend checking out \""Regularizing neural networks by penalizing confident output distributions."
5028,3," Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector."
5029,3, Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction.
5030,1,"\n\nIt seems a bit strange to say \""The property we gain compared to other generative models is that our generator is a nearly optimal map w.r.t. this cost\"" as if this was an advantage of the proposed method, since arguably there isn't a really natural cost in the generative modeling case (unlike in the domain adaptation case); the latent variable seems kind of conceptually distinct from observation space."
5031,3,  The experiments further verify the performance gain compared against the baseline.
5032,1," My rating changes to marginally above threshold for acceptance."""
5033,3,... I'd want to understand how/why and whether we should expect this universally.
5034,3, but constructing a solid framework that will enable stable and solid application and research of these well-motivated models.
5035,3, It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches.
5036,3," The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited)."
5037,3,"/data with similar co-occurrence statistics, in order for your method to be appropriate?"
5038,1,\n\nWhat I like about this paper is that:\n\n1) The experiments are very carefully designed and thorough.
5039,2,I certainly agree [with your point] here but please do not justify it with the selectively chosen and largely incorrect arguments above.
5040,1," Then, the proposal is simple and easy to reproduce and leads to interesting results"
5041,3,"\n\nConcerning the text, some questions/suggestions:\n- Abstract, line 1: I suppose \""In the Chinese society...\""--- are there many Chinese societies?"
5042,3,"\n\nThey also propose that the reason gradients explode is because networks try to preserve their domain going forward, which requires Jacobians to have determinant 1 and leads to a higher Q-norm."
5043,2,"The paper is also unnecessarily sprawling, verbose, and heavy on extended descriptive exposition of other peoples views."
5044,3," For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer."
5045,3, It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells.
5046,1, \n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original.
5047,3, In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs).
5048,2,"The Vygotsky reference is a straight-up drive-by citation, adding nothing except whatever luster he adds to the authors claims."
5049,3," Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017)."
5050,3, (3) It is stated again later as Theorem 2.
5051,1, This paper establishes an interesting connection between least squares population loss and Hermite polynomials.
5052,3,\n3. Are words that are linked via a dependency better than commonly co-occuring words?
5053,2,The investigator is in the top 50% of his field
5054,3,\n5) It was unclear to me why momentum was used in the MNIST experiments.
5055,2,Find your inner nerdâ€”it must be a big part of youâ€”bind and gap it and then dump it in the ocean tied to a large rock.
5056,3,"  But the same efficient search is possible in many of the classic \""discriminatively-trained\"" KB completion models also."
5057,3, \n\n- What is the rationale for setting the gamma (concentration?) parameters to .01?
5058,3,"\n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly"
5059,1," The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting, "
5060,3,"I know that authors are going to publish the code, but this is not enough at this point of the revision."
5061,3,"""In summary, the paper is based on a recent work Balestriero & Baraniuk 2017 to do semi-supervised learning."
5062,3," In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network."
5063,3,"\nFor ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop."
5064,3, So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play?
5065,3, The paper also advances that GPU parallelization must be used to be able to efficiently train the network.
5066,3," By adding more filters or layers in the model while keeping the same FLOPs and parameters, the models with the proposed method outperform the regular convolution models."
5067,3, Does that have any impact on accuracy of OCN?
5068,2,I want to vomit; I cant believe this paper was submitted.
5069,3," This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset."
5070,3, The weights within a depth channel is shared thus maintaining the stationary requirement.
5071,3, How is L_iw affected by the number of samples used in the estimator?
5072,3, \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded.
5073,1,"""Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words."
5074,1,"  BTW, I would suggest to refer to published papers if they exist instead of their Arxiv version (e.g. Hester et al, DQfD). """
5075,3,\n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n
5076,2,This would seem to constitute the very minimum basic scientific requirement for attempting to publish a body of (unoriginal) data
5077,2,Uninteresting. Unpublishable. Reject.
5078,3," Being succinct and applying rigour in editing is part of doing science and reporting findings, and a wise guideline to follow."
5079,3," Three data sets are considered (Swimmer,  Mocap and  Handwriting)."
5080,2,I want to vomit; I cant believe this paper was submitted.
5081,3,CEGIS both selects fewer examples and has a shorter median\ntime for complete synthesis.
5082,3,\n\nSmaller comments:\n\nYou say that you base the Hessian and gradient estimates on minibatched samples.
5083,3, The proposed method was evaluated on a mobile indoor navigation task and a knot tying task.
5084,3," By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics' convergence to a Nash equilibrium."
5085,1,\n\nCorrectness: The paper is technically correct.
5086,3,"\n\n(2) Ablations. The proposed method has multiple ingredients, and some of these could be beneficial in isolation: for example a population of size 1 with an interleaved distillation phase where only the high-reward trajectories are preserved could be a good algorithm on its own."
5087,1,"\n\nA better result, hinting on how \""optimal\"" this can be, would have been to guarantee that the solution to regularised OT is within f(epsilon) from the optimal one, or from within f(epsilon) from the one with a smaller epsilon (more possibilities exist)."
5088,3,The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function.
5089,3,\nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input;
5090,3, My worry is if you're compressing these networks with your method are the weights not treated as binary anymore?
5091,1,"\n-\tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier."
5092,3,  Is there a PIR per image?
5093,1,\n\nMany of the ideas presented are novel.
5094,1, It is the first to study constructing minimal training sets for NPI given a black-box oracle.
5095,3,"\n\nQUALITY: I understand that the main emphasis of this work is on developing faster computational algorithms, which would handle large scale problems, for factorizing this tensor."
5096,3,  The trained residual function can be used to predict a residual z_i for x_i.
5097,3,\n\nThen the paper talks about robustness properties of the momentum operator.
5098,3,  One could even start with a simple Gaussian and linear parameterization of the mean and variance in terms of x. 
5099,3,  And even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model.
5100,3, It is very probably more costful.
5101,3,\n- The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios.
5102,3, It was well motivated from a computational cost perspective hence the use of a hierarchical prior.
5103,3,"\n\nThe paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms. "
5104,3,\n* Are there particular reasons in using policy learning instead of other reinforcement learning approaches
5105,1,"""Overall strength:\nIn this paper, the authors proposed target-aware memory networks to model sentiment interactions between target aspects and the context words with attentions."
5106,3,"""Summary: The paper studies the problem of effectively training Deep NN under the constraint of privacy."
5107,1," Finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance.   \n""."
5108,3,\nThe model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities.
5109,3," The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning."
5110,3,"""\nSummary:\n- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function."
5111,3, Can the authors show that the procedure will always converge?
5112,3," Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models."
5113,1, \n\nClarity:\nThe paper is easy to read.
5114,2,Line 181: on each of the two trees. What does the term two trees refer to?
5115,3, In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference.
5116,1,"\n\nTo summarize, I believe that the paper addresses an important point,;"
5117,3,\nThe techniques used depend on previous work.
5118,3, Would your regularizer still be helpful there?
5119,1,"\n\n* Section 3.2: The choice of how to get low-variance gradients through the ancestor-sampling choice seems seems like an important technical challenge in getting this approach to work, but there\u2019s only a very cursory discussion in the main text."
5120,3," But before that, there are several issues need to be addressed."
5121,3," \n\nPhysical processes in Machine learning have been studied from the perspective of Gaussian processes. Just to mention a couple of references \u201cLinear latent force models using Gaussian processes\u201d and \""Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations"
5122,3,\n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER.
5123,3,"n\nComments:\n- Eq. (16): $j$ in the denominator should be $t_j$.\n"""
5124,3,"\n- I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is."
5125,3,"\n6) Sec 2 para 7: \""L_target is a target objective which can be a negative class probability or in the case of heart failure, predicted BNP level\"" -- for predicted BNP level, are you treating this as a probability and using cross entropy here, or \nmean squared error?."
5126,3,"\nWe apply the CP decomposition to a pretrained network, then restore it back into the dense format, optimize it, and then apply the CP decomposition again"
5127,3,\n- why focus only on extractive QA?
5128,3,\n\nThere is also one step in the theorem that I cannot verify.
5129,3,"\u201d  The main benefit of the GP is not the uncertainty in the predictions, but the marginal likelihood which is useful for kernel learning."""
5130,3,The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms.
5131,3,"  Although it is a common base-line, some choices are not clear: why using a FFNN instead that a CNN which performs better on this dataset; how data is presented in terms of temporal series \u2013 this applies to the Temporal MNIST too; why performances for Temporal MNIST \u2013 which should be a more suitable dataset \u2014 are worse than for the standard MNIST; what is the meaning of the right column of Figure 5 since it\u2019s just a linear combination of the GOps results."
5132,3," There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous."
5133,3,". What happens if you instead treat it as a regression task, would it then be able to hint at intermediates (a batch size of 96) or extremes (say, 512).\"
5134,3," Alternatively, if the subtask graphs were learned instead of given, that would open the door to scaling an general learning."
5135,1,\n- Introduces novel semi-supervised and active learning variants of few-shot classification.
5136,1, The paper can be understood with no problem.
5137,1,. This result is quite nice.
5138,3, but that\ntheir analysis still my provides some additional leverage for such\nlayers.
5139,3,"""This paper is about low-precision training for ConvNets."
5140,2,"presumptuous, ignorant and downright dangerous."
5141,3, The authors consider the setting of a pre-trained image models as generic representations of generated and real images to be compared.
5142,3,"\n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE."
5143,2,This is a perfect example of the worst kind of research in social psychology
5144,2,The rest of this review operates from the assumption that this paper is a sincere attempt at scientific evidence and argument.
5145,3,\n\n- This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy.
5146,1, One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter.
5147,3," Then there is the additional question, why should we care?"
5148,3, However I am fine accepting it.
5149,3,Does it well describe the new space?
5150,1,\n\nThe idea of empirically studying the manifold / topological / group structure in the space of filters is interesting.
5151,1,"(On the positive side, the appendix provides some interesting details on the tasks generations to understand the experiments.)"
5152,3, Would the authors be willing to comment on the importance of the value of h?
5153,3,\nb) the consequence of the approximation errors on the general convergence of the proposed method (consistency and rate)\n\n-
5154,3,\n\nc)  It is not clear is the softsign is used besides the activation function: In page 5 is said \u201cR_BRE can be applied on ant rectified layer before the nolinearity\u201d 
5155,3,"""The paper proposes and evaluates a method to make neural networks for image recognition color invariant."
5156,3," The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small."
5157,3," First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance."
5158,3, This works address a conjecture proposed by Tian (2017).
5159,3,\n-\tThe experimental results how the idea holds some promise
5160,3,\n\nThe authors conclude that in this experimental setting:\n- AT seems to defend models against shared dx's.
5161,3,"""Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc."
5162,3,"  Two algorithms are presented: NN-1 runs open-loop trajectories from the beginning state, and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state. "
5163,1,"""This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved."
5164,1,"\n\nIn general, the proposed model has novelty."
5165,1, and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.
5166,1," If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option."
5167,2,"However, the applicant seems to have run out of steam before he developed a detailed plan and completed the proposal. This is unfortunate"
5168,1,"\n\nThis is an experimental/methods paper that proposes a new algorithm, explained only in general details, and backs up it up with two reasonable experiments (that do a good job of convincing me of point (1) above)."
5169,3," That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000. "
5170,3,"""The paper compares some recently proposed method for validation of properties\nof piece-wise linear neural networks and claims to propose a novel method for\nthe same."
5171,1,\n\n+ The paper is easy to follow.
5172,3," Rainforth et al. suggest this approach, as well as the approach of averaging K^2 noisy estimates, which the theory suggests may be more appropriate if the functions involved are sufficiently smooth, which even for ReLU networks that are non-differentiable at a finite number of points I think they should be."
5173,1,\n\nStrengths\n - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.
5174,3,"  In the first equation on page 5, is tilde y involved?"
5175,3," As far as I can tell, the paper never answers the questions: Why do we need a guide actor?"
5176,3,"  So are do the \""windows\"" correspond to spatial windows, and if so, how?"
5177,1,\n\nThe analysis of the phases in the hyperparameter space is interesting and insightful.
5178,3," 6:\nin the definition of L_{SR}(s, s'), why \\psi takes \\phi(s) as argument?\n\n- in conclusion:\nthat that"""
5179,3, The argument is that we tend to use the same notion of similarity and dissimilarity to define classes (known or unknown) and one can thus expect the similarity function learned from known classes to carry over to the unknown classes.
5180,3," \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes."
5181,3,d.\n\n# Summary\nThis paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks.
5182,3, Is it possible to use hard negatives as noisy / trick commands and evaluate against them for robustness ? 
5183,3," It would be needed to have a way of visually evaluate the similarity between original images and generated images."""
5184,3, It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy.
5185,3, Do you mean \u201csolves/considers both issues in a principled way\u201d?
5186,3," However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized."""
5187,1, The writing of the paper is excellent.
5188,3,The reward is +1 when the agent collects a health kit and 0 otherwise.
5189,3,\n\nSome open questions that I find crucial:\n\n* How exactly is the \u201cstochastic forward-pass\u201d performed that gives rise to the moment estimates?
5190,3,"""SUMMARY:\n\nThe motivation for this work is to have an RL algorithm that can use imperfect demonstrations to accelerate learning."
5191,1,"""The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick."
5192,2,"The paper is silent on the kinds of issues that occupy yards of library shelf-space, and goes well beyond what is warranted by the data"
5193,3," As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1."
5194,3, The only way to get an SMC estimator\u2019s variance to 0 is to drive the variance of the weights to 0.
5195,1,  Experiments show its usefulness 
5196,3," Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell., "
5197,3, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem.
5198,2,"The paper is overlong, very verbose and contains unnecessary repetition."
5199,3," For e.g. \""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks\"", \""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\"", \""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\""."
5200,2,insights that rarely amount to very much beyond the bleeding obvious. [..] the problem with jargon is that any idiot can have a go
5201,3, The MSCOCO captioning and Flickr30K datasets are used for evaluation.
5202,1,  This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.
5203,3," Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion? "
5204,2,"Its like you sat around and dreamed up ideas, except theyre just castles in the air"
5205,1,  \n\nPros\nThe paper presents interesting ideas regarding unsupervised object discovery\
5206,3," Could predicting presence or absence separately be a way to encourage sparsity, since absence of a unit is already representable as a count of zero?"
5207,2,proposal language is frightfully unclear.
5208,3," In the task, there are multiple taxi agents that a model must learn to control."
5209,3, The authors assume that the global model will depict general english.
5210,3,"\""  It is presented as a collection of observations with no synthesis or context for why they are important. "
5211,3, The authors propose a reservoir sampling algorithm for drawing samples from the memory.
5212,3," Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal."
5213,3," For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk."
5214,3,\n\nI am also curious with the effect of pre-trained model from ImageNet.
5215,2,The presentation is of a standard that I would reject from an undergraduate student
5216,3," One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound."
5217,3,"""1. This is an interesting paper - introduces useful concepts such as the formulation of the utility and privacy loss functions with respect to the learning paradigm\n2."
5218,3, The paper focuses on the theoretical results and does not present experiments (the polynomials are also not elaborated further).
5219,3,"""The paper develops a technique to understand what nodes in a neural network are important\nfor prediction."
5220,3,"   \nSecond, in non-convex problems, one can expect curriculum learning approaches to also perform better, not just converge faster. "
5221,3, The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions. 
5222,1, They also provide a tight bound for the one dimensional input case.
5223,3," The main contribution is a) insight into the connection between the MAML procedure and MAP estimation in an equivalent linear hierarchical Bayes model with explicit priors,"
5224,3, However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain).
5225,1,\n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari.
5226,2,"In any aspect that this paper is different from XXX et al., (20XX), it shouldnt be"
5227,3, It provides some insights on the challenges and benefits of replay based memory consolidation.
5228,3,\n\n- The fact that the attack OptMargin is based in L2 perturbations makes it very susceptible to a defense that backprops through the attack.
5229,1,"The authors tackle a very important problem, the one of learning low precision models without comprosiming performance."
5230,1, \n\nAn important feature of this model is it is easier to parallelize and speed up the training/testing processes.
5231,3,"  Specifically, they propose to perform the augmentation on the semantic space representation, obtained from the encoder of this autoencoder."
5232,1, The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product.
5233,2,"This, of course, is disingenuous if not unethical"
5234,1, \n\nThe high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge (though I am not an expert in this space).
5235,1,"\n\n- Significance:\nWhile looking at RNN from an iterative estimation is interesting,"
5236,2,I now turn to my best guess about what the authors might be doing
5237,3,"\n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case."
5238,3,\n\nTheir contributions are:\n\n1. Formulate complex valued convolution
5239,3,"\n\nTypos etc.:\n\n* \u201clearn a particular series intermediate\u201d missing \u201cof\u201d.[[CNT], [CLA-NEU], [CRT], [MIN]]\n\n* \u201cTo do so, we generate on sequence y1:T\u201d s/on/a/, I think?"
5240,3," The justification given is that it is \""to address the difficulty of training due to the complex nature of the problem\"" but this is not really satisfying as the problems are not that hard."
5241,1,"\n\nOn reading the abstract and knowing quite a bit about proximal methods, I was initially skeptical, but I think the authors have done a good job of making their case. "
5242,1, \n\nStrengths:\n- The derivation of the dual formulation is novel
5243,2,Find your inner nerdâ€”it must be a big part of youâ€”bind and gap it and then dump it in the ocean tied to a large rock. -Referee 
5244,3,\n\n* Using character-level models a la Ling et al.
5245,3," Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G."
5246,3, There is no reason to believe that ASG can be faster than CTC in both training and decoding.
5247,2,I am constructing this review more in a stream of conscious thought than a systematic assessment.
5248,1," This paper offers a new conceptual setup to look at the problem and consolidates different views (successor repr, proto values, eigen decomposition) in a principled manner."
5249,3, I would like to see more formal definitions of the methods presented.
5250,3,\n* sec4.1 innaccurate\n* well approximated\n* sec4.2 an curvature\n* (Amari 1989)\n* For the the Laplace\n* O(n^3) : what is n ?
5251,1,\n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data)
5252,3," Also, are ignoring the gradient of the value at the next step?"
5253,3, A few minor presentation issues:\n- ReLu --> ReLU\n\n+
5254,3,"\nBut if the other reviewers argue that the paper should be accepted I will change my score.\n\n"""
5255,3, If these are arXiv give the proper ref.
5256,3," First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points. "
5257,3," I'd re-evaluate my rating after looking at the data in more detail.\n"""
5258,3,Is this a problem in practice (it seems to happen on your curves)?
5259,3,"\n\nIn figure 4a, x-axis should be \""number of landmarks\""."
5260,1,\n9. The examples provided in the appendix are great.
5261,3,\nHow does the likelihood of the model behave under the circumstances?
5262,3," While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized."
5263,3,"\n\nCaption of Fig 1: \""subject/object\"" are syntactic functions, not semantic roles."
5264,1, \n\n* Related Work\nThe authors do a good job describing and listing the papers most related to the current submission.
5265,3,"The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing."
5266,1," This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection."
5267,3,\n\nResults on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match).
5268,3,"""This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training."
5269,3," Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures."
5270,2,The problem is epitomized by almost every word choice in the title (vi
5271,3," For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima."
5272,1," Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM."
5273,3,    The computation of f-y_r\\phi(s_r) makes it hard to understand.
5274,3,"However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\"
5275,3,"\n\nOn the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text. "
5276,3, How diverse are the sketches?
5277,3,\n\nAnother concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported.
5278,3,"""1. Summary\n\nThis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change."
5279,3, I gather the tensor Wa = R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding \\phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time? 
5280,3,"  Whether this good performance is due to your contributions or due to effectiveness of the baseline algorithm, proper analysis and discussion is required and counts as useful research contribution.\n"""
5281,3, \nSo this paper is innovative in two parts:\n- it applies GANs to adversarial example generation\
5282,1, \nIt seems that one of the claimed benefit is that the proposed method is effective at identifying the k.
5283,3, Results are shown on MNIST and Fashion MNIST.
5284,3," All of this analysis provided more insight into the method and helps the reader understand its extents. \n"""
5285,2,Are you kidding?
5286,3," Also note that subspace identification can estimate (A, B, C, D) matrices which is great for control purposes especially for the infinite horizon LQR."
5287,3," For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?\n\nSection 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results."
5288,2,"Can you explain this part a bit further, but without going into detail"
5289,1," Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance."
5290,3," While this analysis was originally used for residual networks, they relate this to any network by letting I turn into an arbitrary initial function. "
5291,2,"The paper is definitely exploratory, but probably not of interest to people other than the author."
5292,3,"  In other words, it seems that if we just draw the box a bit differently, the environment soaks up the nondeterminism, instead of needing to define a new type of Q-value."
5293,3," \n2-\tIt is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume)."
5294,3,".  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \""generic\"" tensor."
5295,3,"""This paper studies the problem of learning one-hidden layer neural networks and is a theory paper."
5296,1,The paper is clearly written and presents the theory and experimental results nicely.
5297,1, The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.\n\nPros: \n1.
5298,2,This is rubbish!!!
5299,3, Why not use the more standard 1/t decay?
5300,1,. Therefore the main advance is in terms of learning speed to obtain this similar performance.
5301,3,"""This paper dives deeper into understand reward augmented maximum likelihood training."
5302,3,"  For example, what is the output of the Eq. (7)?"
5303,3, The reviewer believes that this space could be better spend discussing the theoretical implication of the proposed regularization (see next).
5304,3,The authors re-invent this and find it works better than randomly choosing a gold token or taking the max.
5305,3," Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%."
5306,1,"In other words, the proposed technique is rather stable and can not be easily exploited."
5307,3,"\n* The practical advantages of the proposed approach are twofold:\n1. Given a fixed parameter budget, coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets."
5308,1, This is an important and useful problem in robotics and other\napplications.
5309,1,"\n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting."
5310,1," What is less trivial is estimating (4) for various activations, and that seems to the major methodological contribution."
5311,3," \n\n\n-- Comments and questions to the authors:\n\n1. In the introduction, please, could you add references to what is called \""traditional solutions\""?\"
5312,3,\n\nOverall I found the paper interesting but not ground-breaking.
5313,3," This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks."
5314,3," The method is based on a adversarial training: the generator produces filters, and the discriminator aims to distinguish the activation maps produced by real filters from those produced by the generated ones."
5315,1,\n\nOther questions and comments:\nThe ablation shows 0.7 improvement on EM with mixed objective.
5316,3,"""Summary: The authors show that using visual modality as a pivot they can train a model to translate from L1 to L2."
5317,3," It\nwould be better to take the best of many runs or to somehow show error bars,\nto avoid the reader wondering whether gains are due to changes in algorithm or\nto poor exploration due to bad initialization."
5318,3, \n\nThe authors say they compare to DQfD but the last version of this method makes use of prioritized replay so as to avoid reusing too much the expert transitions and overfit (L2 regularization is also used).
5319,3,   I will happily upgrade my rating of the paper if the authors can address my concerns over prior work in the experiments.
5320,3,"  \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,"
5321,3,\n\nFurther comments:\n- The proposed method to incorporate numerical data seems quite ad hoc.
5322,1,\n\nNice work creating an implementation of fast GGNNs with large diverse graphs.
5323,3," They compare this optimization method with two baselines in MNIST and CIFAR, and provide an analysis of the decision boundaries by their adversarial examples, the baselines and non-altered examples."
5324,3," By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies)."
5325,3,"\n\nIn table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io."
5326,2,It reads more like a diagnosis confirmed by a set of examples
5327,1, They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters).
5328,3,"  It would be helpful to make the description here more explicit and clear."""
5329,3,"\n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \""standard\"" or \""conventional\"" LSTM implementation (e.g., as provided in optimized GPU libraries)."
5330,3,"\nAs the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y)."
5331,3,)\n\nThe work is inspired by previous results for feed forward nets and CNNs
5332,3,.\n\n(1)\tThe SW-SC kernel (Figure 2 (a)) is an extension of the existing shaped kernel (Figure 1 (c)).
5333,3, \nWhy not train (finetune) the considered models using softplus activations instead of exchanging activation nodes?
5334,3,". They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time."
5335,3," For example, the update equations in Adam were specifically designed to correct for this effect."
5336,3, \n- What is the effect of network hyper-parameters?
5337,3,\n\nExperimental Setup and Training Details\n- How was the model optimized?
5338,3, Why not do lots of random initializations for the optimization?
5339,3, It may be more clear to change this phrase to \u201cevolutionary methods\u201d or similar.
5340,1, \n\nThe paper is clear and well written.
5341,1," On the other hand in sea quest and space invaders, where your method does worse, the l2 error is better."
5342,3, Extremely low bit neural network: Squeeze the last bit out with ADMM.
5343,3, \n\nMinor comments: \n\n- What is the meaning of the dashed lines and the solid lines respectively in Figure 1? 
5344,3,"""This paper proposes a supervised variant of Kohonen's self-organizing map\n(SOM), i.e., trained by gradient descent with gradient obtained by\nbackprop, using a grid of RBF neurons which respond to the input only if\nthey are in the grid-neighborhood of the 'winner' neuron (closest to the\ninput)."
5345,3,"\n\n3. Negative images are fast to obtain, but they are oversimplified, and can be obtained via linear transform which is easy for neural networks."
5346,3,"n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem?"
5347,3,\n - why is your definition of generalization that the test set distance is strictly less than training set ?
5348,3," However, if you look closely at some pictures, you can see that they are very different though reported as similar."
5349,3," This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful."
5350,2,"I appreciated how the author assumed that a goodly percentage of
the readership aren't native speakers, so anything academic would be lost"
5351,3,"   My feeling is that this work is a bit preliminary at present,"
5352,3, The work is rather incremental but is competently executed.
5353,3, A very simple queue for the latter is shown to do quite competitively in practice.
5354,3,"\nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain."
5355,3, It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy.
5356,3,"\n\nIt does not make sense to say that \""The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations\"", and then proceed to do exactly that (in canonical coordinates)."
5357,3,"  I'm not an expert in this area but the contribution seems relevant to me, and enough for being published."""
5358,3, How do you deal with this case?
5359,3, The features are selected such that ...  determines the distribution\nof future observations \u2026 Filtering is the process of mapping a predictive state\u2026\u00a0\u00bb
5360,3, How does the proposed technique compare to existing methods in terms of runtime?
5361,1,"""[ =========================== REVISION ===============================================================]\nI am satisfied with the answers to my questions. "
5362,1,"\n\nPlanning lane-change maneuvers is an interesting, important problem for self-driving vehicles."
5363,3,"If only\n  \""the portion of\"" general English must be communicated, why is it validated?"
5364,3,"\n\nSection 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the \u201cbaseline\u201d Random Forest classifier."
5365,3,\n\u2013 The perception-driven control formulation is well-detailed and simple to follow.
5366,2,"When the reader is finished struggling through all the methods and results, he/she is left wondering whether it was worth the time."
5367,3,n\nThe bot performance significantly better than the fully trained agent.
5368,3," I thought, that it should model the joint (empirical) distribution over the labels, and this is part of the dataset."
5369,3,"  Given its reduction of processing-intensive and need for larger number of iterations, how much worse is the random choice (no processing, independent of iterations)?"
5370,3,\n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks.
5371,3,"\nPaper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler."
5372,3,\n- Are the experiments single runs?
5373,3,\n\nThe paragraph after eq. 17 is duplicated with a paragraph introduced before.
5374,1,\n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written.
5375,3, b) and what is the percentage of cycles saved by employing the ISRLU.
5376,3,  The final threshold for 'successful' speedups feels somewhat arbitrary -- why 16ms in particular? 
5377,1,\n- Moving the state of the art in low precision forward
5378,1,"""Hi, \n\nThis was a nice read."
5379,3,\n\nWhat about distributed SGD or asyncronous SGD (hogwild)?
5380,1," In addition, the algorithm in this paper selects the best performing network at each step, which also hampers the discover of the optimal model.\"
5381,3, Is this because of some stopping condition or because of gradient explosion?
5382,3, It is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding (classification) relies on artifacts or confounds.
5383,3,"\nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case."
5384,3, The review of NAT is too brief and makes it too hard to understand the remaining of the paper.
5385,3,". It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function."
5386,3,". \n\n-- In Figure 1, the \u201cG1\u201d on the right should be \u201cG2\u201d;\n-- Section 2.2.1, \u201cX_f\u201d should be \u201cx_f\u201d;\n-- the motivation of having \u201cz_v\u201d should be introduced earlier;\n-- Section 2.2.4, please use either \u201calpha\u201d or \u201c\\alpha\u201d but not both;\n-- Section 3.3, the dataset information is incorrect: \u201c20599 images\u201d should be \u201c202599 images\u201d;\n\nMissing reference:\n-"
5387,3,"  As a side note, I would have expected the authors to use actual behavioral data but instead, the network is trained using artificial trajectories based on \""modified Brownian motion\u201d."
5388,3,"""Learning sparse neural networks through L0 regularisation\"
5389,3,"  Also, what about if we include the bias term so that b + w a is the preactivation value?"
5390,3," \n- I understand that the main advantage of this method is the speedup; however, providing the final accuracy as a function of the nonzero entries for slower methods (e.g. the sparse pruning showed in Fig 1. a) would provide a more complete picture."
5391,3, to be sought for here?
5392,3, Can you provide a citation for this?
5393,3," However, it is not clear to me that the method is invulnerable to novel white-box attacks."
5394,3,"\nAdditionally, the idea of factorized representation idea (describable component and indescribable component) has a long history."
5395,3, RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time.
5396,3, Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron.
5397,3,"""Summary of the paper\n---------------------------\nThe paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training."
5398,3," The second idea is to allow bootstrapping at episode boundaries, but cutting off episodes to facilitate exploration."
5399,2,The introduction is so confusing and purely written that I gave up.
5400,1,"""This is a well written paper on a compelling topic: how to train \""an automated teacher\"" to use intuitive strategies  that would also apply to humans."
5401,3," \n\nMinor comments: \nIn introduction, parameter with zero training error doesn't mean it's a global minimizer\nIn section 2, it is not clear that visualizing loss function is helpful in see the reasons of generalization given minima."
5402,3, \n* Which kind of error would using a convolution architecture for the encoder decrease?
5403,1," \n\nSecond, the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach."
5404,1,"""The topic discussed in this paper is interesting."
5405,3,"""The authors propose the N-Gram machine to answer questions over long documents."
5406,1,\nThe paper is organized well.
5407,3," The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU?"
5408,3,"n\nThe model is a residual gan formulation, where the generator generates an image mask M, and (Input + M) is the adversarial example.\"
5409,1, The quantitative results also support the visuals.
5410,3,"  Besides, as Veit showed, ResNet also shows ensemble behavior."
5411,3,"""This paper deals with early stopping but the contributions are limited."
5412,3,"""Summary:\nThis paper proposes a data augmentation method for one-shot learning of image classes."
5413,1," Experimental evalution is well done against a number of recently developed alternative methods in favor of the presented method,"
5414,1,\n\nFigure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls
5415,3," Another positive aspect of the paper is that the synthesis results can be analyzed, providing insights for the generation process."
5416,3,"""This paper introduces siamese neural networks to the competing risks framework of Fine and Gray."
5417,3, How many pairs were extracted?
5418,3," The evaluation consists of hypernym detection on WordNet and graded lexical entailment, in the shape of HyperLex."
5419,3,  What is the upper bound on the size of PATH lengths you can train?
5420,3," For instance, there are many related papers on:\n\n-taxi fleet management (e.g., work by Pradeep Varakantham)\n \n-coordination in multi-robot systems for spatially distributed tasks (e.g., Gerkey and much work since)\n\n-scaling up multiagent reinforcement learning and multiagent MDPs (Guestrin et al 2002, Kok & Vlassis 2006, etc.)"
5421,2,"Not sure how to say this diplomatically, but the manuscript is really dull"
5422,3,"""This paper proposes a novel regularization scheme for Wasserstein GAN based on a relaxation of the constraints on the Lipschitz constant of 1."
5423,1," Not surprisingly, as this framework is able to learn from way more data (e.g. in Atari), it outperforms the baselines, and Figure 4 clearly shows the more actors we have the better performance we will have. "
5424,3, There are many ways to create rational incentives for neurons in a neural net.
5425,1,"""This paper suggests a simple yet effective approach for learning with weak supervision."
5426,3, and 3) testing the DTP algorithm on locally-connected architectures.
5427,3,"\n- a module transforming the image into a log-polar representation according to the predicted origin,"
5428,3,\n\nThe size of a kernel matrix depends on the sample size.
5429,1, The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.
5430,1," However, the method is not properly motivated."
5431,1, This work makes a pretty significant contribution to such topic.
5432,1," The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty."
5433,3,  The goal function tries to predict states in this latent space.
5434,3,"  You can make an argument for why the one that was ablated was \u201cmore interesting\u201d, but really this is an obvious empirical question that should be addressed."
5435,1, I agree with the second reviewer that the approach is interesting.
5436,1,"\n\nThe contribution to the practice of PSRNNs seems significant (to my non-expert eyes): when back-propagation through time is used, using ORFs to do the two-stage KRR training needed visibly outperforms using standard RFMs to do the KRR."
5437,1,\n\nPositives:\n- The three properties of visual concepts described in the paper are interesting.
5438,3, Are the number of parameters in the proposed approach and the baseline VAE similar? 
5439,3, I would appreciate the authors to elaborate this a bit.
5440,3," For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)? "
5441,3,"  Perhaps future work will see if the results are much different in other languages.\n"""
5442,3,"  As it stands, this is somewhat an unknown, and should be easy enough to demonstrate."""
5443,3," \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective."
5444,3, Is there an elbow that indicates the threshold cut?
5445,3, Is the time budget different for each new generated environment?
5446,3,"""This ms presents a new clustering method which combines deep autoencoder and a recent unsupervised representation learning approach (NAT; Bojanowski and Joujin 2017)."
5447,3,"   This property is visible in I(Z_2,X) for PIB in Figure 3, but otherwise absent."
5448,3, They claim that these augmentation types provide orthogonal benefits and can be combined to yield superior results.
5449,3,"""This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy."
5450,3,"""\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful."
5451,1,"\n-\tThe experiments show that essentially, the latent defenders are stronger than the input defender in most cases."
5452,3, The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary.
5453,3, The method is demonstrated on a toy example and on the task of unsupervised domain adaptation.
5454,1, The paper is technically correct and nontrivial.
5455,3,  The authors devise a heuristic way (based on an innovated measure that combines computational complexity with performance) to select the tensor rank to be used.
5456,3,"\n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability."
5457,1," This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline."
5458,3," In particular, the formulation can be seen in a different way."
5459,3," At best, GANs generate reasonable images that are lower resolutions (e.g. < 128x128)."
5460,3," This work is an extended version of [1], aiming to address the high-dimensional problem."
5461,3," However, a fully\ncooperative agent can be exploited by a defector."
5462,3,"\n\n- The objective of the attention mechanism that sets the dropout mask seems to\n  be the same as the primary objective of classifying the input, and the\nattention mechanism is prevented from solving the task by adding an extra\nentropy regularization."
5463,3, This paper seems to be applying this exact same strategy in training for a cross-entropy classification loss f(.).
5464,2,This made the paper very long and may bore the reader
5465,2,The results look like a smorgasbord of data
5466,3," The paper studies locally open maps, which preserve the local minima geometry."
5467,3," The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices."
5468,3,"  But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick."
5469,1,"\n- I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances)."
5470,3,"""This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes."
5471,2,"The paper comes with proofs, but â€“ at a first glance â€“ they seem to be more cute than useful."
5472,3," I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from \""slope\"" to \""gradient norm\""."
5473,3," In evaluation time, they insert these cells between layers of a network comparable in size to known networks."
5474,3, So I assume there are multiple steps between s and s\u2019?).
5475,3,"\n[2] Larranaga, P. (2002). A review on estimation of distribution algorithms."
5476,3,"It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \""raw\"" bootstrapped DQN) and UCB still looks like it does better."
5477,3,"  They demonstrate the quality of the sequences on sine waves, MNIST, and ICU telemetry data."
5478,1,"\n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value)."
5479,1," I enjoyed reading the paper,"
5480,3,"  The actual setup seems somewhat arbitrary,"
5481,3," To start with, GAN is already training the generator G to match the induced P_G(x) (from P(z)) with P_d(x), and hence by definition, under the generator G, there should be no better prior than P(z) itself (because any change of P(z) would then induce a different P_G(x) and hence only move away from the learning target)."
5482,3," Further, Q-masking largely amounts to simply removing actions that are infeasible (e.g., changing lanes to the left when in the left-most lane), but is seems to be no more than a heuristic, the advantages of which are not evaluated."
5483,3,"""The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row."
5484,3," I would expect to see a comparison of the vanilla prototypical nets against their method for each one of the different scenarios of the free parameters of the S matrix, something like a ratio of accuracies of the two methods in order to establish whether learning the Mahalanobis matrix brings an improvement over the prototypical nets with an equal number of output parameters.  \n\n"""
5485,3,\n\n1. Why is universal perturbation an important topic (as opposed to adversarial perturbation).
5486,1," Upon closer inspection, I read a much stronger introduction and appreciated the summaries at the ends of sections 4.4 and 6."
5487,3,"\n\n\nOverall, the paper is well-written but the novelty and applicability seems a bit limited."
5488,1, It is introduced with 'We ask' followed by two well formulated lines that make up the hypothesis.
5489,3,\n\nHow important is using the pretrained weights from the deterministic RNN?
5490,3,\n\nThis is clearly an application paper.
5491,3," why is the policy a sum of \""p^{cost}\"" and \""p^{reward}\""? "
5492,1,  The proposed method appears to have a modest improvement for few-shot learning.
5493,1,\n\nCons:\n\nWhile we liked both the challenge posed and the idea to solve it we found several major issues with the work. 
5494,1,"\n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal."
5495,3," The dataset, as well as all investigated models/approaches, are existing work."
5496,3, Sufficient experiment results show that the proposed method has better convergence rate than [1].
5497,1,"\n\nSection 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid)."
5498,1," I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al. and Maddison et al. work)."
5499,1, While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.
5500,3,"\n6 .In the evaluation, why are just 12 tasks used in the Amazon dataset?"
5501,3,"\n\nFirst of all, I have to say that the paper is very much focussed on the aforementioned paper, its experiments as well as its (partially speculative) claims."
5502,3,"\n\n\nA lighter version of Algorithm 1 in Appendix F should be moved in the text, since this is the novelty of the paper.\n"""
5503,3, \n\nThe proposed refutation is based on the following experiments:
5504,3, Do you have any guesses as to why this might be?
5505,3," Please give references for the evaluation metrics used, for proper credit and so people can look up these tasks."
5506,3," The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training."
5507,3, The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences.
5508,1,\n\n+ The proposed approach is a relaxation of the constraints on the dual variable for the OT problem.
5509,3,"   \n\nIn the numerical experiment, the parameter \\tau_\\theta is sensitive to the final solution."
5510,3," This will make it less usable, I think it's necessary to provide the training time comparison."
5511,3, Are the differences between SQDML/RAML and ML significant?
5512,3," Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains."
5513,3," In the current statement, the result seems independent to $L$ when $L \\geq 2$."
5514,2,"here is a lot of terminology flung around such as false negatives, false positive and median, first quartile, third quartile"
5515,3,"\nWhile the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive."
5516,3, \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.
5517,2,Ah - now I see a glimpse of promise in this paper - Five pages into the documen
5518,1,\n\nVery nice paper.
5519,3,"This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection."
5520,1," I have only one question:\n\nin the simplified versions, content(x_t) = Wx_t , which works very well (outperforming full LSTM)."
5521,1," For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6)."
5522,2,The current manuscript is deceptive and I do not recommend its publication.
5523,1,"                 \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper. "
5524,3,\n\nConsidered paper is one of the first approaches to learn GAN-type generative models.
5525,3, It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication.
5526,3, \n\nOther comments and questions:\n\n1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel.
5527,1, They are very close to existing algorithms
5528,3," The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs."
5529,3,"  I'm also unsure whether the windows are over spatial extent only, or over features."""
5530,3," In practice, with deep D, trained by single gradient update steps for G and D, instead of the \""argmin\"" in Algo 1., the assumptions of the theory break."
5531,3,  How does the game do on the original task?
5532,3, It would be clearer if they stated explicitly that the alignment is between covariate-specific embeddings.
5533,3, Even storing a 50k x 50k matrix requires about 20GB of RAM.
5534,3," I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation."
5535,3," The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples."
5536,3," The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator."
5537,1," Also, it was interesting how that was found to be high on AAE due to mode-collapse."
5538,3," The only explanation is that this method mimics the genetic algorithm. However, this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability."
5539,2,revise this paper at your own risk 
5540,3,"However, I didn\u2019t see such detailed analysis as in the other papers on controllable image generation."
5541,3,"The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset."
5542,3,  Chebyshev polynomial is used for faster computations.
5543,3,\ncons:\n 1. SVD is a standard tool for subspace and manifold analysis for decades of years.
5544,3," How was the train/val/test split done, etc."
5545,1,\nThe paper shows nice results on a number of small tasks.
5546,3,"""This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations."
5547,3," Is accuracy stable, can it drop back down below the threshold in the next epoch?"
5548,3, What specific optimization is being depicted?
5549,3, The three main decision functions in the sequential process are computed with neural nets.
5550,3,"  The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality."
5551,3,"""\nThe authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL)."
5552,3,  I doubt that a PhD student would be able to reimplement the method and achieve comparable results given the paper at hand only.
5553,3," With this in mind, it is of course completely fine that the results are not better than for real-valued networks.\n"""
5554,3," One possible explanation is as follows: in the zero-shot learning, one has access to large training data to learn the semantic embedding (training classes)."
5555,3,"In general, when we project our update to be orthogonal to the maximal change of the next states value, how do we know it is a valid direction in which to update?"
5556,3," Specifically, the authors say that for each image they produce 5 additional \""virtual\"" data points, but when multiple methods are combined, does this mean 5 from each method? Or 5 overall? If it's the former, the increased performance may merely be attributed to using more data."
5557,3,"  If so, the training time becomes extremely important (and should be included in the \u201cNN Phase\u201d time measurements in Figure 4)."
5558,3,\n\nI found no typo or grammatical errors which is unusual - good careful\njob
5559,3," Note that a square euclidean regularization leads to a regularization term in the dual of the form max(0,f(x)+f(y)-|x-y|)^2 that is very similar to the proposed regularization."
5560,3,"\n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix."
5561,3, My main concerns are the following:\n\n1) One motivation of DFM is that in many applications data is a discretization of a continuous process and then can be represented by a function.
5562,3," If that is the case, could the authors describe this a bit further?"""
5563,1," \n\n*Originality + significance*\n\nTo my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs."
5564,1," The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms."
5565,3,\n\nI'd be interested in how their proposed VGG-based PIR actually correlates with human evaluation.
5566,3,\n2. Should state and verify conditions for application of implicit function theorem on page 2.
5567,3, Have the authors been evaluating many times on the test data?
5568,3,"\n\nCons:\n(1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case."
5569,1,  Authors captured a new facial dataset for their evaluation and reported better results than PCA.
5570,3," In this case, using a teacher model trained on a harder task (CIFAR10) leads to much improved student training on a simpler task (MNIST). Why?"
5571,3,\n\nOne last related literature is pedagogical teaching from computational cognitive science.
5572,3,\nUsing so-called learning curves is a good way to answer this.
5573,3, Then make scatter plots of these quantities against each other.
5574,1,\n- Scales to high-resolution images.
5575,3,"\n\nThe authors write that artificial intelligence has mostly overlooked the role of teaching, but this claim is incorrect.[[CNT], [CNT], [CRT], [MAJ]] There is a long history of research on teaching in artificial intelligence."
5576,1, In a number of places there are clear signs of sloppiness (e.g. undefined citations). 
5577,3," With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.),;"
5578,3,"\n\nThe former set of works, while focused on machine translation also learns a translation table in the process"
5579,3,"""This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks."
5580,3,\n\n************************\nThe authors provide an algorithm-agnostic active learning algorithm for multi-class classification.
5581,3,"\n- It is considered that the GAN are trained without any problem, and therefore work in an optimal regime"
5582,3, Face topologies are so regular that they can even be represented with a height map like geometry encoding in the image plane (See [1'] below).
5583,3, Could the authors comment on how sensitive the method is to this parameter?
5584,1, \n\nClarity: Clear.
5585,3,Paper written so that it's easy for a reader to implement the methods\n\t\u2022\t
5586,1,\n\nComments:\n1) This paper is well motivated.
5587,3," Indeed, when the model goes deeper, the receptive field becomes very close to that of the proposed method."
5588,3," Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text)."
5589,3," Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server."
5590,3," In the first line, shouldn't the first term not contain \\Delta W ?"
5591,2,Perhaps I have just read these papers at monthly intervals and the author had bad luck.
5592,3,"""The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one \""slow\"" trained as usual and one \""fast\"" that gets updated in every time-step based on the slow network."
5593,3,\n- How is the system supervised?
5594,3, A bit more discussion on these choices would be helpful.
5595,3,"  \n\n- I suggest to divide Section 3.1 in two subsections. The first one introducing Stein\u2019s identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title \u201cStein Control Variate\u201d."
5596,3,"""This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles)."
5597,3, \n\nI do have some questions about the training itself.
5598,1," Experiments show that the system achieves a better performance than different subparts of the system (through an ablation study), state of the art and common open source systems."
5599,3,"""The paper discusses the problems of meta optimization with small look-ahead: do small runs bias the results of tuning?"
5600,3,\n\n- I would have liked to see some context as how these results compare to an approach trained with aligned corpora. 
5601,3, I also recommend stating which ideas came from the Sandryhaila and Moura (2013) work in a more pronounced manner.
5602,3,"\n* Graph Convolution: A High-Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)"
5603,3,"\n\nThe morphological agreement task would be an interesting contribution of the paper, with wider potential."
5604,3,\n\nI\u2019d like to see more discussion of why the second stage supervised problem is beneficial.
5605,3," For example, how does this compare to random perturbation (say, zero-mean) of the weights?"
5606,3," However, recent work (e.g. Samek et al., IEEE TNNLS, 2017 or Montavon et al., Digital Signal Processing, 2017) showed that decomposition-based methods (such as DeepLift) provide less noisy explanations than Sensitivity Analysis."
5607,3,"""The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system."
5608,1,\n\nThe proposed approach is novel to my knowledge and overcomes specificity of previous approaches while remaining efficient.
5609,2,"This paper is so bad I cannot even reject it! 
"
5610,1,\n\n\n## Pros / Strengths\n\n+ effort to assess momentum / Adam / other modern methods
5611,3,\n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy.
5612,1,"\n\nIndeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network."
5613,3, I would expect some further elaboration of this question in the paper.
5614,2,This sentence would... put off anyone not being paid to read it
5615,3,\n- The speedup is only measured on CPU.
5616,3,"\n\nIn this regard, I would have expected comparison with other state-of-the-art data augmentation techniques."
5617,3,\n\n\n=Major Comments=\n* It's hard for me to understand if the performance of your method is actually good.
5618,1,  \n\n\n** REVIEW SUMMARY **\n\nThe paper is readable
5619,3," For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration."
5620,1," One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n"""
5621,3," I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC."
5622,3,"  Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016)."
5623,3," \""A simple way to initialize recurrent networks of rectified linear units.\"" arXiv preprint arXiv:1504.00941 (2015)."""
5624,3, Is this on bAbi as well?
5625,3,"\n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data."
5626,3," (I would add an explanation of the spectral estimation in the Appendix, rather than just citing Rodu et al. 2013)."
5627,3,"Lillicrap et al. (2015) benchmarked against 27 tasks, Houtfout et al. (2016) compared in the paper also used Walker2D and Swimmer (not used in this paper) as did [2], OpenAI Gym contains many more control environments than the 4 solved here and significant research is pursing complex manipulation and grasping tasks (e.g. [3]). This suggests the author's claim has already been widely heeded and this work will be of limited interest."
5628,3,"However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures."
5629,3," The main contribution are scaling rules that relate the batch size k used in SGD with the learning rate \\epsilon, most notably \\epsilon/k = const for optimal scaling."
5630,3,\n- The goal of the paper is to address automatically the learning of regularization parameters.
5631,3," However, they do not mention whether\nthey perform a similar hyper-parameter tuning for DDQN, in particular for the\nparameter epsilon which will determine the amount of exploration."
5632,3, Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner.
5633,3, It proposes many heuristics to use the object feature and attention weights to find the correct count.
5634,3,"\nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar."
5635,3,"\n\nIt seems like a good strategy is to subsample, perform Hadamard rotation, then quantise."
5636,1," \n3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods."
5637,3," Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?"""
5638,3,"\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)"
5639,3, This assumption is not met in the HealthGathering environment as several different states may generate very similar vision features.
5640,1," Moreover, the method also yields new algorithms for learning decision trees."
5641,1, \n\nThe paper attempts to make progress in the region between deep learning and functional data analysis (FDA). This is interesting.
5642,3,"""SUMMARY\nThe paper deal with the problem of RL."
5643,3,"    On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems."
5644,2,Its complicated to understand what the objective of the study even is
5645,3,The claimed main contribution of the paper is the taxonomy.
5646,1,\n\nThe authors do a great job walking us through the formulation and intutition of their proposed approach.
5647,3," Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image."
5648,1,"\n\n+ The paper presents a promising application in police composite sketching, which can significantly improve human-in-the-loop search in face modeling."
5649,3, This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations.
5650,3,"  As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments."
5651,3,\n\nWhy not report actual wallclock times?
5652,3, The paper only provides results on the bAbI task.
5653,3,\n\n- What kind of safety constraints cannot be expressed by masking actions?
5654,1, The idea is interesting and insightful.
5655,3,"\n- I think it's quite interesting that the framework seems to also allow answering counterfactual questions for realizations that have been sampled from the model, see Fig 16."
5656,3,"\""  I'd like to see work in this area encouraged."
5657,3, \n\nSmall comments that did not impact paper scoring:\n1) eq 1 we usually don't use the superscript \\gamma
5658,3,  \n\nThere are essentially three scenarios of generalization discussed in the paper:\n        (a) various generalizations of image parameters in the PSVRT dataset\n  
5659,3,"""This paper can be seen as an extension of the paper \""attention is all you need\"" that will be published at nips in a few weeks (at the time I write this review)."
5660,3,  These are short duration sates lasting only seconds.
5661,1, But the results are impressive.
5662,3," The proximal step requries the solution of a positive-definite linear system, so it is approximated using a few iterations of CG."
5663,3,"  Here are a few comments: \n\nI think when talking about modelling the dynamics of the world, it is natural to discuss world models and model based RL, which also tries to explicitly take advantage of the separation between the dynamics of the world and the reward scheme."
5664,3,"\n\nThe experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model."
5665,3,"""The authors propose a defense against attacks on the security of one-class SVM based anonaly detectors."
5666,1,\n\nPROS:\nThe idea is interesting. 
5667,3,"\nFinally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator."
5668,3," Unless the authors meant that it \""is similarly decoded to produce $\\mathbf{\\~x}$."
5669,3,"  Only two examples are not convincing.[[CNT], [EMP-NEG], [CRT], [MAJ]] \n3.\tIn section 3, the authors claimed that (5) models the target and context independently."
5670,3,\n- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger
5671,1,\n\npros\n- the paper is written in a clear and concise manner
5672,3, Do these correspond with some known models?
5673,3,"\n\nTypos: page 2, second-to-last paragraph: firs -> first, page 7, second to last paragraph: and and -> and."
5674,3, A solution using a pairwise convnet followed by hierarchical clustering is proposed.
5675,3, \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.
5676,1," Nevertheless, I think this work is important given its performance on the task."
5677,3," And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability."
5678,1,\n\nThe paper is very well-written and easy to follow.
5679,3,  This contribution is added to the logits coming from each agent.
5680,3, There are then of course two main questions to address (i) which states should be stored and how 
5681,1,\n\nThe paper starts off strong.
5682,3,"\n\n[5] Y Miao, M Gowayyed, F Metze, EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding, 2015\n\n[6] D Bahdanau, J Chorowski, D Serdyuk, P Brakel, Y Bengio, End-to-end attention-based large vocabulary speech recognition, 2016"
5683,3,"You spend too much space talking about specific hyperparameter ranges, etc."
5684,3," \u201cIn the Eye of the Beholder: Employing Statistical Analysis and Eye Tracking for Analyzing Abstract Paintings, Yanulevskaya et al\u201d\nThe architecture may lead in overfitting to users' feedback (being over-fit on the data with PIR measures)\n\n- Page 6-Sec 4.2)"
5685,3,"""This paper focuses on the problem of \""machine teaching\"", i.e., how to select a good strategy to select training data points to pass to a machine learning algorithm, for faster learning."
5686,3, Were these values different?
5687,2,You should consider consulting a competent statistical adviser.
5688,3," \n\n[1] Interleaved Group Convolutions. Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. ICCV 2017. "
5689,2,Black-box modeling exercise using a hodge-podge of data tied together with a poorly-defined model
5690,3,\n  I recommend to add some distance-based outlier detection methods as baselines in experiments.
5691,1,"The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\"
5692,3, What does \u201cusing game rules to infer the existence of unit types\u201d mean?
5693,1,nThe paper is generally easy to understand and clear in their results.
5694,3,"\n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings)."
5695,3, How you use IBM model for supervision.
5696,3," Especially, projecting the data points to a uniform sphere can badly blur the cluster boundaries."
5697,3, This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization.
5698,3,"     After all, when we humans generalize to understanding relationships, exactly what variability is present in our \""training sets\"" as compared to our \""testing\"" situations?"
5699,3, The paragraph below this corollary is only a high level intuition. 
5700,3,The method is to prune the network\u2019s activations at each layer and renormalize the outputs.
5701,1,\n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.
5702,1," Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements."
5703,3, The paper is not talking about the weaknesses of the method at all.
5704,3,\n\n(2) I feel that the model design is the main reason for the good overall RC performance.
5705,3," Modern solvers\nroutinely solve instances with tens of millions of non-zeros in the constraint\nmatrix, but require a strong relaxation."
5706,3,"\n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ? "
5707,3,"\n\u201cOn optimization methods for deep learning,\u201d in Proceedings of the 28th\nInternational Conference on Machine Learning, 2011, pp. 265\u2013272.\n\n) \n\nor even a variant of BFGS which makes a block-diagonal approximation to the Hessian with one block per layer."
5708,3,"\n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?"
5709,3," This means that the upsampling operation is not really upsampling a \""true\"" graph/mesh."
5710,2,"his is the complete review, not an excerpt: Good topic, but i dont get an idea of the results."
5711,1," This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting. "
5712,3," Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only?"
5713,2,Why do you have so many tables? Did you go to Ikea?
5714,3,"\n\nApplied to sequence-to-sequence networks, the approach requires training one encoder, and two separate decoders, that generate the target sequence in forward and reversed orders. "
5715,3, It uses demonstrations to learn in an off-policy manner as in these papers.
5716,3, \n\nWhat is the significance of the difference between BBB and BBB with sharpening in the language modelling task?
5717,3,"""This paper suggests a \""learning to teach\"" framework. Following a similar intuition as self-paced learning and curriculum learning, the authors suggest to learn a teaching strategy,  corresponding to choices over the data presented to the learner (and potentially other decisions  about the learner, such as the  algorithm used). "
5718,2,"I could not find any passage in the MS that would explain to me what is the exact novel idea, proposal, argument, or hypothesis"
5719,3," These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network."
5720,3,  It will be informative to provide results with a single GP model.
5721,3,".\nCons.\n- in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al),"
5722,1,. The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features.
5723,1,"\n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs."
5724,3," Empirical performance is shown on 3 downstream tasks: Product-type classification, Sentiment Classification and Aspect Extraction."
5725,1," \n\nBased on the theory developed, the paper presents a practical algorithm."
5726,3," If it turns out the algorithm is quite robust to the randomness, it is then an interesting question why this is the case."
5727,3," Is (b), (d),(c) in a sequence or is testing moving from plain to different things?"
5728,3," After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all."
5729,3,"  Furthermore, this learning extends to large-scale image datasets."
5730,3,"\n\nFor instance, eq. (3) proposes one model for p(F|X). Eq. (8) proposes a different model for p(F|X), which is an approximation to the previous one."
5731,1,"  \n\nOverall, this paper does seem to identify a concrete problem, and I liked the use of explicit aspect embeddings for sentiment analysis."
5732,3,"\n+When n = O(m log m), the result that \\epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this?"
5733,3,  \n\nSection 2.7 tries to relate the spike-based learning rule to the biologically observed STDP phenomenon.
5734,3, An adversary could choose any policy.
5735,3, What is the perplexity of all the language models corresponding to  Tables 4 and 5?
5736,3,       (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset\n 
5737,3,  What is the variance between the seeds.
5738,1,  This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly.
5739,1, The idea of learning the basic relations between actions and state through self exploration is definitely interesting.
5740,3, The authors evaluate on cifar-10/100 and several out of distribution datasets and this method outperforms the baseline by significant margins.
5741,3, Is there any justification for why this method of injecting noise was chosen ?
5742,3," \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?"
5743,2,This paper does not offer a revolutionary breakthrough
5744,3, \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean?
5745,3,"While this is NP-hard problem in general, the greedy algorithm is 2-approximate"
5746,3,\n\nDo you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics?
5747,3,(The authors reference some signal processing tasks in the introduction
5748,3," Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \""context unit\"" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \""context\"" vectors are exchanged)"
5749,3," The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017."
5750,3, The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).
5751,1,  The paper performs a modest evaluation of such\nsimple models and shows surprisingly good r-squared values.
5752,1, It is novel and should generate further research with respect to understanding its vulnerabilities more completely.
5753,1,"""The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate"
5754,3,"""This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL."
5755,3," In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal."
5756,3,  There is no way to discover a network outside of the principled design space articulated in point (1) above.
5757,1," The paper essentially introduces a method to use off-policy data, which is of course important,"
5758,3,\n\nPros:\n- Baseline performance is exceeded by a large margin\n- Novel use of adversarial perturbation and temperature\n- Interesting analysis
5759,3," \nMost data that are available for learning are in discrete forms and hopefully,\nthey have been digitalized according to Shannon theory."
5760,3,"\u201d However, in table 4, evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC."
5761,3, This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes.
5762,3," \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline."
5763,1,\n\nThe paper is well written. 
5764,1,                      \n- The results are otherwise convincing and clear improvements are shown with the proposed method.
5765,3," Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections."
5766,2,"Lots of work, effort, but no real science."
5767,1," By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent."
5768,1,"In summary, I recommend acceptance."
5769,3,  Other systems seem to have development and test performance closer together. 
5770,3, Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set?
5771,2,"Quite frankly, it seems that the technical language is used more to frighten the reader than facilitate his task"
5772,3,"\n\nWhile the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression."
5773,3," It would be good to clarify this in the text."""
5774,3,\n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other.
5775,3," The proposed embeddings just barely beat the baseline on product classification and sentiment classification, but significantly beat them on aspect extraction task."
5776,3,\n\n\nCons:\n- The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones.
5777,3, I requested for the authors to use their method on the best performing baselines (i.e. Yao et al. 2016 or Liu et al. 2017) or explain why this cannot be done (maybe my request was not clearly stated).
5778,3, Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps.
5779,3,\n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).
5780,3,It is my understanding that Sens-FGSM is not trained to a particular iteration of the \u201ccat-and-mouse\u201d game.
5781,3,\n\nComments:\n1. Paper should cite Domke 2012 in related work section.
5782,3,\n\n=============================================================\n\nThis paper presents an algorithm for training deep neural networks.
5783,3, I would recommend refraining from using these terms here.
5784,3, This is inline with TD-lambda analysis in previous work.
5785,3,\n\nCan the authors guarantee that the variational bound that they are introducing (as defined in eqs. (19) and (41)) is actually a variational bound?
5786,3,"\n\n\nTechnical quality\n==============\n\nThe main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on\na data point to fool the network."
5787,3," As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015."
5788,1, My main concerns about this work are have two aspects: \n(a)\tNovelty\n1.\tThe idea is a good one and is great incremental research building on the top of previous ideas.
5789,1, That is perfectly fine -- and this work is still valuable.
5790,3,. How does the greedy step affect training and decoding?
5791,3,"""This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP."
5792,3,\ Can PixelNN run in real-time?
5793,3, In these cases focusing on the hardest negative reduces performance.
5794,3, However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued.
5795,3,"\n\n-The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. \\alpha_{ij}^l, \\alpha_{ij}^h, \\alpha_{ij}^u) it will learn to attend to \""different regions for different level\""."
5796,3, Am I misunderstanding something?
5797,3,"""This paper focuses on accelerating RNN by applying the method from Blelloch (1990)."
5798,1,  \n- I appreciate that it is difficult to find good test datasets for evaluating causal estimator.
5799,1, \n\nThe improved lower bound given in Theorem 6 is very modest but neat.
5800,1, \n\nClarity:  The paper is fairly clearly written. 
5801,3,"\n\nHowever, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the \nproposed term depending on the parameters of the model (and this depends on the model!)."
5802,3," In summary, I think the paper needs very careful editing for grammar and language and, more importantly, it needs solid experiments before it\u2019s ready for publication."
5803,3, Which kernel is better and how to choose between them in a deep network?
5804,3,"\n\u2013 The paper\u2019s main contribution seems to be a neural network with a GA optimization for classification that can learn \u201cintelligent combinations of features\u201d, which can be easily classified by a simple 1NN classifier."
5805,3," \n\nIn addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM)."
5806,1, the claims are a bit strong for CNN and need further theoretical and experimental verification.
5807,1,  This is encouraging.
5808,3," During model evaluation only the forward decoder is used, with the backward operating decoder discarded."
5809,3, Maybe some subsection titles would help make it feel a bit more cohesive.
5810,3, \n\nSummary\nThe authors study state space models in the unsupervised learning case.
5811,1," This is a very nice contribution.\n"""
5812,3," On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the \""Two problems with variational EM... \"" paper by Turner and Sahani (2010)."
5813,3," If so, it would be necessary to compared the proposed method to some classic methods for identifying k with kmeans, such as the elbow method, BIC, G-means etc, especially since kmeans seem to give much better NMI values.\n\n\n"""
5814,3," It looks like Theorem 2 has already been shown in \""Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d."
5815,1, It extends a series of recent papers correctly cited.
5816,3,"\n\n-- A suggestion: As future work I would be very interested to see if this method can be incorporated into common few-shot learning models to on-the-fly generate additional training examples from the \""support set\"" of each episode that these approaches use for training."""
5817,3," What are \u2018edge devices\u2019, \u2018vanilla parameters\u2019?"
5818,3,\n\nWhy is the stride for the convolutional/deconvoluational layers set to 2 (as stated in Section 2.1)?
5819,3, Could you provide a comparison with EM?
5820,3,"\nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well."
5821,1,"\n\n+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain."
5822,1,\n\nOverall I think the idea proposed in the paper is beneficial.
5823,3,\n- Few-Shot Learning Through an Information Retrieval Lens (Triantafillou et al.)
5824,3,"""\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count."
5825,3,"  They also evaluate realism images using AMT fooling - asking turks to chose the fake between between real and generated images, and obtain substantial improvements on this metric as well."
5826,3," ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!)."
5827,3,  Do you have insights on the role played by the architecture of the inference network and generative model?
5828,1,\n\n\nPro:\n-\tA novel idea of producing natural adversary examples with a GAN
5829,3, \n- Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6).
5830,3,"\n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well."
5831,1, and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels.
5832,3,"\n- Was any kind of regularization used, how does it influence task performance vs. transfer?"
5833,3,"\n\nThe main contribution seems to be the use of the \""graph shift\"" operator from\nSandryhaila and Moura (2013), which closely resembles the one from\nShuman et al. (2013)."
5834,3,"""This paper investigates identity space learning with well-controlled variations using an artistic portraits dataset."
5835,3,"  Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?"
5836,3,"""This paper proposed an end-to-end network that generates computer tokens from a single GUI screenshot as input. "
5837,1," It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques."
5838,3,. A higher rank would correspond to more dependence across time
5839,3, 3) Generalization bound based on stability.
5840,2,Some papers are a pleasure to read. This is not one of them.
5841,3," For example, find the nearest german neighbour of the word \""dog\"" in the common representation space. The authors instead compare with very simple baselines."
5842,1,"\n\n%%% After Author's Clarification %%%\nThis paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly."
5843,3, What is the exact loss for each step in the recurrence of the outputs (according to Figure 5)?
5844,3," \nAlternatively, can the network be trained once for a domain, and then used for every synthesis problem in that domain (i.e. in your experiments, training one net for all possible binary-image-drawing problems)?"
5845,3,  What architecture did you use in your experiments?
5846,1,- The method is specifically designed for online learning with limited hardware ressources.\n\n
5847,3,\n\nSection 5.1:\n- I don't agree with the authors that the topics in Table 3 are interpretable.
5848,3, \n\nComments:\n\n1. How do the PAG scores differ when using a full covariance structure?
5849,3," \nFurther do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model."
5850,3,What is the factor of augmentation
5851,1,"\n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental."
5852,2,It is not hard to develop this methodâ€¦I could write code for this on a rainy afternoon.
5853,3,\n5-\tThe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages.
5854,1,"\n\nThe analysis of the results are interesting, both from the quantitative & qualitative point of view."""
5855,3,"  The main observation is that static sigma-delta coding as proposed in OConnor and Welling (2016b), is not correct when the weights change during training, as past activations are taken into account with the old rather than the new weights."
5856,1,"""The quality of the paper is good, and clarity is mostly good."
5857,2,I do not want to see the paper again.
5858,1," The proposed metric is interesting,;"
5859,1," However, the validation procedure is not clear in the article."
5860,1, \n- The testbed is nice; the tasks seem significantly different from each other.
5861,3,  And how does the performance of the technique depend on the setting of m?
5862,3, also applies to table 4.
5863,1," In general, this manuscript is well written."
5864,3,"""Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules. "
5865,3,"\n- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt."
5866,3,"\n\u201cIn game theory, the outcome maximizing overall reward is named Pareto optimality."
5867,3,"""The paper extends the prototypical networks of Snell et al, NIPS 2017 for one shot learning."
5868,3,  They use a REINFORCE-like algorithm to differentiate through the Monte Carlo.
5869,3," \n\n# Novelty and Significance\n- The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN. "
5870,1,". However, I also believe the authors managed to clear essentially of the criticism in they reply."
5871,3," Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks."""
5872,3,:\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve?
5873,1," Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks,"
5874,3,": Image Pivoting for Learning Multilingual Multimodal Representations, EMNLP 2017 for a good set of references)"
5875,3," This is perhaps more important than improving the baseline method by a few point, especially given that the goal of this work is not to beat the state-of-the-art."
5876,3,"\n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs."
5877,1," Generally, the result is interesting and the presentation is easy to follow."
5878,3,"There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences."
5879,3, This (the space between x and x') I think is more interpretable as the invariance corresponding to the space between z and z_k. Have you tried that?
5880,3,"\n\n* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance."
5881,3, Prog is conditionally independent of X?
5882,3,   Then is there anything new in FAME on the gray images?
5883,3,\nThe section should also emphasize that the models discussed in this paper are only applicable for early stopping in cases where the function evaluation budget N is much larger than 100.
5884,3," It is deduced that HCGD generalizes better, however, this is the case only at the very end of training, while SGD with momentum and ADAM work far better initially."
5885,3," The proposed generator model combines node embeddings, with an LSTM architecture for modeling the sequence of nodes visited in a random walk; the discriminator distinguishes real from fake walks."
5886,3,"\n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention."
5887,3, \n\nThe dev perplexity quoted in Section 4 for a 5 gram LM is very high.
5888,3,"\n[3] Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, Honglak Lee. Decomposing Motion and Content for Natural Video Sequence Prediction. In ICLR, 2017\n"""
5889,1,. It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.\n\n12
5890,3,  But it also requires extra work to ensure they are generating meaningful graphs.
5891,3," In fact, Figure 1 is straight in identifying disentanglement and completeness as a deviation from an ideal bijective mapping."
5892,1,"\n\n=========================================\n\nAfter the rebuttal I've updated my score, due to the addition of FSGM added as a baseline and a few clarifications."
5893,1," These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training."
5894,3,\n\nThe paper mentions that the approach is \u201cunsupervised\u201d. 
5895,3,"\ One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs."
5896,3,"\n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces"
5897,3," If it is the first case, I would be interested to know the latter: It is possible that if all layers are binarized, then the differences between the binarized and non-binarized version become more amplified."
5898,1,. The results are interesting.
5899,3, Having an algorithmic environment would make the description easier. 
5900,3,  Do the papers that prove similar theorems about ConvNets able to handle general L
5901,1,". The proposed combination is straightforward,"
5902,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
5903,3,\n\nRemaining remarks\n- Just a very simple / non-standard ConvNet architecture is trained.
5904,1, The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way.
5905,3," If the contribution of the paper is the \""output stochastic\"" noise model, I think it is worth experimenting with the design options one has with such a model."
5906,3,"""This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions."
5907,2,Didnt like this one 
5908,2,I dont understand thermodynamics.
5909,1,"That being said, I believe that \nsome discussions could strengthen the paper:\n - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating\nuncertainties in the observation or physical evolution models."
5910,3, Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)?
5911,3, Suppose SGD leads to a local minimum of the empirical loss.
5912,3,\n[2] A dirty model for multi-task learning (NIPS)
5913,3, The contribution of this paper of this paper is two-fold.
5914,1,  The results and analysis are informative.
5915,3," \n- In particular, why is the dataset not used for the causal controller?"
5916,1, I think overall it is a good idea.
5917,3, Is there a principled reason AT+FGSM defends against universal perturbations?
5918,3," With PCL, one can use hypothesis testing framework to analyze the detection rate and false alarm of different neural networks against adversarial attacks."
5919,1," While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper."
5920,3,"  However, this submission needs to be improved in terms of clarity and its experiments."
5921,3, Maybe you could motivate your choice at this point.
5922,3,"""The paper argues for structured task representations (in TLTL) and shows how these representations can be used to reuse learned subtasks to decrease learning time."
5923,3,  It would be better to provide some statistics showing how the target-context interaction model outperforms the traditional ones in the special cases like the one shown in Table 4.
5924,3, Would the special cases lead to worse performance and if so why is there a difference?\
5925,3,"\n(ix) Usually, one should think of the Laplace approximation and the resulting Bayes factors more in terms of a \""volume\"" of parameters  close to the MAP estimate, which is what the matrix determinant expresses, more than any specific direction of \""curvature\""."
5926,3," In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16)."""
5927,3, One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the \nassigned pseudo-labels.
5928,3,\n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.
5929,3,"""This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points."
5930,3, Numerical experiments comparing the two implementation or at least a discussion is necessary.
5931,1, And an extensive literature of theoretical results.
5932,3, I guess it was softmax probability.
5933,3, Do you have experiments by fixing the number of layers and varying the hidden size?
5934,3,\n\nThe idea is exciting LDS by wave filter inputs and record the output and directly estimate the operator that maps the input to the output instead of estimating the hidden states.
5935,3,"\n* For Table 8, the similarities are not striking."
5936,3, Additionally for their experiment authors use the SUMO top view driving simulator. 
5937,1," Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others."
5938,3,"""This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j)."
5939,3," For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation."
5940,3, Did you also make changes to the dataset?
5941,3,"""This paper investigates a new approach to prevent a given classifier from adversarial examples. "
5942,3,"\n - p.5. Notation seemed a bit non-standard. I thought most people use \\sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid.[[CNT], [CLA-NEU], [DFT,DIS], [MIN]]...\n - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common."
5943,3, LSTMs as well could have been a point of comparison.
5944,2,"Overally speaking, the manuscript is well written."
5945,3," For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient? "
5946,3, However it looks like the task graph itself is still simple and has limited representation power.
5947,3, Is there a typo?
5948,3, This is partly because of the writing.
5949,3,"Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta."""
5950,3, But these are also the same set of experiments performed by Cai et al.
5951,3,"""This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. "
5952,1,\n\nPros:\n(1) The paper is clearly written.
5953,3, Maybe adding a section about the notation and developing more the intuition will improve the reading of the manuscript.
5954,1,  I do not have any suggestion for improvement.  This is good work that should be published.
5955,3, \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.
5956,1,It is admirable that the authors are seeking to add to the understanding about theory of neural nets instead of contributing a new architecture with better error rates but without understanding why said error rates are lower.
5957,3, The goal of this process is to extract a function that maps any given state to a subgoal.
5958,1,\n\nThe suggested techniques are nice and show promising results.
5959,3,\n\n* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning). 
5960,3," The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to \u201cparallel ordering\u201d and \u201cpermuted ordering\u201d and show the performance gain."
5961,3,"  If there is a clear separation from a different view, show that one instead."
5962,1," The paper makes a nice contribution to the details of deep neural networks with ReLUs,"
5963,3, The improved understanding of SPENs and potential for further work justify accepting this paper.
5964,3, Why is using earth mover distance better than MMD based distance?
5965,3,  Some interesting results on the development set show the importance of things like warm starting on large language model training data.
5966,3," Additionally, they also show that the proposed ensemble method results in better performance than other ensemble methods (For example, ensemble over independently trained models)  not only in combined mode but also in individual branches."
5967,3, What is the cost function and what is the strategy to train on multiple tasks ?
5968,2,"[REDACTED]'s talks are popular, but then so are Ke$ha concerts."
5969,3,"""Summary:\nThis paper investigated the problem of attribute-conditioned image generation using generative adversarial networks."
5970,3,"""This paper proposes replacing fully connected layers with block-diagonal fully connected layers and proposes two methods for doing so. "
5971,3," The difference VGG - ResNets is also interesting, but it would have been interesting to see how this affects the current state of the art in understanding deep learning, something that was done for the \""flat vs sharp\"" dilemma, but is lacking here."
5972,3,"  Doesn't the good\n  performance without TS on, e.g., ResNets in Table 2 imply that the\n  Deep ResNets subfigure in Figure 3 should start out at 80+?"
5973,3,"  The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches."
5974,3, This is especially the case for the RNN experiments.
5975,3, They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.
5976,3," The paper is written in a procedural fashion - I first did that, then I did that and after that I did third."
5977,3,"""This paper proposes a new algorithm to generate the optimal control inputs for unknown linear dynamical systems (LDS) with known system dimensions."
5978,1,"\nTo me, the execution seems sound."
5979,3," In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE."
5980,3," Some typos and minor issues are listed in the \""Cons\"" part below."
5981,1, The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion.
5982,1," I can see the benefit of trainable approach here,"
5983,3, Are we talking about a multi-tenancy setting where multiple processes execute on the same device
5984,1, the approach is only tested in a small 7x7 grid and 2 agents and in a 10x10 grid with 4 agents.
5985,3,"  Hopefully it is possible to streamline the methodology section to communicate the intuitions more easily.\n"""
5986,3, RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning.
5987,3," For now let\u2019s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value."
5988,3," Since it is entirely general, I would rather expect a test on a dozen different data sets."
5989,1," The results show promising results,"
5990,3," In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature."
5991,3," Nevertheless, it is important to note that direct comparison should not be made among approaches with key differences."
5992,3,"""UPDATED COMMENT\nI've improved my score to 6 to reflect the authors' revisions to the paper and their response to my and R2's comments."
5993,3, There are several typos in the proof of Theorem 2.
5994,1, The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task.
5995,3,\n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.
5996,3,"    Let's assume that o(B Until C) is satisfiable, so the conjunction is satisfiable."
5997,3,The idea is character based.
5998,1,"   From a scientific point of view, this seems orthogonal to the point of the paper, though is relevant if you were trying to build a system."
5999,1," \n\nOverall, I believe this paper is a nice contribution to the deep learning theory literature."
6000,3,"""I am overall positive about the work but I would like to see some questions addressed."
6001,3, It would be interesting to see if the additional auto-encoder part help address the issue.
6002,3, The fact that the variable d is equal to 768 is not explicitly stated.
6003,1,\nThe experiments show clearly that a) the components of the proposed pipeline are important since they outperform ablated versions of it and b) the system is better than previous work in those tasks
6004,3," They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering."
6005,1,"\n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms."
6006,3,"  Avoiding the estimation of a dynamics model was stated as a given, but perhaps more could be put into motivating this goal."
6007,3, An alpha-divergence formulation is considered to combine both methods.
6008,3,"  Instead of real interactions, the submission proposes to maximise the activations of hidden units in a separate neural network."
6009,3,\n7. Discussion of weight decay on page 5 seems tangential to main point of the paper. Could be reduced to a sentence or two.
6010,1," \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined."
6011,1,"Experiments are performed on a variety of tasks, demonstrating that the Skip-RNN compares as well or better than baselines even when skipping nearly half its state updates."
6012,3," In the latter, the notion of \""Probabilistic Lipschitzness\"", which is a relaxation of the \""cluster assumption\"" seems very related to the actual work.\n\nReference:\nBen-David and Urner."
6013,1,\n+ New dataset for robot arm pushing objects.
6014,2,There are not enough headings.
6015,3,\n\n- How reliable are the interpretations?
6016,3," It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \""structured noise\"", and (3) Samples from out-of-dataset classes."
6017,1,"\nOverall, the proposed method seems to be very useful for the RWA."""
6018,3, We need a better reason than morphology to want to do source-side dependency parsing.
6019,1,\n\nAll in all the paper is very clear and interesting.
6020,2,"Findings are presented in an anecdotal, descriptive style that I cant interpret."
6021,3,"\u00a0\u00bb \n\nNormally, I should have stopped reviewing, but I decided to continue  since those parts only concerned the preliminaries part."
6022,1,"\n\nTurning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO."
6023,3," \n\nThe paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD."
6024,3,"""\nSummary:\n\nAuthors propose a method which uses a Q-learning-based high-level policy which is combined with a contextual mask derived from safety-contraints and low-level controllers which disable certain actions from being selectable at certain states."
6025,3,"  Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI)."
6026,2,The concluding 'takes a village' sentence is also a bit unoriginal.
6027,1," The proposed method, which makes use of grouping information, seems reasonable and useful."
6028,3,   Please provide numerical support.
6029,3, Can you post the gold German sentence?
6030,3," While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward."
6031,3,"\n  While this does not significantly dilute the message, it would have made it much more convincing\n  if results were given with stronger networks."
6032,3," Moreover, the empirical comparisons are only conducted on MNIST."
6033,3,"""In this paper, the authors present a computational framework for the active vision problem."
6034,1," While the experiments are well done,"
6035,1," Related work is clearly cited, and the novelty of the paper well explained."
6036,2,"I would suggest activating the spellchecker on Word, or keeping the cat from walking on your keyboard"
6037,1, \nThe paper is well presented and I want to underline the importance of this.
6038,3," The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms."
6039,3," For example, what is the difference between the random mask update and the subsampling update (why are there no random mask experiments after figure 1, even though they performed very well)?"
6040,3," My suggestion would be to completely separate these three parts: present a general method first, then use heat sink as the first experiment and airfoil as the second experiment."
6041,1,\n - The proposed model can be easily applied to any VQA model using soft attention.
6042,3, RLSVI is provably-efficient with a state-of-the-art regret bound for tabular learning - you would probably strengthen the case for this algorithm as an extension of RLSVI by building on this connection... otherwise it's a bit adhoc to justify this approximation method.
6043,3," For example, in Eqn. 7, it would be clearer to write out the min over optimization variables."
6044,3, Would it be able to generate an expander graph?
6045,3," As a reader I wouldn\u2019t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2. "
6046,3," In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results."
6047,3, Deciding on these targets is formulated as a combinatorial optimization problem.
6048,3," If that is so, should this paper also compare with STE?"
6049,3," \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand."
6050,1,  The concatenation does fairly comparably to LL in Tables 3&4.
6051,3,"""This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI.\"
6052,3," If the decision boundary are curved, then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curved."
6053,3,"\n\nThe proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games."
6054,3, \n                     Wv ExpectedExpertDirectionInStatesSimilarToS   >\n\nThe learner\u2019s direction in state S is just (S-S\u2019) in feature space.\n\nThe authors model the behavior of the expert as a kernel density type approximator\ngiving the expected direction of the expert starting from a states similar to the one the learner is in.
6055,3," Similarly, the representation analyzed in Figure 7 is promising,but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper."
6056,3,"""\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas:"
6057,3,"  The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation."
6058,3," In the latter case, how long were models trained?."
6059,3," The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic."
6060,3,"\n\nAs a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack."
6061,3," As a non-expert in the field, I'd expect that ordering between pathologic patterns matters more."""
6062,3,\n\nYou claim that FractalNet shows no ensemble behavior.
6063,3, This is a very important point that should be made\nclearer.
6064,2,The candidate has not demonstrated to me the required knowledge in any of the starred areas
6065,3," Second, it is not clear what \""parameters of the regularization\"" means."
6066,1, The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information.
6067,3, Authors should pay attention to explain more detailed analysis about this point in the paper.
6068,3, The paper claims that the approach can deal with the partial observable domain better than the standard methods.
6069,3,\n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.
6070,3," but it is not clear if those results are something difficult to find with other existing standard ways,"
6071,1,"\n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation)."
6072,3,\n\n\nTypoes:\n\nLegend of Figure 2: red lines are error -> red lines are accuracy\nTable 1: test accuracy -> test error\nBefore 6.2: architecture effects -> architecture affects
6073,1," \n\nOn unconditional generation, your hypothesis on uncertainty is interesting and could be tested."
6074,3,"""The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface"
6075,3, Time series modelling is performed via convolutional LSTMs.
6076,3,\n\n2.  If the authors\u2019 encoding scheme really works I feel that they could beef up their experimental results to demonstrate its unqualified advantage.
6077,3,"\n\nPage 7: \""In the supervised setting...\"" This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside."
6078,3," If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result."""
6079,1,\n\nClarity:\nThe paper is very clear.
6080,3, It would be even better to compare CTC and ASG to seq2seq-based models with the same gated convnet.
6081,3, but I'm wondering if this is because more augmented data is used overall.
6082,3,"""This paper provides a new generalization bound for feed forward networks based on a PAC-Bayesian analysis."
6083,1," The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art."
6084,1,"\n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: The main idea is clearly motivated and presented,"
6085,3, Does AESMC give a better generative model?).
6086,3, The main limitation is that the best architectures as currently described are less about discovery and more about human input;
6087,2,I find the author's writing to be very undergraduate-like.
6088,3," \n\nThis paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results. "
6089,3,"\n\n4. Equation 2, bottom: C_in, W_f, H_f, and C_out are undefined at this point."""
6090,3, \n\n- The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction.
6091,3,"\n\nOverall, I think it is a borderline paper."
6092,3,"\nPage 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated."
6093,1," \n\nThe major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy."
6094,2,"First, the paper is for a large part incomprehensible"
6095,3, The experiments show also that in on of the considered case the generalization accuracy is better for the proposed method.
6096,1, The best experimental evidence for the authors\u2019 perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method.
6097,3, Larger scale experiments on ImageNet is also recommended to show how general the conclusion of the paper is.
6098,3,"\n\ni.e. the authors form a vector \""HoW\"" (called history of the word), that is defined as a concatenation of several vectors:\n\nHoW_i = [g_i, c_i, h_i^l, h_i^h]\n\nwhere g_i = glove embeddings, c_i = COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word."
6099,3,"""Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation."
6100,3,"""The paper proposed an extension to the fast weights from Ba et al. to include additional gating units for changing the fast weights learning rate adaptively."
6101,3," If what I think is correct, I think it would be important to show this."
6102,3," But in either case, it will be good to discuss the connections."
6103,3,\n- What is the reader supposed to take away from Table 1?
6104,3,"  I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa?"
6105,3, It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks).
6106,3," I'd encourage the authors to do a more detailed experimental study with more tasks,;"
6107,1, \n\nQuality: The paper is well written.
6108,1,"  For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result."
6109,3, The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game.
6110,2,â€¦this is a antique approach to a modern problem
6111,3," Moreover, mapping features to some appropriate feature space has been widely investigated, including the choice of appropriate mapping."
6112,1,\n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.
6113,3," However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:\n\n-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action."
6114,3," Negative results were reported when the proposed regularization technique was applied to language models, whose output distribution has more entropy."
6115,3, Maybe the manuscript part with the definition of the accuracy measures may be skipped.
6116,3,\n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input.
6117,3,"""The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X."
6118,1,"\n\nOverall, the idea is interesting, as providing an end-to-end trainable technique for distributing the precision across layers of a network would indeed be quite useful."
6119,3, Why T=2 and not 1 or 10 ?
6120,3," \n\nI missed a \""related work section\"", where authors clearly mention previous works on similar datasets."
6121,3, The \u2018semantic\u2019 property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter).
6122,1," However, there are rather little explanations provided to argue for the different modeling choices made, and the intuition behind them."
6123,3,"\n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect. "
6124,3,"\n\nI am willing to adjust my rating when the questions and remarks above get addressed."""
6125,3," but I don't have a really high amount of experience in the field of adversarial examples in order to make my review with high confidence."""
6126,3,"\n\nNota: \n     - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0?"
6127,3," Ideally, one should see the effect of learning with options (and not primitive actions) to fairly compare against the proposed framework."
6128,3,- so the failure to be better than the real-valued alternatives seems unremarkable.
6129,3,  Are they based on different alpha and lambda values?
6130,3, \n-   Equation (5)  should be - O(\\alpha^2)
6131,3," Once again, a more in-depth analysis of this phase behavior would be very welcome."
6132,2,..incoherent babble of unsubstantiated overstatement.
6133,1, \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL
6134,3, The lambda term would then serve as an indicator to how much entropy is necessary.
6135,3, Did you use dropout?
6136,2,I think time will show that inheritance (section 1.5.3) is a terrible idea.
6137,3,"\n\n--Main comment\nAbout the deep network case in Theorem 1, how $L$ affects the bound on ranks?"
6138,3," Just the\n  book, or the whole corpus?"
6139,3, The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads.
6140,1, Experiments show that ISRLU is promising compared to competitors like ReLU and ELU.
6141,3,\nThis result is related to the third conjecture of the paper that is :\n3. the number of the number of mappings which preserve a degree of discrepancy  is small.
6142,3,"\n\n* The word \""layers\"" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder."
6143,3,"\n\nQuestions:\n* How did you optimize the variational parameters for q^* and the flow parameters in terms of learning rate, stopping criteria etc."
6144,3,  This is achieved via an attention mechanism.
6145,3," \n\n The experimental results seem promising, although not earthshattering."
6146,3," I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot."
6147,3,\nWhy did you just apply the rotations only on d_{t}.
6148,3," Adadelta, RMSprop, and ADAM are mentioned explicitly, but what exactly are differences and similarities?"
6149,3,".\n\nMoreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the \""related work\"" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis)."
6150,3, In my view this particular result is an immediate corollary of the result for linear networks.
6151,3, How does thresholding mentioned in Figure 5 work?
6152,3," \n- The difference between Figures 1, 4, and 6 could be clarified. "
6153,1,\n\nStrengths:\nSimultaneous text and image generation is an interesting research topic that is relevant for the community.
6154,3,"It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate."
6155,3, I got confused in this section because eq.9 defines f(x) as a quadratic function.
6156,1, as well as cleaning process provided\n\t\u2022\tEvery figure and plot is well explained and interpreted\n\t\u2022\t
6157,1,"\n\nOverall, I think that this paper could serve as a useful baseline for generating point clouds,"
6158,3,\n- local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs.
6159,1," The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST."
6160,3," Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section."
6161,2,I have rarely read a more blown-up and annoying paper in the last couple of years than this hot-air balloon manuscript
6162,1,"\n- The multi-level attention is novel and indeed seems to work, with convincing ablations."
6163,3, It\u2019s better to train some semi-supervised model to make the settings more comparable.
6164,3," In my opinion, this section could benefit from a little more expansion and conceptual definition."
6165,1,"\n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel."
6166,3, How many such epochs are done?
6167,3, You just introduce this concept without any explanation. 
6168,3, The novelty in the paper is implementing such a regression in a layered network.
6169,3, They authors then extend the approach to \u201cpermuted order\u201d and finally present their proposed \u201csoft ordering\u201d approach.
6170,3, What is your compression ratio for 0 accuracy loss?
6171,3," \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity? "
6172,2,The author gives the impression of having had some mathematical training.
6173,1,"\n\n- Theorem 1 is neat, well done!"
6174,1, \nLearning a meaningful representation is needed in general.
6175,3," The distance that is used is the Euclidean distance over the learned representations, i.e. (z-c)^T(z-c), where z is the projection of the x instance to be classified and c is a class prototype, computed as the average of the projections of the support instances of a given class."
6176,1,\n\nI really like the reverse perplexity measure.
6177,3," Compared with the n-gram replacement used in the paper, which one is better?"
6178,3,"""The authors consider the task of program synthesis in the Karel DSL."
6179,3, The paper claims that this may help to prevent model collapsing.
6180,3, The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it?
6181,3," Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet)."
6182,3,"\n\ncons:\n\n- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement. "
6183,3, Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2.
6184,3, The authors here propose a method for sidestepping this issue by using the random exploration of an agent to learn generalizable skills which can then be applied without any specific pretraining on any new task. 
6185,3, In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688).
6186,1,"""The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph."
6187,3,  It would have been interesting to compare not only with SFNN but also to a model with the same architecture and same gradient estimator (Raiko et al. 2014) using maximum likelihood.
6188,3, Can it solve the halting problem?
6189,3, The first one is a style discrepancy loss to enforce that the discrepancy between the learnt style representation for a source sentence and the target style representation is consistent with a pre-trained discriminator.
6190,1,\n\nThe authors do provide a novel formulation and demonstrate the gains on a variety of concrete problems taken form the literature.
6191,3, The neural network in charge of predicting the next frame in a video implicitly generates flow that is used to transform the previously observed frame into the next.
6192,3,"""The paper proposes a model for abstractive document summarization using a self-critical policy gradient training algorithm, which is mixed with maximum likelihood objective."
6193,1, I also find the empirical results encouraging.
6194,3,"""This paper investigates multiagent reinforcement learning  making used of a \""master slave\"" architecture (MSA)."
6195,1," Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice."
6196,1,"n\nOverall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side. "
6197,3,"""This paper suggests a reparametrization of the transition matrix."
6198,3,"\nWith systematic comparisons like that, it would be easier to understand where the gains in performances are coming from."
6199,3,"\n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments."
6200,3," A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly."
6201,3," However, from the\nequations in the paper it seems that the logit is set to zero."
6202,3,"""The paper proposes a \u2019Cross View training\u2019 approach to semi-supervised learning."
6203,3,\np3. Are you using an L2 weight penalty?
6204,3,"\n\nThe first paper is the most related, also using homomorphic encryption, and seems to cover a superset of the functionalities presented here (more activation functions, a more extensive analysis, and faster decryption times)."
6205,2,The supportive tone of this reviewâ€¦ took some effort.
6206,3," As a consequence, those two methods need to take as input a new reward function for every new map."
6207,3," For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below)."
6208,1, \nThe experimental results are convincing enough to show that it outperforms other active learning algorithms.
6209,3, I think slack variables come from (Cortes et al 95). 
6210,3," \n\nThe methods reviewed prior work which the authors refer to as \u201cparallel order\u201d, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn\u2019t be the case."
6211,1,"Observing 25% of the pixels gives almost no information about the identity of the digit and it makes sense that it\u2019s hard to encode the future, however, 50% of the pixels give a good idea of what the digit identity is."
6212,3, Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient?
6213,3," This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf"
6214,1,\n\n\nREVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments.
6215,3,"\n\nSimulation results compare MADQN with Dijkstra's algorithm as a baseline, which offers a myopic solution where each agent picks up the closest customer."
6216,2,"While I wont advocate strongly for its publication, I also would not object to its publication in the journal"
6217,3,"\nHowever, there is no reason to automatically expect that this will decrease the importance of shape."
6218,3,"  Then 2.5 pages are spent on channel pruning.[[CNT], [PNF-NEU], [DIS], [MIN]] \nI would have liked if the authors spent more time justifying why we should trust their method as a privacy preserving technique (described in detail below)."
6219,2,"No new insights, no important question addressed, no problem solved. -.."
6220,1,". Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution."""
6221,2,This is a sin of omission!
6222,3,"  This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n"""
6223,1,"""The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches."
6224,3, \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?
6225,3, \n\n- Empirical results do not clearly show the advantage of the proposed method over state-of-the-arts.
6226,3," In section 3.2 they explain that \""Our attack uses existing optimization attack techniques to...\"", but one should be able to understand the method without reading further references."
6227,3," Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure."""
6228,3," Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed."
6229,3," Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people."""
6230,3," They won\u2019t look as flattering, but will properly report our progress on this journey."
6231,1,"\n\nApart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out."
6232,1," Nevertheless, I do think there are some interesting ideas theoretically and algorithmically."
6233,3,"\n6. Section 4: How does the KRU compares to the other parametrizations, in term of wall-clock time?"
6234,3," Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper."
6235,3, Complexity for solving the Babi tasks could be added later.
6236,3,"\n\nand\n\nJ. Ngiam, A. Coates, A. Lahiri, B. Prochnow, Q. V. Le, and A. Y. Ng,"
6237,3," Is it possible to change more than one attribute each time in this method?"""
6238,1,"\nAdditionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models."
6239,3,\n\nChapter 5 takes a stochastic differential equation as a starting point.
6240,3," What this means is that the agent has a reservoir of n \""states\"" in which states encountered in the past can be stored."
6241,3, So I\u2019m not sure how to interpret them: should they have been run for longer?
6242,3," In effect, this paper extends the existing literature suggesting end-to-end branching. "
6243,3, Any differences in terms of theory and experiment?
6244,1,"\n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance."""
6245,1,\n- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel.
6246,3, \n- It seems from Figure 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end.
6247,2,"Unfortunately the paper itself is full of so many fundamental misunderstandings, I don't even know where to begin in criticizing it"
6248,1, The authors propose a centralized solution to the problem by adapting the Deep Q-learning Network model.
6249,3, I understand that it is not an issue in the test set.
6250,3," The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences."
6251,3,\n\nMinor: fix margins in formula 2.7.
6252,3,"""The paper claims that image captioning systems work so well, while most recent state of the art papers show that they produce 50% errors, so far from perfect."
6253,3,"""## Review Summary\n\nOverall, the paper's paper core claim, that increasing batch sizes at a linear\nrate during training is as effective as decaying learning rates, is\ninteresting but doesn't seem to be too surprising given other recent work in\nthis space."
6254,3,\n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS.
6255,1,"\n\nSignificance\nThe relaxed dual formulation and OT/Monge maps convergence results are interesting and can of of interest for researchers in the area,"
6256,3, It also shows that the phenomenon that sharp minima lead to worse result can be explained by Bayesian evidence.
6257,1,\n\nPositives:\n\n- An interesting new idea that has potential to be useful in RL
6258,2,We regret that some of the remarks made by Referee 1 were not edited before being sent to you. -Editorial Assistant on behalf of Edito
6259,3, Can the VGG be instead trained from scratch in an end-to-end way in this model?
6260,3,". For instance, in the case of MNIST, rotations with different degrees are applied"
6261,3," For this reason, I vote for a borderline accept."
6262,3,"""The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization."
6263,3,"\n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator."
6264,3,"\nDumoulin, Shlens and Kudlur (2017)\nhttps://arxiv.org/abs/1610.07629"
6265,3,  \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept.
6266,3,"\n- Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics."
6267,3, The authors propose the CrescendoNet which is without residual connections.
6268,3,"""This paper is an extension of the recent language style transfer method without parallel training pairs (Shen et al. 2017)."
6269,3, This will be clear if you provide the pseudo-code for learning.
6270,1, The numerical experiments show that using CCC strategy leads to an increase in the proportion of efficient equilibrium outcomes.
6271,1, The proposed approach seems to be of interest and to produce interesting results.
6272,3," As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain."
6273,2,"As it is often the case with conceptual developments, most of it doesnt make a lot of sense, in the..."
6274,3,\n- Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful.
6275,3," For visual information, authors use a pretrained CNN (with fine tuning)."
6276,3, A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion.
6277,3," Then, the model generates a program, based on the extracted tuple collection and the question, to find an answer."
6278,3," Several complex but discrete control tasks, with relatively small action spaces, are cast as continuous control problems, and the task specific module is trained to produce non-linear representations of goals in the domain of transformed high-dimensional inputs."
6279,1,  The paper is very well written and quite clear.
6280,3," This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition. "
6281,3," They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article."
6282,3," More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces."
6283,3," The authors point it out, but a further investigation might be interesting."
6284,1, The experimental analysis of the approach is very convincing and confirms the author\u2019s claims.
6285,3,"\n\nIndeed, the authors have commented: \""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures."
6286,3, Does it correspond to the seconds per update step or the overall training time?.
6287,3,"But, the paper does not provide in-depth discussion on this."
6288,3, It would also be nice to compare with Soudry et. al.
6289,2,(although I admit that here is a possibility they might turn out to be correct in that they are guessing right)
6290,3,"""This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust."
6291,3," While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data."
6292,1,"""Pros:\nThe paper is easy to read."
6293,3, Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data? 
6294,3,"  In particular, how to ensure the semantic space u to be same as the vocabulary semantic space?"
6295,3,"\n\n  The paper uses LSH structures, computed over the set of examples,"
6296,1,"The approach proposed by the authors is an excellent first step in this direction, and they provide a convincing argument for why a previous approach (joint optimization) did not work."
6297,1," The results might be of theoretical interest,"
6298,3,"\n10. Also, table 3 could be greatly improved by providing more ablations such as results for (n=16, d=8), (n=4, d=4), etc."
6299,3," How well does the framework scale to more complex scenarios, e.g., multiple types of manipulation together?"
6300,3,  I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.
6301,3,\n\nI believe the design choices made by the authors to be valid in order to get things to work.
6302,1, \n\nMinor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix).
6303,3," Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios."
6304,3,\n-- The authors show that filters with different characteristics are responsible for different aspects of image modelling
6305,3, While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation.
6306,3, Sample-based RL did not originate in 2008.
6307,3, Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points.
6308,2,Reviewer : I do not have any additional comments. I can let this manuscript pass.
6309,3,"\n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics."
6310,3," \nIn experiment 3 instead, the defined setting is supposed to give imporance to colors, as stated at the end of page 4."
6311,3," Besides, what is the effect when having more and more support images?"
6312,1, \n\nThe paper is written well and provides good insights (mostly taxonomy) on the existing methods for neural network-based clustering.
6313,3," Why is this?\n\n* I'm confused by the difference between Table 6 and Table 4? Why not just include the TLM results in Table 4?\n\n\n\n\n\n\n"""
6314,3, It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings.
6315,3,"  If it were, this would be a much sought breakthrough."
6316,1,"\n\nPaper may be useful to practitioners who are looking to implement something like this in production."""
6317,3, \n\nThe authors give an example of an adversary manipulating the input in order to draw the activation to specific features to draw confusing/malignant explanation maps.
6318,1, \n\nThe work itself is interesting and can provide useful alternatives for the distribution over the latent space.
6319,3," The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms."
6320,3," Are the types of classes in the seen/unseen classes important, I'd expect at least multiple runs of the current experiments on (E)MNIST."
6321,3,"""This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models."
6322,3,  but it would be important to argue that those options are likely to be useful again under some task distribution.
6323,3,"""The paper proposes a method to train deep multi-task networks using gradient normalization."
6324,3," To force the probability near the endpoints you have to use alpha, beta < 1 which results into a \u201cbowl\u201d shaped Beta distribution."
6325,3," I appreciate if authors can provide more results in these settings.\n\n"""
6326,3," \n\nI am not an expert in this area, so it is difficult for me to judge the technical merit of the work."
6327,1, Experiments on 3 different tasks showcase the potential of the proposed method.
6328,3," The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts."
6329,3," The authors compare their method, named Sequential Regression Models (SRM) in the paper, to previously proposed methods such as BNN, LCE and LastSeenValue and claim that their method has higher accuracy and less time complexity than the others."
6330,3,\n\nExperiments on two domains are presented.
6331,1,"\n\nOverall, I think it is a nice demonstration that non-recurrent models can work so well,"
6332,1, This focus is mainly on medical image classification but the approach could potentially be useful in many more areas.
6333,3,  \n- Why does the method require two images?
6334,3, \nThe main idea of the paper is to project examples into an RK Hilbert space\nand performs convolution and filtering into that space.
6335,3,"  First, I'd suggest acknowledging these works and discussing the differences to your work."
6336,3,".  It may be possible to motivate and demonstrate the methods of this paper in other domains, however."
6337,3,\n\n2. Clarification on the unit ball constraints.
6338,3,"\n\nSecondly, the experiments could have more thorough/stronger baselines."
6339,3,"""The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network."
6340,3," \n\nThe paper's most clear contribution is the observation that the objective results in multiplicative compositionality of vectors, which indeed does not seem to hold in CBOW."
6341,3," They iteratively explore all\npossible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive\noracle ensures that the procedure converges."
6342,3,"\n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately."
6343,3," Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP."
6344,3, Experiments on both convolutional and recurrent networks are used for evaluation.
6345,3,\n\n(2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp. 
6346,1,\n2. Good empirical evaluation with ablations.
6347,1," The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis,"
6348,1,"""This paper proposes a novel adaptive learning mechanism to improve results in ergodic cooperation games."
6349,3,"""This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase."
6350,3, What is your network updating toward?
6351,3, . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?
6352,3,\n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components.
6353,3," In addition to further discussion on this point, this result also suggests evaluating other recently proposed \""minor changes\"" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016)"
6354,1, The experimental results also sufficiently demonstrate the proposed advantages of the model.
6355,2,I recommend the publication even if I am not impressed
6356,3, Results further outline the resemblance between the compared methods.
6357,3,"\n\n2) Page 2 minor typos\nWe study training problem -->we study the training problem\nIn the regime training objective--> in the regime the training objective[[CNT], [CLA-NEG], [DFT], [MIN]]\n\n3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al"
6358,3," However, I couldn\u2019t find any details on how the phrasal verbs were chosen, or what (if any) held out data was used for tuning the models."
6359,3, What type of\ntuning did you do to choose in particular the latent dimensionality and the\nlearning rate?
6360,3," \n- It would have been nice to see results on other tasks than domain adaptation, such as synthetic image generation, for which GANs are often used"
6361,3," The student model is a deep learning model (MLP, CNN, and RNN were used in the paper)."
6362,3,"""The authors tackle the problem of estimating risk in a survival analysis setting with competing risks."
6363,3,\n\nCons\n- Requires access to a black-box oracle to construct the dataset.
6364,3, The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch.
6365,1," As this is an important component of the proof of the main result, the paper would benefit from an explanation of this step?\n"""
6366,3,"\n- a network predicting a polar origin,"
6367,3, This generated image is then sent to the discriminator.
6368,1,"\n\nThe method seems to give significantly lower kernel approximation errors,"
6369,2,"So, I guess the [XX] theory was dead on arrival when it was proposed."
6370,3,"\nAlso, why highlight the problem of authorship attribution of Shakespear's work in the introduction, if that problem is not addressed later on?"
6371,1,"\n(ii) \""One benefit in using the GP is due to its Bayesian nature, so that predictions have uncertainty estimates (Equation (9))."
6372,1,"""The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing."
6373,3,"\n\nGILAD-BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER, K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In Proceedings of The 33rd International Conference on Machine Learning (2016), pp. 201\u2013210."
6374,3," If we add another class, no matter how do we define the borders, there will be one pair of classes for which the transition from one to another will pass through the region of a third class."
6375,3,  That could be worthy of some (brief) discussion.
6376,3, In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance.
6377,3, \n\nThe training process looks highly elaborate with a lot of hyper parameters.
6378,3,\n\nThe authors present multiple qualitative and quantitative evaluations.
6379,3," Here, the main goal is to use evidence-based arguments to distinguish good from poor local minima."
6380,1,"\n\nOverall: There's the start of an interesting idea here,"
6381,3,\n-\tWhat are the hyperparameters values use for the LSTM\
6382,3,"""This paper studies a new architecture DualAC."
6383,3, I might have missed something here.
6384,3, If not we have to count the distillation frames as well.
6385,3, The authors should also analyze the sensitivity of speedup and accuracy drop depending on the learning rate for \u2018Rank selection\u2019.
6386,3,\n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals.
6387,3, Addressing each and every concern is quite important:\n\n1) Speed.
6388,3, I do not understand how the \u201crepresentative units\u201d are selected and where the \u201clate\u201d selectivity on the far right side in panel a arises if not from \u201cearly\u201d units that would have to travel \u201cfar\u201d from the left side\u2026 
6389,1," Reward shaping allows the proposed model to outperform simpler baselines, and experiments show the model generalizes to unseen graphs."
6390,3," For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner."
6391,3, In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set. 
6392,3," \n\nOverall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders."
6393,3,"\n\nThe authors break down the \""inference gap\"" in VAEs (the slack in the variational lower bound) into two components: 1. the \""amortization gap\"", measuring what part of the slack is due to amortizing inference using a neural net encoder, as compared to separate optimization per example."
6394,3," However, the questions themselves could have been formulated differently: \nQ1: the way Q1 is formulated makes it sound like the covariates could be both discrete and continuous while the method presented later in the paper is only for discrete covariates (i.e. group structure of the data)."
6395,3,"  While the forward RNN is trained to predict the next timestep, its hidden representation is forced to be similar to the representation of the backward RNN in the same optimization step."
6396,3, It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results.
6397,3,"\n\nQuestions:\n\n1. What is \\rho on page 4?.[[CNT], [null], [QSN], [MIN]] I assume it is some nonlinearity, but this was not specified.[[CNT], [null], [QSN], [MIN]]\n2. On page 5, it says the merge block takes as input two sequences.[[CNT], [null], [DIS], [MIN]] I thought the merge block was defined on sets?"
6398,3,"""This paper proposes an interesting variational posterior approximation for the weights of an RNN."
6399,3, \n- page 6: \u201con the last full precision network\u201d: should probably be \u201con the last full precision layer\u201d\n
6400,3, The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning.
6401,3," According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit."
6402,3,"\""\n\n* I didn't find Table 1 particularly illuminating."
6403,3,"""The article \""Do GANs Learn the Distribution?"
6404,3,\n\nSeveral references I suggest:\nhttps://arxiv.org/abs/1706.08500 (FID score)\nhttps://arxiv.org/abs/1511.04581 (MMD as evaluation)
6405,1," This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks. "
6406,3,"""In the paper, the authors discuss several GAN evaluation metrics."
6407,3, Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.
6408,3,"\n5) \""We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\"" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model."
6409,1," The resulting representations are show to be useful \u2013 they produce SOTA results on preposition selection by a decent margin (on a practically useful and recently studied tasks that is arguably the task that best reflects on the quality of the learned representations) and good (but not quite SOTA) results, without further linguistic features beyond POS tags, on the much more studied task of preposition attachment disambiguation."
6410,3,"  They do not mention whether they train the previous methods on the new dataset, and some of their reported improvements may be because of this."
6411,3, I would therefore like to ask the authors to put a lot of effort into improving the presentation of the paper.
6412,3,"""The paper presents results across a range of cooperative multi-agent tasks, including a simple traffic simulation and StarCraft micro-management."
6413,1,\n\nStrengths\n- Use of graph neural nets for few-shot learning is novel.
6414,1,"\nThe tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets."
6415,3," \n\n* The paper overstates the contributions of Q-masking, emphasizing improvements to data efficiency among others."
6416,3," (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure;"
6417,3," If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3."
6418,3,  The comparisons for the Twitter dataset were even based on character-level with word-level.
6419,3," Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning."
6420,3, Is there a way to support this experimentally?
6421,3,And 3) how important is the HDDA and the specific variant used (which is not clear) important: is it important to model the Gaussians using a sub-space?
6422,2,"Many questions on the text, for example, cause embarrassment in understanding the text"
6423,3,"\n\n- In Section 2, the authors review ideas of so-called random kernel sparsity."
6424,3," In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations."
6425,3, It is a multi-view extension of variational autoencoder (VAE) for disentangled representation.
6426,3,"""The authors propose using piecewise linear activation functions with contraints to make it continous."
6427,3, Specially a formal representation of the method should be included.
6428,1," \n\n2) The word embeddings used seem to be sufficient to capture the \""knowledge\"" included in the corpus."
6429,3,"""This paper presents two methods for imposing a margin on discriminative loss functions, one which uses the margin between the reference transcription and alternatively hypothesized transcriptions (LMLM), and another which compares all alternative candidates and uses a margin between those with a better system objective (WER or bleu) and those with a worse system objective (rank-LMLM)."
6430,3,"\n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required."
6431,3," First, if we trained the proposed model with starting from \""zero\"" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?"""
6432,3,"\n\nForming the predictive distribution explicitly is intractable, so the paper suggests training a\nneural net to map from a subset of inputs to the predictive distribution over outputs."
6433,1," \n\nI like the idea of the paper,;"
6434,3, It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots.
6435,3, This is important as stacking LSTMs is a common practice.
6436,3,"\n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012."""
6437,3," The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs."
6438,3,"""The paper discusses dropping out the pre-softmax logits in an adaptive manner."
6439,3, This would help interesting applications inform what is wrong with current theoretical views.
6440,3,"\nThis paper also uses a log-polar transform, but lacks the focal point prediction / STN."
6441,3,"\n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions?"
6442,3," Specifically, they observe that:\n\n(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet."
6443,3,"""*Summary*\n\nThe paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs)."
6444,3," Many Markov games do not have a cooperative equilibrium, so this paper restricts attention to those that do."
6445,3, The weights thus change according to the imposed neighborhood relationship and depending on the class labels.
6446,3,"\nSpecifically, results were presented for multiple defenders and some ablation experiments were highlihgted"
6447,3, since I think it much makes it clearer to explain
6448,3,"\n\nAs a minor remark, please make figures readable also in BW."
6449,3,"\n\n----------------------\n\n\nThis paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs."
6450,3," As it stands, it's not even clear if larger \nvalues are better or worse."
6451,1, The papers are written well and easy to follow.
6452,3," I'd highly recommend the authors to cut a third of their text, it would help focus the paper on the actual message: pushing their new algorithm."
6453,1,  The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility. 
6454,3,"\n\nRegarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods."
6455,3, This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard.
6456,3,"\n- For the interpolation results you say \""we output the argmax\"", what is meant?"
6457,3,"  In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained."
6458,3," \n- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the \""leave-one-out\"" version of the estimation."
6459,3," Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'..."
6460,3," As shown in in Fig.8, the estimated dimensions and original dimensions are very different."
6461,3, Actually understanding it at depth and validating the proofs and validity of the experiments will require some digestion.
6462,3,  They then show that the result matrix satisfies a restricted isometry condition.
6463,3,"""The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective."
6464,3, An externally valid measure would strengthen the results.
6465,3, This modifies the generator objective.
6466,3,"\n2) How does the approach generalize to other languages? (Presumably well, but something to consider for future work.)"
6467,2,"Though the objective of the paper pretends to be ambitious, there are a significant number of problems that limit the study's usefulness."
6468,3, One could do a method that completely distort the image and therefore will be classified with as a class.
6469,3,"2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate]."
6470,3, Similar questions for the 5\n   models in the 3D rendering experiment.
6471,3,   \n\nWould this proposed strategy have thwarted the Russian tank legend problem?
6472,1," While the learning architecture is not original in itself, it is \nshown that a proper physical regularization greatly improves the performance."
6473,1,\n\nIt is admirable that the authors use an interesting (and to my knowledge novel) data set.
6474,1,"\n\nOverall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found \""in the wild\""."
6475,2,"Through streamlining, more meaningful references and restriction to the essentials, the manuscript may still be saved"
6476,3,\n- The decoder model has randomness injected in it at every stage of the RNN.
6477,3," For comparison the existing methods ICA, Tucker2 should also be evaluated for the baseline corrected data, to see if it is the constrained representation or the preprocessing influencing the performance."
6478,3,"\n- \""The episode length (time budget) was randomly set for each episode in a range such that 60% \u2212 80% of subtasks are executed on average for both training and testing."
6479,3, i.e. Could you please elaborate on what's different (in terms of learning) between 3.2 and a normal latent Z that is definitely allowed to affect different classes of the data without knowing the classes?
6480,3,"If it is done by hyperparameter tuning with cross-validation, the training cost may be too high."
6481,3, How would it compare to other methods on those problems?
6482,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
6483,3, TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices. 
6484,3," The task itself is not too complex which involves 10 objects, and a small set of deterministic options"
6485,2,This is a antique approach to a modern characterizationproblem
6486,1," It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy."
6487,3,"\nAnd just because something works on MNIST, does not mean it works in general."
6488,2,The last two sentences of the summary greatly exaggerate the value of this paper and the usefulness of its conclusions.
6489,3," \n\n4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this."
6490,3,"\nSignificance - Apart from the issue of intractable inference which is arguably a large limitation of this work,"
6491,3, \n\nThe goal here is to make the target sentence generation non auto regressive.
6492,3,\n\nThere has been a number of highly relevant papers.
6493,3,"\n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models."
6494,3, Is this because of the MCTS approach?
6495,3,"  \n\nObjects are matched across consecutive frames using non parametric (not learnable) deterministic matching functions, that takes into account the size and appearance of the objects."
6496,3,"\n\n2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)?"
6497,3, The way the authors propose to do it is to have the agent follow a random policy.
6498,3," Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD."
6499,3," \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling."
6500,3,"  Experiments on conditional generation (speech recognition, image captioning), and unconditional generation (MNIST pixel RNN, language models) show the effectiveness of the regularizer."
6501,3, \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?
6502,3," Correspondingly, the derivative of C is a small negative value."
6503,3,\n\n- The presentation can be improved.
6504,3,"\n\nThe training data is synthetic, allowing for arbitrarily large training sets to be generated."
6505,3,"\n3. Due to the randomness from multiple operations, the experiments need to be run multiple times, and see if the output clustering is sensitive to it."
6506,3,\n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem?
6507,3, \n\n- This paper has a lot of orthogonal details.
6508,3,"\n\nThe approach is interesting and the paper is well-written,"
6509,3,\n\n\nCLARITY\n\nThe details of the memory based kernel density estimation and neural gradient training seemed\ncomplicated by the way that the process was implemented.
6510,3,"""This paper proposes to join temporal logic with hierarchical reinforcement learning to simplify skill composition."
6511,3,\n\nSo the learning dynamics for VAN and ResNet are very different because of this scaling. 
6512,3,"\n\nOne empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3])."
6513,1,\n- Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance.
6514,3," In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement."
6515,3," \n\nThis paper replaces the manual addition of a distractor sentence with a single word replacement where a \""narrator\"" is trained adversarially to select a replacement to fool the question answering system."
6516,3," Should that be z_t ?"""
6517,3,"\n\nThe authors present two ENAS models: one for CNNs, and another for RNNs."
6518,1," I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art."
6519,3," \n\nInstead of saying that in passing why not explicitly state it in key places, including the abstract and title?"
6520,3, Presumably this statement needs to be made while also keeping mind the number of importance samples.
6521,3," \n\n(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer."
6522,1,"       \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off)."
6523,3," \n(v) Why is an incorrect \""pseudo\""-set notation used instead of the correct vectorial one?"
6524,3," The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions."
6525,3," However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem."
6526,3," Methods like neelakantan et al's multisense embedding, for example, which the work cites, can be used in some of these evaluations, specifically on those where covariate information clearly contributes (like contextual tasks)."
6527,3,"\n\n---\n\nReferences \n\nCai, J., Shin, R., & Song, D. (2017)."
6528,3,"\n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) ."
6529,3,"""In the medical context, this paper describes the classic problem of \""knowledge base completion\"" from structured data only (no text)."
6530,3,"\n\nIt would also be nice to see some visual representations of images perturbed with the new perturbations, to confirm that they remain visually similar to the original images."
6531,3,"""The paper presents a method to parametrize unitary matrices in an RNN as a Kronecker product of smaller matrices."
6532,3, Could we not also apply this to negative examples?
6533,3,"\n\nWith regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks."
6534,3," \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose."
6535,1," A major contribution is certainly the combination with an adversarial loss, which is a non-trivial task."
6536,3, Why/when is the 2nd term in (19) small?
6537,3,\n\nThe paper is a follow up on previous work by Machado et al. (2017) showing how proto-value functions (PVFs) can be used to define options called \u201ceigenoptions\u201d.
6538,3,"  The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images."
6539,3,"\n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?"""
6540,3,\n \nDo you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues?
6541,2,"Frankly, she knows nothing about X or Y and should stick with what she does know."
6542,2,"Publication of this paper will not advance our knowledge in any shape or form, it will just result in other researchers pointing out how bad this study actually is"
6543,3,\n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.
6544,2,"Exhaustive pages of statistical and meta-analytic data and results which seem rather redundant are presented, about 60+ pages of minute analyses of each possible particle of data."
6545,3,"\n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases."
6546,2,This paper contains neither theory nor research.
6547,3," When a tensor is decomposed as a sum of rank-1 tensors (outer products), the number of operations in a DNN forward pass decreases leading to a faster testing runtime."
6548,3,"""MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure."
6549,3," I would also change the caption to use \""meta-training losses\"" instead of \""training losses\"" (I believe those numbers are for the meta-loss, correct?)"
6550,2,Use of words like performativity (is that a word?)
6551,3," \n\nWhile the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments."
6552,1," Moreover, the authors themselves suggest how to proceed along this line of research with further improvements."
6553,3, It would have been helpful if the author(s) could have made a formal statement. 
6554,3, Why should we introduce the convolution stochastic layers?
6555,3,  The fragility of DNN models to marginally perturbed inputs themselves is well known.
6556,3,"  The faces experiment is\nsimilar to previous work done by Martin (2011) and Kontsevich\n(2004) but unlike that previous work does not investgiate whether\nclassification features have been identified that can be added to an\narbitrary image to change the attribute \""happy vs sad\"" or \""male vs female\""."
6557,3,"\n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017."""
6558,2,Jargon-riddled assertions are made as though the meagre data substantiate the claims.
6559,3," Probably better to introduce branch for key methods, parallel sampling/ translation broadcasting and inadaptive or adaptive model."
6560,2,"I understand Wikipedia is not the best source of information, howeverâ€¦based on the information from Wikipedia, your hypothesis breaks down"
6561,3,\n\n2. The authors measured the actual speedup on a single CPU (Intel Core i5)..
6562,1,"\n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN"
6563,3, The plot does not explicitly account for the distillation phase.
6564,3," Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers."
6565,3," \""Adversarial agents\"" is also misleading because those agents act like automata."
6566,1,  \n2. TR seems to lose generalization more gracefully than SGD when batch size is increased.
6567,3,Or is the layer shown (e.g. \u201cstage3layer2\u201d) the penultimate layer?
6568,3," As a result, the results are not suggesting significance or generalizability of the proposed method."
6569,3,"  In general, it treats all object proposals as nodes on the graph."
6570,1," the problem is described neatly in conjunction with the past work,"
6571,1,"\n+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims."
6572,1,"\n\nThe connection between ASG, CTC, and marginal log loss has been addressed in [9], and it does make sense to train ASG with the partition function. "
6573,3,"\n\nWhile I can definitely imagine that your network architecture is well-designed and a good choice for image classification tasks,"
6574,3," \nIn this case, perhaps expressing heuristic rewards as potentials as described in Ng\u2019s shaping paper might solve the problem."
6575,3, This makes training of such networks cumbersome.
6576,1," Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work."
6577,3," (c) has no reference equation, does it?"
6578,3,"\n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks?"
6579,1," As a comparison, when we define a random sub-goal the resulting reward is a very sparse signal that would almost certainly not give rise to useful myopic policies."
6580,3,"  In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1."
6581,3, What would happen if the learning rate is not reset at the beginning of each cycle?
6582,1, The main difference from previous work is restricting the context to be close to a preposition.
6583,3,The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem. 
6584,3, This could help validate the hypothesis: how does the estimated rank vary with the number of components?
6585,3,"""This paper studies learning to play two-player general-sum games with state (Markov games) with imperfect information."
6586,3,"""The authors propose a new defense against security attacks on neural networks."
6587,3, It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores.
6588,3, \n-\tCan the authors address the earlier comment on how their approach provides \u201cguarantees for preserving neural net functionality approximately\u201d?\n\n
6589,3, \n- how many new data points are finally added into the training data set?
6590,3," Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model."
6591,3, I had to read a lot of sections more than once and use details across sections to fill in gaps.
6592,3," External/internal?\n \n\u201cSpecifically, it is applicable to various methods as described below \u2026\u201d Related papers should be cited."""
6593,3,"The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv"
6594,2,There are far too many typos and grammatical mistakes. I started to correct these but got annoyed.
6595,3," \n\nOn the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently."
6596,3," Did the authors try truncating after more words (e.g., 10k)?"
6597,3, This latter technique is similar to randomized coordinate descent.
6598,2,"I would propose skipping the entire first page, which is bewildering and distracting"
6599,3, I think a language modeling benchmark and/or a larger scale question answering dataset should be considered.
6600,3, however this paper is difficult for me to follow.
6601,1,"\n\nIn general, I think the paper is written clearly and in detail."
6602,3," \nHowever, if the next step in the expert\u2019s path was difficult to approximate, \nthen the reward for imitating the expert would be lower."
6603,3,"  The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general-purpose method for program synthesis example selection."
6604,1,\n2. It provides useful insights of model behaviors which are attractive to a large group of people in the community.
6605,3, \n\nOther issues are listed as follows:\n(1). Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3?
6606,3," In those scenarios, the proposed method might not work well and we may have to resort to the gradient free methods. "
6607,3,\u201d\n7. Page 7: \u201cThis shows that a good initialization is important for this task.
6608,3,"""The paper describes a neural end-to-end architecture to solve multiple tasks at once."
6609,1,"\n\nTo summarize, I like the work and I can see clearly the motivation."
6610,3,"\n- Finally, a minor point: I will challenge the authors to justify their claim\n  that the learned generative model is \""useful\"" (their word)."
6611,1,". In my view, it is not a correct statement to call the feature extraction from a CNN unsupervised feature learning as the referred CNNs in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as well. "
6612,3,"""This paper investigates an effect of time dependencies in a specific type of RNN."
6613,3,"\nTo enable training with SGD, the authors calculate the centre within a mini batch"""
6614,1,Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels.
6615,2,It was as if they walked in with a blank sheet of paper.
6616,3,"\n- You state: \""singular values of an IP layer behave according to the MP distribution even after 1000s of training iterations."
6617,3, This is in general not true.
6618,3,  It may also lead to simpler solutions.
6619,3,  for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .
6620,3, It would be more clear if you can provide a complete pseudo-code of the learning procedure.
6621,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
6622,1,\n\nThe idea is interesting and trendy.
6623,3,"""Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same."
6624,3,  Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.
6625,3,\n\nThe authors also prove a byproduct result stating that identifiability holds for Leaky ReLU networks with one hidden layer.
6626,3," In practice, how large must the weight matrix be to observe this behavior?"
6627,3, This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.
6628,3, Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices.
6629,3," You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear."""
6630,3," \n\nAt some point in the first 4 pages it would be good to explain what is meant by ``hard\u2019\u2019 functions (e.g. functions that are hard to represent, as opposed to step functions, etc.) \n""."
6631,3, The conclusion in the paper is that training at high momentum requires additional training epochs before $A$ reaches its equilibrium value. 
6632,3," \n\nOne has to wait till we go into the experiments section to read something like:\n\""Lastly, although in theory, we need full gradient and full Hessian to guarantee convergence, calculating them in each iteration is not practical, so we calculate both Hessian and gradient on subsampled data to replace the whole dataset\""\nfor readers to realize that the authors are talking about a stochastic mini-batch method."
6633,3, (end of page 2)\n- Is it correct that the same representation stored in the NE table is used twice?
6634,1,"\n\nAlthough related work is discussed in detail in section 1, it remains unclear how exactly the proposed algorithm overlaps with existing approaches."
6635,1, The addressed problem is challenging and the proposed idea seems interesting.
6636,1,\n\nThe paper demonstrates improvements in a number of public datasets.
6637,3, More regularization required?
6638,3, All of the approaches seem to perform about the same
6639,3,"  The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels)."
6640,3,"""This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators)."
6641,1," Experiments show that the method helps to find paraphrases of phrasal verbs, as well as improve downstream performance on preposition selection and preposition attachment disambiguation."
6642,3,\nNote that item 2) is relevant in many other setups in the deep learning framework and is often overlooked.
6643,1, The paper is very well written and has nice visualisation of demonstrating weights.
6644,2,Pleasepleaseplease with sugar on top remove sentences that sound like Hegelian light bulb moments while not meaning anything
6645,3," A conv net on the image is combined with that on a state image, the state being interpreted as rechable pixels."
6646,3,\n\n6th line of 5.1 theta_l is initialised uniformly in an interval -> could you explain why and/or provide a reference motivating this ?
6647,3,"\n\nPage 3: \""using virtual observations (originally proposed by Qui\u00f1onero-Candela & Rasmussen (2005) for sparse approximations of GPs)\""\n\nThe authors are citing as the origin of virtual observations a survey paper on the topic."
6648,3,"   \n\n\n- \""Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate..."
6649,3,"\n\nThe R_AC, penalizes correlation between responses of different nodes."
6650,3,"""Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape."
6651,3,"This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help."
6652,3,\n2.\tThe dataset is motivated as consisting of four challenges (described in the summary above) that do not exist in the existing RC datasets.
6653,1, I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).
6654,1," The most useful part of the paper is the empirical evidence to\nbackup this claim, which I can't easily find in previous literature."
6655,3,"Namely, the author train a generative adversarial network (GAN) to adversarial examples for a target network."
6656,1,\n- The experiments cover different settings with different task difficulties.
6657,3,"""This paper proposes a neural clustering model following the \""Noise as Target\"" technique."
6658,3," However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit"
6659,2,"XXX is inserted purely for fashion, adds nothing, and reflects the authors belief/wish that fashionable papers regardless of logic or content have a larger chance of acceptance. Our community needs to combat this sort of unreflexive pseudo-scholarship"
6660,1,"\\n\n\nOverall, I like the paper, I like the algorithm and I think it is a valuable contribution."
6661,3,"""It is rather difficult to evaluate the manuscript. A large part of the manuscript reviews various papers from the active vision domain and subsequently proposes that this can directly be modeled using Friston\u2019s free energy principle, essentially, by \u201canalogy\u201d, as the authors state."
6662,1,"  The numeric examples, although quite toy, provide a clear illustration."
6663,3,"\n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0."
6664,3," The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?"
6665,3," With an appropriate fix, lemma 4 gives a cleaner set of assumptions than previous work in the same space (Nguyen + Hein \u201917), but yields essentially the same conclusions."
6666,3," Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks."
6667,3," The annealed importance sampling technique of Wu et al (2017) for\nestimating bounds on a generator's log likelihood could be easily applied in\nthis setting and would give (for example, on binarized MNIST) a quantitative\nmeasurement of the degree of overfitting, and this would have been preferable\nthan inventing new heuristic measures."
6668,3,"\n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB."
6669,3,. These structures are extrapolated  by \n1) mapping the graph nodes into an embedding using an algorithm like node2vec\n2) compressing the embedding space using pca\n3) and extracting 2D slices from the compressed space and computing 2D histograms per slice.
6670,3,"In Section 4, the authors show their method on permuted MNIST."
6671,3,\n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer.
6672,3, This is not clear at all after reading the paper.
6673,3,\nand form a predictive distribution over the output for all the remaining possible inputs.
6674,1,\n\nDespite the rich literature of this recent topic the related work\nsection is rather convincing.
6675,1,Hence I strongly recommend accepting this paper.
6676,1," The paper is well-written, reasonably scholarly, and contains stimulating insights."
6677,3," \n\nWith regard to the learning rule: while the rule is formulated in terms of spikes, it should be noted that for neuron with many inputs and outputs, this update will have to be computed very very often, even for networks with low average firing rates."
6678,3,\n\n- Quality of figures.
6679,3, \n\nQuestions/Comments:\n\n- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method. 
6680,3," Also, the fact that the importance sampling ration is always cut at 1 (or not used at all) is inherited from these papers too."
6681,3, More baseline besides DC-SBM could better illustrate the power of GAN in learning longer random walk trajectories.
6682,3,"""# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples."
6683,1, The proposed method works well on CIFAR and MNIST datasets.
6684,1, The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1).
6685,3," In fact, a VAE would be nicely suited when proposing to work with low-dimensional latent spaces."
6686,3,". Also, the caption for Table 2 could contain more information regarding the network outputs."
6687,3,"  For example, analyzing whether the latent stochastic variables may shown to actually help with generalization of composition of primitive commands. \n\n """
6688,3,\nA Learned Representation For Artistic Style
6689,3," Are they all the continuous control tasks available in open ai?\n\n\n \n\n\n\n\n"""
6690,3,"  There are theorems,  but they are trivial, straightforward applications of importance sampling."
6691,3,"\n\nE.g. in the beginning of the 2nd paragraph in Introduction, the authors write \""Generative models like GANs, VAEs and others typically define a generative model via a deterministic generative mechanism or generator ...\"". While I think the use of a **deterministic** generator is probably one of the unique features of GAN, and that is certainly not the case with VAE, where typically people still need to specify an explicit probabilistic generative model."
6692,3,"""The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset."
6693,3, Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion)
6694,1,\n\n(2) The experiment is minimal and even the given experiment is not described well.
6695,3, Please proof read.
6696,3,"\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.]."
6697,3,\n\nI would like to see more explanations of Figure 4.
6698,3," For the videos, it may be useful to indicate when the goal is detected, and then let it run a couple more steps and stop for a second."
6699,1,"\n\n\nMODEL & ARCHITECTURE\n\nThe PATH function given a current state s and a goal state s', returns a distribution over the best first action to take to get to the goal P(A)."
6700,3,A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting.
6701,3,"\n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n"""
6702,3,"\n\n* \""On average, 2H(X|Z) elements of X are mapped to the same code in Z."
6703,3,"\n\n8. In figure 2, how are rounds constructed?"
6704,3,"""`The papers aims to provide a quality measure/test for GANs. "
6705,3," After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task."
6706,1,"\n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks."""
6707,1,"It was a nice result that only optimizing ROUGE directly leads to lower human evaluation scores, despite the fact that that model achieves the best ROUGE-1 and ROUGE-L performance on CNN/Daily Mail."
6708,3," What is new compared to [Finn et al. 2016] is that the authors managed to train the network in combination with an adversarial loss, which allows for the generation of more realistic images."
6709,3, The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X (an introduction to C# offered on edx) and problems on the Microsoft CodeHunt platform.
6710,2,"Most part of methodology is useless, most of paragraphs are inrelevant to the main topics"
6711,3, Is there any mistake in the description of the abstract?
6712,3," but given the empirical nature of this paper I would have liked to see results on more interesting datasets (Celeb-A, CIFAR-10, really anything but MNIST)."
6713,3, Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined.
6714,3,". The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples."
6715,1,"\n\nOn the other hand, the theoretical part of the paper is not really improved in my opinion,"
6716,2,In order to be able to publish this manuscript it need [sic] to be rewritten in the form of a scientific article
6717,3, The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label.
6718,1,"\n\nOverall, I like this paper and the approach of extending TS-style algorithms to Deep RL by just taking the final layer of the neural network."
6719,3, In light of this (and of the paper being quite a bit over the page limit)- is this material (4.2->4.4) mostly not better suited for the appendix?
6720,1," Overall, this paper contributes a useful new dataset that can be quite useful for reading comprehension."
6721,3,". Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time."
6722,3, They have been extended to 2-d later on (Spatio-temporal TDNNs)
6723,3,"\nThe curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only."
6724,3," More specifically, there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2."
6725,1,"\nHowever, the result is supported by exhaustive experiments making the result highly convincing."
6726,3,"\n \nMinor \n1. In Table 1, where is sigma defined?"
6727,3," In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks."
6728,3," The first part (up to page 11), focuses on better understanding the exploding gradients problem, and challenges the fact that current techniques to address gradient explosion work as claimed."
6729,3, The only thing the player knows is the best adversarial policy.
6730,3," Also, if this test is diagnostic, why use X-rays for diagnosis in the first place?"
6731,3, Change your claim that VAN is equivalent to PReLU.
6732,3," For example, in binary classification tasks, a very small S_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each other."
6733,3," They find many properties reminiscent of neurons in the mammalian entorhinal cortex (EC): grid cells, border cells, etc."
6734,1," As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models."
6735,3, But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply.
6736,3, The resulting method is\ncalled BDQL.
6737,3,"  It could have more contribution if the authors model the interactions within the attention model itself, instead of a simple prediction layer, which is problem-dependent."
6738,3, The authors propose pixel deconvolutional layers for convolutional neural networks.
6739,1, The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing.
6740,1,\n\n+ Originality: \n- The main direction of incorporating human feedback in the loop is original.
6741,2,I think the N-mixture modeling should be abandoned: it is clear that they [the authors] do not understand this class of models
6742,1,\n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.
6743,3,"\n\nFirst, as explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes)."
6744,1, \n\nThe experiments are complete and the writing is good.
6745,1,\n\nI like the idea of this paper.
6746,3,"""Summary:\n\nUsing a penalty formulation of backpropagation introduced in a paper of Carreira-Perpinan and Wang (2014), the current submission proposes to minimize this formulation using explicit step for the update of the variables corresponding to the backward pass, but implicit steps for the update of the parameters of the network."
6747,3," The original min/max formulation of the WGAN aim at minimizing over all measures, the maximal dispersion of expectation for 1-Lipschitz with the one provided by the empirical measure."
6748,1,\n\nPaper Strengths:\n* The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet-BC L = 118 k = 35 e = 3).
6749,1, \n\nPros:\n- The paper is well-written and clear.
6750,1,\n\nI think the idea is interesting and novel..
6751,1, Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time.
6752,1,"nThe paper is well-written, relevant and interesting"
6753,3, The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information.
6754,3," In any case, competing with an active leader board would be much more compelling."""
6755,1,"""This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means)."
6756,3, You motivate the choice of \\alpha^* by wanting to maximize the reduction in the local linear approximation to \\mathcal{C} induced by the update on w. 
6757,3, These are used in a 1-NN classifier and a (smoothed) Naive Bayes classifier.
6758,3, and if the authors are able to address it satisfactorily I will increase my score. 
6759,3," Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels. "
6760,3,\n\nExperiments on RL planning tasks would be interesting to see (Maybe on a simple/predictable environment).
6761,3," With BLEU score differences being so low, the authors should specify how statistical significance is measured;"
6762,3, Did you consider larger dataset for evaluation?
6763,3,A penalty term is added that forces an agreement between the hidden states of the two decoders.
6764,3, Was the same procedure done for the experiments in the paper?
6765,3,"  The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation."
6766,3,  The forward RNN predicts forward in time and the backward RNN predicts backwards in time.
6767,3,"\n- Most Image Caption papers which use a pre-trained CNN model, do fine-tune the image feature extractor to improve the results (see Vinyals et al. 2016)."
6768,1, The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption.
6769,1," I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs."
6770,3,"""This paper analyzes the effect of image features on image captioning."
6771,3,"  \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling."
6772,3, \n 2) 3d voxel occupancy grids with a small resolution.
6773,3,\n\nI verified most of the math.
6774,3," if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes."
6775,3,  The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.
6776,3," If yes, how do you explain the differences?"
6777,3," However, the authors do not cite enough work on such multimodal representation learning (for example, look at Spandana et. al."
6778,3, This may allow you to avoid truncation altogether.
6779,1,\n\nRevised Review:\nThe main idea of the paper is very interesting and the work presented is impressive.
6780,3,\n\nThe Q&A task could be used to describe a simpler system with only a decoder accessing the DB table.
6781,2,"However, the applicant seems to have run out of steam before he developed a detailed plan and completed the proposal. This is unfortunate"
6782,3, They also claim that this analysis can be used to improve the performance of the models.
6783,3,\n\nWhy were so many of the chosen datasets have so few training examples?
6784,3," A simpler speech task such as Keyword Spotting could also be investigated.\n"""
6785,3," I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model?"
6786,3,"  The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach."
6787,3, Is it because the total number of training images drops?
6788,3,"  \n\n- The authors write \""\\alpha_i W c_i does not explicitly depend on the target word t."
6789,3, Why is GLU chosen? Why is dilation used?
6790,3,"\""On Valid Optimal Assignment Kernels and Applications to Graph Classification\"", NIPS 2016.\"
6791,3, The proposed approach does not rely on attention based model.
6792,3, This paper proposes to generate text using GANs.
6793,1, The experimental results seem solid and seem to support the authors' claims.
6794,3,\n- Is there a typo in the bound given by eq. (17)?
6795,1, A moderate improvement compared to other approaches is observed for all data sets.
6796,2,Not good.
6797,1, The example in Table 1 is very good; but more examples (especially involving the quantitative comparison) are needed to demonstrate the claimed advantages.
6798,3,  The first dataset is the well-known benchmark MNIST also presented in a customized Temporal-MNIST.
6799,3,"""This paper proposed a class of control variate methods based on Stein's identity."
6800,3," Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks."
6801,3,"""The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples."
6802,3," \u201d  The surprise would be the initial reaction of \u201cwhat\u2019s that on my car?  Is it dangerous?\u201d but after identifying the object as non-threatening, the emotion of \u201csurprise\u201d would likely pass and be replaced with appreciation."
6803,2,nless the authors performed some pagan ritual before euthanizing the animals I would use killed (or euthanized) instead of sacrificed
6804,3,Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task.
6805,3,"\n- Experiments: The experiments are not enough at all, More specifically, the paper didn't establish any baseline to show the difficulty of this problem and the dataset."
6806,3,"  Not the interpretation itself is fragile, but the model."
6807,3,"\n\nf) It is supposed that the L_1 regularization motes the weights to be informative, this work is doing something similar."
6808,1," It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,"
6809,1," Overall, the reported functionality is nice,"
6810,3,"I believe you mean standard attention, if so, please explain why standard attention is semantic."
6811,3,. I will give a few examples to highlight the point.
6812,1," It is a very interesting set up, and a novel idea."
6813,3," \n\nAs far as I understand, the methods for learning backbone structure and the skip-path are performed independently, i.e. there is no end-to-end training of the structure and parameters of the layers."
6814,1,"\n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty."
6815,3," Based on the magnitude of this profile coefficient, which determines the importance of this `filter,\u2019 individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner."
6816,3,"  However, this equation is the key proposal of the paper. "
6817,3, Their paper provides a proof that in the non-parametric case the optimum on NCE objective function is at the data distribution with normalisation constant either learned or held fixed (0 or any value you like).
6818,3, The paper advocates two primary metrics: accuracy of the predicted motion and perceptual realism of the synthesized images.
6819,3," I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer."""
6820,3, What was the goal here?
6821,1, The paper contain many interesting contributions
6822,3,  Computation of the bound requires integration over multiple layers (equation 15).
6823,1,\n- The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting.
6824,3,"""This paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network VGG19 due to Simonyan and Zisserman."
6825,3,"\nThe Bayesian posterior distribution is obtained by combining an assumed\ngenerative model for the data, data sampled from that model and some prior\nassumptions."
6826,3, The core technique is to construct a coreset of points whose labels inform the labels of other points.
6827,3, The approaches are evaluated in grid worlds with and without other agents.
6828,3, To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word).
6829,3, The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games.
6830,1,  I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct.
6831,2,"Mr. Beard is very courageous to give references- if someone did look at another book, I am sure they would stop reading Mr. Beard'-Feynma"
6832,3, \n\nA few more minor comments:\n(i) How are you training a GP exactly on 50k training points?
6833,3,\n\n7) the experiment in section 6.4 should be presented as an extreme case of that of figure 7.
6834,3, I suspect that the model is also computationally efficient: can the authors report training time for different datasets?
6835,1, \n\nThe experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks.
6836,3, One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work.
6837,3," \n- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate?"
6838,3, There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations).
6839,1,\nThe algorithm then chooses optimistically over the distribution induced by the ensemble.
6840,3,  The author extend the  original algorithm from two-folds:\n\n1. A style discrepancy loss that pushes the style vector of the sentences away from target style
6841,3,"\n - there is a sentence that doesn't end at the top of p.3: \""... the original GAN paper showed that [ends here]."
6842,3,"\n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017."
6843,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
6844,3," At the moment, the description in section 3 is fuzzy in my opinion. [[CNT], [PNF-NEG], [CRT], [MIN]]Interesting information could be:\n- how is the performance of the NMT system?"
6845,3," They ablate the choice of K, the binary nature of the gate weights."
6846,3," For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling."
6847,1,   \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.
6848,3,"\n- Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi."
6849,3," but I got a little lost in the 3 cases on page 4.[[CNT], [null], [CRT], [GEN]] For example, what is r?"
6850,1, This is a strong contribution
6851,3,\n\n- How useful is r-squared as a measure of performance in this setting?
6852,3," Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution."
6853,3," Here is a hook into that literature: Goldman, S., & Kerns. M. (1995)."
6854,3, But how can it describe the robustness?
6855,1,\n\nSGD comparison: \u201cfixed learning rate.
6856,3," One of the main distinctions is using filter-wise normalization, but it is somehow trivial."
6857,3,\n\nWhat is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)? 
6858,3, I\u2019m curious if this phenomenon still holds if authors use float64 in the experiments.
6859,1, The idea deserves a publication.
6860,3,"""The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator."
6861,3,\n\n- The extrapolation of the value function approximator can also contribute to why the limited horizon MC method can see beyond its horizon in a sparse reward setting.
6862,3,"\ng)\tClearly, with a large T (number of RW steps), the RW is not modeling just a single community. Is there a way to choose T?"
6863,1," The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM."
6864,3, Will a ResNet(32) show similar performance?
6865,1," Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet."
6866,3, A discussion on the difference would be helpful.
6867,3,The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution.
6868,3, What is driving those difference in results?
6869,1,"""The paper develops an interesting approach for solving multi-class classification with softmax loss."
6870,3," Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model."
6871,3," In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better."
6872,1,\n- This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.
6873,1,"\n\nOverall, I think this is a good paper and its core contribution is definitely valuable: it provides a novel analysis of an algorithmic task which sheds light on how and when the network fails to learn the algorithm, and in particular the role which initialization plays."
6874,1, Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. 
6875,1,"""The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks."
6876,3, This three networks need to be clearly described; ideally combined into one end-to-end training pipeline.
6877,3,"\n\nFigure 6a shows visualisations by different techniques and is evaluated \""by looking at it\""."
6878,3, Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization.
6879,3," To do so, they first motivate a new measure of gradient size, the Gradient Scale Coefficient which averages the singular values of the Jacobian and takes a ratio of different layers."
6880,3,\n\nReferencing is okay.
6881,3," Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution)."
6882,3, Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.
6883,1,\n\nPros: \n-Authors provide some empirical evidence for the benefits of using their technique.
6884,3," \n\nThe contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks."
6885,3,"""The paper presents a provable algorithm for controlling an unknown linear dynamical system (LDS)."
6886,3,\n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p.
6887,3,\n- What is the exact mechanism of generating a representation for NE EECS545?
6888,1,  This perspective leads to some interesting theoretical results and some new interpretation.
6889,2,The authors have not bothered to learn the first thing about the theories they are hoping to refute with ill-designed experiments and muddled rationale
6890,1," \n\nIt is nice that the authors use \""counterfactual regularization\""."
6891,3,"""In the centre loss, the centre is learned."
6892,1,"""Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models"
6893,3, The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story.
6894,3,... but I'm left wondering how/why?
6895,3,"""This paper propose a simple method for guarding trained models against adversarial attacks. "
6896,3," Moreover, the authors filter the \u201cnormal\u201d samples using those (p.7 top), which makes the entire exercise a possible circular argument."
6897,3,"\n\n6. When comparing training curves with LSTM, it might be helpful to also include the complexity comparison of each iteration."""
6898,3," They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \""context units\"" vector by a scalar."
6899,3, Just test it and report.
6900,2,The regression analysis is rubbish. Let's see what happens when you do this properly.
6901,3,\n\n\n4) Conclusion\nThe paper proposes a method for training RNN architectures to better model the future in its internal state supervised by another RNN modeling the future in reverse.
6902,3,"  Or is it fine to have an incomplete example set?\n\n"""
6903,3, \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive.
6904,3," \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines."
6905,3, Can you show some analysis of your model results that confirm/deny this hypothesis?
6906,3,\n\nMinor Comment:\nThis paper needs more proper proof-reading.
6907,3,Large successful evaluation section provided\n\t\u2022\t
6908,3," In particular, tie in the stepsizes tau and tau_theta here."
6909,3, Scalable Bayesian optimization using deep neural\nnetworks.
6910,3," Note that equation 4 is simply mentioning attention computation, not the proposed positional attention."""
6911,3,\n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental.
6912,3," After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding."
6913,3, The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback);
6914,1," \n\nThis paper offers a promising direction for language modeling research,"
6915,3,"\n\nExperiments on known datasets are interesting, but none of the results are competitive with current state-of-the-art results (SOTA), despite what is said in Appending D."
6916,3," As well, the considered attacks are in L2 norm, and the distortion is measured in L2, while the defenses measure distortion in L_\\infty (see detailed comments for the significance of this if considering white-box defenses)."
6917,3,"\n\nThe random matrix theory part of this paper is intriguing, but left me wondering \""and then what?"
6918,3,.\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks.
6919,3," How would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil, without relying on any computational simulation at all?"
6920,3,"The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients."
6921,1," The ideas are interesting,"
6922,3, Can the system improve further if speaker-adaptive features are used instead of log mels? 
6923,3,\n\nMinor Asks:\n(1) Clarification on how the error rates are defined.
6924,3,\n\nThe work of Konidaris et al [1] is a more appropriate reference for this work (rather than the Thomas reference provided).
6925,3," An eigenoption is defined as an optimal policy for a reward function defined by an eigenvector of the matrix of successor representation (SR), which is an occupancy measure induced here by a uniform policy."
6926,3,".In particular, the concave player plays the FTRL algorithm with standard L2 regularization term."
6927,2,"On my opinion, the approach the authors are using is trivial and the problem they are solving is made up."
6928,3, A network is trained to learn the transformations that minimize the Wasserstein distance between distributions.
6929,1," I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3."
6930,3, Note that the notion of dynamic expert is present in the SEARN paper too.
6931,3,\n\n2. The relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear.
6932,3,\n\n+ Clarity: \n- Yosinski et al. 2014 citation should be Nguyen et al. 2015 instead (wrong author order / year).
6933,3, How do you deal with words (or even the whole string) for which you have no word embedding?
6934,3, How do the authors propose to move past narrow definitions of factors of variation and handle more complex variables?
6935,3,". The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\\gamma}, which can be regarded as the inverse function of the original GAN G_{\\theta}, is trained to learn a map from the original input data space to the latent z-space."
6936,3, In terms of the model this is a relatively small extention of Raffel et al 2017.
6937,3,"""The authors present a method for learning word embeddings from related groups of data."
6938,3," If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected)."
6939,3,"""The authors test a CNN on images with color channels modified (such that the values of the three channels, after modification, are invariant to permutations)."
6940,3,"\n\n- I'd like a more explicit accounting of whether 0.00006 seconds vs\n  0.024 seconds is something we should care about in this kind of\n  work, when the steps can take minutes or hours on a GPU."
6941,3,  Perhaps another problem that has an explicit divide and conquer strategy could be used instead.
6942,3,"""The paper analyzes the the effect of increasing the batch size in stochastic gradient descent as an alternative to reducing the learning rate, while keeping the number of training epochs constant."
6943,1," \n\nIn summary, getting net2net to work for architecture search is interesting."
6944,3," The fact that multiplication often involves public weights is used to speed up computations, wherever appropriate."
6945,1," \n(-) The gains are reasonable,"
6946,3, The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles.
6947,1,"\n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive."
6948,1,\n\nThere are several things to like about this paper:\n- The problem of efficient exploration with deep RL is important and under-served by practical algorithms.
6949,3," Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals."
6950,3,"  While for the explicit gradient, it is assessed at the current point, and it is an unbiased one."
6951,1,"?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above."
6952,2,"The paper descends into nonsense, never to return, on line 44."
6953,3,\n\n\nPOST REVISION COMMENTS:\n\n- I didn't reread the whole thing -  just used the diff tool. 
6954,3," In the experiments, it is strange that this method can also achieve comparable or better results."
6955,1,"\n\nThe paper is well written, and easy to follow."
6956,3,"""The authors propose a mechanism for learning task-specific region embeddings for use in text classification."
6957,3,\n\nWhat is C in eq. 9?
6958,3,"\n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated."
6959,2,"The Gettysburg Address was only 272 words, and was one of the most powerful speeches in history. This paper, on the other hand, is over 8,000 words and says absolutely nothing at all."
6960,3,"""\n\nThis paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating."
6961,1,"  The idea of using class label to adjust each weight\u2019s learning rate is interesting and somewhat novel,"
6962,3," What is missing from this picture is when a blind walk cannot reach such a state, e.g. when a narrow corridor is present in the environment."
6963,3,Many different evaluation measures defined to measure different properties of the project\n\t\u2022\t
6964,1, Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning.
6965,1,\n\nComments:\n\n-The weak convergence results are interesting.
6966,3,"""The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights."
6967,3," However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem."
6968,1,"""The authors provide a novel, interesting, and simple algorithm capable of training with limited memory."
6969,3," and  the results on datasets are not explained either (and pretty bad). A waste of my time."""
6970,1, but ReLUs network perform well in practice.
6971,3,\n+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of\nsemantic relatedness between the source and the target sets\n\nFew notes and questions
6972,3, This is not an accurate portrayal.
6973,3, I am also wondering why authors have not tried their method on at least one more task?
6974,3," For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap."
6975,3, The number of centroids learned from NATAC may not be good for k-means clustering.
6976,3,\nIt would be informative for the research community to see the relationship of training time (how long it takes in hours) versus how fast it learns (iterations taken to learn).
6977,3, The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.
6978,3,"n\n\nThe author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series."
6979,3,"\n\nFig 3: This figure studies \u201cthe first and last layers of each network\u201d. Is the last layer really the last linear layer, the one followed by a softmax?"
6980,1," The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset),;"
6981,3,. It might be only complex when the number of dependency layer is large.
6982,3,"  In particular, how does the variational posterior change as a result of the hierarchical prior? "
6983,3,"""This paper borrows the idea from dilated CNN and proposes a dilated convolution based module for fast reading comprehension, in order to deal with the processing of very long documents in many reading comprehension tasks. "
6984,3,"""This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images."
6985,3,"\nThe authors evaluate their methods on two tasks, program generation and molecule generation."
6986,1,\nThe key novel idea is to learn a pairwise similarity function using the examples from the known classes to apply to examples of unknown classes.
6987,3," \n\nThe solution proposed in this work is to have past activations decay exponentially, to reduce this problem."
6988,3," The decoder then maps these new data points into feature space, obtaining in this way the image feature representations that, along with the feature representation of the original (real) image will form the batch that will be used to train the one-shot classifier."
6989,2,The MS is like a group of old wise men sitting around mulling over the terrible state of the world
6990,2,All these data are given in the author's Ph.D thesis. Does this material need to be published?
6991,3,"   Heck, in practie ridge regularization will also do something similar for many function classes. "
6992,2,A few points fall into the category of So What' h/
6993,3, An alternative route is suggested in my detailed review.
6994,3," For example, Hazzan & Jakkola (2015) in \u201cSteps Toward Deep Kernel Methods from Infinite Neural Networks\u201d consider GP constructions with more than one hidden layer."
6995,3, It would be nice to discuss computational cost as well.
6996,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
6997,3,\n\nOriginality: I am not familiar enough with adversarial learning to assess the novelty of this approach.
6998,3, Is that correct?
6999,3,"""The paper proposes a large experimental analysis with the goal of evaluating the generalization capabilities of CNNs."
7000,3," Because of this unknown, I could not understand the experiment setup and data formatting!"
7001,3,  Basically how hand picked were the options?
7002,3,\n\nMinor\n- write in the description for table 1 what task the accuracies correspond.
7003,3, The motivation for this measure is that it is invariant to simple rescaling of layers that preserves the function.
7004,3, What are the confidence intervals? 
7005,3, --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort. 
7006,1, \n\nThey obtain significant quantitative improvements over the previous 2 papers in this domain (video prediction on tabletop with robotic arm and objects) on both type of metrics - image assessment and motion accuracy.
7007,3,"""The authors propose the use of a gamma prior as the distribution over \nthe latent representation space in GANs. "
7008,3,"\n\nThese strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained \non some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting\nmultiple times a large dataset), then they are used on a final target task with again few labeled data and large \nunlabeled samples but beloning to a different set of categories."
7009,3,"""This paper aims at better understanding the functional role of grid cells found in the entorhinal cortex by training an RNN to perform a navigation task."
7010,3,\n\nThe paper uses three manners of adversarial attacks and formulate saliency as the gradient that is particularly influential to the final classification output.
7011,3, \nA subgoal is defined as a linear transformation of the distance traveled by an agent during a transition.
7012,3," This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words."
7013,3, \n\n- Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications.
7014,3,"\n\n1) The paper emphasizes the \""fully-differentiable tree planning\"" aspect in contrast to VPN that back-propagates only through \""non-branching\"" trajectories during training."
7015,3, The MSE objective (optionally interpolated with a 2nd-order tensor) is optimized incrementally by SGD.
7016,1,"\n\nIn summary, I think the paper can be accepted for ICLR."
7017,3, \n8. The performance on One-Layer Subspace Network (with only the input features) could be added.
7018,2,An article like this is just a waste of peer-reviewing resources
7019,3,"  Out of curiosity, is it intended as an experiment to verify the need for better baselines? Or as a 'fair' training procedure?"
7020,3," The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch."
7021,3," To evaluate the first two, the authors use estimates (lower bounds of log p(x)) given from annealed importance sampling and the importance sampling based IWAE bound (the tighter of the two is used)."
7022,1," The method seems solid, and the writing is pretty good."
7023,3,"\n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n"""
7024,3," Ideally, this type of approach should allow not only to generate images from an observational distribution of labels (e.g. P(Moustache=1)), but also from unseen interventional distributions (e.g. P(Male=0 | do(Moustache =1))."
7025,3,"""The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that \""obfuscates\"" the document, i.e. changing words in the document."
7026,3, This will make the comparison much more clear.
7027,1, \n\nThe experimental results show the effectiveness of the approach.
7028,3,"""The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision."
7029,3, The experimental results show that the span based model performs better than the model which generates the answers.
7030,3,  The objective is ambitious an deserve attention.
7031,1,\n- Good prediction results
7032,1," I also like the \u201cShared Attention\u201d - it makes a lot of sense to say that if the \u201csemantic\u201d attention mechanism has picked a particular word, one should also attend to that word\u2019s head; it is not something I would have thought of on my own."
7033,3,"\n\nSecondly, the experimental analysis is not very extensive. "
7034,3," If desired, the results of timed comparisons can also be reported, but reporting just a timed comparison with an artificial limit of 1 second may mislead some readers into thinking that we are farther along than we actually are."
7035,3," However, they're sometimes to briefly described to be understood by most readers."
7036,2,Is this a joke?
7037,2,"Table 4 seems unnecessary given figure 8. Indeed, figure 8 also seems unnecessary

-R2 has been watching Tidying Up"
7038,3,"\""\nHere the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance."
7039,1,\n\nFinally the experimental results are okay
7040,3,"""In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies."
7041,1,". In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy. "
7042,3,"\n- Not clear what \""Ensure\"" means in the algorithm description."
7043,1,\n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field.
7044,3," By contrast, the DCD method in [Ref3] can achieve 0.54."
7045,1, The time and sample complexities of this approach are polynomial in all relevant parameters.
7046,3," Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?"
7047,2,Reject â€“ More holes than my grandads string vest!
7048,3,"  As such, it ought to be evaluated on other types of problems to demonstrate this generality."
7049,3,"\n\nThe authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs. "
7050,1, This is the main take-away from this paper.
7051,3,"""This paper considers the problem of Reinforcement Learning in time-limited domains."
7052,3," For example, it can be used for deduplication using A (eq 1) as the similarity matrix."
7053,3," Also, authors claim that their method consume less computation time than reinforcement learning."
7054,3,\n2.) Model generalization to data-sequences longer than the training set.
7055,3,\n\nClarifications:\n- See the above mentioned clarification issues in 'major weaknesses'. 
7056,3," some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26\u201330."
7057,1,  \nWhile the paper is very well-written
7058,3," Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion."
7059,3," Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency?"
7060,3,\n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen.
7061,3,"  The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways."
7062,3, -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins?
7063,3,\n\nGraph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter.
7064,1,. More evidence in the paper for why it would work on harder problems would be great.
7065,1,"\n \nIn light of the above, I have increased my score since I find this to be an interesting approach,"
7066,3,"  Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems."
7067,1,"\n\n# Novelty and Significance\n- The problem considered in this paper is new,"
7068,3,"""The paper consider a method for \""weight normalization\"" of layers of a neural network. "
7069,1,"  I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings."""
7070,1," But it\u2019s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR."""
7071,3,"\nI think this could explain the better experimental results\n\nIn the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper."
7072,3,"""The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM)."
7073,1, I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA.
7074,1,\n\n2. The proposed method is shown to outperform a few baselines empirically.
7075,3, (a) To retrieve the key (a vector) given the value (a string)  as the encoder input.
7076,3,"  One way to overcome this potential problem is to add projection matrices between layers that will do some mixing, but this will blow the number of parameters."
7077,3,"  However, since the task is *video* prediction, it seems more natural to show small video snippets rather than individual images, which would also evaluate temporal consistency."
7078,3,"\n\n=Minor Comments=\n* \""We also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optima\""\nGenerally, people add noise to the gradients, not the values of the parameters."
7079,1,\n\nThe experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy.
7080,3," However, when RL is applied to navigation problems it is tempting to evaluate the agent on unseen maps in order to assess weather the agent has learned a generic mapping & planning policy."
7081,1," Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection."
7082,3," After estimating the values of this parametrization, the authors formulate the problem of finding optimal control inputs as a large convex problem."
7083,3,\n- an agent trained on random maps does much worse on fixed random maps that an agent trained on the same maps its being evaluated on (figure 4)
7084,3, I think this would also shed some light as to why this approach alleviates the problem of mode collapse.
7085,3,"  But one could use a small value of K (say, K=5)."
7086,1,"\n\nOther than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible."
7087,2,"The title of the paper indicates that the study was an RCT, but was it?"
7088,3," Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n"""
7089,2,"...But it would take me a long time to try and figure this out, so I am approving publication  "
7090,3,"  Autonomous Agents and Multi-agent Systems (AAMAS), 2016"
7091,1,\n\n- a number of approaches developed for the basic idea
7092,2,This paper is about combining inflexible specifications in a flexible way.
7093,1," The interesting part of these experiments is that the noise is not stationary, and this is quite characteristic of real-world applications."
7094,1,"  The paper is also well written, a bit dense in places, but overall well organized and easy to follow."
7095,3,"\n4. Add computation time (wall-clock) for all the experiments, to see how it compares in practice (this could definitively weight in your favor, since you seems to have a nice CUDA implementation)."
7096,2,The paper does have some moderately interesting results. But...nothing took me by surprise or wonder
7097,3,"""The paper proves the weak convergence of the regularised OT problem to Kantorovich / Monge optimal transport problems."
7098,3,\n2.  SRM needs to gather training samples which are 100 accuracy curves for T-1 epochs.
7099,3,\ Can this be generalized to arbitrary PGM structures? 
7100,1," It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded."
7101,3," However, to me it seems that even in 15 game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it."
7102,3, I think even alphaGo uses some form of supervision.
7103,1,\n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.
7104,1,"""The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation."
7105,3," The proposed approach closely mirrors that of Nakayama and Nishida, 2017 in that image-aligned text is encouraged to map to similarly to the grounded image."
7106,3, \n(2) The overall objective in Section 5 is broken.
7107,1, The overall structure of the paper is appropriate.
7108,3, An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting.
7109,3, This is crucial to understand the advantages of the BC technique.
7110,3," \n\nThe main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift."
7111,1,  The authors claim that this limitation of the simulation is made valid by the ability of the low level controller to incorporate prior knowledge and perfectly limit these actions.
7112,1, So this paper could have real-world impact.
7113,3,\n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images.
7114,3,"  ideally, the discrete encoding be simply a discrete approximation of the continuous encoding."
7115,1, \n\n3 The authors conducted experiments on three benchmark datasets and show promising performance of CrescendoNet .
7116,1,\n\nPros:\n1. The paper is well written and easy to read.\n2.
7117,3," A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers."
7118,1,"  The paper is clearly written, and has useful ablation experiments."
7119,3," Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients."
7120,1,\n\nNegatives\n- While the exhaustive analysis is extremely useful
7121,3,"\"" This is true of any autoencoder."
7122,3,"\n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)"
7123,3,"""Summary:  The paper applies graph convolutions with deep neural networks to the problem of \""variable misuse\"" (putting the wrong variable name in a program statement) in graphs created deterministically from source code."
7124,3," By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks."
7125,3, There is also no analysis of what happens for adversarial examples for the detector.
7126,3," No, z_L is a vector of latent variables."
7127,1,"""\nI thank the authors for the thoughtful response and updated manuscript."
7128,1,  \n\nThe idea is nice.
7129,3, I do not see a clear novelty in the proposal.
7130,3, It would be more useful to predict links that are removed with an unknown bias.
7131,3," Then,  PACT is combined with quantizing the activations."
7132,2,I would drop the first clause in the paper title. It does nothing other than degrade the scientific integrity of the work
7133,3," In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization."
7134,3,"\n\nReferences:\n\n[1] Grosse, Roger, et al. \""Exploiting compositionality to explore a large space of model structures.\"" UAI (2012).\n[2] Duvenaud, David, et al. \""Structure discovery in nonparametric regression through compositional kernel search.\"" ICML (2013).\n"""
7135,1,\n\nThis contribution is not bad for an empirical paper.
7136,3,\n\n- the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite.
7137,3, I would think this should be less-than-or-equal
7138,1,"\n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design."
7139,3, How does SGD with a batch size of 1 compare to TR with the batch sizes of 512 (CIFAR10) or 1024 (STL10)?
7140,3, The claim is that the found input represents the non-linear transformations that the layer is invariant to
7141,3, What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated?
7142,1,
7143,3,"  Since the explanation algorithms investigated in this papers merely react to the interactions of the model to the input and thus are unsupervised processes in nature, the explanation methods merely show the model's reaction to the change."
7144,3,". The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters."
7145,3," I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions."
7146,3," This in itself is not even new, but the authors replace a linear\noutput layer with squared error (proposed in another, earlier paper) by a\nsoftmax layer with cross-entropy."
7147,3, \n\nComments:\n\n1. Why force the Lag. multipliers to 1 at the end of training? 
7148,3,"Further discussion would be helpful.\n"""
7149,3, The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations.
7150,3, In particular it targets\nGraph Convolutional Networks.
7151,3,"""This paper proposes a ranking-based similarity metric for distributional semantic models."
7152,3,"""The main idea of this paper is to replace the feedforward summation\ny = f(W*x + b)\nwhere x,y,b are vectors, W is a matrix\nby an integral\n\\y = f(\\int W \\x + \\b)\nwhere \\x,\\y,\\b are functions, and W is a kernel."
7153,3," Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?"
7154,1,. The latter would be more exciting.
7155,1, The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.
7156,3,"  In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \""context unit\"" of size d x K, where d is the embedding size and K the region size."
7157,2,"The text is marred by conceptual messiness, careless reasoning, sloppy phrasing and  inadequate acquaintance with the relevant literature"
7158,3, What is the total computation time of the different variants and baselines
7159,3," \n\nIn equation 2, please check the measure of the mixture."
7160,1,"\n* Perhaps counterintuitively, experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training."
7161,3,\n- Le and Zudema use pooling and this paper uses weighted sum.
7162,3,"It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer."
7163,1," But as the paper shows, it\u2019s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow."
7164,3,. The word pairs are further filtered and clustered to improve the representation
7165,2,Ive never read anything like it &amp; I do not mean it as a compliment
7166,3, What happens when it is set to 10^4 or 10^6?
7167,3, This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks.
7168,1,"""The approach solves an important problem as getting labelled data is hard."
7169,3,The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.
7170,1,"The best parameters found by Harmonica improve over the hand-tuned results for their \""base architecture\"" (ResNets).\"
7171,3,"  Regarding the latter methods: what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish, and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be made."
7172,1,\n\n(significance) This is a promising idea.
7173,3," As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent."
7174,3," I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans."
7175,1," \n\nOverall: I like the idea this paper proposes,"
7176,3,  How to generate the neighborhood in Neigh(\\hat{u}_i) on page 5?
7177,3,\n\nThe authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*.
7178,1,"""The idea of using MCMCP with GANs is well-motivated and well-presented"
7179,3,\n\nMinor edits: Page 1. 'significantly match and improve' => 'either match or improve'\n\nAdditional notes:
7180,3,"\n\nAdditionally, I do not think the current gridworld task adds anything to the experiments, I would rather actually see this on a more interesting linear function approximation on some other simple task like Mountain Car than a neural network on gridworld."
7181,3,"""This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches."
7182,1, The paper is easy to read and its experiments show the effectiveness of the method.
7183,1,"\n+++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository"
7184,3, A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN.
7185,3, The algorithm requires that every step is crystal clear.
7186,3,". Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n"""
7187,3," Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype."
7188,1,"""This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way."
7189,3,  Section 3 continues with a sensitivity analysis now considering performance and storage of the method.
7190,3,The authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pairs.
7191,3,"  Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization."
7192,1,  Substantial data set of 29m lines of code.
7193,3,"""This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor."
7194,1, Nevertheless the paper is quite well written.
7195,1,"\n\nStrengths:\n\n- Reasonable approach, quality is good"
7196,1,.\n\nI enjoyed reading this paper.
7197,1,\n\nThe paper is very clear and easy to follow.
7198,3," Especially the latter will be appreciated."""
7199,3,The idea is quite simple as pruning using the connection in the batch norm layer
7200,1," So, in order to have options that lead to more direct, \""purposeful\"" behaviour, the agent must first wander around in a random, purposeless, way, and hope that this will lead to a reasonable exploration of the state space."
7201,1,\n\nPros:\nThe paper is well written and easy to follow.
7202,1,"\n\nI vote accept."""
7203,3,"\""\n\nIn the conclusion, \""optimal for deterministic objective\"" should be \""deterministic objectives\"""""
7204,3, \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm.
7205,3," What would happen if the higher reward was +2, or more interestingly, if the game was extended to allow agents to fish medium-sized fish (+2), in addition to small and large fish."
7206,3,"    The implementation of SPL seems to hurt performance in some cases (slower convergence on the IMDB dataset), can you explain it?"
7207,3,"  In particular, the authors propose a new framework to \""sharpen\"" the posterior."
7208,3,"""The main contribution of this paper are:\n(a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion,"
7209,1,\n2. Cheap soft unitary constraint
7210,3, \n\n- I would be keen to see eigen options being used inside of the agent.
7211,1, This is a merit compared with existing protocols used in generation evaluation.
7212,3, Is it a snapshot of the Jul 14 one?
7213,3," By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space)."
7214,1," Overall, I really liked this paper."
7215,2,"Thus, there is nothing new in the manuscript that warrants publication in Science or most other journals"
7216,1,  \n\n\nThe paper presents an interesting idea and is fairly well written.
7217,3,"\n\n- in (2), the hat is missing over \\Psi\nin the definition of v_\\pi(s), r only depends on s'?"
7218,3,\n- It may be helpful to indicate the standard deviations of the experimental results.
7219,3," The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves."
7220,1, \n\nThe presentation of the paper is clear.
7221,3," If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches?"
7222,3,"\n- Sec 4.6: the authors refer to the \""order net\"" beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline."
7223,1,"\n\u2013 Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency. "
7224,2,I am concerned that so many trees died for this to be the result.
7225,3,\n\nMy rating of this paper would remain the same.
7226,3,  Why not include Bayesian linear regression and\n  Gaussian process regression as baselines?
7227,1,"\n+ The idea of using Tumblr data seems interesting, allowing to work with a large set of emotion categories, instead of considering just the binary task positive vs. negative."
7228,3," To fix this the paper proposes to learn a second GAN to learn the prior distributions of \""real latent code\"" of the first GAN."
7229,3,"""This paper proposes to learn vector representations of prepositions by learning them as tensor decompositions of a triple of a left word (maybe head), the preposition, and right word (maybe dependent). "
7230,3," I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is."
7231,3, The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix.
7232,3,\n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial.
7233,3, The authors suggest that such a model could help with disentangling factors of variation in data.
7234,3," The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere."
7235,3,"""MARS is suggested to combine multiple adversaries with different roles."
7236,3,"n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width). "
7237,3,"""Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention."
7238,3, The proposed approach captures several popular few-shot learning approaches as special cases.
7239,3," In particular, I have serious problems understanding:\n\n1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ?"
7240,3, This should not be a surprise.
7241,1,  I found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compelling.
7242,3," From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function."
7243,3,"\nAn algorithm that can work with off-policy data has also been developed by [Nachum, 2017] (in the form of a Bellman residual minimization algorithm, as opposed to this work which essentially uses a Fitted Q-Iteration algorithm as the critic)."
7244,3, \n\n4) The experimental section can be significantly improved.
7245,3, they propose to use a latent variable gan with one continuous encoding and one discrete encoding.
7246,1," There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties."
7247,3, Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG.
7248,3, \nFigure 3 and Figure 5 illustrate the brain maps generated for Collection 1952 with ICW-GAN and for collection 503 with ACD-GAN.
7249,3,"CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise"
7250,3,"""This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis."
7251,3, Please include your description of how your method can be extended to networks which do allow for skip connections.
7252,3," Also, Page. 3. What does \u201cpartial descriptions\u201d mean?"
7253,3," The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm."
7254,3,"\n\nAn interesting avenue would be if the subtask graphs were instead containing some level of uncertainty, or representing stochasticity, or anything that more traditional methods are unable to deal with efficiently, then I would see a justification for the use of neural networks."
7255,3,\n\n- In the last section authors mention the intent to do future work on atari and other env.
7256,3, The authors also highlight that their sample complexity depends only logarithmically on the time horizon T.
7257,1,"\n\nI overall like the paper's theoretical results,"
7258,1,\n\nPapers 's pros :\n- clarity\n- technical results
7259,3," However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway."
7260,1,"""This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network."
7261,3," \nDifferently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores."
7262,3, Morever this fine-tuning experiment needs more details: is it based only on parameter initialization\nor there are some fully frozen network layers?
7263,3, \n\nThey choose to analyze mazes because they have many nice statistical properties from percolation theory.
7264,3," \n\nContribution: \nAs discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units."
7265,3," \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c."
7266,3, The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules).
7267,1,"This provides an interesting and, as far as I can tell, novel view on MAML."
7268,2,For the sake of time I have listed only a few (thirteen!) of the most glaring errors
7269,3," \nMore recently, the noise-shaping hypothesis has been tested with physiological data:\nChklovskii, D. B., & Soudry, D. (2012)."
7270,1,"\n\nThe best-claimed method of the method, called \""Hybrid method\"" is also mentioned only in passing, and that too in a scratchy fashion (see end of subsec 4.3):"
7271,3," For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment).\"
7272,3,"Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation."
7273,1," As a result, a gradient descent algorithm converges to the unique solution."
7274,3,\n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\
7275,3," Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question."
7276,3, Some insights might be useful there.
7277,3,  \n \nShould provide a citation for DRQN
7278,3," On a related note, the authors only seem to report results from a single random seed (ie. deterministic architectures are trained exactly once)."
7279,3, The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets \u2013 this is called \u201csuper-convergence\u201d.
7280,3," Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop."
7281,3," The authors evaluate their method based on a mechanical turk survey, where humans are asked to judge the realism of the generated images; additionally, they propose to measure prediction quality by the distance between the manually annotated positions of objects within ground truth and predicted frames."
7282,3,"""The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments."
7283,3," Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization."
7284,3,\n- Experimental gains on small data sets
7285,3, \n\n- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V.
7286,3," \n[Ref2] Xavier Bresson, Thomas Laurent, David Uminsky, James von Brecht. Multiclass Total Variation Clustering."
7287,3, The authors may think about including a comment on that issue.
7288,3, But one concern that I have is regarding the evaluation metrics used for it.
7289,1," Even though LCW performs better than others in this circumstance,"
7290,3,.\n\n3) The authors argue that their method is preferable to graph kernels in terms of time complexity.
7291,3,"\n\nHave the authors considered training the net with small random perturbations added to the samples, to compare the \""vanilla\"" model to the more robust one, which has seen noisy samples, and compared explanations?"
7292,3,". First of all, it took me a very long time to figure out that the middle block of results corresponds to taking a pretrained CRF energy and amortizing inference by training an inference network."
7293,3,"""This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner."
7294,3, Is mini-RTS a deterministic environment? 
7295,3, It is hard to judge the significance of this extension.
7296,3,? What are do the shaded regions on plots indicate?
7297,3,"""The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language."
7298,3,"""This article tackles the extraction of sentiments at a fine-grained level."
7299,3,"  Here, this is replaced by an implicit regularizer."
7300,3, It is hard to understand sufficiently well what the formalism means without more insigh
7301,2,"There is too much detail provided which is not relevant to the paper, or any for that matter."
7302,2,"This piece offers nothing newâ€¦is poorly written, analytically weak and repeatedly inaccurate. h/"
7303,3,"""This paper considers a dichitomy between ML and RL based methods for sequence generation."
7304,3," But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling?"
7305,3, \nIn some domains this can be a much better approach and this is supported by experimentation.
7306,3,"\n\nRegarding the experimental setup, how are the hyper-parameters for the baseline tuned?"
7307,3," Without understanding this first result, it\u2019s difficult to decide to what extent the rest of the paper\u2019s results are to be believed."
7308,1, \n\nThe idea is original to the best of my knowledge and is presented clearly.
7309,1, An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown.
7310,3," I suspect the results will still be meaningful, but the appropriate analysis is essential to be able to interpret the human results."
7311,3,"""In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs."
7312,3,. Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ? 
7313,2,The figures are dishonest and not all that useful.
7314,3, Perhaps the authors can clear this up in the text after sec 4.3.
7315,3," Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope? "
7316,1, I found no technical\nerrors.
7317,3, the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context.
7318,3," While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture."
7319,3,"""This paper proposes \""spectral normalization\"" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function."
7320,3, Maybe they could be relegated to the appendix?
7321,3,"\n- In the section \""Open Problems in RAML\"", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q."
7322,3,  I find the authors\u2019 claim that FAME is performing superior global modeling interesting. 
7323,3,". What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output?"
7324,3," Why is the proposed approach better than the\npreviously published ones, and when is that there is an advantage in using it?"
7325,3,"\n\n(2) In Theorem 1.1, the notation is slightly unclear because B^T is only defined later."
7326,2,"Why dont you just send copies of this to the two people in the world who care about it, and forget the publication route?"
7327,2,Ive never read anything like it &amp; I do not mean it as a compliment
7328,2,I just dont get the point of this
7329,3, The idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learning.
7330,3,"""The paper proposes a set of benchmarks for molecular design, and compares different deep models against them. "
7331,3,"\n- In the experiments, why are the input images downsampled to 320x320?"
7332,1," though it involves large amount of experimental efforts, which could be impactful."
7333,3," \n\nThe authors claim that \""Through our analysis and experiments, we note that the implicit ensemble behavior of CrescendoNet leads to high performance\""."
7334,1," \n\nThe paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field."
7335,3, An autoencoder objective is used to produce meaningful tuples.
7336,1,\n\nThe paper is fairly clear and these extensions are reasonable
7337,1,\n* the paper is clearly written and easy to understand
7338,1,\n\nThis paper is easy to understand
7339,3,  Then f_v is one of the centroids (named VCs).
7340,2,The lack of theory is painful at times
7341,3," In this sense, the paper would really produce a more significant contribution is the authors can include some ideas about the ingredients of a RNN model, a variant of it, or a different type of model, must have to learn the compositional representation suggested by the authors, that I agree present convenient generalization capabilities."
7342,3,"""Summary:\n\nThis paper proposes an adversarial learning framework for machine comprehension task."
7343,3,"  I think what the authors intend to note here is that the W parameters are independent of this, or as expounded upon in the concrete example that follows, that context words and weights W both are."
7344,2,This paper is so devoid of content it should be rejected outright.
7345,1,"\n\nOverall, pretty interesting result and solid contribution."
7346,1, It could lead to new insight on automating design of neural networks for given problems.
7347,3, It is indeed a good method for dealing with the variance.
7348,3,\n\nCan the proposed approach help when we have recurrent value functions?
7349,3,  In some sense are we are defining a model for the action taken by the expert?
7350,3, Is it purely to improve likelihood of the fitted model (see my questions on the experiments in the next section)?
7351,3, If I'm not mistaken K=1 is regular ELBO and not IWAE/AESMC?
7352,1,\n- This paper spends a lot of time re-deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets.
7353,3, Is it a PCFG?
7354,1, \n\n\n\nPros - \n* This paper attempts to add the understanding of neural nets instead of only contributing better error rates on benchmark datasets.
7355,3,. Why is it important to separate class from style?
7356,3,"  There is a footnote on page 7 regarding Bayesian posterior sampling -- I think this should be brought into the body of the paper, and explained in more detail."
7357,3," For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations."
7358,3,\n\n- The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there.
7359,3,  How does your implementation compare to the original work?
7360,3,\n- It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect.
7361,3,"\n\nQuality:\n\nI am very concern about the authors stating on page 1 \""we sample from the\nposterior on the set of Q-functions\""."
7362,3," I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required."
7363,3," Although there are some (minor) differences between the implementation with Mirowski et al. 2016,"
7364,1,"\n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases."
7365,1," Using this system, the authors are able to harness much more compute to learn very high quality policies in little time."
7366,3," The proposal is to replace the non-linearity in half of the units in each layer with its \""bipolar\"" version -- one that is obtained by flipping the function on both axes."
7367,3,"\n\nI believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al."
7368,3,"\n\nMore generally, putting the biological arguments aside, why would a 2D neighborhood relationship be helpful?"
7369,3,"""Summary: \n\nThe authors consider the use of attention for sensor, or channel, selection."
7370,3,"I recommend comparing the Jacobian w.r.t the phi(s) and tau(s) inputs to the Path function using saliency maps [1, 2]; alternatively, evaluating final policies with out of date input states s to phi, and the correct tau(s) inputs to Path function should degrade performance severely if it playing the role assumed."
7371,3,Can this be explained by comparing the variational free-energy.
7372,2,It was agonizing for this reviewer to read a total of nine pages describing the overall methods.
7373,3, Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright.
7374,2,This is a paper struggling not to die
7375,3,\n\n1. System identification.\nSubspace identification (N4SID) won't take exponential time.
7376,3," Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field."
7377,3, Are these the results after all agents had converged?
7378,3," These are still the main results of the paper."""
7379,3,What is the variance of the mini batch estimate?
7380,3,\n\n- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta.
7381,1, \n\nPros:\ni) Detailed review of the existing work and comparison with the proposed work.
7382,1,\n\nPros:\n-- Efficient model
7383,2,I dont know what the heck the red highlighted text is supposed to mean and left it as is.
7384,3," \n\nNotice the following suspiciously similar paragraphs from the two papers:\n\nFrom \""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\"":\nWe also look at an environment where strategies must be learned from raw pixels. "
7385,3,"  Is syntax checking used at each step of token generation, or something along these lines?"
7386,3, Is it clear that this step in needed? 
7387,3, the motivation behind the semi-supervised and active learning setup could use some elaboration.
7388,3,"  Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings."
7389,3,\n - an experimental evaluation of the methods on the cifar 10 dataset
7390,3, But all the other components seem to have been demonstrated previously in other papers.
7391,3,"\n2. For the same accuracy, coupled ensembling yields significant parameter savings."
7392,3,"""This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples."
7393,3, They notably show\nhow physical priors on a given phenomenon can be incorporated in the learning process and propose \nan application on the problem of estimating sea surface temperature directly from a given \ncollection of satellite images.
7394,3," This could have been done via empirical work, for instance:\n- Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1."
7395,1,"""This paper proposes an interesting  approach to prune a deep model from a computational point of view. "
7396,3," I think the authors should spend time on better motivating the choice of invariance used, as well as on testing with different (potentially new) architectures, color change cases, and datasets."
7397,1,"\n\nPros:\n1. Well-written paper, with clear contributions."
7398,1,\n\n- Numerical study shows some promise of the proposed method.
7399,3, This might be a relevant comparison to add to establish more clearly that it is the implicit step that yields the improvement.
7400,3," On the other hand, in this semantic space, by construction, we are guaranteed that similar concepts lie near by each other."
7401,3,\nThe resultant policy is a function of the expert traces on which it depends.
7402,3, In this work we propose a simple method to extend these benefits to the context of deep networks without skip-connections.
7403,1,"\n- The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up."
7404,1,"\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description."
7405,1," Using features sur as time-series (TS), Architecture Parameters (AP) and Hyperparameters (HP) is appropriate, and the study of the effect of these features on the performance has some value."
7406,3," but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.\n"""
7407,1, \n\n\nComment: I kinda like the idea and welcome this line of research.
7408,3, It will be interesting to compare with some existing second-order optimization algorithms for deep learning.
7409,3," The authors provide a possible mechanism to explain these results, by analyzing classification performance as a function of baseline firing rate."
7410,3, They also examine the effects of the temperature and step size of the perturbation.
7411,3," The authors should provide more competing algorithms in batch mode active learning."""
7412,3,"""This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions."
7413,1," It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework."""
7414,3,"Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? "
7415,3,"As a result, I am wondering whether this separated question embedding really brings new information, or the similar improvement can be achieved by increasing the size of LSTM layers."
7416,3,". The paper postulates that learning should happen on shallower networks first, then on a deeper network that uses the GAN cost function and regularizing discrepancy between the deeper and the small network. "
7417,3,"""The main contribution of the paper is a method that provides visual explanations of classification decisions."
7418,3,  The proposed method is evaluated on a small Markov chain and a simulated Baxter robot.
7419,3,"\n\n[Ref1] Zhirong Yang, Tele Hao, Onur Dikmen, Xi Chen, Erkki Oja. Clustering by Nonnegative Matrix Factorization Using Graph Random Walk.In NIPS 2012."
7420,3, This probably leads to more parameters in the convolutions.
7421,3, The proposed filter relaxes this requirement by forming the weights as the output of a two-layer perception.
7422,3,"\n\n[1] Wang, Weiran, Honglak Lee, and Karen Livescu. \""Deep variational canonical correlation analysis.\"" arXiv preprint arXiv:1610.03454 (2016)."
7423,1," \n\nThe network architecture is suitably described and seems reasonable to learn simultaneously similar games, which are visually distinct."
7424,3," In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?"""
7425,3,"  What if we just selected the one past domain that was most similar to the new domain, by some measure?"
7426,3,"\n\n- In light of the time series aspect being the main contribution, a\n  really obvious question is: what does it learn about the time\n  series?"
7427,1,"\nIn addition, the authors collected the largest sketch dataset, I know of."
7428,1,n\nThe paper is well written and easily to follow.
7429,3, A feature of previous optimization based methods is that a user may specify the amount of perturbation (epsilon).
7430,2,"I read it again, this time squashed between two large people on the delayed flight home, and still enjoyed reading it"
7431,1,"""Clarity \nThe paper is well-written and clear."
7432,3,"""The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,"
7433,3, But the future work on bandits is already happening:
7434,3,"""The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems."
7435,2,"Though less enthused about manuscripts novelty, this reviewer does admire the hard work of your group."
7436,1,"\n\n- In the perceptual evaluation procedure, the \u201c1 second\u201d restriction is artificial and makes the evaluated methods appear better than they are."
7437,3,"\n8. I would like to see some experimental verification that the proposed method significantly reduces the dropout gradient variance (page 6), if the authors claim that tuning dropout probabilities is an area they succeed where others don't."
7438,1,"\n\nThe experiments are very convincing, both numerically and visually."
7439,3, They also show experimentally that penalized gradients stabilize the learning process.
7440,3,\n6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful.
7441,3, Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally.
7442,3,"""This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se)."
7443,3,"\n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition. "
7444,1,n\nIn summary the algorithms are novel variants of SGD
7445,1,"\n\nIt is interesting how strong the denoising effect is, as simply a byproduct of the adversarial regularization."
7446,3, It appears sometimes the entropy loss (which is not the main contribution of the paper) is essential to improve performance; this obscures the main contribution.
7447,3,". To establish such a behavior, the authors propose early stopping as well as other heuristics."
7448,1," A note on the QA results: The QA results are certainly good enough to be in the range of \""good systems\"","
7449,3,. Can the authors provide better examples here?
7450,1,  \n\n\n\nPros:\n\nImportant and challenging topic to analyze and any progress on unsupervised learning is interesting.
7451,3," The \""super-convergence\"" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models."
7452,3,"   However, we may find policy pi_1 that makes A true and B false (in general, there is no single optimal policy) and find pi_2 that makes A false and B false, and it will not be possible to satisfy the phi_1 and phi_2 by switching between the policies."
7453,3,"""\nThis paper revisits an interesting and important trick to automatically adapt the stepsize."
7454,3,  The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.
7455,3, Does each component is related to a certain topic?
7456,3,"\n5. General: How does your method compares with other factorization approaches, such as in \""Factorization Tricks for LSTM Networks\""?"
7457,3,  \n\nI believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed.
7458,3," Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures."
7459,1, I enjoyed the investigation of the effect of L_2 regularization on qualitative optimization behavior.
7460,3,\n- the ability of the model generalizing to composition of primitive commands seem to depend heavily on the whether the action is seen during training.
7461,2,"I'm not inclined to suggest acceptance because I haven't enough elements to do so.

Significance: 7/10
Soundness: 8/10
Presentation: 9/1"
7462,1, although it outperforms GAR on 7 out of 13 categories;
7463,3, Thus now the classification is based on the Mahalanobis distance: (z-c)^T S_c (z-c).
7464,3," Since there are multiple customer and taxi agents, there is a multi-agent coordination problem."
7465,3," This should be explicitly said and the parameter value should be stated.\nPage 4: \n-\tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers."
7466,3," They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%)."
7467,3, Using data generated by a pre-trained network is usually not representative of what will happen in real life.
7468,3,"\n(2). In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases. Could you still say that your generated \u201cadversaries\u201d leading to the wrong decision from LeNet? "
7469,3, Or at least not in an intuitive way.
7470,3,"""This paper thoroughly analyzes an algorithmic task (determining if two points in a maze are connected, which requires BFS to solve) by constructing an explicit ConvNet solution and analytically deriving properties of the loss surface around this analytical solution."
7471,1,"""The paper is well written and clear"
7472,3," To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties. "
7473,3," \n\nBoth in terms of general visual classification (only MNIST is used, while it would be nice to see results on CIFAR and/or ImageNet as in Bendale&Boult 2016), as in exploration of different scenarios (different number of unseen classes, different samplings) and ablation of the method (independent training, using OCN for hierarchical clustering, influence of Auto Encoder)."
7474,3, Numerical experimental results are also presented to verify the theory.
7475,1," \n- The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper."
7476,3,   From (a) to (b) the red points move closer to the center while in (c) they move further away (why?).
7477,3," In addition, what is done at test-time if the sequence length differs from the sequence length at training time?"
7478,3," Do you also not train the recurrent matrix in the other models (RNN, LSTM,...)?"
7479,3," I would say that it is \""similarly decoded to $\\mathbf{c}$\"", since it is \\mathbf{c} that gets decoded."
7480,3, A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.
7481,3,\n\n- \\phi in Fig 2 should be explained by the caption.
7482,3,"What's the point of designing a network without skip connections?\n"""
7483,3,\n- Is log-likelihood a good loss here?
7484,1," Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start. "
7485,1,  The paper is also well written.
7486,3,"\n\np 1: authors write: \""Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989)."
7487,3, Similar comparison should be done with off-policy fitting in Q-Prop.
7488,1," The weight matrix is maintained normalized, which helps accuracy."
7489,3,"""This paper proposes a new character encoding scheme for use with character-convolutional language models."
7490,3,\n\n\nPros:\n- The model achieves SOTA on SQuAD among published papers.
7491,3, How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)?
7492,1,"""This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. "
7493,3, Such comparison has been carried out both in theory and practice for simple low dimensional environments with linear (and RKHS) value function approximation showing how TD methods can have much better sample complexity and overall performance compared to pure MC methods.
7494,3,There is only one mostly minor issues with the algorithm development and the experiments need to be more polished. 
7495,3," The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion)."
7496,1," The proposed meanChar architecture doesn\u2019t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn\u2019t extremely satisfying."
7497,2,"...to show their similarities or differences, as well as, advantages or disadvantages by your own letters.."
7498,2,The authors offer no credible theoretical reason why we should care about this result.
7499,3," Namely, the following questions need to be answered:\n\n1. Does using linked-word-pairs truly raise the state of the art?"
7500,3,There is a literature which follows on from the F&J paper.
7501,3, Some theoretical guarantees for the efficiency of reservoir sampling are provided.
7502,3," I would suggest highlighting equations 2 and 4 in some way (for example, proposition/lemma + proof), so that the casual reader can quickly see what the main results are."
7503,1," As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better."
7504,3,"""The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR."
7505,3," It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change."
7506,3," The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier). "
7507,2,I would suggest the authors to have some native English speaking to go through it
7508,3,"   In Hazan's paper they mention that the system id portion, at least, seems to work with non-symmetric, and even non-linear dynamical systems (bottom of page 3, Hazan 2017)."
7509,2,I have rarely read a more blown-up and annoying paper in the last couple of years than this hot-air balloon manuscript
7510,2,The proposal is largely descriptive and mostly a fishing expedition. It will be great if they catch some interesting or unexpected fish
7511,1,\n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs.
7512,3,"\n \n \nMajor\n1. What is the model/baseline in Tables 3, 4, 5?"
7513,3,"""After reading the rebuttal:\n\nThe authors addressed some of my theoretical questions."
7514,1,"Minimizing a quadratic f(x) = .5||x||^2 will converge in one step with a step-size of 1 because this is well-conditioned; on the flip side, slow convergence comes from lack of strong convexity, or with strong convexity, ill-conditioning of the Hessian (like a stiff ODE)."
7515,1," The empirical evaluations, analysis and comparisons to existing methods are well executed."
7516,1," It has been improved since, through different sets of experiments and apparently a clearer presentation,"
7517,3,"""The submission describes an empirical study regarding the training performance\nof GANs; more specifically, it aims to present empirical evidence that the\ntheory of divergence minimization is more a tool to understand the outcome of\ntraining (i.e. Nash equillibrium) than a necessary condition to be enforce\nduring training itself"
7518,3," \nCombining self-organising with classification.[[CNT], [null], [DIS], [GEN]] \nComparing learned representations from different models."
7519,3,"\nIt's also common practice to analyze the representations learned, in\nmany deep learning papers."
7520,3," How does this method perform for more realistic data, even e.g. MNIST ?"
7521,2,The author should abandon the premise that his work can be considered research
7522,3," Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature."
7523,3,"\n- please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017) \nshould be\nGraph Neural Network (Nowak et al. (2017))"
7524,3," For image classification, the performance of proposed method is below its predecessor Xception network."
7525,1, The model achieves good results on bAbI compared to memory networks and the relation network model.
7526,3, Experimental evaluation considers a dataset of 200k images of a teapot with varying pose and color.
7527,1,\n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new.
7528,3," \n\nThe proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process."
7529,3,"\n\n- Overall, the only contribution of the paper seems to be the modification to Ba et al. is the Eq. (8)."
7530,3,"  The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?"
7531,1,"  The linear models do very well, which means it should be\n  possible to look at the magnitude of the weights."
7532,3,"""The authors consider a Neural Network where the neurons are treated as rational agents."
7533,3," We'd like the number of bits parameter to trade off between accuracy (at least in terms of quantization error, and ideally overall loss as well) and precision."
7534,3,"If I understand correctly, Stachenfeld et al. discussed this result, but didn't prove it."
7535,3,I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)?
7536,3,"  Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN."""
7537,3,"\n\n2. Indeed, AlexNet is a good seedbed to test binary methods."
7538,1, The results seem to show that a delayed application of the regularization parameter leads to improved classification performance.
7539,3, It could have been interesting to know if there are more insights / lessons learned in this process.
7540,3," \n\nThis paper's contribution are quite moderate, as the proposed method seems to be a very natural extension but it is backed up by lots of numerical results. "
7541,3," At some level, I question whether the proposed framework is doing any more than just value function propagation at a task level, and these experiments would help resolve this."
7542,3, \n\n1. Returns are defined from an initial distribution that is stationary for the policy.
7543,3," Accordingly, my main point is the following: the model is indeed learning the task, as measured by performance on training set, so authors are only showing that the solution selected by the RNN does not follow the one that seems to be used by humans."
7544,3," In practice the class means are \""learned\"", yet regularised towards the batch class means."
7545,1, The paper is interesting and easy to follow.
7546,1,"\n* The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture."
7547,3, One issue is that basic baselines could more clearly illustrate what is going on.
7548,3,"""The paper proposes a method for identifying representative examples for program\nsynthesis to increase the scalability of existing constraint programming\nsolutions."
7549,3," It is easy to train since its low resolution, but also means a lot since it a relative complicated scene."
7550,3,"  \n\nFirst, please systematically compare your methods with existing methods on the widely adopted benchmarks including MNIST with 20, 100 labels and SVHN with 500, 1000 labels and CIFAR10 with 4000 labels."
7551,3,"""[Overview]\nIn this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user's questions as well."
7552,3,"\n\nYou say that \""First, path-wise training procedure significantly reduces the memory requirements for convolutional layers, which constitutes the major memory cost for training CNNs."
7553,3,"  how the \""good emulation property\"" is exactly measured ?)."
7554,3,"\n\nThe text refers to a Figure 3 which does not exist, probably means Figure 2."
7555,3,   In other words samples in the same category will have the same weight \n\nError bound is derived.
7556,3,  I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.
7557,3, \n\nUsing the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting.
7558,3,Can you provide more details about how you run the data assimilation model in the experiments? Did you use your own code?
7559,3, It would be helpful if the authors could comment on the dependence between T and L.
7560,1,"\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed."
7561,3, As a\nconsequence results on decoding learning task using low resolution\nimages can end up worse than with the actual data (as pointed out).
7562,3," The motivation behind all this is to learn the input features to the SVM as opposed to hand-crafting them, and use the generalization ability of the SVM to do well on tasks which have only a handful of training examples."
7563,3, Could you explain how classes are predicted given a test problem?
7564,3,"\n\n[1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017.\n[2] Tri Dao, Christopher De Sa, Christopher R\u00e9. Gaussian Quadrature for Kernel Features. NIPS 2017"""
7565,3, \n\n\nFigure 4:\nB) What does it mean to feed two vectors into a Tanh?
7566,3,"\n\nSecond, from a purely methodological point of view, STANs boil down to learn the optimal linear combination of the input sensors."
7567,3, How about the performance and pairwise KL divergence?
7568,3," but there is a real lack of theoretical guarantees motivating the penalty form chosen, such a theoretical development could allow to understand what may rule the choice of an ideal value for lambda in particular."""
7569,1, The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising.
7570,3," \n\nThe main idea in the paper is that RNNs applied to Seq2Seq case are learning a representation based only on \""memorizing\"" a mixture of constructions that have been observed during training, therefore, they can not show the compositional learning abilities exhibit by humans (that authors refer as systematic compositionality)."
7571,3," Authors are suggested to discuss in more detail.\n"""
7572,1," This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings."
7573,3, Tuning the architecture of the single multi-layer NN adversary might be as good?
7574,3,The DReLU is supposed to remedy the problem of covariate shift better.
7575,1,\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks.
7576,3," \n\n[1] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom."
7577,3, What is the intuition behind the random walks and graphs of Fig 6?
7578,1," In addition to examining the effectiveness, authors also performed experiments to explain why OPTMARGIN is superior."
7579,3,"\n(b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses,"
7580,3,"  In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients."
7581,3,"\n\n\nNote: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard."
7582,3,"n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments)"
7583,3, This new layer is referred to as \u2018pixel deconvolution layer\u2019 and it is demonstrated on two tasks of semantic segmentation and face generation.
7584,3,"\n\nReferences:\nHenaff, Mikael, Arthur Szlam, and Yann LeCun."
7585,3," In contrary, in the proposed approach, the auto-encoder model (with 10 hidden layers) is learned using 50 training samples in AwA, and 200 images of birds (or am I missing something?)."
7586,1, This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting.
7587,3," From what I understood from the paper, the only technical contribution is the use of a so called 2-pass decomposition, which is simply an implementation of projected gradient descent on the set of rank-R tensors."
7588,1,  This makes the high level learning strategy more efficient because it does not have to explore these possibilities (Q-masking).
7589,1," The proposed technique has some limitations,"
7590,1,"   \nThe idea is nice and simple,"
7591,3," but instead uses a single synthetic data point in dimension 5, and k=1."
7592,3,"  The model is simple: tensor factorization, where the covariate can be viewed as warping the cosine distance to favor that covariate's more commonly cooccuring vocabulary (e.g. trump on hillary and crooked)"
7593,3, Please clarify.\n2) It is not clear to me how the model learns to generate specific OOV variables.
7594,3," Thus, they are essentially learning the skip connections while using a human-selected model."
7595,2,"Generally, some extra clarification might be nice to improve clarityâ€¦"
7596,1,"\n\n* Using raw waveforms as audio modality is very interesting,"
7597,3,\n\n- The mechanism in 2.2.4 feels a little like\n  http://aclweb.org/anthology/D17-1015
7598,2,"Currently, the impression is that this was simply another piece of research/ consultation."
7599,3," However, it only identifies hidden units that are important for\na class, not what are important for any particular input."
7600,3," If it is the local quadratic approximation, how is it correlated to the original function?"
7601,1,"A well-written manuscript,;"
7602,3, It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S.
7603,1,   \n\nPros:\n+ The paper is written clearly and easy to read
7604,3,"\nAuthors use the dataset provided for Starcraft: Brood war by Lin et al, 2017."
7605,3,"  The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings."
7606,3, The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines.
7607,1, Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods.
7608,1," The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation."
7609,3,"""The principal problem that the paper addresses is how to integrate error-backpropagation learning in a network of spiking neurons that use a form of sigma-delta coding."
7610,3,"This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803)."
7611,1,"  The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality."
7612,3, \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks.
7613,3," However, I am not familiar enough with the literature on Bayesian evidence, or the literature on sharp/broad minima, and their generalization properties, to be able to confidently say how original this work is."
7614,3," The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%)."
7615,3,"\n\nSince these earlier results existed, and approximation-amortization decomposition is fairly simple (although important!), the main contributions of this paper are the empirical studies."
7616,3, The data are just targets\nobtained by the q-learning rule.
7617,1,"""Pros: \nThe paper proposes a \u201cbi-directional block self-attention network (Bi-BloSAN)\u201d for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient."
7618,2,It is a collection of good ideas that do not add up to or advance to any meaningful conclusion
7619,2,I think the audience will eat him alive. But I want to be there to hear it.
7620,3," They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet."
7621,3,"\n\n- Similarly, for Algo 2, add references."
7622,3,It seems that much less would be sufficient from figure 4?
7623,2,There is no need to test these hypotheses. They have been tested a long time ago. It is in all...
7624,3,\n\n2. Develop a similar relaxation for deep neural networks.\nThe author already mentioned that they are pursuing this direction.
7625,3, This is a direct result from definition of conditional probability.
7626,2,This theoretical framework is dark and depressing. But it fits your study well
7627,3," Likewise analyzing results for varying number of classes (e.g. 100 classes with Cifar-100, 1000 classes with ImageNet), with imbalanced datasets is required."
7628,3," \n\nIn Section 4.2, you need to refer to Table 2 in the text."
7629,1,\n\n2. Non attribute part (Z) is explicitly modeled in the framework.
7630,3,"""This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE."
7631,3,   The authors need to address the following:  \n\nFirst (very important): You need to show experimentally how your method compares against regular neural nets (with specific fixed forms for their activation functions such relus etc).
7632,3,"""The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive."
7633,3,  What hyper-parameters are used.
7634,1,"  For the application of images, using text description to refine the representation is a natural and important research question."
7635,3," Also, the results section seems to switch off between calling the method CCN and LCO interchangeably."
7636,3," I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner."
7637,1," \n\nNevertheless, while the idea behind the proposed approach is definitely interesting,"
7638,3,  \n2)\tFrom modifying learning rates to weighting samples.
7639,1," In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile."
7640,3, It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best.
7641,3, A different question is whether that regime is actually useful.
7642,2,This is depressing! So much work with so little science -..
7643,3, Do the authors have any intuition why?
7644,3," Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched."
7645,1," On the other hand, although the experiments are well designed and illustrative,"
7646,3," Which is the real issue, noise or curvature?"
7647,3," \nAdding a small delta to the sample x shifts its position in data space, completely altering the prediction rule applied by the model due to the change in proximity to another section of the decision hyperplane."
7648,2,Why exactly this task? I can think of a zillion other cognitive tasks
7649,3,"""The authors show how techniques typically applied to real-valued networks (e.g. with real-valued inputs and parameters) can be straighforwardly generalized to complex-valued networks (e.g. with complex-valued inputs and parameters)."
7650,3,"\nThe authors provide some detail about the actual implementation of their model, section 4, but the in depth details required at ICLR are missing."
7651,1,"  \n\nPros:\n- The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights."
7652,1,"\n\nOverall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered."
7653,3,"""I am not sure how to interpret this paper."
7654,3,"\n  of SMAC, BO, and other approaches that are trying to model the map\n  from these choices to out-of-sample performance."
7655,3,"\n\nand citations within, plus quick googling for more recent works."
7656,3, and \n(2) Extract the penultimate layer output as features to train a conventional classifier such as SVM.
7657,3,\nWhat is exactly the communication protocol?
7658,3,\n\n* The caption for Fig 5 should explain what each of the sub figures is.
7659,3,"\nI think estimating the operator from the input to the output is interesting, instead of constructing (A, B, C, D) matrices, but this idea and all the techniques are from Hazan et. el., 2017."
7660,3," Thus, in the PPD the only (jointly) winning\nmove is not to play, but selfish agents are again tempted to defect and try to score points even though\nthis decreases total social reward."
7661,3, This is an example of a constraint which cannot be expressed by masking actions and in fact requires breaking the top speed limit for a bit in order to be safer in the longer term.
7662,1,\n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al).
7663,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
7664,3, A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods.
7665,3,"  Therefore, we address the following questions.[[CNT], [null], [DIS], [GEN]]  Will reinforcement learning work even if we consider each unit as an autonomous agent \u201d\nIs there any citation for the claim \u201cFrom a micro perspective, the abstraction capability of each unit contribute to the return of the entire system\u201d ? "
7666,3, \n How is it implemented in practice? 
7667,1,"\n\nAs the main idea and the proposed model is simple and intuitive, the evaluation is quite important for this paper to be convincing."
7668,3,\n\n5) bottom of p.4: use hat{L}_gamma = 1 instead of L_gamma =1 for more clarity.
7669,2,I want to vomit; I cant believe this paper was submitted.
7670,1, The experimental results are very good and give strong support for the proposed normalization.
7671,3," They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach."
7672,1,"""This paper presents a novel method for spike based learning that aims at reducing the needed computation during learning and testing when classifying temporal redundant data."
7673,3, This seems to suggest that the only thing that the master can communicate is action information?
7674,3,\n\nFigure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case? 
7675,3,\n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al.
7676,3," Here I list some of my questions:\n+About the MBEM algorithm, it\u2019s better to make clear the difference between MBEM and a standard EM. Will it always converge? What\u2019s its objective?"
7677,3,"""Summary of paper:\n\nThe authors present a novel attack for generating adversarial examples, deemed OptMargin, in which the authors attack an ensemble of classifiers created by classifying at random L2 small perturbations."
7678,3," This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue)."
7679,3,\n\n1. Why is this model successful?
7680,2,It seems like you are torturing the data until the model converges.
7681,1, A mixture of experts layer further improves performance.
7682,2,"Overall, I think this manuscript is a waste of time."
7683,3,\n2. Can you introduce the Risk -R in the paper before using it in Theorem 4.1\n3
7684,3,  The proposed method is theoretically analyzed and experimentally tested.
7685,3, The model is inspired by current knowledge about long term memory consolidation mechanisms in humans.
7686,3, Theorem 5 follows easily from this.
7687,1, The paper is clearly written and easy to understand
7688,3, It uses the notion of a locally open map to draw equivalences between local optima and global optima.
7689,2,"An exercise in feature manipulation, of the brainless kind"
7690,3," The authors formulate the learning problem as a minimax problem which tries to choose diverse example and \""hard\"" examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function."
7691,3,"""This paper proposes a new method, called VCL, for continual learning."
7692,1, \n\nPros: \n+ The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel.
7693,1, The paper is written clearly and is easy to follow.
7694,3," By \""circumscribed in two image domains\""?"
7695,3, The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix.
7696,3, Were these\n   viewpoints chosen randomly?
7697,3,\n4. The authors can indicate the application scenario of this work.
7698,3, Movielens comes to mind.
7699,3,I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly.
7700,3,"""This paper addresses the problem of one class classification."
7701,3,\n* It would be interesting to see a version of figure 5 where the horizontal axis is the number of epochs.
7702,3, The update steps describe gradient ascent.
7703,3, How do you estimate the diffusion parameter D?
7704,3, This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.
7705,1," The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement)."
7706,1,\n\n- The adaptive component seems to provide improvements for small dataset sizes
7707,3," This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets."
7708,3,"""The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer."
7709,3,"""* EDIT: Increased score from 5 to 6 to reflect improvements made in the revision."
7710,3,"\""\n-Please explain what is meant here by 'hand crafted information', my understanding is that the f^i in figure 1 of that paper are learned modules?"
7711,2,Table 5 is a beast. I have no idea what it is trying to say because it terrifies me
7712,3,\n\nQuestions:\n- Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping.
7713,3,Mentioning of failures and limitations demonstrates a realistic  view on the project\n\t\u2022\t Complexity and time analysis provided\n\t\u2022\t
7714,3, (b). Graph-Structured Representations for Visual Question Answering. Teney et al. arXiv 2016.
7715,1,\n3. Experiments performed on public datasets.
7716,3,  This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy.
7717,1,\n- The paper is overall well-written and clear.
7718,3," Visual Genome (Krishnan et al., not cited) and SIND (Huang et al., not cited) are two datasets which are both larger than Flickr30k and have different image distributions from MSCOCO."
7719,3,\n\nThe comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform.
7720,3, \u2028\n(TODO) I would recommend that the authors test their approach on such setting.
7721,3," Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct."
7722,3,\n\n1. The authors tested out this new activation function on RNNs.
7723,3," Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution."
7724,3,"\n\n[2] Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., & Precup, D. (2011)."
7725,3,". Query Learning Strategies Using Boosting and Bagging."""
7726,3,"\n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)?"
7727,3, Of which dimension?
7728,1," I find the definition of the \""state\"" in this case very interesting."
7729,3,"\n1) It would be nice if the paper were to explain, from a theoretical perspective, why large evidence should correspond to better generalization, or provide an overview of the work which has shown this (eg, Rissanen, 1983)."
7730,3," Furthermore, while the highlighted contribution is the named entity table, it is always used in conjunction to the database approach."
7731,3,"\n\nThat said, I could also imagine a scenario where there is no benefit to generating multiple noisy samples for a single example versus a single noisy sample for multiple examples."
7732,3,\na) It is known that unsupervised clustering methods can achieve 0.97 accuracy for MNIST.
7733,3, which may beg the question if the algorithmic contributions are buying much for their added complexity?
7734,3," \n\nIt is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w)."
7735,3," To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline."
7736,1, The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017).
7737,1,"""Pros:\n1. A new DNA structure GAN is utilized to manipulate/disentangle attributes."
7738,3," Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units.."
7739,3," Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7)."
7740,3," Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer?"
7741,3," Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections."
7742,3," For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\"
7743,2,I find this a smart-looking house built on a weak and shifty foundation
7744,3,"""The manuscript proposed to use prefix codes to compress the input to a neural network for text classification."
7745,3,"""The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article)."
7746,3,"  If the learning rate were optimized together with batch size (eg, keeping aN/B constant), would the generalization gap still appear?"
7747,3," \""Deeply-supervised nets.\"" Artificial Intelligence and Statistics. 2015."
7748,3," However, it remains unclear why SPENs are the right choice for an energy function. "
7749,3, The paper explores in particular:\n1.) Model generalization to unknown data similar to the training set.
7750,3,"""This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL."
7751,1," I could not verify its novelty, but this seems to be a great contribution"
7752,3,  Small batch sizes\nsometimes make it easier for many machines to be working simultaneously.
7753,3,"\n\nBecause of the plethora of VAE models used in video prediction [1] (albeit, used with pre-structured latent spaces), there has to be atleast one VAE baseline."
7754,3," \n\nFurthermore, I think I misunderstood parts, because the scaling in MNIST and CIFAR was different and I didn't see why (for MNIST, it was proportional to block size, and for CIFAR it was independent of block size almost)."
7755,3,\n\n- How computationally expensive is this approach take compared to MagNet or other adversarial approaches? 
7756,3, It would be good to show whether part of its efficiency comes from effective image-guided navigation: does a partial image match entail with targetted navigation (e.g. matches in the right side of the image make the robot turn right)?
7757,3," They mention some subtle details that must be taken into account, such as scaling the plot axes by the filter magnitudes, in order to obtain correctly scaled plots."
7758,1,"  The work on dynamic environments was an interesting step:  it would have been interesting to see how the \""models\"" learned for the dynamic environments differed from those for static environments."
7759,3,   but could be expanded into a nice contribution eventually. 
7760,3,\nThe main concern is the motivation of the two-pass decomposition.
7761,3, They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory.
7762,3,\n\nI don't see what Socher et al. (2013) has to do with the loss in equation (7).
7763,3, \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar.
7764,3," Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems?\n"""
7765,3," While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC."
7766,1, Essentially you perform a local exploration rule in parameter space... and sometimes this is great -
7767,3,\n\nPros:\n- Jointly optimizing forming of groups and placing these seems to have merit\
7768,3," How do features of networks pre-trained on ImageNet, and then fine-tuned for the medical domain, compare to features learned from medical images from scratch?"
7769,3,"\n\n2) Even if for sin activation functions, the analysis is NOT complete. "
7770,3,\n- Are the image features fixed or learned?
7771,3," \n- In Figure 2, even though the diff norm fluctuates, the cosine similarity remains almost constant."
7772,3, \n2. The property of TR decomposition is that the tensors can be shifted (circular invariance).
7773,3,"""The paper proposes to use 2-d image representation techniques as a means of learning representations of graphs via their adjacency matrices."
7774,3,"\n3) \""simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging"
7775,3," (That is, how many experimental runs were averaged or was the experiment run once?)."
7776,1,"\n- While overall the writing is clear, in some places I feel it could be improved"
7777,3,  Does the generalization gap not appear when no momentum is used?
7778,3,"\n\nReferences:\n[A]: Upchurch, Paul, Noah Snavely, and Kavita Bala. 2016. \u201cFrom A to Z: Supervised Transfer of Style and Content Using Deep Neural Network Generators.\u201d arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.02003.\n\n[B]: Kingma, Diederik P., Danilo J. Rezende, Shakir Mohamed, and Max Welling. 2014."
7779,3," They apply this approach to multi-modal (several \""intentions\"") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks."
7780,3,\n\nAnother point associated with the evaluation of the proposed approach is the method used to qualitatively assess options in the Atari experiments described in Section 4.2.
7781,1, The literature review seems to cover and categorize well the field.
7782,3,"Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is"
7783,3," Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty."
7784,1,"\n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct,;"
7785,3,"""The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones)."
7786,1,\n\nMain Comments:\nThe paper is very well written and clearly states and explains the contributions.
7787,2,Was the white noise random?
7788,3,\n\nThe authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best.
7789,3,"\n4. However, computing and inverting the Fisher information matrix is computationally expensive."
7790,3," I'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap."
7791,2,You have put in a lot of effort answering a question that should have never been asked
7792,1," Therefore I've upgraded my rating, and due to better understanding now, als my confidence. """
7793,3, It also outperforms existing methods in terms of coverage and compositionality.
7794,1,\n\nThe idea is important and this paper seems sound
7795,3,"In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA."
7796,3,"""This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings."
7797,3,"  The symbol i is an index into $CV_{i}$, the cut-value of the i-th largest unique weight-value."
7798,3,\n\nThe organization and presentation of the paper need some improvement.
7799,3, Their technique can be applied on top of existing GANs and can address issues such as mode collapse.
7800,3,"\n\nQ(A) is sometimes taken to mean the true posterior (i.e., eq. (31)), sometimes a Gaussian approximation (i.e., eq (32) inside the integral), and both are used interchangeably."
7801,3," Command of related work is ok,"
7802,3," If averaging paths leads to ensembling in CrescendoNet, it leads to ensembling in FractalNet."
7803,3," Aside\nfrom the WGAN critic needing to be optimized for more steps, couldn't the\nremaining components be trained jointly, with a weighted sum of terms for the\nencoder?"
7804,3,\n2. Lacks in sufficient machine learning related novelty required to be relevant in the main conference
7805,2,"By the end of the paper, I was left with the impression that the lid had been lifted off a big can of..."
7806,3," Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider."
7807,3," The algorithm, tagged 'Consequentialist Conditional Cooperation', uses outcome-based accumulative rewards of different strategies established during prior training."
7808,3,"  \n\n- It would seem to me at first glance that the most natural means of overcoming the problem discussed at length toward the end of Section 3 would be to add an additional layer, which would facilitate interactions between the attention-weighted word embedding (\\alpha_i c_i) and aspect embedding (v_t)."
7809,1," The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community."
7810,3, please can you describe the distributions in a more precise way?
7811,3, I would argue that there is a lot of evidence for local inhibitory connection in the cortex.
7812,3, So the paper feels directed to an audience with less background in neural net LMs.
7813,3," This must be mentioned in section 4.2 \""does parameter space noise explore efficiently\"" because the answer you seem to imply is \""yes\"" ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D"
7814,1," By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements."
7815,1,"  But to the best of my knowledge, this is the first paper that applies this concept to the open world classification task."
7816,3,"""This paper presents methods to reduce the variance of policy gradient using an action dependent baseline."
7817,1,"""The topic is interesting however the description in the paper is lacking clarity."
7818,3," However, I think that it should be for pair in the same layer. It is not clear in the paper."
7819,2,I basically stumbled across every second sentence
7820,1,"""In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward."
7821,3," Unfortunately, the contribution seems rather small to be accepted for ICLR."
7822,3," For me, this is not the pain point in this tasks."
7823,2,The authors should refer to the super interesting article on this topic in Wikipedia.
7824,3,  The proposed method jointly trains an RNN encoder with a GAN to produce latent representations which are designed to better encode similarity in the discrete input space.
7825,2,"You know that this table is unreadable â€“ at least one of the authors of this paper must have said this is unreadable, lets put in a series of bar plots with the numbers written on the bar. I agree with that person."
7826,1, \n\n2. Experiments are comprehensive.
7827,3,"\n* I wish the authors would show that they get a *useful* model eventually - for example, can this be used to denoise other images from the dataset?"
7828,3," \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity."
7829,3,\nOriginality: this paper introduces block diagonal matrices to structure the weights of a neural network.
7830,1," \n- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.\n"""
7831,3,  The same issue is found in Figure 2.
7832,3, The weights of the NN like structure are optimised using a genetic search algorithm which optimises the cross-validation error of a nearest neighbor classifier.
7833,3,\n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.
7834,1," The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems."
7835,3," So if this works, it could be an impactful achievement.."
7836,1,\n\nPros:\n- The presentation is clear and easy to follow.
7837,1,.\n\nThis is a really interesting perspective on probing invariances and should be explored more.
7838,3," In Section 4.1, the target x and y have time steps from t1 to t2."
7839,3,\n\n4. The transferrability of features in Section 6 is an interesting problem to explore.
7840,3,"\n\nA few questions/comments:\n1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to?"
7841,3,"""This paper focuses on solving query completion problem with error correction which is a very practical and important problem. "
7842,3, It might be better to add errorbars to those curves
7843,2,Some self citations may be easily taken out without harming the paper -..
7844,3, Therefore it is my opinion that reinforcement learning approaches to SLAM lack a concrete goal in what they are trying to show.
7845,3, Efficient KRU span a restricted subspace whose elements might not compose into structures that are expressive enough.
7846,2,"You aimed for the bare minimum, and missed!"
7847,3,"\n\n[1'] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, D. Samaras. Neural Face Editing with Intrinsic Image Disentangling. CVPR 2017"
7848,3,\n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals.
7849,1,\n\nIt is an interesting paper with a novel approach to multi-task learning
7850,3,\n\n2. Does the Deep Metric network always return a '64-dim' vector?
7851,1, The method is validated on simulations as well as in cfDNA and is s\nhown to provide increased precision over competing methods.
7852,1,\n\nClarity\n\nThe paper is well-written.
7853,1,"""This is a well-written paper with good comparisons to a number of earlier approaches."
7854,3, They accept different inputs (raw pixels vs edges).
7855,1,"\n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU."
7856,2,English need to be corrected by an english speaker
7857,3,"n- the method is a simple feed-forward network, so it is very fast to compute\"
7858,3," It seems you are scaling to mini-batrch gradient to be in expectation equal to the full gradient (not normalized by N), e.g. it scales ~N."
7859,3,"\n\n[1] Oord, Aaron van den, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. \""Wavenet: A generative model for raw audio.\"" arXiv preprint arXiv:1609.03499 (2016)."""
7860,1,"""Although the problem addressed in the paper seems interesting,"
7861,3,"""In this paper an alternating optimization approach is explored for training Auto Encoders (AEs)."
7862,3,\n(2)\tThe CW-SC kernel (Figure 2 (c)) is very similar to interleaved group convolutions.
7863,3, It is therefore not surprising that reducing the number of updates does not deteriorate the test set accuracy.
7864,3, \nf)\tIsn\u2019t learning the random walk sample path a much harder / higher-dimensional task than it is necessary?
7865,3,\n\nii) What is the standard for creating the questions?
7866,2,Well Written. 
7867,3, These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution.
7868,3,"  It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques."
7869,3," However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space."
7870,3, This seems to mean that we should not expect for q -> p when K increases?
7871,3," For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs."
7872,3,\n\n-- Comparison with existing work: There has been a lot of work recently on one-shot and few-shot learning that would be interesting to compare against.
7873,3, Bhatnagar et al. (2009) propose a solution by projecting the error on the tangent plane to the function at the point at which it is evaluated.
7874,3,\nFor example if we set all the variances to be a unit matrix than the KL is collapsed to be a simple symmetrical Euclidean distance.
7875,3,"n\n2) Continuing on the above point, this is essentially about learning a common multimodal representation and then decode from this common representation."
7876,1," The idea of structured matrices in this context is not new, but the diagonal block structure appears to be. "
7877,3," \n\nMajor concerns:\n1.\tThis work brings some modifications to the prediction layer, which is a bit trivial."
7878,3," Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance."
7879,1,"\n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods."
7880,3," Where the label 'semi-supervised' is used (page\n4) is actually wrong: yes the labels are used, but of course it is the\n*gradients* which show up in the update, not the labels themselves\ndirectly.[[CNT], [EMP-NEU], [DIS], [MIN]] It's also not true that there is little research in understanding\nthe formation of internal representations.[[CNT], [EMP-NEU], [DIS], [MIN]] There is a whole subfields of\npapers trying to interpret the features learned by deep networks, and much\nwork designing learning frameworks and objectives to achieve better\nrepresentations, e.g, to better disentangle the underlying factors."
7881,3,?\n- Do you think your approach would benefit from having a few parallel training points?
7882,3," \n\nI encourage the authors to develop the problem and method further, as well as the analysis and evaluation. \n"""
7883,3, \n\n**EVALUATION AFTER AUTHORS' REBUTTAL**\nThe reviewer has read the responses provided by the authors during the rebuttal period.
7884,3,\n \nMore description should be provided to explain the reward visualization on the right side of figure 2. What reward?
7885,3, Do the multiple input distributions actually help?
7886,2,"My summary assessment of the paper is as follows:
Introduction: Poor
Background: Poor
Methods: Poor
Results: Poor
Discussion: Poor
Conclusions: Poor 
Overall assessment: Poor
Recommendation: Reject
Further Comments: Why was this submitted to a journal?"
7887,1, The ideas are straightforward and make sense given the current trends in the field.
7888,3,"\n\nIn particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness."
7889,3,   A concluding observation is that CNN models learn and generalize the structure content of images.
7890,1, I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever.
7891,1,"""(Score before author revision: 4)\n(Score after author revision: 7)\n\nI think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly."
7892,3," I would have preferred to see an extensive grid search done to find the best possible \\lambda, then seen how well the proposed method does compared to this."
7893,3," For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy."
7894,3, It would be helpful to add a few simple and intuitive baselines in the experiments.
7895,3,"""This paper introduces a skip-connection based design of fully connected networks, which is loosely based on learning latent variable tree structure learning via mutual information criteria. "
7896,3, Is it due to a good initial network structure?
7897,1,"""The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper."
7898,1, Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data.
7899,1,\n\nThis is a fairly strong paper.
7900,3," The paper shows that even under a random policy, the eigen options can lead to purposeful options\n\n"
7901,3," Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n"""
7902,3,  It would be very helpful to include algorithms.
7903,1," \n\nOverall speaking, this work is quite interesting."
7904,3,"  Furthermore, only feedforward and locally connected networks (CNN) are considered since their architecture is considered more biologically plausible than convolutional neural networks."
7905,3,"""This paper propose an adaptive dropout strategy for class logits."
7906,1, This paper improves on the upper bound given by [2] and the lower bound given by [1].
7907,3,"""The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced."
7908,3," \n\n(2) Moreover, the augmenting features x_i^A (regardless A=F, G, or H), are in the same space as the original features x_i. Hence x_i^A is rather an augmenting instance than additional features. What makes feature augmentation better than instance augmentation?"
7909,3, Experiments are made in En-De and En-Ru.
7910,3,"\n\nSome points:\n1. The introduction uses \""scalability\"" throughout to mean something closer to \""ability to generalize.\"" Consider revising the wording here."
7911,3,"  Given a new dataset, how to determine it for a good performance."
7912,1,\n\nSignificance: The problems the authors consider is worth exploring further
7913,3," For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved."
7914,3, Could the authors perhaps comment on how well these metrics would work in the semi-supervised case?
7915,2,It amounts to story-telling ! 
7916,3,"""This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU."
7917,3, \n\n5) Overall: in this paper the authors come up with a method for learning objects from Atari games video input.
7918,3,"""This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression."
7919,3,"\n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing \u201csum_reduced\u201d\n-\tISTA \u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative. "
7920,1,"  For example, it seems that the proposed projected gradient descent method leads to better speedup results in VGG as opposed to Resnet, with very similar reduction in accuracy."
7921,3," Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2."
7922,1,\n- The idea is novel and impactful if its evaluated properly and consistently.
7923,3," Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib."
7924,2,"The first and last paragraphs are weak and read in a style unlike the rest of the paper, like poor advertising copy."
7925,3, Could the authors justify why minimizing w.r.t. W_1 is still a quasi-convex optimization problem?
7926,1, The good aspect of this paper is that it has some performance improvements.
7927,2,"For a section on thought, very little seems to have gone into it."
7928,3, It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.
7929,3,"For a GAN noise vector a plausible filter set is created, and for a data sample a set of plausible activations are computed. "
7930,1," The technical proofs of the paper are in appendices, making the main text very smooth."
7931,1,The discussion on the deficiencies of the naive LP approach is mostly well done.
7932,3, The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end).
7933,1, The experimental results show that the propped model outperforms tree-lstm using external parsers.
7934,3, Do you have any insights are speculations regarding this?
7935,1," \n\nOverall, the idea seems relevant and there are some good findings but I am sure that image captioning community is already aware of these findings."
7936,3, PSNR is evaluated on predicted frames -- PSNR does not appear to be explicitly defined but I am taking it to be the metric defined in the 2nd paragraph from the bottom on page 7. 
7937,3,  This is a minor modification.
7938,3, I think this more counts as careful engineering of the SAN model rather than a main innovation.
7939,1,\n\u00a0\nPositives:\n\u00a0\n\t\u2022\tNovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed\n\t\u2022\tVery good abstract:
7940,3,"\n- In table 4, why do Hill et al lstm and bow perform much better than the others?\n"""
7941,3,"""The authors propose a method for performing transfer learning and domain adaptation via a clustering approach."
7942,3,\nb) Discussion of the experimental results is not sufficient
7943,3," The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition."
7944,3," Is there an economic motivation?[[CNT], [CNT], [QSN], [MIN]] Is it just a different way to train a NN?"
7945,1, The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems.
7946,3, The paper has scope for improvement.
7947,1,"""The paper is written well and clear."
7948,3,\nThe paper already mentions about this direction and it would be interesting to see the experimental results.
7949,1," If the LM part really is to model grammaticality and coherence, a general LM might be suitable as well."
7950,3,"\n- I'm not convinced that page 4 and the \""Bayesian\"" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \""this is similar to particle filter\"" and add the mathematical derivation after, rather than as if it was some complex formula derived."
7951,3," \n\nThe fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space."
7952,3,"\n\nThe second idea is that episodes may terminate due to time out, but we should include the discounted value of the time-out termination state in the return."
7953,3, I think I might have missed something here.
7954,3,"\n\n-  Please also discuss the dependence of your algorithm with respect to the starting baseline function \\phi_0."""
7955,3,\n   transformations chosen?
7956,3,"\nThen the expected expert direction for learner state S is:\n\n     SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 )"
7957,3, Can you explain why this is possible?
7958,2,"This sounds nice, but in fact it is vague. There are many instances of this kind of nice-sounding vagueness."
7959,3,\n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?
7960,3, This result is obtained almost directly by writing the bound on Lipschitz-continuous function\n | f(y)-f(x) | < L || y-x ||\nwhere x = x_0 and y = x_0 + \\delta.
7961,3,"""This paper presents a method for matrix factorization using DNNs."
7962,1,\n\nThe methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment.
7963,3,\nThe agent then samples from this posterior for an approximate Thompson sampling.
7964,1,\n\nResult interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used.
7965,1,"""Summary:\nThis paper proposes a new approach to tackle the problem of prediction under\nthe shift in design, which consists of the shift in policy (conditional\ndistribution of treatment given features) and the shift in domain (marginal \ndistribution of features)."
7966,3," However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. "
7967,3," \n\n2) With reference to the previous point, the experiment 1 in Figure 2 provides a standard example of domain shift."
7968,1, The paper is well written and the experiments are interesting.
7969,3, and (2) not specific to face images.
7970,3," As I said, the authors should rephrase the definition of explicit grounding, to make it clearly distinguished with the previous work I listed above."
7971,3,"""General comment\n==============\nLow-rank decomposing convolutional filters has been used to speedup convolutional networks at the cost of a drop in prediction performance."
7972,1,"\n\nFirst of all, I was surprised on the short length of the discussion on the state-of-the-art."
7973,2,Are you trying to be funny? Dont.
7974,1,"  This paper helps others to better understand the vulnerabilities of DNNs."""
7975,1,"\n\nClarity: The paper is relatively clear,"
7976,3,"The idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis."
7977,1, The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution.
7978,3," During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation."
7979,1," Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future."
7980,3, There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher').
7981,2,"Authors wanted to make title catchy but actually finished trivial title, a la Daily mail"
7982,3,\n2. Evaluates how informative the latent states are via state reconstruction.
7983,3," To combat this problem, the authors suggest modifying the state representation of the policy to include an indicator of the amount of remaining time. "
7984,3," Then it is stated: \""the number of hidden neurons, as well as the structures for the deep neural networks\nwere empirically tried, and the results of the best settings were registered for comparison\"" (1st paragraph Sec. 3)."
7985,3," However, the experiments to support the idea do not seem to match the motivation of the paper."
7986,3,"\n* Section 4.2. From table 2, it seems that all permutations are used for training which is rather large for molecules of size 20."
7987,3,"\n\nIt would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix."
7988,3, \n\nI would also like to see some analysis of what's actually being learned by the teacher.
7989,3,"""This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss."
7990,1,\n+ well written
7991,1,"\n\n\nPaper Strengths:\n- Despite being simple technique, the proposed pixel deconvolution layer is novel and interesting."
7992,3, how do you initialize h? fully at random ?\n\n5) seems the paper is following implementation by Kim et al. what happens if the discriminator is like in cycle GAN acting on pixels.
7993,3," The authors apply their technique to two datasets, namely, the Omniglot dataset and the TIMIT dataset and show that their model does a reasonable job in these two tasks."
7994,3," The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method."
7995,3, \nWhat many independent seeds where used for training?
7996,3,"n\n-For counting the minimal complexity mappings, I wonder if one can make a connection with Algorithm robustness of Xu&Mannor(COLT,2012) where instead of comparing losses, you work with discrepancies.\n\nTypo:\nSection 5.1 is build of -> is built of\n"""
7997,3," Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures."
7998,3, I think that\nwithout an experiment the small section on non-conjugacy should be removed.
7999,1," Assuming that the result carries over to ConvNets, I find this result to be very interesting."
8000,2,I now turn to my best guess about what the authors might be doing.
8001,3, Did you run any statistical significance test on the evaluation results?
8002,3," They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning."
8003,1,"\n\n* The experiments are sufficient to demonstrate the viability of the approach,"
8004,3,"  They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be."
8005,3, And what is the relation of the black/white and orange squares?
8006,3," In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational."
8007,3, Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions.
8008,1," The different ideas are clearly stated and discussed, and hence open interesting questions and debates"
8009,3,"""This paper presents a method based on GANs for visualizing how humans represent visual categories."
8010,1, Contextualization with respect to previous work is adequate.
8011,2,"This paper is under-referenced, conceptually impoverished, and poorly written."
8012,3,"\n\n3. More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation."
8013,3," I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation)."
8014,2,"The manuscript makes 3 claims: The 1st we've known for years, the 2nd for decades, the 3rd for centuries."
8015,3,"   I would recommend this work for a workshop presentation at this stage.\n"""
8016,3,"\nThe paper should present a comparison with such kinds of models.\n"""
8017,1, \n\n2. The authors proposed a simple method to ground the language on visual input.
8018,3,"\n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering."
8019,3, A few additional comments are as follows:\n\n  \u2022 The plot in Figure 3 is the leading collection of results to demonstrate the dominance of the authors' adaptive weight approach (CAR) over the A3C (TD(0) estimates) and LRA3C (truncated TD(lambda) estimates) approaches.
8020,3," For (1), the authors show that the DNN has a tighter bound on the depth."
8021,1, The framework is evaluated on a relevant multi-task problem.
8022,3," -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion."
8023,3, What is the relevance/applicability of the method given this context?
8024,3,"  but I think authors should improve the aspects I mention for its publication.\n"""
8025,3, Does it mean that you have not run enough iterations for the baseline methods?
8026,1,"\n  \nThe papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures"
8027,3,"\nThe authors propose to first embed each image into a feature space, and then feed this learned representation into a auto-encoder that handles the projection to and from the semantic space with its encoder and decoder, respectively."
8028,3,"n\n2. Regarding the partial observability, each agent knows the location of all agents, including itself, and the location of all obstacles and charging locations; but it only knows the location of customers that are in its vision range."
8029,3,"\n\nTo keep my text readable, I assume we are working in feature space\ninstead of state space and use different letters for learner and expert:"
8030,3, \n\nCons:\n\n- The proposed approach follows largely the existing work and thus its technical novelty is weak. 
8031,1,"\n\nThe paper is interesting and well written, the proposed regularization makes sens since it is basically a relaxation of the constraints and the numerical experiments also suggest it's a good idea."
8032,3,"""The paper proposes a budgeted online kernel algorithm for multi-task learning."""
8033,1," Although  I found the results useful and potentially promising,"
8034,3," In particular, authors aim to show that units behave as binary classifiers during training and testing."
8035,3," \n2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets,"
8036,3,  The model is shown to improve on the published results but not as-of-submission leaderboard numbers.
8037,1,"  \n\nOn the whole I believe that the task and method are interesting, and experiments convincingly demonstrate the utility of Skip-RNNs compared to the author\u2019s own baselines."
8038,1, The proposed data augmentation is a general one and it can be used to improve the performance of other models as well.
8039,3," In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric."
8040,3,Authors use the image modality as a bridge between two different languages and the agents learn to ground different languages to same image based on the similarity
8041,2,It is hard to imagine researchers of this caliber being unaware of a half-century the field X
8042,3,\n\nQuestion:\n-\tWhat is the cost of constructing orthogonal random features compared to RF?
8043,2,"The manuscript is too long for what the authors have to say. However, additional text is required as outlined below."
8044,3, some of the algorithms tested are similar algorithms that have already been proven to work well in practice.
8045,3,\n3) It is easy to envision a (full knowledge) attack where the attacker knows the architecture and parameters of the detector and then devises an attack to fool both the detector and the original classifier. 
8046,3, This paper's introduction provides precisely 1 statement to suggest that may not work for deep learning.
8047,3,"\n\n3. Only one attribute can be \""manipulated\"" each time?"
8048,3, Why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselines?
8049,3," First of all, errors learned from the noisy data sources are constrained to exist within a word."
8050,1,"\n\nIn my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision."""
8051,3,"\n\nd) The authors found hard to regularize the gradient $\\nabla_x D(x)$, even they tray tanh and cosine based activations."
8052,3,"\n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation?"
8053,1," The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on."
8054,3,\nThere is only one layer here so we don\u2019t have complex non-linear things going on?
8055,3," \n\nIn order to decide when to stop, an independent goal detector is trained which was found to be better than adding a 'goal-reached' action to the PSF."
8056,3,"""Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks."
8057,3, Can you comment more on how to pick that value in real-world settings? Just saying sufficiently many (Section 4.2) is not sufficient.
8058,3, One could think that it makes a somewhat incremental contribution with respect to the more complete work (both theory and practice) from [Bartlett et al. 2017].
8059,3, The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix.
8060,3," \n\nThere have been several works that have noted that lambda can and perhaps should be changed as a function of state (Sutton and Barto, White and White [1], TD-Gammon)."
8061,3, whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures.
8062,3, The game is parameterized with the rewards of +1 and +3.
8063,3," Some qualitative analysis, or even feature ablation study would be helpful."
8064,1,\n- Experiments on multiple tasks and datasets confirm the efficacy of the method
8065,3,"\n\nThe text says that rand+cegis selects 70% of examples of the proposed approach,\nbut figure 4 seems to suggest that the numbers are very close -- is this initial\nexamples only?"
8066,1,  I myself am very curious about what would happen and would love to see this exchange catalyzed.
8067,3," The practical aspects are also interesting, because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains; these ideas could clearly be developed in future work."
8068,3," \n\nOverall, the results didn't warrant the complexity of the method."
8069,3, \n\nThere is also a toy example created to show that this approach works well compared to the RNN based approaches.
8070,3, The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference.
8071,3," Each node is represented with (dx, dy) along with one-hot representation of three different drawing status."
8072,3," However the exposition needs to significantly improve for the paper to be ready for ICLR."""
8073,3,"  And note that the \""w(u)\"" defined in this reference is the lambda*w*(alpha) optimal relationship defined in this paper (but without the 1/lambda factor because of just slightly different writing; the point though is that strong duality holds there and thus one really has equality)."
8074,3,". However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as \u201csmaller-norm-less-informative\u201d assumption. "
8075,3, Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction.
8076,3, The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy.
8077,3, A Gaussian model is used for representing these confidences as covariance matrices.
8078,3,"\n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data."
8079,3,  It is not the case that the adversarial loss was simply removed.
8080,3," With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2)."
8081,3,\n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)?
8082,1,"\n\nSummary:\n\u2014\u2014\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method."
8083,3, What are the theoretical advantages of using reservoir sampling?
8084,3,"?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n"""
8085,3, \n1) Is the network given a sequence of program/solution input?
8086,3, They also observe that the ID drops with each successive layer.
8087,2,rampant and unsupported speculation
8088,3," (It would seem that *more* robustness is better than less, but the text says that lower values are \nchosen.)"
8089,1,"\n\nThe paper is well presented, the result is explained and compared to other results, and the proofs seem correct."
8090,3, Each subtask execution is represented by a (non-learned) option.
8091,3,\n\nThe idea of using dilated convolutional networks as drop-in replacements for recurrent networks should have more value than just reading comprehension tasks.
8092,3,"\n\n\n[1] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst."
8093,1,"\n\nIn brief: quality is high, clarity is high, originality is high, and significance is medium."
8094,3," Are there any good automated metrics, and how well do they correspond to human judgement?"
8095,3," I had particular difficulty accepting that the phase 1 GD iterates would never hit the boundary if the quadratic was strongly convex, although I accept that it is true due to the careful choice of step size and initialization (assumptions 1 and 2)."
8096,2,The authors last name sounds Spanish. I didn't read the manuscript because I'm sure it's full of bad English
8097,1," These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017),"
8098,1,\n- That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration.
8099,3," \n\nIf we trust the authors, then the paper seems good because it is fairly unusual."
8100,3,"""This paper views graph classification as image classification, and shows that the CNN model adapted from image net can be effectively adapted to the graph classification."
8101,3,. But the training of the GAN is in itself a problem.
8102,1,\n- The experimental results seem to be complete for the most part.
8103,3,"   \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting."
8104,3,"  If s_0 is start of string, the s_0:m is of length m+1 not length m."
8105,3, Given the ongoing review decisions/issues I'm putting my review slightly below accept..
8106,3,   This seems to be the fundamental question -- do random projects help in the train_D | test_C case?
8107,3," since the approach is interesting for out of sample data,"
8108,2,This manuscript is obviously not suitable for publication in high impact factor journals.
8109,3," The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks."
8110,3," \n\n2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments."
8111,3, It also make some connections to random matrix theory.
8112,3," If it can be integrated into the training end-to-end, it might be better."
8113,3,\n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial.
8114,3," Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon."
8115,3," Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting.[[CNT], [EMP-POS], [APC], [MAJ]]\n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch?"
8116,2,Did you have a seizure while writing this sentence? Because I feel like I had one while reading it.
8117,1," It is interesting the \""psychological\"" analysis that the authors present in Section 6."
8118,2,The title showed promise...
8119,2,The project is lack of interest.
8120,3, The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.
8121,3,"""The paper \""A Deep Predictive Coding Network for Learning Latent Representations\"" considers learning of a generative neural network."
8122,3," I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting."
8123,3," \n\nMore detailed comments\n1. Instead of considering multi-class classification as one-vs-all binary classification, can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays."
8124,3,"""The majority of the paper is focused on the observation that (1) making policies that condition on the time step is important in finite horizon problems, and a much smaller component on that (2) if episodes are terminated early during learning (say to restart and promote exploration) that the values should be bootstrapped to reflect that there will be additional rewards received in the true infinite-horizon setting.\n\n1 is true and is well known."
8125,3,"\n\nFigure 3: This can be inferred from the text (I think), but I had to remind myself that \u201cIW train\u201d and \u201cIW test\u201d refer only to the evaluation procedure, not the training procedure."
8126,1,  This is would be worthwhile to appreciate the benefit of the proposed approach.
8127,1, I think there is some merit in the work.
8128,3," For weighted automata, the reference Droste and Gastin considers weighted word automata and weighted logic for words."
8129,3," \n\nPresumably, once one had found a path ( (s, a0), (s1, a1), (s2, a2), \u2026, (sn-1,an-1),  s\u2019 ) one could then train the PATH policy on the triple (s, s\u2019, a0) ?"
8130,3,  The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.
8131,3,"\nDuring training, the agent explores by itself related (but different) tasks, learning a) how actions affect the world state,"
8132,3," However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work."
8133,1,"""This paper adds an interesting twist on top of recent unpaired image translation work."
8134,3," The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function."
8135,3,"  Through a series of experiments and a newly defined dataset, it exposes the short-comings of current seq2seq RNN architectures."
8136,3,"\n* Section 2.2 starts talking about \""deterministic layers h\""."
8137,3,". According to Fig. 2, the conditional probability in the product operator should be revised to p(a_t | x_{1:t}, a_{1:t-1}), and the independence approximation to remove a_{1:t-1} from the conditions should also be noted in the paper."
8138,3,  A variety of experiments are conducted that demonstrate the efficacy of the proposed methodology.
8139,1, It seems right and valid.
8140,3, The work also investigates various schemes for selecting negative samples.
8141,3, Why not just enforce this constraint by doing projected gradient descent?
8142,3," For example, what are the necessary and sufficient conditions for an attacking method to be undetectable?"
8143,3,"""This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU."
8144,3," So by subtracting features from successive states, the method mainly encodes the action as it almost encodes the one step dynamics in one shot."
8145,3,"\"" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something?"
8146,3,\n- a final classifier performing the desired classification task.
8147,3,"\n\n* You have a complicated method for constraining the parameters to be in [-0.5,0.5]."
8148,3,"""This paper presents an iterative approach to sparsify a network already during training."
8149,1,. \n\nPros:\n\nThe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory. 
8150,3,Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.
8151,3,"\n\nLack of new Insights\nThe visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than \""something non-trivial is going on in these networks\"". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images). "
8152,3,"""The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance."
8153,3,"\n\nIn figure 2, top row, please display the learning rate on a log scale."
8154,1,"\n\nIn summary, this is an interesting and well-written paper on a timely topic."
8155,3,"""The authors extend the ResNeXt architecture."
8156,3," But isn't this exactly what neural networks do \u2013 learn intelligent combinations of features optimized (in this case, via GA) for a downstream task?"
8157,3," This is a limitation, as the model space is not as flexible as one would desire in a discovery task."
8158,1,"\n\nThat said, overall the paper is a nice contribution to dialogue and QA system research by pointing out a simple way of handling named entities by dynamically updating their embeddings."
8159,3," Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too."
8160,1," See more detailed points below in Weaknesses.[[CNT], [null], [DIS], [GEN]]\n\n**Strengths**\nI like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. "
8161,1,". This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. """
8162,3,"""In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence."
8163,3,"\n6. Just claiming the generalization capability of deep networks is not enough, need to show how much the model can interpolate or extrapolate?"
8164,3, With the same plot one could sell SGD as the superior algorithm.
8165,3,"""This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector)."
8166,3," The weightings are defined in terms of an exponential decay on each dimension at each time step (given by the forget gate), which lets the cell be computed sequentially in linear time rather than in the exhaustive quadratic time that would apparently be necessary for this definition."
8167,3, It would be useful to explain more why this is needed.
8168,3,(3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase.
8169,3,"""The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or \""batches\""."
8170,3, I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick.
8171,3,\n\nExperiments in a simple car simulator on a task which requires the car to take a certain exit while navigating through traffic are presented with two baselines: 1. a greedy policy which navigates to the right-most lane asap and then follows traffic till the exit is reached.
8172,3, \nThis constraints may not be necessary if instead they used proximity space representation.
8173,2,I have received one highly negative review and wondered whether it could be overcome by other reviews. I conclude that the answer is no
8174,3, That seems important to explain more thoroughly than is done in the current text.
8175,3, The architecture consists of a \u2018slow\u2019 network which provides weight updates for the \u2018fast\u2019 network which outputs the predictions of the system.
8176,2,"The whole paper reminds me of a paper of a couple of years ago, which I didnt like."
8177,1,  This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.
8178,3,"\n\n* Page 2: In general the notion of separating the latent space into content and style, where we have labels for the \u201ccontent\u201d is an old idea that has appeared in the literature and should be cited accordingly."
8179,3,"?\n\n2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree."
8180,3, \n\nLemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria.
8181,1, The idea of using similar sentence pairs as cluster-to-cluster translation is interesting.
8182,3, \n\nWhere exactly is input injection used?
8183,1, I thus appreciate the authors\u2019 effort.
8184,1,\n+ Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem
8185,3, See detailed comments below for problems just in the introduction. 
8186,3,\n\nMy concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model.
8187,3, The transform allows for explicit rotations and swaps of the hidden cell dimensions.
8188,3, Is it the % of times that the classifier is confused?
8189,3, I understand that GANS (and Wasserstein GAN) is a relatively young community and that references list can be short but their is a large number of papers discussing regularized optimal transport and how the resulting problems are easier to solve.
8190,3," Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics."
8191,2,It is not clear what the author wants to accomplish. - Reviewer 
8192,3,"""Overview:\nThis paper proposes an approach to curriculum learning, where subsets of examples to train on are chosen during the training process."
8193,3,\nWould it not be fine if the attention mechanism did a perfect job of selecting\nthe class ?
8194,3,"""Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy."
8195,1,  Nice ablation studies.
8196,2,My first concern is that I dont get it.
8197,1," I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here."""
8198,3,"\n\npage 3:\n- \""f that plays the role of an appraiser (or critic)...\"": this paragraph could be extended and possibly elements of the appendix could be added here.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Section 4: the way clipping is presented is totally unclear and vague.[[CNT], [PNF-NEG], [CRT], [MIN]] This should be improved.[[CNT], [PNF-NEU], [SUG], [MIN]]\n- Eq (5): as written the distribution of \\tilde{x}=tx+(1-t)y is meaningless: What is x and y in this context?"
8199,3,"\ It does so from the perspective of \""Bayesian model comparison\"", where two models are compared based on their \""marginal likelihood\"" (aka, their \""evidence\"" --- the expected probability of the training data under the model, when parameters are drawn from the prior)."
8200,2,You were either in a rush or you do not care too much about getting your paper accepted
8201,3, 3) final classifier for one-shot learning is learned on augmented image space with two (if I am not mistaken) fully connected layers.
8202,3,"""Quality:\nThe paper appears to be correct\n\nClarity:\nthe paper is clear, although more formalization would help sometimes\n\nOriginality\nThe paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know."
8203,3,"\n\nIn the experiments the authors compare with classical backpropagation, but they do not compare with \nthe explicit step of Carreira-Perpinan and Wang?"
8204,3," I strongly recommend to the authors, to provide technical details of topologies used, hyper parameters and any other important detail that would help a third party research to reproduce these results."
8205,3,  I am also concerned to see test performance significantly better than development performance in table 3.
8206,3, First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions.
8207,1, I enjoyed learning about the authors\u2019 proposed approach to a practical learning method based on the information bottleneck.
8208,3," The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model."
8209,2,"There is an over-reliance on sophisticated statistics at the expense of good old fashioned, scientific thought"
8210,3," In the experiments, authors only stated that \u201cwe fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph\u201d, and then the graph is used to train existing models as the input of the graph."
8211,3,"""This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal."
8212,1,"  While the formulation in terms of pre-post spike-times is interesting, the result is clearly different from STDP, and ignores the fact that e_t refers to the backpropagating error (which presumably would be conveyed by a feedback network): applying the plotted pre-post spike-time rule in the same setting as where STDP is observed will not achieve error-backpropagation."
8213,1,"""= Quality = \nOverall, the authors do a good job of placing their work in the context of related research, and employ a variety of non-trivial technical details to get their methods to work well."
8214,3," They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016."
8215,3," Then, these image features are organized in a fully connected graph."
8216,3,  The coreset construction requires one to construct a set of  points which can cover the entire dataset. 
8217,3,  Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks.
8218,3," In future revisions, this should be rectified."
8219,3, in Vendrov work they defined a partial ordering.
8220,3,The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks.
8221,1," This type of study is important to give perspective to non-standardized performance scores reported across separate publications,"
8222,1,  Positive empirial results support the proposed regularizer.
8223,3,  Is this a substantial fraction of the time of the games studied? 
8224,3," The (Mirowski et al, 2016) paper shows that a neural network-based agent with LSTM-based memory and auxiliary tasks such as depth map prediction can learn to navigate in fixed environments (3D mazes) with a fixed goal position (what they call \u201cstatic maze\u201d), and in fixed mazes with changing goal environments (what they call \u201cenvironments with dynamic elements\u201d or \u201crandom goal mazes\u201d)."
8225,3," Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript."
8226,1,"  Of particular interest is [3], which reports 98.9% accuracy with a 100-unit LSTM initialized with orthogonal and identity weight matrices; this is significantly higher than all reported results for the sequential MNIST task."
8227,1,\n2.\tThe paper is well written making it easy to follow.
8228,3,"  The first property (correctness) is a more essential property of this quantity, rather than the second (appropriateness as an example selection measure)."
8229,3," In\nProceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015), ACM, pp. 1310\u20131321."
8230,2,This looks like a very early draft
8231,3," I'm not very familiar with SQuAD dataset, but the results seems worse than \""Reading Wikipedia to answer open-domain questions\"" Table 4 which seems use a vanilla LSTM setup."
8232,2,The authors have gained some attention in the community: most of this is due to the wrong reasons
8233,3,"""The authors describe a method for encoding text into a discrete representation / latent space"
8234,2,However I deplore the fact that this paper has been created at all
8235,3,"  In European Conference on Computer Vision, pp. 297\u2013312, 2014a."
8236,1,\n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable.
8237,1,\n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem.
8238,3," Please add error bars (based on Figure 4, I would imagine they are quite large)."
8239,2,Why chase a gene in this ridiculous organism?
8240,3," So if the proposals are missing a single object, this cannot really be counted."
8241,3," What are the instructions given to the workers?[[CNT], [CNT], [QSN], [MIN]]\n- In section 4.2. the authors state \""We also simultaneously learn a corresponding inference network, .... granular human biases captured\""."
8242,1," \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing;"
8243,3,\n- introduces a simple branch-and-bound approach.
8244,1," Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets."
8245,3,\n\nFigure 5:\na) What was the rational for stopping training of CommNet after 100 epochs?
8246,1,"Also, the recently proposed continuous relaxation of random variables seemed relevant."
8247,3,"""This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN."
8248,1," For me, it is a clear accept."
8249,1,\n+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks
8250,3, The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples.
8251,3,"  Also, the figure shows that the performance of Adam+BackProp is worst than Adam+ProxProp even though the training loss of Adam+BackProp is smaller that of Adam+ProxProp."
8252,3,"  Or are the keys for already found entities retrieved directly, by value?"
8253,3,\n\n(Assessment)\nBorderline. Refer to the Cons section above.
8254,3, What's similar vs dissimilar is trained with a binary classifier.
8255,3,"""The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks."
8256,3,  Are there any theoretical results which can be leveraged from the stochastic processes literature?
8257,1," \n- Since there are existing methods to generate images from a textual description (e.g. Zhang ICCV 2017, \""StackGAN\""), Fig. 10 merits a comparison to those."
8258,3," This involves producing some additional data points, either by adding noise to the projected semantic vector, or by choosing a number of that vector's nearest neighbours."
8259,3,. The main novelty is about using word pair embedding to improve the Topic model
8260,3," The non-conservative part leads to the fact that the dynamics of SGD\tmay show oscillations, and these oscillations may even prevent the algorithm from converging to the 'right' local optima."
8261,1,"""I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution."
8262,3,"\u201d Let\u2019s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons?"
8263,3," The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors."
8264,3," It is well known that CNN features are much better if enough data is available.\n """
8265,3," Typically, the regularization depends on both hyperparameters and parameters."
8266,3,"  Eq. 5 clarifies this implicitly, but would be good to state outright. """
8267,3, What if multiple sets of z can generate the same observations and how does the evaluation of disentanglement fairly work if the underlying model cannot be uniquely recovered from the data?
8268,3,"However, the paper does not properly relate their results, assumptions in the context of the existing literature."
8269,1, Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.
8270,3," Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like."
8271,3," U. Gutmann and J. Corander, \u201cBayesian optimization for likelihood-free inference of simulator-based statistical mod- els,\u201d Journal of Machine Learning Research, vol. 16, pp. 4256\u2013 4302, 2015. \n\nG. da Silva Ferreira and D. Gamerman, \u201cOptimal design in geostatistics under preferential sampling,\u201d Bayesian Analysis, vol. 10, no. 3, pp. 711\u2013735, 2015. \n\nL. Martino, J. Vicent, G. Camps-Valls, \""Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models\"", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017."
8272,1," While the revised version contains more experimental details,"
8273,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor."
8274,3," Since most theory around divergence minimization is based\non the unmodified loss function for generator G, the experiments carried out in\nthe submission might yield somewhat surprising results compared the theory."
8275,3," A good historical hook to intelligent tutoring is Anderson, J. R., Boyle, C. F., & Reiser, B. J. (1985)."
8276,1," It shows convincingly that standard NMT models completely break down on both natural \""noise\"" and various types of input perturbations."
8277,3," A more complete reference is \""handbook of weighted automata\"" by Droste."
8278,3,Those clarification issues are important to address.
8279,3," \n- the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for \""ord\"" the input is a WxHx1 matrix, but for \""ord\"" and \""cifar\"" the input is a \""WxHx3\"" matrix."
8280,1," The proposed framework, task graph solver (NTS), consists of many approximation steps and representations: CNN to capture environment states, task graph parameterization, logical operator approximation; the idea of reward-propagation policy helps pre-training."
8281,1," \n\nI am very happy to see experimental evaluations on real robots, and even in two different application domains."
8282,2,"Although no ground-shaking breakthroughs are made, it is worthy of publication"
8283,3, I would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method.
8284,1, The paper is easy to follow although it is on an adequate technical level.
8285,3,What would happen if Y is random or the activation is ReLU?
8286,3, Could you comment on this problem?
8287,3," In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric."
8288,3,\n6. SGD also needs to update at least d times for all d latent tensors.
8289,3, The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer.
8290,3,"\"" Not only is it impossible to reproduce a paper without any architectural details, but the\nresult is then that Fig 3 essentially says inputs -> \""magic\"" -> outputs."
8291,1, The representation is clear with detailed empirical studies.
8292,3," \n\nHowever, the formulation is very similar to \""[1] Semi-supervised Question Retrieval with Gated Convolutions\"" 2016 by Lei, and \""Deriving Neural Architectures from Sequence and Graph Kernels\"" which give theoretical view from string kernel about why this type of networks works."
8293,2,The experimental design is a bit funny
8294,1,\n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written.
8295,1,"The idea is straightforward,"
8296,3, The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs.
8297,1, \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight\u2019s learning rate is somewhat novel.
8298,2,"it reads like someone who searched around the literature without much authentic understanding of social science, methodology or statistics"
8299,3,"\u00a0\u00bb\n\u00ab\u00a0 This approach is fast, statistically consistent, and reduces to simple\nlinear algebra operations."
8300,3,"""The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL)."
8301,1, the paper is well-written and the results are strong;
8302,3, Please clarify this in the paper explicitly.
8303,3," For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different."
8304,3, More explanation needed here. 
8305,3," Given this conclusion, I would have expected a discussion about \nthe difference between learning with all data at the same time or with fine-tuning in two different \nsteps."
8306,1,"\n\nThe consistency properties are nice,"
8307,3," Why is Genz & Monahan 1998 better than other alternatives such as Monte-Carlo, QMC etc?"
8308,1, but the impact may be.
8309,3, \n\nAlso note that multidimensional CLT here is glossed over.
8310,1,"""The paper addresses the problem of tensor decomposition which is relevant and interesting."
8311,3," Also, I guess we are assuming the obj is strongly convex?"
8312,3, The experiments demonstrate that the proposed algorithm are competitive with standard backpropagation and potentially faster if code is optimized further.
8313,3," The technique consists of first training a model by constraining its trace norm, which allows it to be well-approximated by a truncated SVD in a second fine-tuning stage."
8314,3," but there is a number of ways in which it can be improved, detailed in the comments below."
8315,3,\n\nThe use of a non-parametric definition for the activation function should be contrasted with the use of a parametric one.
8316,3,"  Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences)."
8317,3,\n\nI would agree that the idea of using dilated CNN (w/ residual connections) instead of BiLSTM could be a good solution to many online NLP services like document-level classification tasks.
8318,3,\n- The introduction also states that the authors will propose ways to improve image captioning.
8319,3," The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting."
8320,3," \nFinally, in order to corroborate the quantitative results, authors should have reported some visual experiments in order to assess whether a change in c_j really correspond to a change in the corresponding factor of variation z_i according to the learnt monomial matrix."
8321,3,"\n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL."
8322,3,"  However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification)."
8323,3," If it is purely a scaling factor, how is the scale quantified? "
8324,1,"To my knowledge, the explanation of universal perturbations in terms of positive curvature is novel."
8325,1,. \n\nClarity: The paper is well-written and clear
8326,3, \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic.
8327,3, Then apply any classifier trained on the true distribution on the resulting x* = G(z*).
8328,3, The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.
8329,3, I suggest to put more information into its caption.
8330,3,"""The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach)."
8331,3," The authors showed that across the test images, they were able to perturb the ordering of the training image influences."
8332,3,"""The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix."
8333,3, \n\nCan you clarify the contributions of the paper in comparison to the R3NN?
8334,3," So instead of showing a quick \u2018hack\u2019, I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided.\n"""
8335,3," \n\n\nIn a couple of places early in the paper, you mention that the neural net computes \u201cthe probability\u201d of examples."
8336,3,  But in fact later they admit that the problem of optimizing the alignment is a non-convex problem and the authors end up with a couple of heuristics to deal with it.
8337,3,  My question here is \u2014 are gradients being re-used intelligently as suggested in Section 3.1?
8338,1," That connection might be worth discussing because removing the regularizing (i.e., metabolic cost) constraint from your RNNS makes them learn representations that differ from the ones seen in EC."
8339,3," Does it happen on this dataset only or it is the case for many datasets? """
8340,3,"""The main issue is the scientific quality."
8341,3,"\n5.\tIn the third phase of data collection (Paraphrase RC), was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2, or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3?"
8342,1," This approach is useful even when there is only weak supervision to provide the \""similarity/dissimilarity\"" information."
8343,3,"""The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel."
8344,3, It would be useful to clarify whether this is happening.
8345,3," The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching."
8346,3," To me the reported performance plateau with number of map size is normal expected behavior, only symptomatic that this deep RL agent has finite capacity."
8347,1,"  The idea of using class label to adjust each weight\u2019s learning rate is interesting and somewhat novel,"
8348,1,"\nPros: interesting idea, relevant theory provided, high-quality experiments"
8349,3," However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions."
8350,3,  Experiments search for effective CNN architectures for the CIFAR image classification task.
8351,3," On top of that, I feel the execution of the paper leaves much to be desired. \n"""
8352,1,\n\nPros:\n- The paper is nicely written and good to follow.
8353,1,. The author should also compare this extension of VAECCA.
8354,3, Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset.
8355,3,"""The main concern of this submission is the novelty."
8356,3,\n- Eqn 11 should use \\simeq.
8357,1,"   \n\n\nAnyway, I believe the paper is interesting and the authors are exposing interesting facts that might be worth to spread in our community, so I rate the paper as slightly over the acceptance threshold."""
8358,1,"\n\nPros:\nSimple, interesting idea\nWorks well on toy problems, and able to prevent divergence in Baird's counter-example"
8359,3, As is done in the baselines usually.
8360,3, The proposed method introduces hyper-hyperparameters which may be hard to tune.
8361,3," It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position."
8362,1,"\n\nPaper is mostly very clearly written,"
8363,1, The model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed before.
8364,3," In order to make sure the second \nequality in Equation 2 holds, p_mu (y|x,t) = p_pi (y|x,t) should hold as well."
8365,3,\n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.
8366,3," SAP does not perform as well as adversarial training, but SAP could be used with a trained network."
8367,3," This is presented in the same figure and paragraph as the CIFAR results,"
8368,2,"Who are and where did they come from? That is, why was this obsucre and arbitrary method chosen?"
8369,1,"  Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good."
8370,2,I found every single reading of every theorist mentioned in this article seriously wanting
8371,1,"n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong."
8372,3," Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015)."
8373,1," \n\nOverall, I think this dataset is very useful for RC."
8374,3, though I have a few questions below that prevent me for now from rating the paper higher.
8375,3," The baseline seems to be best performing on \""all cars\"" and \""non-red cars\""\n\nIn order to be at an appropriate level for any publication the experiments need to be much more general in scope.\n"""
8376,1,\n- Intensive experiments to validate the performance.
8377,1," Some findings in this submission indeed look interesting,"
8378,3, (b) why only slight overlap is used in practice?
8379,3," However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge."
8380,3,\n- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity
8381,1,"\n\nExperiments indicate that the  approach is most successful when the regularized RNNs are conditional generators, which emit sequences of low entropy, such as decoders of a seq2seq speech recognition network."
8382,3," As noted in the paper, the generation of syntactically and semantically valid data is still an open problem."
8383,3,\n\nMinor:\n- What is the difference between LSTM and left-branching LSTM?
8384,3,"\n\n- Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer?"
8385,3,"\nMore arguments are desirable for the advantage of this paper, i.e. quantitative\nevaluation of diversity of generated text as opposed to LSTM-based methods."
8386,3, Isn't there any existing benchmark where this could have an impact?
8387,3," FDA is the research field that formulated the ideas about the statistical data analysis of data samples consisting of continuous functions, where each function is viewed as one sample element."
8388,3," \n\nTo improve readability, the authors should propose a diagram of the network, summarizing all notations"
8389,3,"""This paper deals with the problem of learning nonlinear operators using deep learning."
8390,3," For examples of such methods, one may see the paper \""Transform Invariant Auto-encoder\"" (by Matsuo et al.) and references therein."
8391,3,"\n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger."
8392,1,"\n\nOverall, the paper only has a few interesting observations,"
8393,3,"\nThe method is derived by considering the \""rewiring\"" of an (artificial) neural network as a stochastic process."
8394,1,"\nExperiments show that on the navigation task, the proposed approach outperforms\na variety of baselines under both a normal data condition and one requiring\nstrong generalization."
8395,3," However, the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernels."
8396,3, The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region.
8397,3,"\n\nTo learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm."
8398,3,"""The paper proposes a modified approach to RL, where an additional \""episodic memory\"" is kept by the agent."
8399,3,"""The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis."
8400,1," I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score."""
8401,3,\n\nI think finding analogies that are not exact matches is much more compelling.
8402,3, The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.
8403,3," Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed."
8404,3,"\nThis \""hybrid\"" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time."
8405,3,"\"" I would encourage the authors to revisit Le and Zuidema (2015), especially section 3.2, and consider the technical innovations over the existing work."
8406,3,"\n\nHowever, I hope that you address some of the concerns I have raised in this review."
8407,2,You have two many misprints
8408,3, The loss on \\pi_S should be made explicit.
8409,3, There is an entire research field about closing the reality gap and transfer learning.
8410,3,"Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization)."
8411,1,\n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\
8412,3," A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear."
8413,3, Is it because it does not achieve SOTA?
8414,3," While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like \u201c\\lambda_rec\u201d and \u201c\\lambda_norec\u201d"""
8415,1," \n\n While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. "
8416,2,"This is a nicely done paper but the sample is small, the measures uninformative and the findings have only very weak relevance to policy."
8417,1,"Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees."""
8418,1," As this is a mostly theoretical paper, pushing experimental results to the appendix does make sense,"
8419,1,\nIt is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt.
8420,3,"\n\n* Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high."
8421,1,\n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning.
8422,1, The paper is well written
8423,3,"\n\nComparison with predicted parses by Spacy are by no means \""gold\"" parses..."
8424,1, \n\nQuality: The empirical results (including a video of an actual robotic arm system performing the task) looks good.
8425,3,"\n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance."
8426,3, The evaluation with the sequence of checkpoints was created by using every fifth image.
8427,3, I assume this is an artifact of the way the goal recognizer is trained.
8428,1," - except for MultiCopy where it trains faster,"
8429,1,"It is easy to follow, and succinct while being comprehensive."
8430,3,"\n\nEquation (2) shows that the learnable filters g are operating on the k-th power\nof the normalized adjacency matrix A, so when K=1 this equals classical GCN\nfrom T. Kipf et al."
8431,1, It also provides some sufficient conditions for the non-linear cases.
8432,1, It is a novel setup to consider reservoir sampling for episodic memory.
8433,3," It's well accepted that in any partially observed domain, inclusion of the latent variable(s) as a part of the agent's observation will result in a fully observed domain, less state-aliasing, more accurate value estimates, and better performance."
8434,3,"\nThe presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.\nThe effect of using the faster approximation on performance also remains to be investigated."
8435,3,"""The main goal of this paper is to cluster images from classes unseen during training."
8436,1, \n\nI must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task). 
8437,3," In particular, the authors adopt the Wasserstein distance to define the ambiguity sets."
8438,1," If no LM was used, then the choice of reporting results in terms of only CER is reasonable,"
8439,3," \nFor this purpose, authors employed a reinceforcement learning approach to\noptize a prediction from masked text."
8440,3," \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term."
8441,3," Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc."
8442,3,\nOptimizing the PIB objective is intractable and the authors propose an approximation that applies to binary valued stochastic networks.
8443,3,"In this method, two new types of kernels are developed, namely the spatial-wise and channel-wise sparse-complementary kernels."
8444,3," It feels rather contrived to focus so much on the datasets with exact matches since,;"
8445,3, This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true. 
8446,3," Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation."
8447,3,"\n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss)."
8448,3, I would have expected the same runtime.
8449,1, \n\nOriginality: This is an original approach.
8450,3,   Why is this.
8451,3, \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline?
8452,3, But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method.
8453,2,The discussion is inappropriate and the new content is generally poorly written
8454,1,\n3. Experimental results.
8455,3, Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data.
8456,3,\nBy using K > 1 the method is able to leverage information at a farther distance\nfrom the reference node.
8457,3, The work would be stronger if the authors can extend this to higher dimensional time series.
8458,3," Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks."
8459,3," Therefore, the authors derived the update formulate based on the analytical continuation technique."
8460,3, Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers.
8461,3,". Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal."
8462,3,"""This paper presents a neural architecture for converting natural language queries to SQL statements."
8463,3," In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions."
8464,3,"""This paper presents a nearest-neighbor based continuous control policy."
8465,1,In all experiments CCC-based agents fare better than agents operating based on a specific strategy.
8466,1, The results seem like they may be very important.
8467,3, When that is done it would make an exciting contribution to the community.
8468,3," The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA."
8469,3, I.e. are we talking about a space of 10 total classes or 10000 total classes?
8470,3,"""The paper proposes a method for few-shot learning using a new image representation called visual concept embedding."
8471,3, I was aware that the features mostly refers to the inputs to softmax.
8472,3, Some exploration of this issue or commentary would be valuable.
8473,1," The proposed method presents a possible way of better modeling the future,"
8474,1," I find it nice how they benefited from context (left context and right context) by solving a \""fill-in-the-blank\"" task at training time and translating this into text generation at test time."
8475,3, Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act.
8476,3,\n\n---\n\nThe authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey.
8477,3,"""This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets."
8478,3,"\n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W."""
8479,3," Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly."
8480,3,\n\n5. Figure 4: the authors should show the same plot for more convolutional layers at varying depth from both VGG and ResNet.
8481,3, There should be some theoretical and/or empirical study of its effect on quality of the solution.
8482,3,"""The paper describes learning joint embedding of sentences and images."
8483,2,"So, what is the point of this?"
8484,2,Please also consider making the submission looking less like an advertising booklet rather than a research paper
8485,3,"\n\n4. In section 3.2, it may be clearer to explicitly point out the use of the \""3-sigma\"" rule for Gaussian distributions here."
8486,1,"\n\nOverall the paper is a descent one, but with limited novelty."
8487,1,n\nThe paper presents an interesting approach which achieves good performance.
8488,3, Grounding the proposal with some theoretical or conceptual arguments could have made the proposal sounder.
8489,3," I expect these updates to be reflected in the final version of the paper itself as well. """
8490,3," For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline."
8491,1,\n\nPros:\n1. The required time for architecture searching is significantly reduced.
8492,3, \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.
8493,2,"Publication of this paper will not advance our knowledge in any shape of form, it will just result in other researchers pointing out how bad this study actually is"
8494,3,\n\n- A similar classification of collaborative filtering models with covariates is proposed in this thesis (p.41):\nhttps://tspace.library.utoronto.ca/bitstream/1807/68831/1/Charlin_Laurent_201406_PhD_thesis.pdf
8495,1," While the measure shows interesting trends towards a linear behaviour for simpler methods,"
8496,3,\n\nReferences issues:\n- harmonize citations: if you add first name for some authors add them for all of them: why writing Harold W. Kuhn and C. Vilani for instance?
8497,3,"\n[5] Hausknecht, M., Lehman, J., Miikkulainen, R., & Stone, P. (2014). A neuroevolution approach to general atari game playing."
8498,1,2014\n\nPros:\n- Propose a sound approach to mix two complementary strategies for domain adaptation.\n- Great empirical results.
8499,3, What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?
8500,3," Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model."
8501,3," If so, this should be stated explicitly."
8502,3," Here's one reference (others should be easy to find): \""SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs\"" by Boehm et al, IEEE Data Engineering Bulletin, 2014."
8503,1, The results in Section 5.2 seem to make a lot of sense and\n  show the big contribution of the model.
8504,1,  This seems reasonable.
8505,3,"   I think the right citation is \u201cIncorporating invariances in support vector learning machines\n\u201c Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 \u201cTraining invariant support vector machines.\u201d ""."
8506,1, \n\nComments:\n\nThe findings suggest the effectiveness of that approach. 
8507,3," While it is better than a \nconterpart of MLE (MaskedMLE), whether the result is qualitatively better than\nordinary LSTM is still in question."
8508,3,\n\ncons\n- please provide the value of the diffusion coefficient for the sake of reproducibility
8509,3,"   From what I can tell, Hazan's paper introduces the idea of wave filtering (convolution of the input with eigenvectors of the Hankel matrix); the filtered output is then passed through another matrix that is being learned online (M)."
8510,3,\n[====================================END OF REVISION ================================================]\n\nThis paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning.
8511,2,There is so much that is wrong with this paper that it is difficult to know where to start
8512,3," In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters."
8513,3, It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores.
8514,3, Explicitly mentioning this may help the reading grasp the formulation.
8515,3,"\n\nIf the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal."
8516,3, This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations.
8517,3, And quantify spatial tuning somehow (a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations).
8518,3, The number of state updates that the model learns to use can be controlled with an auxiliary loss function. 
8519,3," It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing."""
8520,1,"\n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented."
8521,3,\n-- Details of the validation sets used to tune the models.
8522,3," Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected."
8523,3, The authors mention that word dropout can be considered as its special case which randomly drops words without any prior.
8524,3, It is only mentioned that the data is augmented with translations and horizontal flips.
8525,3, This would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the Tucker2 representation to be removed by the preprocessing.\n\n\n\n
8526,1,"\n\nOverall, I would argue that this paper is a clear accept."""
8527,3,"\n\nFigure 6: Looking at those 2D PCAs, I am not sure any of those method really abstracts the rendering away."
8528,3, While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary.
8529,1,\n\nExperiments: appreciated the wall clock timings.
8530,3,"  The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior."""
8531,3,"\n- Figure 2: can you plot the various transition types (normalized, un-normalized, ...) in the plots?"
8532,3,"""The paper studies the global convergence for policy gradient methods for linear control problems."
8533,1," Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset."
8534,1,\nExperiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables.
8535,3,"\nFinally, it would be nice to see where the algorithm falls short, and where there is room for improvement."
8536,3, A baseline of regular Q-learning should be included for these simpler domains.
8537,1,"n\nThe experiments are pretty robust, and they show that their method is better than the proposed baselines"
8538,3," How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW"
8539,1,\n\nI have some question and some consideration that can be useful for improving the appealing of the paper.
8540,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
8541,3,"""The paper presents a multi-task architecture that can perform multiple tasks across multiple different domains."
8542,3," Although DCN+ is an improvement of DCN, I think the improvement is not incremental."
8543,3,"\n\nPros and cons:\nPros: the method is simple to implement, the paper lists for what kind of datasets it can be used."
8544,2,and I'll refer to you as the author despite the effort you have put into ensuring that I know who you are...
8545,3,\n(c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task.
8546,3,"\n -  would any 7-7 split work just as well (ie, cross-validate over the 14 domains)"
8547,3,  The characteristics and theoretical analysis of the proposed method are discussed and explained.
8548,1,"\n\nWhile the strength of this paper is clearly the good writing as well as rigorous experimentation,"
8549,3,\n\nTheoretical analysis is presented to show the performance of any selected subset using the geometry of the data points.
8550,3, Only the topology of the cells themselves are designed.
8551,1, but the original contributions are adequately identified and they are interesting on their own.
8552,3,"  Is that what is meant by the \""defense network\"" (in experiments bullet 2)?"
8553,3," Given these previous works, the contribution and novelty of the paper is limited."
8554,3," The authors may hence consider\nimproving the LP relaxation, noting that the big-M constraint are notorious\nfor producing weak relaxations."""
8555,3," \n    In section 2.1 the authors mention that facial landmarks have been detected using a 'pre-trained ensemble-of-regresion-trees detector (Gerbrands, 1981)'."
8556,3," \n\n2, In the paper, there is an assumption about the peak of random feature \""it is a natural assumption on realistic data that the largest peaks are close to the origin\""."
8557,3, but not convincing enough.
8558,3," \n\nPresumably the linear Wk transform helps us pick out the important dimensions of similarity between S and Ej.\n\nMapping the learner and expert directions into subgoal space using Wv, the heuristic reward is\n\n   Rh = B <   Wv (S-S\u2019),  \n                    Wv SUMj  < Wk S, Wk Ej > ( Ej - Ej\u2019 ) >"
8559,3, The key idea is that the agent learns a shared representations for tasks with different visual statistics
8560,2,"This article describes [XY]Â, a research project investigating methods for game design. The paper is poorly written [full review"
8561,3, The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.
8562,3,"""Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights. "
8563,3," These two papers need to be cited:\n\nRudolph et al., NIPS 2017, \""Sturctured Embedding Models for Grouped Data\"":"
8564,3,"\n \nFurthermore, I would want to question the practical usage of having an 'even faster' method for generating adversarial examples."
8565,1,"\nThe experimental section is extensive, and offers new insights into both the presented algorithm and baselines."
8566,3," There is nothing wrong with the fundamental idea itself,"
8567,3," \n\n- in page 2, what do the numbers mean at the end of each sentence? Probably the figures? "
8568,3," The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \""expected policy gradient\"" by Ciosek & Whiteson."
8569,3," They  learning a linear mapping from the latent code to an idealized set of disentangled generative factors, and then define information-theoretic measures based on pseudo-distributions calculated from the relative magnitudes of weights."
8570,3," They empirically test this out for networks 50 layers deep and 100 layers wide, where they find that some architectures have exploding gradients after random initialization, and others do not, but those that do not have other drawbacks."
8571,3,"  The authors describe these confidences variously as: \""some notion of confidence that the agent has in the value function estimate\"" and \""weighing the returns based on a notion of confidence has been explored earlier (White & White, 2016; Thomas et al., 2015)\""."
8572,3,\nThe method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided.
8573,3," Their architecture is a deep feedforward network where each layer takes as input two images: one corresponding to the original maze (a skip connection), and the output of the previous layer."
8574,1, The experiment results seem solid and the proposed structure is with simple design and highly generalizable.
8575,1,"\n\nMinor comments:\n- P.1, L.5 in the third paragraph: architexture -> architecture[[CNT], [CLA-NEG], [CRT], [MIN]]\n- What does \""Cor\"" of CorGAN mean?[[CNT], [null], [QSN], [MIN]]\n\nAFTER REVISION\nThank you to the authors for their response and revision."
8576,3," Could you provide some further comments on this?\n\n"""
8577,3, Does the solution obtained with the optimization can be run as efficiently?
8578,1,"""\nThe paper was fairly easy to follow,"
8579,2,The figure is a mystery to me. Where did all this sound energy go? It defies the basic laws of physics.
8580,3,"""SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent."
8581,1,\n* The relation between the vertex and spectrum domain is well elaborated and nice (although neither important for understanding nor implementing the approach).
8582,3, The authors experimented on the UCI data sets with different activation functions to show the efficacy of their proposed method.
8583,3," It is known that even basic multi-layer perceptrons (MLPs) result in much lower classification errors, e.g., for MNIST. LeCun et al., 1998, is a classical example with less then 3% error on MNIST with many later examples that improve on these."
8584,1," However, the weak convergence results are good."
8585,1,"""Summary: \n\nI like the general idea of learning \""output stochastic\"" noise models in the paper,"
8586,3, Does higher extractiveness correspond to higher or lower system ROUGE scores? 
8587,3," From the adversary's standpoint, it would be easier to manipulate inputs than latent variables."
8588,1,"\n  This isn't necessarily a problem, it's fine for models to not do everything well,\n  but it's a stretch for the authors to claim that these results are a positive\n  aspect of the model."
8589,3,"  I am including this additional information in my review, for use by the area chair, but otherwise leaving my assessment as-is.\n"""
8590,3, \n\nThe method proposed by the authors is very similar to Lipton's approach.
8591,3, And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).\
8592,3,Are the choices of decoder etc. similar?
8593,3,"""This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization."
8594,1,\n\nReview: \nI very much like the paper.
8595,3," by 2) reproducing the results in the same way as in the original work,"
8596,3,"First of all, \""neural networks are good at generalizing to examples outside their train set\"". This depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample space."
8597,3,\n\nThe suggested methods are empirically evaluated on a number of settings.
8598,3," Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity?"
8599,3," In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN."
8600,2,It is clear that this manuscript will not win a beauty contest.
8601,1,"\nThis line of work is specially relevant since it attacks one of the main bottlenecks in learning complex tasks, which is the amount of supervised examples."
8602,2,The conclusion is something of a shaggy dog.
8603,3," Examples include \""QUASAR: Datasets for question answering by search and reading\"", \""SearchQA: A new q&a dataset augmented with context from a search engine\"", and \""Reinforced Ranker-Reader for Open-Domain Question Answering\""."
8604,3, \nAuthors refer to prior work for methods to learn this backbone model. Liu et.al (http://www.cse.ust.hk/~lzhang/ltm/index.htm) and Chen et.al. (https://arxiv.org/abs/1508.00973) and (https://arxiv.org/pdf/1605.06650.pdf).
8605,3,"\n\n- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation?"
8606,1, Experiments were conducted on various data sets and experimental results were reported.\n\nPros:\n* Studying semi-supervised learning techniques for deep models is of practical significance.
8607,3, Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers. 
8608,3, There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works.
8609,3, The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples.
8610,2,The authors should completely revise it after completing the study and by following common scientific standards
8611,3,  \n* I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now.
8612,3,"""The paper proposes a method  for learning object representations from pixels and then use such representations for doing reinforcement learning."
8613,1,"\n\nWhile the method is of interest, there are more recent mutation callers that should be compared."
8614,3,\n- What happens when we use adversarial attacks different from FGSM?
8615,3,"  While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters."
8616,3, The experimental results show that the model is able to achieve the state-of-the-art performance on CNN/Daily Main and New York Times datasets.
8617,3," Why is it reasonable given: (i) the challenge in defining appropriate rewards (i.e., it's not clear to me what would constitute the right reward for this problem);"
8618,3,\n\nA key element in PSRNN is to used as an initialization a kernel ridge regression.
8619,3,The recommended length is 8 pages + 1 page for citations.
8620,3, it is hard to conclude much about under what circumstances one representation or model is better than another.
8621,3,"   The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \""fidelity\"" of the weak label when training the student at the final step."
8622,1, \n\nOriginality:  Training very deep networks have always been important in Deep Learning and the idea of using class label to adjust each weight\u2019s learning rate is somewhat novel.
8623,3,  How do you\n  explain that it converges to a an objective value that is so much worse?
8624,3,\n\n4. Why not comparing to Bootstrapped DQN since the proposed method is based on it?
8625,3,"""Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning. "
8626,3," \n\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments."
8627,3,"\n\nFor generative modeling on MNIST, \""784d vector\"" is less clear than \""784-dimensional vector\""."
8628,3," \n\n[1] Kulesza, Alex, and Ben Taskar. \""Determinantal point processes for machine learning.\"" Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012): 123-286."""
8629,3, The correction requires only two extra multiplications per model parameter and update step.
8630,3,  Max pooling on graph is done by using Graclus multilevel clustering algorithm.
8631,3," The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule."""
8632,3," Given N inputs and output, this method allows one to specify a linear transformation with O(log(N)) parameters, and perform a forward and backward pass in O(Nlog(N)) time."
8633,3, Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks.
8634,1, both method and experiments are presented well. 
8635,1, The idea to represent task graphs are quite interesting.
8636,1,"\n\nIn general, the paper proposes an idea to tackle an interesting problem."
8637,3," For instance recently Wu et all in ICCV2017, bu also many other papers."
8638,3, A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets.
8639,3, This is demonstrated in comparison to weight normalization in Figure 4.
8640,3, (mixture pointer mechanism + REINFORCE)
8641,3, \n\nThe mathematical basis for this paper is actually introduced in [3] and a single-layer version of the current model is developed in [4].
8642,1,  The idea is simple and well explained.
8643,3,"n\nI am looking forward to the test performance of this work on SQuAD."""
8644,3,"""The paper claims to develop a novel method to map natural language queries to SQL."
8645,3,"  However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area."
8646,3, I had a number of clarification questions spefically on this section:\n- Am I correct that the results in this section do not use the trace-norm regularization at all?
8647,3," On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet)"
8648,1,"\n\nI like the idea of learning an \""output stochastic\"" model -- it is much simpler to train than an \""input stochastic\"" model that is more standard in the literature (VAE, GAN) and there are many cases where I think it could be quite reasonable. "
8649,1," Though it makes sense that your regularization might lead to a better estimator, you don't seem to have shown so either in theory or empirically."
8650,1," The discussion is largely based on a sequence of experiments, some of which are interesting and insightful."
8651,3,  The connexion to PSRNN is very tenuous since the main results are about the regression part.
8652,2,The findings are not novel and the solution induces despair.
8653,3, It is very unlikely that these\ndefaults are optimal across the different benchmarks.
8654,3,"  In more typical HCI experiments the gender distribution and ages ranges of participants are specified as well as how participants were recruited and how the game was motivated, including compensation (reward) are specified."
8655,3,  First the tasks it was tested on are very simple -- 2 synthetic tasks plus 1 small-scaled task.
8656,3, It is a reasonable idea and authors indeed show that it works.
8657,3," For the mobile robot, is the robot learning some form of traversability affordances, e.g., recognizing actions for crossings, corners, and obstacles?"
8658,3,"\n- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354)."
8659,3,\nThe latent variables plus a one-hot-encoding representation of the relation is used to reconstruct the input entities.
8660,3, Why is this an interesting problem?
8661,3," Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?"
8662,3,"\n\n- The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t."
8663,3," Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS."
8664,3, Face mesh graph is represented by Fourier basis of graph Laplacian and therefore convolution operator is defined in Fourier space.
8665,3,"""OVERVIEW: The authors present results from several state-of-the-art generative models trained on a facial dataset for learning a general facial identity space."
8666,3,"\n\n(3) In Section 1.2 (Tracking a known system): \""given\"" instead of \""give\""[[CNT], [CLA-NEG], [CRT], [MAJ]]\n\n(4) In Section 1.2 (Optimal control): \""symmetric\"" instead of \""symmetrics\""[[CNT], [CLA-NEG], [CRT], [MAJ]]\n\n(5) In Section 1.2 (Optimal control): the paper says \""rather than solving a recursive system of equations, we provide a formulation of control as a one-shot convex program\"". Is this meant as a contrast to the work of Dean et al. (2017)?"
8667,3,\n\n\nClarifications:\n- See the above mentioned issues with the exposition of the technique.
8668,3,How about cases when computing Z is intractable?
8669,2,This sentence is so hard to digest it gave me reflux
8670,3, (or perhaps does it not matter due to the convolutional structure?)
8671,3,"""Summary:\n\nIn this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA)."
8672,1," If the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the Stanford Question answering dataset or TriviaQA."
8673,3,"""The paper introduces a novel method for modeling hierarchical data."
8674,3, What are the class you are interested in?
8675,2,"This paper is definitely not suitable to PNAS, or, for that matter to any other journal."
8676,3,"\n\nQ: For the multiple outputs, the k neighbor is selected at random?\n"""
8677,3, I think this information would be very useful to the community in terms of what to take away from this paper.
8678,3,"""Summary of the paper: The paper analysis how well difference target probation (DTP) - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like CIFAR-10 and ImageNet. "
8679,3, What happens if you start with random vectors?
8680,3,"\n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?"
8681,3,"""In this paper, the authors propose to have a different learning rate depending on the class of the examples when learning a neural network."
8682,3,"page 5: \u201cthe the degree\u201d , \u201cspecified as (\u2026.) followed by\u201d -> , \u201cas (\u2026.) followed by\u201d ?,\n- This notation probably stems from the code, but SAME and VALID could be nicer described as \u201c0 padding\u201d and \u201cno padding\u201d"
8683,3, Can authors provide more intuition on these precise definitions?
8684,3,"\n\nCurrent approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval."
8685,1,I buy the premise and think the work addresses an important issue.
8686,3,"  Assumption 1 alludes to this but doesn't specify what is \""small\""?"
8687,3, The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically.
8688,3," For example, if we use the single shared question embeddings, but increase the size from 128 to some larger number like 192, can we observe similar improvement."
8689,1," \n\nIn general, this is an interesting direction to explore, the idea is interesting,;"
8690,1, The approach is methodological similar to using expected likelihood kernels.
8691,3, The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN.
8692,3, but one problem is that how to obtain the proper solution of equation (3)?
8693,3," As discussed above, it is encouraged to elaborate other potential causes that led to performance differences."
8694,1,"\n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1."
8695,3," Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction."
8696,3,"Note\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say."""
8697,3, I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper.
8698,1, I am looking forward to seeing work on the research goals outlined in the Future Directions section.
8699,3, Have the authors considered lifting this restriction for classification and if so does performance improve?
8700,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
8701,3,"""This paper presented a multi-modal extension of variational autoencoder (VAE) for the task \""visually grounded imagination."
8702,1, The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods.
8703,3," The main technical contribution is the search procedure to find minimal training sets and pare down the observation size, and the empirical validation of the idea on several algorithmic tasks."
8704,1,\n\nI found the paper interesting
8705,3," The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next."
8706,3,"  However, I think it would make much more sense to mention how their work relates to other neural network architectures which learn algorithms (such as the Neural Turing Machine and variants) or reasoning tasks more generally (for example, memory-augmented networks applied to the bAbI tasks)."
8707,3, \u201c - The paper you give exactly solves for the nonlinear function approximation case.
8708,3, \n\n\nMinor details:\nPersonally I\u2019m not a big fan of abusing colons (\u201c:\u201d) instead of points (\u201c.\u201d).
8709,3," The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations. "
8710,3,"\n\nThe main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e.,"
8711,3," However, it is really nice to be able to decouple the hidden size and the number of recurrent parameters in a simple way."
8712,1, The two limits (sampling from marginals instead of optimal coupling and differentiability of the critic) are interesting and indeed suggest that one can do better but the examples and observations are well known in OT and do not require proof in appendix.
8713,3,"  It seems that this\napproach only works due to the peculiarities of the formulation of the only task that is considered,\nin which the program maps a pixel location in 32x32 images to a binary value."
8714,3, Does the model try to reconstruct the state of the current time-step or the future?
8715,3," For example, if we are minimizing the changes our update will make to the value of the next state, what would happen if the next state is equivalent to the current state (or equivalent in our feature space)? "
8716,1,"\n\n3. Based on the experiment results, this proposed method outperformed previous methods (TD-GAN, IcGAN)."
8717,3," So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the \""classification\"" function is pushed down to lower layers, as the upper layers are reduced in size."
8718,1, \n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations.
8719,3,"\n\nMinor comments:\n\nIn appendix C, Table 4 caption: you say target sentence is \u201cTrg\u201d but it is \u201cRef\u201d in the table."
8720,3,\n\nAnother major concern is that the technical contributions of the proposed model is quite limited.
8721,3," Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) . "
8722,3," Typically this problem is split into a (non-convex) system ID step followed by a derivation of an optimal controller, but there are few guarantees about this combined process."
8723,3,\n\nPros:\n- Generating programs with neural networks is an exciting direction
8724,3, The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.
8725,3, The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.
8726,3, \n* Section 3.1. Constraints can be introduced to impose structural properties of the generated graphs.
8727,3," The authors should analyze if this also holds true for ResNet and Inception, which are more widely used than VGG16."
8728,3, The wording here was confusing.
8729,1, By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).
8730,3,\u201d The conclusion is obvious when we train the network to make it invariant to colors and textures.
8731,3," \n\nUtilizing a trained GAN, the authors propose the following defense at inference time."
8732,1,\n- Training and verification sets are automatically generated by the proposed method.
8733,3," That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level"
8734,1,"\n\nDespite these questions, though, this paper is a nice addition to deep learning applications on software data and I believe it should be accepted.\n\n"""
8735,3," Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think. \n"""
8736,3," I do agree now on Figure 5, which tips the scale for me to a weak accept. "
8737,3,"  Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n"""
8738,3,\n\nThe matrix completion approach is based on robust PCA.
8739,1," On the positive side, the paper is mostly well-written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks."
8740,3," My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity."
8741,3,"""The authors introduce a novel novel for collaborative filtering."
8742,1, In that sense this paper could be useful to researchers wanting to better understand this field.
8743,3, What exactly makes them dynamic?
8744,1," The method leads to better performance than using no external resources,"
8745,1,\nThis is a significant result in language modeling and a milestone in deep learning reproducibility research.
8746,3,"""This paper proposes a (new) semantic way for data augmentation problem, specifically targeted for one-shot learning setting, i.e. synthesizing training samples based on semantic similarity with a given sample ."
8747,3,The robot learns inverse and forward models through autonomous exploration.
8748,2,"This is such a promising topic, but I was very disappointed in this paper combining a substantial amount of author talent."
8749,1,"\n\nPros: new and interesting result, theoretically sound."
8750,1,"\nIt is true that the authors use strong regularization techniques (drop out, external knowledge of words embedding...)."
8751,3," In that case the soft ordering could\n  actually learn the optimal depth as well, repeating identity layer beyond the option number of\n  layers."
8752,3, \n\nThe proposed DAuto is essentially DANN+autoencoder.
8753,3, The kernel is fixed.
8754,3,"    \n\nLastly, the authors evaluate the learned model on link and node prediction tasks and state that the model's so-so performance supports the claim that the model can generalize."
8755,3,"\n\n2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out. "
8756,1," \n\nWhile most of the paper is well written,"
8757,3," What the authors call \""intelligent mapping and combining system\"" for the proposed system is simply a fully connected neural network.[[CNT], [null], [DIS], [GEN]] Such systems have been largely investigated in the literature."
8758,3,"  I will suggest to give a better intuition of the whole idea and move a lot of mathematical details to the appendix.  \n"""
8759,3, There needs to be a cohesive story that puts the elements together.
8760,3," We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters."
8761,3," One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy."
8762,3,\n\nCaptioning Model and Table 1\n- The authors use greedy (argmax) decoding which is known to result in repetitive captions.
8763,3,"\n\nThe explanation of the cause of \""super-convergence\"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments."
8764,1,\n\nOverall the paper is well-written and quite clear.
8765,3, \nThe authors use deep Q learning from Mnih et al 2015 to learn their optimal policy.
8766,3,"\nReferences:\n[1] Vincent Vanhoucke, Andrew Senior, and Mark Mao, \u201cImproving the speed of neural networks on cpus,\u201d in Deep Learning and Unsupervised Feature Learning Workshop, NIPS, 2011.\n[2] Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin, \u201cOn the efficient representation and execution of deep acoustic models,\u201d Proc. of Interspeech, pp. 2746 -- 2750, 2016.\n\n9. Minor comment: The authors use the term \u201cwarmstarting\u201d to refer to the process of training NNs by initializing from a previous model."
8767,3,\n4.) A recreation of a similar problem in the machine translation context.
8768,3,\n\n6. How does the narrator choose which word to obfuscate?
8769,3," Also, could authors please throw some light on why this might be happening?"
8770,1," The observations are interesting, despite being on the toyish side."
8771,3,"""This  paper is  on ab important topic : unsupervised learning on unaligned data."
8772,3,\n \n\u201cThis paper unifies both issues\u201d sounds very weird.
8773,3, Earlier work on tree kernels (in terms of defining tree distances) may be related to this work.
8774,3,\n- it suggests an interesting connection between a traditional model and Deep Learning techniques\
8775,3," The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings."
8776,3," The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task."
8777,3,That are the functions used?
8778,3, What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)?
8779,3,"Theoritically, it solves the soft attention problems."
8780,3, Showing such results would be more convincing.
8781,3, The belief is that this learning style mimics human learners
8782,1," The method is novel, and the paper is generally well written."
8783,1, Thus the novelty of this aspect of the paper is overstated.
8784,3,"\nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal."
8785,3,"Otherwise, the objective won't be a proper probability distribution."
8786,3,". It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered,e.g. does the proposed sparse learning method converge at the same rate as the others?"
8787,1,The paper is well organized and clearly written.
8788,3," \nThe idea presented in this paper is that instead of performing data augmentation in the image space, it may be useful to perform data augmentation in a latent space whose features are more discriminative for classification."
8789,3,"\n\nQuestions:\n- \""Inventing plausible fine details while preserving identity\"" -- since identity is created and there is no ground truth, where does the line between \""fine detail\"" and \""new identity\"" lie?"
8790,3,"\n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W."
8791,2,The manuscript reads much like an unrevised masters level paper.
8792,3,\n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\
8793,1,I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA).
8794,3,"\n\nSome further points:\n\n- There are several hyperparameters set to the \""standard\"" or \""default\"" value, like Adam's beta parameter and the batch size/BPTT length."
8795,3,"\n\nOriginality: \nThe paper heavily depends on the approach followed by Brutzkus and Globerson, 2017."
8796,2,The rest of this review operates from the assumption that this paper is a sincere attempt at scientific evidence and argument.
8797,3, What is the big picture behind this claim?
8798,1," The authors discover some interesting characteristics of MC based Deep-RL which may influence future work in this area, and dig down a little to uncover the principles a little."
8799,3, The proposed models are learned by imposing a soft constraint on the computational budget to encourage skipping redundant input time steps.
8800,3," Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored."
8801,3," In my opinion, the paper is in a preliminary stage and should be refined."
8802,1," However, only one type of tasks is used."
8803,3," If this is still enough for ICLR, the paper could be okay."
8804,3," As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc)."
8805,1,"\n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach."
8806,1, \n- The experiments show the better behavior of the method compared to adversarial training for domain adaptation
8807,3, The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework.
8808,1, \n\n\n2) Pros:\n+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.
8809,3," If not, I\u2019d be curious as to why."
8810,3,\n\nWhy is the Related Work section at the end?
8811,3," By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level."
8812,3, It investigates what sort of image representations are good for image captioning systems.
8813,3,"\nFor those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not."
8814,3," For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. "
8815,3," However, I would not worry too much about that issue, as the same techniques presented in this paper apply to any weighted linear averaging algorithm."
8816,3,"""This work proposed a reconfiguration of the existing state-of-the-art CNN model architectures including ResNet and DensNet."
8817,3," The paper proposes an actor-critic algorithm, called Normalized Actor-Critic (NAC), based on the entropy-regularized formulation of RL, which is defined by adding the entropy of the policy as an additional term in the reward function."
8818,3, ATreeC is an actor-critic architecture that uses a softmax over TreeQN.
8819,3,"  Secondly, it views SGD with different settings as introducing different levels of noises that favors different minima."
8820,3,"The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it."
8821,1," The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations."""
8822,3, RMN reduces the complexity to linear time for the bAbi dataset.
8823,1,"\n\nOverall, the paper is well-written."
8824,3,"\n\nEven on the SQuAD data, the sentence-level processing seems sufficient: as discussed in this paper about Table 5, the author mentioned (at the end of Page 7) that \""the Conv DrQA model only encode every 33 tokens in the passage, which shows that such a small context is ENOUGH for most of the questions\""."
8825,2,"There is hardly any paragraph (even in the abstract) that is not messy, disorganized, confusing, that does not contain mistakes (some are quite embarrassing), redundancies, abusive shortcuts or discussions that sound absurd."
8826,3,"  \n\nIf convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:\nVARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING"
8827,1, \n\nThere are several things to like about the paper:
8828,1, The experimental results also look promising.
8829,3, I also have the following questions regarding the theoretical contributions:\n\n(A) The authors emphasize the logarithmic dependence on T.
8830,3, But adversarial images should be visually undistinguishable from original images.
8831,3,\n\nBaselines: why is Shin et al. (2017) not included as one of the baselines?
8832,3,  MLE-based methods penalize syntactically different but semantically equivalent programs.
8833,1,"\n\n(2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights."
8834,3," Visual concepts were introduced in Wang et al. 2015, which are clustering centers of feature vectors in a lattice of a CNN."
8835,3,\n\n- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc?
8836,3," However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution."
8837,3," While previous approaches assumes the order of shared layers are the same between tasks, this paper assume the order can vary across tasks, and the (soft) order is learned during training."
8838,3," Indeed the functional model considered in the linear case is very similar to Eq. 2.5 or Eq. 3.2. Moreover, extension to nonparametric/nonlinear situations were also studied."
8839,3,"\nThe subtleness of this work over most other video prediction work is that it isn't conditioned on a labeled latent space (like text to video prediction, for example)."
8840,3,"""The authors present a deep neural network that evaluates plate numbers."
8841,1, The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy.
8842,3, It would be much better evaluated thought a mechanical turk test.
8843,3,\n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution.
8844,1, The performance improvement is expected and validated by experiments.
8845,3," It is shown that through combining the generator architecture and the GAN training objective function, one can learn a foreground--background decomposed generative model in an unsupervised manner"
8846,3,"  The network for the MNIST dataset is similarly only 3 layers deep (input, hidden, output)."
8847,3,"""Summary of paper:\n\nThis work proposes an extension to an existing method (Franceschi 2017) to optimize regularization hyperparameters."
8848,3,"""This paper discusses the application of word prediction for software keyboards."
8849,3,"\n\nThe data-generating process for the considered model, given in Figure 2, seems to be consistent with Figure 1 of the paper \""Domain Adaptation with Conditional Transferable Components\"" (by Gong et al.). Perhaps the authors can draw the connection between their work and Gong et al.'s work and the related work discussed in that paper."
8850,1, It seem to be promising when using transfer learning.
8851,3,\n\nThe approach obtains strong results on the CNN/Daily Mail and NYT datasets.
8852,3,\n----------- EDIT -----------\nAfter reading the publications mentioned by the other reviewers as well as the following related contributions\n\n* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)
8853,2,Having read through this a couple of times I have ended up feeling rather depressed.
8854,3,\n\n3 The authors proposed a path-wise training procedure to reduce memory requirement in training.
8855,3,"\n- the \""blank\"" states do *not* model \""garbage\"" frames, if one wants to interpret them, they might be said to model \""non-stationary\"" frames between CTC \""peaks\"", but these are different from silence, garbage, noise, ..."
8856,2,"You aimed for the bare minimum, and missed!"
8857,3,  Binary tree generated in pooling layers are kept for unpooling layers in decoder network.
8858,3,\n- In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze?
8859,3, How sensible is the algorithm to these hyperparameters?\
8860,3,  It would be good if the authors spent more time developing intuition (through visualization for example) and move some of the more technical proofs to the appendix.
8861,3," The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation)."
8862,3, though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.
8863,1,"\n\nSo, the paper is relevant and well presented."
8864,1,\n\nStrengths\n\n- The proposed model is a generic meta-learning useful for both classification and reinforcement learning.
8865,3,"\nHowever, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks."
8866,3," But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates."
8867,3,"""This paper introduces a fairly elaborate model for reading comprehension evaluated on the SQuAD dataset. "
8868,3,"  Is there any reason this sort of approach wasn't used previously, even though this vein of thinking was being explored for example in the semi-dual algorithm?"
8869,1, The paper is well-written: I enjoyed the mathematical formulation (Section 3).
8870,3,"\""\n - should state in the abstract what your \""notion of generalization\"" for gans is, instead of being vague about it."
8871,1, The approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward.
8872,3, How important is the AE in the loss?
8873,3,"  Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data."
8874,1," as such, I'd recommend the paper for acceptance."
8875,3,\nI would also be great to have intuitions on why a single continuous filter works betters\nthan 20 discrete ones (if this behaviour is consistent accross initialization).
8876,3,\n\nThe paper closely follows Hein and Andriushchenko (2017).
8877,3, The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm.
8878,3,  One way to analyze whether this happens is to predict the identity of the task from the hidden vectors.
8879,2,Uninteresting. Unpublishable. Reject.
8880,1, Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community.
8881,1, or if the success of this paper is mostly due to clever processing of text features using DNNs.
8882,3, Also Maei is the first author on this paper.
8883,3,"\n\nThe key idea is to reformulate the problem as a convex minimization of a \""double-sum\"" structure via a simple conjugation trick."
8884,3,"""The paper proposes a variance reduction technique for policy gradient methods."
8885,1, This search enables the authors to come to convincing conclusions regarding the shortcomings of current models.
8886,3, What are the runtimes?
8887,3,"\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level."
8888,1,"""After reading the authors's rebuttal I increased my score from a 7 to a 6."
8889,2,"The study is poorly conceived, inadequately conducted and the conclusions made by the authors do not necessarily follow from the results"
8890,3, It is not clear why that is not used or at least compared to the method presented.
8891,3," Question for the authors: in the limit of infinite training\ndata and model capacity, will the neural network training lead to a model that will reproduce the\nprobabilities in 3.3?"
8892,1, \n\nThe idea proposed is fairly straight-forward.
8893,1, I think that this is a good approach to the problem that could be used in real-world scenarios.
8894,3," Are they completely different models? Or are the C^I used when learning the HDDA model (and how)? \nIf these are separate models, how are they used in conjunction to give a final density score?"
8895,3,\nThe experimental performance reported Table 1 remains small and largely within one standard deviation of competitor methods.
8896,3,\n- The learning of such soft combination is done jointly while learning the tasks and is not set\n  manually cf. setting permutations of a fixed number of layer per task.
8897,3,"\n\nI ignore the ReLU here, but I assume that is operates element-wise and just clips negative values?"
8898,1,"\n2) Novel and simple \""trick\"" for generating OOV words by mapping them to \""local\"" variables and generating those variables."
8899,1, The paper leverages a novel method in determining the coefficient of relative entropy.
8900,3,\n\nThe authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539)
8901,1,\n\nThis is a very interesting area and exciting work.
8902,1,\n- The technical novelty seems incremental (but interesting) with respect to existing methods.
8903,3, \n- Fig 1: which model is used to generate the conditional sample? 
8904,1,\n\nRevision: I thank the authors for the updates and addressing some of my concerns.
8905,3,"  I think the idea here is that the expression \nTrans\u2019(  (s,s\u2019) , a ) represents the n-step transition function and \u2018a' represents the first action?"
8906,3,"\n\n* Equation 3: Should there be a (1/K) in Z?"""
8907,3, \nc) This plot is disconcerting.
8908,3,"""This paper proposes a simple problem to demonstrate the short-horizon bias of the learning rate meta-optimization."
8909,2,What is this muck?
8910,2,"This is a disaster. I could continue, but you see my point."
8911,3, but without more insights it's difficult to judge how generally useful they are.
8912,3,\n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.
8913,1,"\n\nPros:\nThis method combines the contributions of a few previous works, and obtains a stronger and more general model."
8914,2,"They include practical examples and code samples, and (in a slightly bizarre move) source code will be made available on acceptance."
8915,3," \n- How are the networks trained, with what objective, how validated, which training images?"
8916,1,\n\nThe provided phase analysis and its relation to the depth of the network is also very interesting.
8917,3,"\n\nFor MNIST and MNIST-Fashion experiments, the motivation is mentioned to be similar to Srivastava et al. (2015), but in that study the corresponding experiment was designed to test if deeper networks could be optimized."
8918,3,.\n\nCons:\n1) Coreference eval: No details are provided for how the data was annotated for the coreference task.
8919,1," To me, the word2vec addition paraphrases also look quite good."
8920,2,With the appropriate revisions these results could provide a very limited contribution to the field.
8921,3," The authors suggested one possibility, namely that the server and some workers are located on the same machine and the workers take most of the computational resource"
8922,3,"""The article proposes to use dense skip-connections on the \""vertical\"" (between-layers) connections of recurrent networks."
8923,3,"  In order to see change in the softmax output score, especially if the already dominant prediction score is further increased, a lot of change has to happen to the outputs of the layer serving as input to the SoftMax layer."
8924,3,\n+ The authors show that one of the existing methods can fairly successfully fool humans to believe its synthetic results are actual human faces.
8925,3," In fact, ideas for evolutionary methods applied to RL tasks have been widely studied, and there is an entire research field called \u201cneuroevolution\u201d that specifically looks into which mutation and crossover operators work well for neural networks."
8926,1,".\n\nOverall, it\u2019s nice a nice study of the query completion application."
8927,3," Thus, these non-core units for a particular class could be core units for separating another class from the remaining ones."
8928,3,\nIn this paper the authors investigate \u201cthe use and place\u201d of deep reinforcement learning in solving the autonomous lane change problem they propose a framework that uses Q-learning to learn \u201chigh level tactical decisions\u201d and introduce \u201cQ-masking\u201d a way of limiting the problem that the agent has to learn to force it to learn in a subspace of the Q-values.
8929,3," In particular the paper uses a Caffe reference model on top of the adjacency matrix, rather than learning a method specifically for graphs."
8930,1, The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described).
8931,1,"\n\nThe draft is well-written, and the method is clearly explained."
8932,3,"\n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes"
8933,1," \n\nOffering a related work \""feature matrix\"" that helps readers keep track of how\nprevious efforts scale learning rates or minibatch sizes for specific\nexperiments could be valueable."
8934,3, It is proposed to optimize the formulation using the method of Lagrange multipliers.
8935,3," \n\nOriginality/Significance\nWhile the architecture is new, it is based on a combination of previous ideas about fast weights, hypernetworks and activation gating and I\u2019d say that the novelty of the approach is average."
8936,3,"  When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful."
8937,3," It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward."
8938,3,"  It's nice to have it very clear, since \""gradient step\"" doesn't make it clear what the stepsize is, and if this is done in a \""Jacob-like\"" or \""Gauss-Seidel-like\"" fashion."
8939,3,\n(3) what was the actual computational cost for the BBB RNN and the baselines?
8940,3,"\n[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein"""
8941,1,\nThe idea is simple and it seems to work for the presented examples.
8942,3,\n\nI do have a few questions that might help further improve the draft.
8943,3,"  To be competitive, deeper CCNNs would likely need to be trained."
8944,3,"""The paper presents a multi-modal CNN model for sentiment analysis that combines images and text."
8945,1,"http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed\u2026\n\nOverall Assessment: \n\n"
8946,3,  This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings.
8947,1,"\n\nThe idea proposed in the paper is quite interesting,"
8948,3," Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones."
8949,3,"\n\nSide notes:\n- DCN is already quite commonly used abbreviation for \""Deep Classifier Network\"" as well as \""Dynamic Capacity Network\"", thus might be a good idea to find different name."
8950,1,\n\nPros\nExperiments seem well done.
8951,3,\n\nThe key issue is that the justification for the constrained gradients is lacking.
8952,3,"""Summary:\nThe paper proposes a new dataset for reading comprehension, called DuoRC."
8953,3,\n\n4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy?
8954,3," For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real."
8955,1,\nImprovements on top of Dual-AC with ablation study show improvement.
8956,3,"\n\u2013\u00a0What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult?"
8957,2,This last version is riddled with redundant information and circular phrases.
8958,1,\n\nSignificance: The problem is important.
8959,1,\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks.
8960,3, They show that their method more efficiently suffers on permuted MNIST from less degradation.
8961,3,\n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.
8962,1," Especially it borrows the cyclic loss from the image style transfer, which provides a reasonable regularization to the text style transfer model."
8963,2,"And finally, the references are a mess."
8964,1,  Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method.
8965,3," Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement."
8966,3," Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation."
8967,3,\n- The objective function also seems somewhat non-intuitive.
8968,3," Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary."
8969,3," Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication"""
8970,3, Another clarification is the bAbi performance over Entnet which claims to solve all tasks.
8971,3,"\n\nInstead of the longer intro and related work discussion,;"
8972,3," Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions."
8973,3,"\n\n2. The authors should try more complicated datasets, like CIFAR-10."
8974,2,"The scales appear to be not divided into quartiles as stated, but rather are based on the distribution of the data (into quartiles)."
8975,3," What is the effect, in terms of convergence, in modifying the gradient in this way?"
8976,3,  A comparison is needed.
8977,3,"Further, experimental results of problems with previously untacked sequence lengths are reported."
8978,3," If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should."
8979,1," Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously."
8980,3,"\n\nThe main technical difference of the present work compared from the  main prior work (Vendrov, 2015) is that in addition to mean vector representation they use here also a variance component."
8981,2,Based on these my recommendation would be that the present manuscript cannot be published in this or any other journal
8982,3,"  (you don't need them, but also why number if you never refer to them later?"
8983,3,"\nI have a few comments and questions:\n1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise?[[CNT], [PNF-NEU], [QSN], [MIN]] If bit-wise, can you elaborate why?[[CNT], [PNF-NEU], [QSN], [MIN]] I might have missed something.[[CNT], [CNT], [CNT], [CNT]]\n2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c?"
8984,1, The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results.
8985,1,\n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1)
8986,3," An alternative could be: \nSec 1: Introduction \nSec 1.1: Related Work\nSec 2: Causal Models\nSec 2.1: Causal Models using Generative Models (old: CIGM)\nSec 3: Causal GANs\nSec 3.1: Architecture (including controller)\nSec 3.2: loss functions \n...\nSec 4: Empricial Results (old: Sec. 6: Results)\n- \""Causal Graph 1\"" is not a proper reference (it's Fig 23 I guess)."
8987,3," It would have been nice to experiment with increasing model complexity to study such effect. """
8988,1, although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class).
8989,3,"""This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework."
8990,3, The reasons for this choice are discussed and linked to theoretical properties of OT.
8991,3,"""The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time."
8992,3,"""The paper proposes a neural network architecture for centralized and decentralized settings in multi-agent reinforcement learning (MARL) which is trainable with policy gradients."
8993,3,\n\nThey consider learning VAEs using two different choices of inference networks with (1) fully factorized Gaussian and (2) normalizing flows.
8994,1, \n- experimental results are encouraging
8995,3,Do the authors have any insight on this?
8996,3, How is k chosen? Is it fixed or dynamic?
8997,1,\n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning.
8998,3,"""The paper proposes a method which jointly learns the label embedding (in the form of class similarity) and a classification model."
8999,3, The final sentence probably relates back to the CI approach.
9000,1,"\n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read."
9001,3, This suggests a fairly significant sensitivity to this hyperparameter if so.
9002,3," This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis."""
9003,3," Whereas neither of these components are new, to my knowledge, nobody has combined all three of them previously."
9004,3, The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree.
9005,3,\n\nThe paper uses much space to show how to compute gradients in the proposed\narchitecture: there is obviously no need for this in a day and age where\ngradients are automatically derived by software.
9006,3, \n\nQuestions with respect to dynamical systems point of view: Eq. 4 assumes small value of h.
9007,1," Based on the review taxonomy, the authors presents a mixed objective which aims for bretter clustering performance."
9008,1,  The motivation is clear and the idea is simple and effective.\
9009,3," Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment."""
9010,3," I think that the problem would be better handled that way than with the proposed strategy,;"
9011,2,This paper is desperate. Please reject it completely and then block the author's email ID so they can't use the online system in the future.
9012,3,\n\n(9) Lemma 3.2: Is \\hat{D} defined in the paper?
9013,3," However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English."
9014,3,"\nAuthors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines."
9015,1,"\nThe paper is well written, quality and clarity of the work are good."
9016,3,"""The authors propose a probabilistic framework for semi-supervised learning and domain adaptation."
9017,3,\n- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering
9018,3,  This could help parallelize neural network training considerably.
9019,2,Clearly you can tell that I have become distracted by the style of reporting results. I have in fact given up trying to understand it
9020,1,\n\n\nThe writing and organization of the paper is very well done. 
9021,2,This kind of prose simply borders on cruelty against the reader. And the conclusion is the intellectual equivalent of bubblegum.
9022,3, This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution.
9023,3,\n\nThere is another theoretical point that could be clearer.
9024,3," For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight."
9025,3,\n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9) 
9026,3, Typos didn't influence reading.
9027,1," Although the method could mix well when applied to those particular experiments,"
9028,3, A domain confusion loss is added to learn domain-invariant feature representations.
9029,3," Also, the authors should make this dataset available for replicability."
9030,3,"""Summary of paper - The paper introduces the Gradient Scale Coefficient and uses it to demonstrate issues with the current understanding of where and why exploding gradients occur."
9031,3,"\n\n4. Please, could you explain the last sentence of Sec. 4.3 that says \""The drawback here is that the agents will not be able to generalize to other unseen maps that may have very different geographies.\"" In particular, how is this sentence related to partial observability?"""
9032,1, It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI.
9033,2,"Nothing about the paper is objectively concerning, but I am left unconvinced"
9034,3," With this approach, one can generate text at test time by setting all inputs to blanks."
9035,3,"\n\nThere's a lot that I like about this paper, in particular the ablation\nstudy to examine which pieces matter, and the evaluation of a couple\nof simple models."
9036,3," As such, this may have very little relevance for the actual problem of cfDNA."
9037,3," Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space),"
9038,3,"  Are the synthetic time series clearly multimodal, or do they display some of the mode collapse behavior occasionally seen in GANs?"
9039,3,"""The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results."
9040,1,"\n\n(The transfer from positive->negative on an ambiguous example was interesting: Original \""service is good but not quick\"" -> \""service is good but not quick, but the service is horrible\"", and \""service is good, and horrible, is the same and worst time ever\""."
9041,3, Does it have anything to do with the paper as I didn't see it in the remaining text?
9042,3,"""\nThe authors present a study that aims at inferring the \""emotional\"" tags provided by Thumblr users starting from images and texts in the captions."
9043,3,"  However, I would have preferred to see more quantitative evaluation and less qualitative evaluation, but I understand that doing so is challenging in this domain."
9044,1, \n\n\n--------------\nStrengths:\n--------------\n\n- The paper is fairly clearly written and the figures appropriately support the text.
9045,3, Have authors performed any experiments ?
9046,3," And btw, don't you need to put E_z[ ... ] around the 2nd term on the r.h.s. ?"
9047,3, How does the proposed method handle large images?
9048,3, \u201d Maybe this can be substantiated by experimental results (e.g. a comparison against Pointer Networks [4])?
9049,3,"\""  In this task,  the model learns a joint embedding of the images and the attributes"
9050,3,". Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time"
9051,1,". That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization"
9052,3,\n- The generalization ability of composition over primitive commands.
9053,3,  In Advances in Neural Information Processing Systems (pp. 503-511). (see Figure 5A for the circuit implementing a Predictive Sigma-Delta encoder discussed by you)\n\nb.
9054,3, A theoretical result shows that the number of low-discrepancy (between cross-domains) mappings of low complexity is rather small.
9055,3, This underscores an algorithm that uses a prosocial adjustment of the agents rewards to encourage efficient behaviour.
9056,3,"\nIs there a specific reason for doing so?\n\n"""
9057,3,\n\nThe exposition leaves ample room for improvement.
9058,2,"your figures are provided at 72 dpi, which is fine for childrens games, but professional quality images require 300 dpi"
9059,3, Labelled data and unlabelled data are therefore lie in the same dimensional space.
9060,1, \n\nOverall the paper is very well written.
9061,3,n2) Report the results of a large study of many of the surveyed models on a large number of datasets.
9062,1,The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before.
9063,1,  \n\nBoth extensions are reasonable to me.
9064,3,\n-Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your 'master'.
9065,2,"Probably switching to Bayesian stats will be too difficult for many scientists in the less intelligent fields (e.g., psych)"
9066,3," Named entities, and rare words in general, are indeed troublesome since adding them to the dictionary is expensive, replacing them with coarse labels (ne_loc, unk) looses information, and so on."
9067,1,\nThe paper is mostly clear and the idea seems nice.
9068,3,"\n\nMinor:\n- Typo: Page 4 Line 7 \""Note that this algorithm use the similar strategy\"": use -> uses"""
9069,1," They therefore propose using a more-biased but lower-variance bound to train the inference parameters, and the more-accurate bound to train the generative model."
9070,3,\n\nThe paper can be significantly improved if the authors compare the performance and decoding speed against CTC with the same gated convnet.
9071,3," It trained 20 epochs for the studied case of CIFAR, so 1-3 minutes per epoch on the CPU can be implemented with zero overhead while the network is training on the GPU."
9072,3," Could this submission show some fine-tune experiments?"""
9073,3,"\n\nThe paper may have a valid point that differential privacy is hard to work with, in the case of Deep NN."
9074,2,"It appears the authors has generated reports in a hurry, compiled, and presented as an article."
9075,3," It would be good to add a section gathering such attempts at evaluation previously made and trying to unify them under the proposed framework.\n"""
9076,1,"\n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning."
9077,1, \n\n- Figure 1 is helpful to clarify the main idea of a VHE.
9078,3," The paper makes a number of observations about the learning or transfer learning of shape bias vs. color by controlling and manipulating the input of training data based on shapes, negative shapes, and random images."
9079,1,"\n\nThe distance based loss is novel, and significantly different from prior work."
9080,1," In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission,"
9081,3, Yet it is unclear how this informs us about the quality of the learned forward models f.
9082,3, I searched a bit but it is not possible to find any kind of similar results.
9083,1,"  Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward."
9084,2,"I really dont like to be harsh in my reviews, butâ€¦"
9085,3," Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches."
9086,3," \n\nWhat is proposed seems to be twofold:\n- instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound."
9087,1," Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing."
9088,3,"(Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD."
9089,3, But the authors clearly have only the stochastic min-batch implementation of the algorithm in mind.
9090,3, More detail here would be helpful.
9091,3,"\n\n- Poor experimental validation\n\nWhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised manner, "
9092,2,"I haven't read the word audacity yet in commenting on my reviews, so that is a first. 

- Reviewers response to a response to their revie"
9093,3,\n\nFractalNets amd DiracNets (https://arxiv.org/pdf/1706.00388.pdf) have demonstrated that it is possible to train deep networks without skip connections and achieve high performance.
9094,3," The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed."
9095,1,\n\n4) the experiments in figure 3 show that the tailored data augmentation can be done even for a subset of the\nclasses and still work well for all of them.
9096,3,"\n\n[1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015\n[2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016\n[3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016 \n[4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017\n[5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017\n"""
9097,3," The authors show that this can be interpreted as standard variational inference on an extended space, and that the true posterior can only be obtained if we can target the true posterior marginals at each step of the SMC procedure."
9098,3,"\n    -- \""Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\"""
9099,3, Because IS suffers from collapse even in the time dimension I would expect the optimal proposal parameters learnt from a IWAE-type objective will collapse to something close to the the standard ELBO.
9100,3,"\n \nI was able to work out the intuitions behind the heuristic rewards, but I still don\u2019t clearly get \nwhat the Q-value factorization is providing:"
9101,3, The results obtained on the considered problem
9102,2,It reads like papers often do when they are written in LaTeX. Reject.
9103,3," They second part of their article, where they test the examples created by their models using behavioral experiments was less convincing. "
9104,3,"\n\nAs a summary, this paper would benefit significantly with a more extensive overview of the existing relevant models, clarification on the model details mentioned above and a more through experimental evaluation with more datasets and clear explanation of the findings."""
9105,3,"""Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane."
9106,3," It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings."
9107,1," \n\nQuality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs. \n"""
9108,2,Usually climate studies do not show a good method for the proposed research. This is one of them.
9109,1,"""\n-----UPDATE------\n\nThe authors addressed my concerns satisfactorily."
9110,3," \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence."
9111,3," Arguably, the goal of RL algorithms is to learn to exploit their environment as quickly as possible in order to attain the highest reward."
9112,3,.\n\nThe second question is about the window size $w$
9113,1,\n\nImportance: \nUnderstanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work.
9114,1," The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed."
9115,2,I started to review this but could not get much past the abstract
9116,3,"""Summary: \nThe paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an SVM model."
9117,3,"\n- The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN."
9118,1," Overall, I think that the paper proposes some\ninteresting ideas,"
9119,1,\n\nOriginality: Above average.
9120,3, \nComparison of Maximum Likelihood and GAN-based training of Real NVPs\nhttps://arxiv.org/pdf/1705.05263.pdf
9121,1, I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there).
9122,3,\n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?
9123,3," Perhaps, one could try to explore the zero-shot learning setting, where there is a split between train and test classes: training the autoencoder model using large training dataset, and adapting the weights using single data points from test classes in one-shot learning setting."
9124,3,There are some concerns about this network that need to be clarified:\n1. sigma is never clarified in the main context or experiments\n2.
9125,1,"""In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods."
9126,3," As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image."
9127,3, Details are also given on how the authors are able to achieve realtime completion
9128,3, It introduces only some small technical novelty inspired by soft-k-means\nclustering that anyway seems to be effective.
9129,1,"Once these details are done correctly, the experiments support the relatively well-accepted hypothesis that flat minima generalize better. "
9130,1, Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated.
9131,1,\n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm. 
9132,3," What would be a good measure for an \""effective latent representation\"" that substantiates the claims made?"
9133,3, What is u? This is revealed in the latter sections but should be specified here.
9134,3,"\n2. If linked words does improve topic modeling, why does it do so?"
9135,1," The proposed ideas for the extension seem natural (i.e., use of SR and CNN)."
9136,3,Can the authors address the earlier comment about \u201ca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms\u201d
9137,3," This would demonstrate the real tradeoffs between bias, variance, and computation."
9138,1,"  I feel that while the proposed solution is very intuitive, and probably works as described,"
9139,3,DGA detection concerns the (automatic) distinction of actual and artificially generated domain names.
9140,3,"""This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers."
9141,3,"""This paper proposes a device placement algorithm to place operations of tensorflow on devices."
9142,3, This should be supported by some quantitative results.
9143,3," \n\n============================================================================================\n\nThis paper propose to use the reconstruction loss, defined in a somewhat unusual way, as a regularizar for semi-supervised learning."
9144,3,"\nIn addition, it could be very meaningful to provide some experimental results on linguistically distant language pairs, such as Japanese and English, or simply reversing word orders in either source or target sentences (this might work to simulate the case of distant reordering)."
9145,3,"\n\nI wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations."
9146,3,"\n7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.\n\nOverall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6). "
9147,3,"\n\nWhat is a \""base-level learner\""? I think it would be useful to define it more\nprecisely early on."
9148,2,"I read the first 6 pages of the paper. [â€¦] Sorry, I cant finish reading this paper. I certainly will not recommend anyone else."
9149,1,"  Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG."
9150,3,. The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it.
9151,3,"\n\n\nConclusion:\n\nSince RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem."
9152,1,"\n\u2013 The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling"
9153,1,"n\nIn general, I think the idea of using saliency to detect adversarial attacks is very good and represents an interesting avenue of research."
9154,3, The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration.
9155,3," \nWhen used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label."
9156,3,"\n\nSection 4.4: It\u2019s very very good that you compared to \u201cflat attention\u201d,"
9157,3, Presumably phi is not actually 1-dimensional?
9158,1, \n\nLearning both inverse and forward models is very effective.
9159,3, It seems that A2 should also be similar with A since only one bit in A2 and A1 is different.
9160,3,\u201d. What does \u201csmoother convex\u201d mean?
9161,3,"\n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others."
9162,3, That said I found the intuitive story a little bit difficult to follow;
9163,3," I'd point out that this construction can be done in parallel, so it's less of a computational burden."
9164,3,. It seems that the methods outperforms existing methods for learning graph representations
9165,1," \n\nAs strong points, the paper is easy to follow and does a good review of existing methods."
9166,3, Which forward model is trained from which model-free agent?
9167,1,\n\nSignificance: Average. 
9168,3," There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6)."
9169,3," It is basically a straightforward generalization of the idea of punishing, which is common in \""folk theorems\"" from game theory, to give a particular equilibrium for cooperating in Markov games."
9170,3," The approach is evaluated experimentally on several tasks such as outlier detection, supervised analogy recovery, and sentiment analysis tasks."
9171,1,"\n\nI liked Section 3, however while it is true that all methods differ in the way they\ndo the filtering, they also differ in the way the input graph is represented\n(use of the adjacency or not)."
9172,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean 
9173,3,"Several examples are used to show that even visually similar adversarial examples can have very different saliency maps, which motivates using these saliency maps to detect adversaries."
9174,3, The value for a bin is the (normalized) number of nodes falling into the corresponding region.
9175,3,"""Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights."
9176,1," It does not require training from scratch for each architecture, thus dramatically saves the training time."
9177,3,\n\nThe draft does need some improvements and here is my suggestions.
9178,1,"\n\nA super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models (Table 1 row 8),"
9179,2,"Unfortunately, I cannot recommend this paper for publication because its contents violate the laws of physics"
9180,3,"""Summary: This paper addresses the problem of training deep fully connected networks (FCNs) by introducing a class-dependent learning rate to each of the network weights."
9181,1, It is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable examples.
9182,1,n- Paper is clearly written\
9183,2,"This is clearly a submission that needs to be shredded, burned, and the ashes buried in multiple..."
9184,3,"\n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment."
9185,3,\n\nThe SMC algorithm used is a sequential-importance-sampling (SIS) method.
9186,3,"\n\n-Major: When I look at figure 4D, I see that the proposed approach *also* only provides the master with the sum (or really mean) with of the individual messages...? So it is not quite clear to me what explains the difference.\n\n\n*In 4.4, it is not quite clear exactly how the figure of master and slave actions is created."
9187,3," However, the agent's continuation function (gamma : S -> [0,1]) can specify weightings on states representing complex terminations (or not), completely independent of the behavior policy or actual state transition dynamics of the underlying MDP."
9188,3," \n\nTheorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014."
9189,1,"\n\nOverall, while the paper is well written and makes some interesting points,"
9190,3,"\n\n\""four automaton states Q\u03c6 = {q0, qf , trap}\"": Is it three or four?"
9191,1,"  This is an interesting objective but since no interactive experiments presented in this paper,"
9192,3,"\n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). "
9193,3," Moreover, many of the multi-task baselines obtain a worse performance than a simple perceptron (which does not account for multi-task relationships). \n"""
9194,3, This would have been useful to study in itself.
9195,3,. As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors.
9196,1,"  Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max."
9197,1,"\""recent studies\"", \""several studies\"", etc.\n- \""i.e, the improvements\"" -> \""i.e., the improvements\""\n\"
9198,1,\n- The main idea of the proposed method is clear.
9199,1," In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets."
9200,3," Thanks to the reviewer for clarifying this.\n"""
9201,3," The core idea is two fold: when passing a new image through the (already trained) classifier, first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second, add a temperature to the softmax."
9202,3, Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)? 
9203,3, They first consider the dynamics generated by the following procedure.
9204,3,  The idea is to learn a feature extraction network so that the image classification network performs well and the image reconstruction network performs poorly.
9205,3,  Why are two buffers necessary?
9206,3,  The method is evaluated only against their own VAE-based alternatives. 
9207,3, This paper assumes the link function is invertible.
9208,1,"\nOverall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs."
9209,3,"\nAlso, how about using A^2 in GCN or making two GCN and concatenate them in\nfeature space to make the representational power comparable?"
9210,3,"\n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part."
9211,3," As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. "
9212,3,"  The basic idea is to use a multi-step dynamics model as a \""baseline\"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased."
9213,3," For example, the authors mentioned sparse or non-sparse loss functions."
9214,3,"\n\nMinor comments:\n- Sec 1: \""... optimization techniques like Adam, Attention, ...\"" -> Attention is not an optimization technique, but part of a model;"
9215,1,"   The idea of extracting policies corresponding to individual automaton states and making them into options seems novel,"
9216,3, Or can this replaced by a single BFGS style step?
9217,3,"\n\nAlso, is the setting the same as in Weston et al (2015)?"
9218,1,"\n\nOverall, the proposed method is novel \u2014 even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant."
9219,1, \n\nOverall the authors seems to have captured the essence of a large number of popular CF models and I found that the proposed model classification is reasonable.
9220,3, I disagree. Let's look at an example. Consider ResNet first.
9221,3,"""This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training."
9222,1, The result is yes and the authors show how differently the tuning can be compared to tuning the full run.
9223,1,\n\nEdit: Thanks for the fixes and clarification of essential parts in the paper.
9224,3,"\n\nComment:\n\n1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap. "
9225,1,"""This paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources."
9226,1,"\n\nClarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version."
9227,3,\n_______________\nORIGINAL REVIEW:\n\nThis paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations.
9228,2,Was this an undergraduate class assignment?
9229,3,  You also need to define the evaluation metrics used.
9230,1,"""\nGENERAL IMPRESSION:\n\nOverall, the revised version of the paper is greatly improved."
9231,3," The numbers in the tables are good but I have several comments on the motivation, originality and experiments."
9232,3," In the introduction section, the paper claims that \""the expert must set M to a value that is larger than the time horizon of the currently considered task\"" when mentioning the limitation of the previous work."
9233,1,"\n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks."
9234,3," It is shown that certain color invariant \""input\"" layers can improve accuracy for test-images from a different color distribution than the training images."
9235,3, This group of related work should also be discussed.
9236,3," I will try to summarize the main novel (i.e., not present elsewhere in the literature) results of these:"
9237,3," Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder."
9238,3,"\n\nFig 1 shows \u201cthe histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers."
9239,3, Such an example would dramatically improve the paper's readability.
9240,3, A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task.
9241,3, Figure 2 and 3 could be next to each other to save space.
9242,3,  It is also not mentioned whether the other methods that the authors compare to are re-trained on their newly proposed training dataset.
9243,3," Anyway, a clearer explanation would be helpful."
9244,3," Then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent, which in turn would improve the performance of the reader as well."
9245,1,\n- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks)
9246,3, The paper then argues that LSTM is redundant by keeping only input and forget gates to compute the weights.
9247,3," With various agreement measures, it removes or merges edges and count the final nodes."
9248,3, \nThe only special point of the open-word classification task defined in this paper is to employ the constraints from the similarity/difference expected for examples from the same class or from different classes.
9249,3," In addition, it would be useful to see how the method put forward in the paper compares with other (reward-shaping) techniques within MARL (especially in the perfect information case in the pong players' dilemma (PPD) experiment) such as those already mentioned."
9250,3,\n\nSec 5.2 states \u201cneuron partitions the inputs in two distinct but overlapping categories of quasi equal size
9251,3, These patterns often appear in biological datasets. 
9252,3, What happens if you set the threshold using one out of distribution dataset and then evaluate on a different one?
9253,1," The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters."
9254,3," 2015.\n\n[2] Lee, Chen-Yu, et al."
9255,3, (This is based on my understanding that each data point consists of one image with multiple entities and one description that only refers to one of the entities).
9256,3,\n\nMinor comments\n=============\n1. The authors should use \u2018significantly\u2019 only if a statistical hypothesis was performed.
9257,3," The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting."
9258,3,"\n\nHowever, the first (theoritical) contribution is to make the link\nbetween matrix decomposition and sampling based objective."
9259,3, The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.
9260,3,\n- Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive?
9261,2,"The data are weak, based on very small samples, not analyzed properly, and based on measurements  in the wrong medium. I see little value."
9262,3," Since the main contribution is to use an existing algorithm to tackle a practical application, it would be more interesting to tweak the approach until it is able to tackle a more realistic scenario (mainly larger scale, but also more realistic dynamics with traffic models, real data, etc.)."
9263,1, \n\nPros:\n1) The general task of learning distributions over network weights is interesting
9264,3," Those results show that as structural information is removed, the GGNN's performance diminishes, as expected."
9265,3, Therefore correlation of the image features with the captions is weaker that it could be
9266,2,It reads as if the author were giving a lecture and wandered off point to tell an interesting story.
9267,3," For example, some deer or truck pictures."
9268,3," In the variable trace embedding, the input to the model is given by a sequence of variable values."
9269,3,. \n\nPros:\n- The proposed method leads to state of the art results .
9270,3,\n\n(7) In Definition 2.3: What does the parenthesis in \\phi_j(1) denote?
9271,3,"""The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks \u2014 i.e., the existence of a single perturbation which causes a network to misclassify most inputs."
9272,1, The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks.
9273,1,"\n\n(2) An advantage over prior work, this approach integrates architectural evolution with the training procedure. "
9274,3,"\n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment."
9275,3,\n\nExperiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines.
9276,3,  This is more of a shortcoming than a fundamental issue.
9277,3,", you will have to run node2vec several times to reduce the variance of your resulting discretized density maps."
9278,3,"\n\nCons:\n - notation (i.e. precise definition of r_{i,c})"
9279,3," The proposed architecture could identify the \u2018key\u2019 states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory."
9280,3,"  In the empirical section, several RNNs are trained using this approach, using only ~ 100 recurrent parameters, and still achieve comparable results to state-of-the-art approaches."
9281,3," Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \""soft labels\"" for whether or not pairs of target data belong to the same class."
9282,2,We regret that some of the remarks made by Referee 1 were not edited before being sent to you.
9283,3,\nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation.
9284,1,"but the paper presents useful results,"
9285,3,"\n- In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control."""
9286,3, What is a_t in\nequation 9?
9287,3," So, does the training data have a particular string as the ground truth answer for such questions, so that a model can just be trained to spit out that particular string when it thinks it can\u2019t answer the questions? "
9288,1, Strong results in multiple settings indicate that the proposed method is effective and generalizable.
9289,3," I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6)."
9290,3," That\u2019s a lot. Given that we are thus optimizing with (K+1) times the number of weights of the network, I am not sure that comparing with the vanilla network trained in usual way is fair."
9291,3," For example, see Miyashita, Lee and Murmann 2016, and Hubara et al."
9292,3,\n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.
9293,3,"""This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time."
9294,3, What does the lookup table consist of? 
9295,3,"""This paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next state\u2019s value."
9296,1," On the contrary, the model is heuristic, and simple, but the descriptions are unclear."
9297,3," So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. """
9298,1,\n\nThis is an extremely impressive manuscript and likely to be of great interest to many researchers in the ICLR community.
9299,3," Here, w has almost converged to its optimum w* = 1."
9300,1," Overall, TR seems like an interesting idea,"
9301,3," The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping)."
9302,3,"\n\nMinor points:\n- Is there a benefit to having a model that jointly predicts unit presence and count, rather than having two separate models (e.g., one that feeds into the next)? "
9303,3,"  Also since $\\rho_\\pi$ is define with a scaling $(1-\\gamma)$ (to make it an actual distribution), I believe the definition of $\\eta$ should also be multiplied by $(1-\\gamma)$ (as well as equation 2)."
9304,1,  It is interesting that the Task function is able to encode the higher level structure of the TAXI problem\u2019s two phases.
9305,3," Since L is NON Convex, it could not be automatically considered as bounded."
9306,3," \n\nRegarding the experimental part, it can not make strong support for all the claims."
9307,3," In addition, preliminary experiments comparing among different categories are also provided. "
9308,3," However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation)."
9309,3,"\n\nTable 1: \u201c52th percentile vs actual 53 percentile shown\u201d. \n\n> Table 1: The more fuzzy, the higher the percentile rank of the threshold"
9310,2,This is a rather pedestrian treatment of a popular and well-reported topic
9311,3," \""Deeply-supervised nets.\"" Artificial Intelligence and Statistics. 2015."
9312,3,"  \n3) Batch normalization is popular, especially for the convolutional neural networks."
9313,3," Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis."
9314,1," \n\n** REVIEW SUMMARY **\n\nThe paper reads well, has sufficient reference."
9315,3,", the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning"
9316,1," \n\u2022\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time."
9317,1," Here are a few comments:\n\nI think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix."
9318,2,"For a section on thought, very little seems to have gone into it."
9319,3," \n\nThe spectral convolution methods have been applied to mesh data structures for about 5 years now, as stated in the paper as well [Bruna et al. 2013], [Defferrard et al. 2016], [Bronstein et al. 2017], [Li et al. 2017] ..."
9320,3,". In the proposed protocol, the model \u2018sees\u2019 the generated data D_gen (which is fixed before training) multiple time across epochs. "
9321,3," Suggest to rephrase \""unreasonable\"" to something more positive. "
9322,3," At each iteration, SGD samples a subset of training samples and labels."
9323,3,\n\nOriginality\nThe main contribution of the paper is similar to multiplicative gating.
9324,3,"\n\n4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size?"
9325,3, though the introduction could have motivated the problem a little better (i.e. why would we want to do this).
9326,1,"\n\nSignificance: I believe that this work is quite significant in two different ways:\n1) \""Bayesian evidence\"" provides a nice way of understanding why neural nets might generalize well, which could lead to further theoretical contributions."
9327,3,"""The paper studies the local optima of certain types of deep networks."
9328,1," Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning."
9329,1,\n\nSignificance \n\nThe approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models.
9330,1," This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5."
9331,3," However, Gal and Ghahramani\n(2015) actually follow Hern\u00e1ndez-Lobato and Adams, 2015 so the correct\nreference should be the latter one."
9332,3,"\n\nGeneral Questions\n\nI am  wondering how come you didn't consider a geometry image representation of the meshes, and went for a slightly more general, and yet very confined alternative (the adjacency requirement, which in some sense is the same type of constraint as geometry images)."
9333,3," For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration."
9334,3, I have updated my score to 6 accordingly. 
9335,2,What is a systematic review? I have never heard of an unsystematic review.
9336,3,"\n\nIn section 5, the authors mention briefly some works from the RNN domain & the classical use case of memory networks."
9337,3,1\n\nMajor:\n\n- The size of the generated images is up to 26x31x22 which is limited\n(about half the size of the actual resolution of fMRI data).
9338,2,Authors rarely followed the advices
9339,1," the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work. """
9340,1, The pictorial explanation for how the CNN can mimic BFS is interesting
9341,3," Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used)."
9342,3," However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step."
9343,3,"\n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models."
9344,3," If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems."
9345,3, 2) make minimal change in the middle so that the attack is not detectable.
9346,3,The proposed technique is validated both empirically and theoretically.
9347,1,\n- The paper is well-written.
9348,3, Why is this term included. Is this term not equal to 0?\
9349,3,\n\n* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side).
9350,3,"\n- there are issues with the experiments: the choice of data sets is questionable (all data sets are very small so there is not need for online learning or budgeting; newsgroups is a multi-class problem, so we would want to see comparisons with some good multi-class algorithms; spam data set might be too small), it is not clear what were hyperparameters in different algorithms and how they were selected, the budgeted baselines used in the experiments  are not state of the art (forgetron and random removal are known to perform poorly in practice, projectron usually works much better), it is not clear how a practitioner would decide whether to use update (2) or(3)"""
9351,3, The theoretical results in the paper are supported by experiments.
9352,3," I However, the proposed method departs from the theory that justifies the variational autoencoder."
9353,2,"This is a disaster. I could continue, but you see my point."
9354,1," After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs."
9355,3," If the authors feel otherwise, please comment on why this is the case."
9356,3," Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity."
9357,1," Please clarify.\n\n---------------\nI have updated my scores as authors clarified most of my concerns."""
9358,3, The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly. 
9359,1,\n\nThe paper is written clearly
9360,3," \n\n- \""In a VHE, this recognition network takes only small subsets of a class as input, which additionally ...\"": And that also clearly leads to loss of information that could have been used in learning."
9361,3,\n- We know finding options is the hard part about options\
9362,3, What is the disagreement between L1 penalty and prediction quality?
9363,1," With the increasing focus on applying RL methods to continuous control problems and RTS type games, this is an important problem and this technique seems like an important addition to the RL toolbox."
9364,3, The nice part of doing RL is that it provides ways of actively controlling the exploration.
9365,1, \n\nPros:\n- Good solution to an interesting problem.
9366,3,"  For instance, the proposed method should be compared  with the discriminative approaches including VAT and [1] in terms of the training efficiency."
9367,3," Consequently, the aggregation of all class-specific core units could include all hidden units of a layer."
9368,3,"\n\nSince this is so important to the results, more analysis would be helpful."
9369,3, \n\nThe results in Baird\u2019s counter example are discouraging for the new constraints.
9370,3,"  \nFinally, there are some minor issues here and there (the authors show quite some lack of attention for just 7 pages):[[CNT], [CNT], [CRT], [MIN]] \n-\tTwo times \u201cget\u201d in \u201cwe get get a decoding scheme\u201d in the introduction;"
9371,1, I think that this paper should quantify the effect of an increase of $L$.
9372,3,"  In addition, ASG requires additional forward-backward computation for computing the partition function (second term in eq 3)."
9373,3," This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community."
9374,1,\n\nI\u2019m not fully convinced by the interpretation of Eq. 5: \u201c\u2026 d is inversely proportional to the norm of the residual modules G(Yj)\u201d.
9375,3,  And even then maximize transfer with respect to what?
9376,3, In other words the authors propose to do regularized OT (possibly with a new regularization term) and should discuss that.
9377,1,\n\nCons\n-------\n\nNone
9378,1,"\n\nThe paper is generally well written, easy to read and understand, and the results are compelling."
9379,1,"\n\nIt\u2019s a nice, simple idea."
9380,3,"  \nMy main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper. The efficiency of such a search method decreases very fast when the dimensions of the z-space increases."
9381,3, More experiments with different number of layers and different architecture like ResNet should be tried to show better results.
9382,1,".\n\nOverall, I believe the paper is interesting but not ready for publication."
9383,3, Citation?
9384,3,\n-\tWhat is the definition of H the Hadamard matrix in the discrete orthogonal joint definition?
9385,3,  \n- I think Section 5 can be put in the Appendix - it's essentially an illustration of why the weight scaling is important. 
9386,3, \n\n3.  What do different points in Fig 3 and 4 represent.
9387,1," Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others)."
9388,3,"\n\nMy main concern (and complaint) is not technical, but application-based."
9389,3,   Was the performance significantly worse without the Raiko estimator?
9390,3, Bridging the gap between stochastic gradient MCMC and stochastic optimization.
9391,2,"I believe that there are important questions in this area, questions that have intellectual merit, but the PI has not found anyâ€¦"
9392,3, They distinguish theirs approaches into 1) structured updates and 2) sketched updates.
9393,2,"You do not use the empirical data for the analysis, but the empirical data uses you"
9394,1,\n2. New latent variable model bound that might work better than classic approaches.
9395,3," This problem is often regularized by adding a \""gradient penalty\"", \\ie a  penalty of the form \""\\lambda E_{z~\\tau}}(||\\grad f (z)||-1)^2\"" where \\tau is the distribution of (tx+(1-x)y) where x is drawn according to the empirical measure and y is drawn according to the target measure."
9396,2,"This paper must be rejected, because the work it describes is clearly impossible."
9397,3,"""This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied."
9398,3, Actually t-SNE on raw MNIST pixels is not bad at all.
9399,1,  \n\nPros.\n- the paper is clearly written.\
9400,1,\n-            Provides valuable insight into the MAML objective and its relation to probabilistic models\n\n
9401,3, Could you comment why this would not happen?
9402,3," Since you are not running experiments on Atari here, it is invalid to simply assume that such a second network is needed."
9403,2,I would like to urge the authors to improve the language description to make the manuscript much clear and logic.
9404,3," The second paper uses arithmetic circuits rather than HE, but actually implements training an entire neural network securely."
9405,3,"""This paper proposes using a feedforward neural network (FFNN) to extract intermediate features which are input to a 1NN classifier."
9406,3," There are many such methods, see for example \n\nJiwei Li, Dan Jurafsky, Do Multi-Sense Embeddings Improve Natural Language Understanding?"
9407,2,"Alarming errors, even if they could be easily corrected, gave a sense of carelessness that makes me hesitant to recommend it for revisions"
9408,2,This paper is fluently written and meticulously researched. I do not recommend it for publication.
9409,3,"""Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences."
9410,1,"  It represents an elegant combination of ideas, and a well-rounded combination of theory and experiments."
9411,3,\n\nI wonder whether this approach could be generalized to other classes of words or morphemes.
9412,3, Is there any reference for them?
9413,3, \n\nAll the conclusions are based on one or two datasets.
9414,3, The other methods use only the first epochs of considered deep neural network to guess about its curve shape for epoch T.
9415,3,"\n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments."
9416,3,\n   - Why  use a NMT model without attention? That is not standard and does not make sense to use when a better baseline model (with attention) is available ?
9417,3, This work actually explains the role of initialization for learning BFS and how certain types of initialization lead to poor solutions.
9418,3, The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network.
9419,3," Learn to generate graphs is a key task in drug discovery, relational learning, and knowledge discovery."
9420,3," As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network."
9421,3,\n\nCons: \n\n- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices 
9422,1," Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results,"
9423,3,"\""  However, E can be intra-community because communities are partitioned by METIS."
9424,3, The auxiliary softmax layers take different views of the input for prediction.
9425,3," On the complexity of teaching. Journal of Computer and Systems Sciences, 50(1), 20-31."
9426,2,"Given the way the paper was structured, I felt I had to read most of it."
9427,3," In the\ncase addressed in the paper, what is the likelihood $p(D|\\theta)$ and what are\nthe modeling assumptions that explain how $D$ is generated by sampling from a\nmodel parameterized by \\theta?"
9428,1,"\n-\tPag7 in 3.2 \u201cdiscussed in 1\u201d is section 1?[[CNT], [EMP-NEU], [QSN], [MIN]]\n-\tPag14 Appendix E, why the labels don\u2019t match the pictures;[[CNT], [EMP-NEU], [QSN], [MIN]]\n-\tPag14 Appendix F, explain better the architecture used for this experiment."""
9429,3," Also, it could be interesting also to discuss how the results in Table 2 and 3 compare to human classification capabilities, and if that performance would be already enough for building a computer-aided diagnosis system."
9430,3,"""Language models are important components to many NLP tasks."
9431,3,"""This paper introduces an appealing application of deep learning: use a deep network to approximate the behavior of a complex physical system, and then design optimal devices (eg airfoil shapes) by optimizing this network with respect to its inputs."
9432,2,"Written in parts like an experience track paper, minus the experience"
9433,3,"""The paper proposes to learn a custom translation or rotation invariant kernel in the Fourier representation to maximize the margin of SVM."
9434,2,It is more of a blog post than a research article
9435,3, 2) when two neurons in a layer compute the same function.
9436,3," How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language?"
9437,3,\n\nRe: Speed. I brought up this point because this was a bulleted item in the Introduction in the earlier version of the manuscript.
9438,1,"  The generative approaches based on VAEs and GANs are time consuming, but according to my experience, the training of VAE-based methods are stable and the topology generalization ability of such methods are good."
9439,3, I would be curious to know what the FID looks like as a 'gold standard'.
9440,3,"  I fear the paper is not ready yet, but I am not opposed to publication as long as there are warnings in the paper about the shortcomings."
9441,3,  They empirically verify that the features they learned lead to good quality SVM classifiers.
9442,3,\n\n\nAdditional comments:\nThe paper needs a considerable amount of polishing.
9443,1, The experimental section is strong and it has evaluated across different datasets and various scenarios.
9444,3,\nOptimal control inputs are restricted to be inside the unit ball and overall norm is bounded by L.
9445,2,If you want to solve a puzzle you could just do sudoku.
9446,3,\n\nSummary of review:\n\nThis is an incremental change of an existing method.
9447,3,"\n\nPart of the motivation for this paper is the goal of scaling to very large sets of examples.[[CNT], [null], [DIS], [GEN]] The proposed neural net setup is an autoencoder whose input/output size is proportional to the size of the program input domain."
9448,1, \n\nPros:\n\n1. It is a novel approach which trains the placement end to end
9449,3,"\n\nThe motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \\x, then one encounters the curse of dimensionality as one obtains finer and finer discretization."
9450,3, \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem.
9451,3,"\n\nConcerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion."
9452,3," This was the initial setting of analysis for most of original results that rely on the proximal operator (e.g., Lions and Mercier 1970s)."
9453,3, This may just be a matter of taste.\
9454,2,A failing course paper written by an undergrad
9455,3," \n\n- All the comparisons are based on \""epochs\"", but the competing algorithms are quite different and can have very different running time for each epoch."
9456,3,  Did the authors try experiments with less embedding matrices?
9457,1,\n--The experiments were well carried through.
9458,3, The authors seem to restrict themselves to convolutional networks in the first paragraph (and experiments) but don't discuss the implications or reasons of this assumption.
9459,3, What advantage does the proposed algorithm offer to an user that a more deterministic algorithm cannot?
9460,3," \n\n* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: \n\nEfficient inference in occlusion-aware generative models of images,\nJonathan Huang, Kevin Murphy."
9461,3, The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right. 
9462,1, It may also be useful to develop further insights into current models (although the authors do not go that route).
9463,3,\n\nIt would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion. 
9464,1,\n\nOriginality\nTo my knowledge the method proposed in this work is novel.
9465,3," Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound?"
9466,3, \n\\partial L/ \\partial w (1 + \\alpha(\\mu_w)^T y (\\partial L / \\partial z \\partial w) = \\partial L/ \\partial w + O(\\alpha)  satisfied if and only if \\partial L / \\partial z \\partial w is bounded.
9467,1,\n\n+ new and large dataset
9468,3," It is generally used a synonym for \""nonlinearity\"", so please use it in this way."
9469,3,"   In Equation 5 this is mysteriously replaced with v_t, which is the target aspect embedding and so may have been the intention for u all along?"
9470,3, The method is evaluated with Resnet -50 using synthetic delays.
9471,3,"  \n\nThere is a lot of white space around the plots, which could be used for larger more clear figures."
9472,3, They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize.
9473,1,"\n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks."
9474,1,\n\nEdit: After the authors rebuttal I have increased the rating of the paper:
9475,3,"""Strengths:\n\n-\tThere is an interesting analysis on how CNN\u2019s perform better Spatial-Relation problems in contrast to Same-Different problems, and how Spatial-Relation problems are less sensitive to hyper parameters."
9476,3,\n\nIn equation (10) is z_l=z_l^k or z_l^(k+1/2) (I assume the former).
9477,3,\n\nHowever the paper in its current form has a number of problems:\n\n- The authors state that a major shortcoming of previous (efficient) unitary RNN methods is the lack of ability to span the entire space of unitary matrices.
9478,3, I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.
9479,1, and the contribution seems to be novel and significant.
9480,3, They find that the complex-valued networks do not in general perform better than real-valued networks.
9481,3," A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 \u201cResults\u201d p. 3 is not really results but part of the methods."
9482,3," For example, the higher bound of the memory required can be reduced to about 40% for a Crescendo block with 4 paths where interval = 1.\"""
9483,3,  They systematically characterize the fragility of several widely-used feature-importance interpretation methods.
9484,3,\n-\tSection 3.2 Dataset \u201cAs for TIDIGIT\u201d: \u201cAs for GRID\u201d(?)
9485,1,"\n\nOverall, I think it is a good paper and deserves to be accepted to the conference."
9486,3, The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.
9487,3,"\n\n\nAfter revision:\nSome of my comments were addressed, and some were not."
9488,3,"""In their paper \""CausalGAN: Learning Causal implicit Generative Models with adv. training\"" the authors address the following issue: Given a causal structure between \""labels\"" of an image (e.g. gender, mustache, smiling, etc.), one tries to learn a causal model between these variables and the image itself from observational data."
9489,3," In pixel by pixel MNIST, some of the legends might have some typos (FC uRNN), and you should use \""N\"" instead of \""n\"" to be consistent with the notation of the paper."
9490,1,\n\nPros:\n- very easy to follow idea and model
9491,1, The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision.
9492,3," Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks."
9493,1,. The derivation and analysis seems correct.
9494,3," E.g. \""The top stochastic layer z_L in FAME is a fully-connected dense layer\""."
9495,3,"\n\nAnother interesting avenue would be to learn the options associated with each task, possibly using the information from the recursive neural networks to help learn these options.\n\n\nThe proposed algorithm relies on fairly involved reward shaping, in that it is a very strong signal of supervision on what the next action should be."
9496,3, I believe that such material is more appropriate to a focused applied meeting.
9497,3," When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example."
9498,3," \n\nFinally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited."
9499,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
9500,3,"""Overview\n\nThe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets."
9501,1,"\n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling)."
9502,3, More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method.
9503,1,".\n\nThe experimental part is satisfactory, and seems to be done in a decent manner."
9504,3," As a user, I would be interested in the typical computational cost of both \""MCMC sampler training\"" and MCMC sampler usage (inference?), compared to competing methods."
9505,3,"\n\nFor the evaluations, there are no other tensor-based methods evaluated, although there exist several well-known tensor-based word embedding models existing:\n\nPengfei Liu, Xipeng Qiu\u2217 and Xuanjing Huang, Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model,  IJCAI 2015\n\nJingwei Zhang and Jeremy Salwen, Michael Glass and Alfio Gliozzo."
9506,1,"\n\n\nPros\n\nThe paper addresses an important application of deep networks, comparing the performance of a variety of different types of model architectures."
9507,1,"\n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,"
9508,3," Some additional discussion of the results would be appreciated (for example, explaining why the proposed method achieves similar performance to the LSTM/OPSRL baselines)."
9509,3, It was also mentioned that increasing the regularization parameter size increases the convergence rate.
9510,3, Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough.
9511,3,"? The x-axis on plots, does it include the data required for crossover/Dagger"
9512,3," Snell et al use a soft kNN classification rule, typically used in standard metric learning work (e.g. NCA, MCML), over learned instance projections, i.e. distances are computed over the learned projections."
9513,3, The constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next state\u2019s value.
9514,1,\n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem.
9515,3, I was assuming that those are the same system but did not see the numbers match each other.
9516,3, The work is contrasted to tit-for-tat approaches that require complete observability and operate based on expected future rewards.
9517,2,I dont see how your approach has potential to shed light on a question that anyone might have.
9518,3," To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered."
9519,3,"""SUMMARY\nThe major contribution of the paper is a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. "
9520,3,"\n  \u2022 On page 3, when Gt is first mentioned, Gt should really be described first, before the reader is told what it is often replaced with. "
9521,3,  Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?
9522,3, Following a short review section per section.
9523,3, Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison.
9524,3,\n? p.6: How does \u201cRatings Only\u201d work as DistMult gets no information of the specific entities?
9525,3," In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector."
9526,3,"""* Paper Summary\nThis paper addresses the problem of learning a low rank tensor filter operation for filtering layers in deep neural networks (DNNs)."
9527,1, The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft.
9528,3, One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected?
9529,3," Also, predictive coding has been commonly used in neuroscience:\nSrinivasan MV, Laughlin SB, Dubs A (1982) Predictive coding: a fresh view of inhibition in the retina."
9530,2,The use of the word surgeries to mean surgical procedures is a linguistic atrocity
9531,3, The algorithm is tested on three datasets.
9532,3, This claim needs to be discussed clearer as it is not clear to me why this would be the case.
9533,3," The authors a) extended existing decomposition techniques by an iterative method for decomposition and fine-tuning convolutional filter weights, and b) and algorithm to determine the rank of each convolutional filter."
9534,2,The proposal is poorly written and unfocused with only brief moments of meritorious thinking
9535,2,This is not a paper. This is part of â€¦ something. It cannot be reviewed and should be rejected right...
9536,3, It would be a good workshop paper.
9537,3,"  However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one."
9538,3," At least this is my\nopinion, I would like to encourage the authors to be more precise and show in\nthe paper what is the exact posterior distribution over Q-functions and show\nhow they approximate that distribution, taking into account that a posterior\ndistribution is obtained as $p(theta|D) \\propto p(D|theta)p(\\theta)$."
9539,3,"\n-- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages)."
9540,3,"""The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x)."
9541,3,It could have taken more spaces in the paper.
9542,3,"""The paper considers distribution to distribution regression with MLPs."
9543,3," \n\n\""satisfies task the specification)\"" -> \""satisfies the task specification)\"".[[CNT], [CLA-NEG], [CRT], [MIN]] \n\nFigure 4: Tasks 6 and 7 should be defined in the text someplace."
9544,3," Also, not sure I see why SEARNN can be used on any task, in comparison to other methods."
9545,3," I think which algorithm is faster depends on values of L, T and M."
9546,3, See Dynamic Programming and Optimal Control and references too it.
9547,3,"\nOverall, I rate this manuscript in the top 50% of the accepted papers."""
9548,3, It extends results of Ge and collaborators for the fully observed symmetric case to non-symmetric problems.
9549,3,"""The paper is a pain to read."
9550,3," To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors)."
9551,3,"  Therefore, how to choose this parameter is essential."
9552,3, The weight assigning (write) network is optimized for maximize the expected rewards.
9553,1," It also showed good online A/B test performance, which indicates that this approach has been tested in real world."
9554,3,Is this data low rank? 
9555,3, \n\n# Detailed comments\n\n1. The distinction between parameters and hyperparameters (section 3) should be revised.
9556,1," I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples."
9557,1,I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.
9558,3,\n\nThe proposed method is using Elman nets as the base RNN. 
9559,3,\n\n\nDetailed comments:\n- The upper bound used to derive the formulation applies to a logistic regression classifier.
9560,1,. Both quantitative and qualitative results are provided.
9561,3,"""Summary:\nThe authors present a paper about imitation of a task presented just during inference, where the learning is performed in a completely self-supervised manner."
9562,3, \n2. The listed site for videos and additional results is not active.
9563,3, The authors should at least compare with one-layer conditional GAN. 
9564,3,"\n\n4. The experimental part is ok to me, but not very impressive."
9565,3,"\n  * relation between the p(y_i = 1) (in PCN) and g(x_p,x_q) in Eq 2 could be made more explicit, PCN depends on two images, according to Eq 1, it seems just a sum over single images."
9566,3," The main claimed advantage is that it doesn't require the knowledge of the actions taken by the expert, only observations of states."
9567,3,"""This paper studies the control of symmetric linear dynamical systems with unknown dynamics."
9568,3," The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960."
9569,3, I was wondering if the problem is from the tanh activation function (eq 2).
9570,3, This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR.
9571,3," Therefore, I think this section should be shortened."
9572,3,\n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning.
9573,3,Is the offline attention baseline unidirectional or bidirectional?
9574,1,"\n\nRevision:  Having read the author response for this paper, I am encouraged by the updated baseline for WSJ and the additional explication about the competing Fisher systems."
9575,3,n\nSummary: \n\nThe authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty.
9576,3," \n\nOriginal Review\n=============\nSummary:\nThe contribution of this paper is a method for training deep networks such that skip connections are present at initialization, but gradually removed during training, resulting in a final network without any skip connections."
9577,2, DONT SEE WHATS SO HARD I CLEARLY SAID TURN RUEAT
9578,3,"   This seems a bit surprising, or at least would seem to warrant further explanation."
9579,3,"  The parameters of the FFNN are updated via a genetic algorithm with a fitness function defined as the error on the downstream classification, on a held-out set."
9580,3, This is in turn optimized (approximately) by alternating between gradient-based loss minimization and submodular maximization.
9581,2,This result would be great if it were true
9582,3," Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus."
9583,3," Thus, they are essentially learning the skip connections while using a human-selected model."
9584,3,"""The authors propose a new CNN approach to graph classification that generalizes previous work."
9585,3,\n2. The authors should explain why choosing VGG19 for analysis.
9586,3, Here are some specific questions:\n\n1. How are the keys generated? 
9587,3, The state trace embedding combines embeddings for variable traces using a second recurrent encoder.
9588,3, It combines the following ingredients:\na) a population-based setup for RL\nb) a pair-selection and crossover operator\nc) a policy-gradient based \u201cmutation\u201d operator\nd) filtering data by high-reward trajectories\ne) two-stage policy distillation;
9589,3," However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat. "
9590,3,"\n\nThe \""window size\"" of the local reordering layer looks like the \""distortion limit\"" used in traditional phrase-based statistical machine translation methods, and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model; small window sizes may drop information about long dependency."
9591,3,  \n- Why does NATAC perform much better than NATAC-k?
9592,3, The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image.
9593,3, They observe that PSRNN with orthogonal random features leads to lower MSE on test set than both PSRNN and LSTM and seem to reach lower value earlier in training.
9594,3, The distribution \ncould be replaced with a normalizing flow. 
9595,3,"  The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks."
9596,2,Well written. (entire review
9597,3, Frames required from the environment? 
9598,2,"Did all 5 authors say,Yes, this is a piece of work I am proud to have my name on?"
9599,3,\n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.
9600,3,"""In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model\u2019s runtime."
9601,1," Second, decades of empirical results illustrating good performance of TD compared with MC methods. "
9602,3,"  \n- To claim \""task-agnostic\"", you need to try to apply your method to other NLP tasks as well."
9603,3, Just because a network performs averaging of different paths and individual paths perform worse than sets of paths doesn't imply that ensembling as a mechanism is in fact the cause of the performance of the entire architecture.
9604,3,"\n\n6. The generator reversal procedure needs to be carried out once for each data point separately, and also when the generator has been updated, which seems to be introducing a potentially significant bottleneck into the training process."""
9605,2,"Although it doesnt make for a flashy title (which the authors like more than British tabloids do), there is an alternative interpretation"
9606,3, Results show that intra-attention improves performance for only one of the datasets.
9607,3,This seems like a key claim to establish.
9608,3,"\n\n3) \""The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward\""\nIndeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider."
9609,3," \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n"
9610,1,  The proposed approach claims two advantages over a baseline maximum likelihood estimation-based approach.
9611,3," The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations, argue experimentally that they occur in (some) neural networks of practical interest."
9612,3," As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically"
9613,3," For instance, it could describe more the advantages over reinforcement learning."
9614,3,"Previous methods, e.g. FGSM, may work on arbitrarily sized images."
9615,3,\nThe authors should evaluate on these datasets to make their findings stronger and more valuable.
9616,1," The proposed approach is simple and has an appealing compositional feature,"
9617,3,  Adam White and Martha White.
9618,3," I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches."
9619,1," Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning."
9620,3,It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods.
9621,3,\n\nQuality: Ok. The claims appear to be sufficiently verified in the experiments.
9622,3, \n\n==\n\nThe authors do not put into perspective their approach.
9623,3,. The authors could be more concise when reporting results
9624,3,"\n\nFirstly, in federated learning, each client independently computes an update to the current model based on its local data, and then communicates this update to a central server where the client-side updates are aggregated to compute a new global model."
9625,3," Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models)."
9626,3," But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing."
9627,3," Shouldn\u2019t be something like \u201cuniqueness\u201d, that is how unique is an image in a batch of images be a better indicator?"
9628,3," I think this section and the previous (\""The objective of unsupervised learning\"") could be combined, removing some repetition, adding some subtitles to improve clarity."
9629,3,"\n\nI didn\u2019t understand the r^in, r&out representation in section 4.1. These are given by the domain?"
9630,3, \n\nI also think that this method really ought to be evaluated on some other domain(s) in addition to binary image drawing.
9631,3,   Would it have fixed the Google gorilla problem?Why or why not?
9632,3,"\n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments."
9633,1,". Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework."
9634,1," \nOverall, I like the paper."
9635,3,".\nB. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both"
9636,3," A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity."
9637,3,"""The authors investigate knowledge distillation as a way to learn low precision networks."
9638,1,"""The paper describes some interesting work"
9639,3,\n\n- I think the analysis of section 5 is fairly trivial. 
9640,2,Have you no command of the English language?
9641,3,"  Furthermore, how could FAME advance the previous state-of-the-art?"
9642,3,"\n\nAccording to the authors the core claims are:\n\""1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation."
9643,3,"  At the moment in the last section you mention \""We have validated networks of Gaussian Process Neurons in a set of experiments, the details of which we submit in a subsequent publication."
9644,3,\n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.
9645,3," Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models."
9646,1,\n- Paper is clearly written
9647,3, Is there any difficulties with power law graphs?
9648,2,This literature review is nothing more than a merry dance around the books
9649,3," I was also not sure why there is a need to copy items from the input question, since all SQL query nouns will be present in the SQL table in some form."
9650,3,\n\n* The training procedure of the model is not explained in the paper.
9651,3,"\nThese experiments show that modern sequence to sequence models do not solve the systematicity problem, while making clear by application to machine translation, why such a solution would be desirable. "
9652,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
9653,3," In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model."
9654,3," Why cannot you use the typical evaluation procedure for collaborative filtering,"
9655,1," Moreover, this leads to good performances, so there is no needs to have something more complex."
9656,3,This paper proposes to use automatic differentiation to compute the inverse function efficiently.
9657,3," Especially since uncertainty estimates are required, a Gaussian process would be the obvious choice."
9658,2,I dont believe in simulations
9659,1,"In particular,  the P@5 and P@20 numbers are only slightly better"
9660,2,This article reads like the work of a reasonably competent undergraduate.
9661,1,"\""\n\nThe experiment on fast Hyperband is very nice at first glance, but the longer I think about it the more questions I have."
9662,1,"The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training."
9663,3, I think matching the number of hidden units could be helpful.
9664,3,"""This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page."
9665,3," It would be nice to know (1) how the various models perform\nat QA in both ZS1 and ZS2 settings, and (2) what the actual performance is NAV\nalone (even if the results are terrible)."
9666,3,"\n\nExperimentally, the results are rather weak compared to pure model-free agents."
9667,3," The authors claim that the previous SotA result was carefully fine-tuned with a low learning rate, and that in this paper they used only default fine-tuning with a high learning rate."
9668,1,"\nTherefore, the paper must show that this new method performs better in some way compared with previous methods."
9669,2,"And finally comes the conclusion, which is the intellectual equivalent of bubblegum."
9670,3, GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.
9671,3,  The main idea is to apply CNN on Hilbert maps of the data.
9672,3, \nCons: nothing major.
9673,3," It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation."
9674,2,Being first is not sufficient. I could be first to do a backflip off a building with no net but that doesnt make it a good idea
9675,3,"""This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning)."
9676,3," Inspired by recent works on large-batch studies, the paper suggests to adapt the learning rate as a function of the batch size."
9677,3," In the abstract, what does \u2018two-pass decomposition\u2019, \u2018proper ranks\u2019, \u2018the instability problem\u2019, or \u2018systematic\u2019 mean?"
9678,3," \n\u201cSubsequently, we present that learning counterfactual return leads the model to learning optimal topology\u201d => Do you mean \u2028\u201cmaximizing\u201d instead of learning."
9679,1,  The M in MVProp in particular seems to be very useful in scaling up to the large grids.
9680,2,"As far as this reviewer can tell, what the authors present is not so much a model as it is an equation."
9681,3," If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum."
9682,1,\n\nSignificance:\n\nThe paper's contributions are significant.
9683,3," However, for some key steps in the proof, they refer to other references."
9684,1,"\n\nI think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice."
9685,1,\n\nQuality\n\nThis paper does not set out to produce a novel network architecture.
9686,1,\n(6) MC methods can outperform TD methods with more complex and high dimensional perceptual inputs.
9687,3,"  This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here?"
9688,3,It would be\n  interesting to see what would the method learn if the number of layers was explicitly set to be\n  large and an identity layer was put as one of the option.
9689,3, Is there any specific attempt to visualize or understand the embeddings inside the tree?
9690,1,"Pro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\n"
9691,3," However, your response to one of the sibling\ncomments suggests that it's still a \""mixed\"" training setting where the sampled\nQA and NAV instances happen to cover the full space."
9692,3,\n\nThe Latent variable interpolation experiment could also use more explanations.
9693,2,"The ideas seem to be a combination of wishful thinking, poor chemical insight and limited understanding of the techniques involved"
9694,1,"\n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases."
9695,2,"Looking at the general poor quality of the paper, Im surprised by the list of the co-authors"
9696,1," Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means."
9697,3," \n- It looks like the typos in the equations got fixed\n- The new phrase \""enables to learn to plan\"" seems pretty awkward."
9698,3,"  Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs."
9699,3," See https://sites.google.com/site/neighborembedding/mnist\nc) For 20 Newsgroups dataset, NATAC achieves 0.384 NMI."
9700,2,"It is very lengthy, full of mistakes, irrelevant information, and completely fails to attract..."
9701,3, It also evaluates the number of regions of small networks during training.
9702,3,"\n - qualitative analysis could be extended\n - writing could be improved  """
9703,1,\n\nDetails:\nhave been successfully in anomaly detection --> have been successfully used in anomaly detectionP
9704,3,"""The paper proposes a neural architecture to map video streams to a discrete collection of objects, without human annotations, using an unsupervised pixel reconstruction loss."
9705,3," What is the take-home message of the paper?"""
9706,2,The authors should refer to the super interesting article on this topic in Wikipedia.
9707,1," In summary, a very nice paper."
9708,2,I would have preferred to read a meta-analysis
9709,3,"  Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\"
9710,3,"""The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick."
9711,3,  This quantity is \u2018the probability of an input being assigned the correct output.
9712,3," \n\nLast, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks."
9713,1, The experimental evaluation presents indeed interesting results.
9714,3," The teacher also supplies an uncertainty estimate to each predicted label.[[CNT], [null], [SMY], [GEN]] How about the heuristic function?"
9715,2,Hypothesized is such an ugly word
9716,3, Maybe adding plots to the paper can help.
9717,3," For example, how well do humans classify these sketches?"
9718,3," \n1. \u201cIf we consider TD-learning using function approximation, the loss that is minimized is the squared TD error."
9719,1, \n\nThis paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap.
9720,1,\n\nThe key idea is a smart evolution scheme.
9721,1," \n\nThe overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher-dimensional and stochastic environments."
9722,3," The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins."
9723,3,"\n\nFor the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine \u201ccontexts\u201d that were not observed during training."
9724,1, The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs.
9725,3," In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer."
9726,3," Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets."
9727,3, Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly.
9728,1,"""This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function."
9729,3,"\n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning."
9730,1,"\nI like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function, which makes implementation simpler compared to VPN."
9731,1,  There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations.
9732,3," Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly."
9733,3,\nA (not too large) translation of the input image therefore does not change the log-polar representation.
9734,3,"\n\nThe description of the model is laborious and hard to follow. Figure 1 helps but is only referred to at the end of the description (at the end of section 2.1), which instead explains each step without the big picture and loses the reader with confusing notation."
9735,1," \n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible."
9736,1," \n\nOverall, this is an interesting paper."
9737,3,\nb)\tWhat is the spatial independence assumption needed for such a generator?
9738,3,"\n\n- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks."
9739,3,. The max-pooling operation is then used to obtain a vector representation of size d.
9740,3," The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs."
9741,3,"""This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization. "
9742,3,"However, this reduction grows to infinity the larger the update is."
9743,3,"""This paper presents a tensor decomposition method called tensor ring (TR) decomposition."
9744,1,"\n\nThe approach, and its constituent contributions, i.e. of using RL for program synthesis, and limiting to syntactically valid programs, are novel."
9745,1,"\n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial). "
9746,3,"\"" \n\nIt is clear that regularization should play a significant role in shaping the decision boundaries."
9747,3," Experimental results on MNIST, CIFAR-10 and SVHN are reported to show the compression performance."
9748,3,"\n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1)."
9749,3, The NaaA framework is based on the idea of treating all neural network units as self-interested agents and optimizes the neural network as a multi-agent RL problem.
9750,3," I suggest shortening Section 2, and it should be possible to combine Sections 3 and 4 into a page at most."
9751,1,"""This is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning, evolutionary computation and reinforcement learning in a novel way."
9752,3," Consistent use of the A, T, and S' dataset abbreviations would help."
9753,1,"\n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale."
9754,3," The assumption here is that labels (aka outputs) are easily available for all possible\ninputs, but we don't want to give a constraint solver all the input-output examples, because it will\nslow down the solver's execution."
9755,3,"\n\nSome recent references that warrant a mention in the text:\n- both of these learn optimizers using longer numbers of unrolled steps:\nLearning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017\nLearned optimizers that scale and generalize, Wichrowska et al, ICML 2017\n- another application of unrolled optimization:\nUnrolled generative adversarial networks, Metz et al, ICLR 2017"
9756,3,"\n\nFinally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18?"
9757,3, but\n- A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set.
9758,3,   Also the attack model is described without considering if the adversary knows the learner's algorithm.
9759,2,This paper is fluently written and meticulously researched. I do not recommend it for publication.
9760,2,"It is not clear what percentage of the eligible people are not applying due to being lazy, inertia, or principles, which makes it hard to gauge the size of the potential market. [The people that the reviewer is referring to are immigrants"
9761,3," So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills?"
9762,1," \nThe paper is very easy to follows, and the results are explained in a very simple way."
9763,2,default settings?? huh???
9764,3, The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments
9765,1,\n\nBy interpreting the NCE in terms of matrix factorization allows the\nauthors to better explain this learning criterion and more\nspecifically the self-normalizing mechanism.
9766,3," In addition, it would be useful to investigate how this particular problem is different than a binary classification problem using CNNs."
9767,1,"\n\nPros:\nWell written, thorough treatment of the approaches."
9768,3," They also do not require pre-training and re-training, but just a single training procedure."
9769,2,The english languish should be improved
9770,3, What is a powerful model?
9771,1,"\n\nComment: I kinda like the idea of using chart, and the attention over chart cells."
9772,3, The robot then uses the learned parametric skill functions to reach goal states (images) provided by the demonstrator.
9773,3,\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments.
9774,3, The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types.
9775,1,"\n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison? "
9776,3," For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction?"
9777,3,"  If this is the case, how to prevent it from happening?"
9778,3, Did the model used for drawing Fig. 7 converge?
9779,3,"  In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage."
9780,3, Which architecture is used for building the mapping ?
9781,1,"\n\nWhile the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts),;"
9782,1, I like the presentation and writing of this paper.
9783,3, The 2-dimensional space is binned using an imposed grid structure.
9784,2,The proposal is largely descriptive and mostly a fishing expedition. It will be great if they catch some interesting or unexpected fish
9785,3, The dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable.
9786,3," 1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,;"
9787,3, Is it just choosing the most common class?
9788,3," Section 4.1 states\""We use a method similar to the DAGGER algorithm\"", but what is your method."
9789,3,"\n\nIt should also be noted that I was asked to review another ICLR submission entitled \""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\"" which amazingly introduced the same \""Pong Player\u2019s Dilemma\"" game as in this paper."
9790,1,\n\nMy comments / feedback: \n\nThe paper is well written and the problem addressed by the paper is an important one.
9791,3," If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS."
9792,3,"  Nevertheless, showing that these two ideas work also for RNNs in combination (and seeing, e.g. the nice relationship between Kronecker factors and unitary) is a relevant contribution."
9793,1,"  The instability of a gradient method on non-strongly convex-concave saddle point problems (like the bilinear form of Figure 1) is a well-known property, and many alternative *stable* gradient based algorithms have been proposed to solve saddle point problems which do not require transforming them to a minimization problem as suggested in this paper."
9794,1, Results are also interesting
9795,2,Data is presented as though it were reliable observation when it could equally well be described as unwarranted slander
9796,2,This work is stuck in the past. The referee would rather talk about the future - some of the senior co-authors were the future once!
9797,3, Why is it assumed that human intuition is necessarily good? 
9798,3,\n- medium resolution of the resulting prediction
9799,3, The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison.
9800,2,"So overall we do not recommend a resubmission, but can let you try if you insist

- The Journal that accepted the resubmission"
9801,3," This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \""unused\"" until the present task in hand.\n\n"
9802,3,They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity.
9803,3, \n\nOriginality\nThe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy.
9804,3,\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do.
9805,3," Clearly, the distribution of information over the agents is crucial."
9806,2,(although I admit that here is a possibility they might turn out to be correct in that they are guessing right)
9807,3,"\n\nIn addition to being quite bold in claims, it is also somewhat confrontational in style."
9808,1,\n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition.
9809,3,The multi-scale representation allows for better performance at early stages of the network.
9810,1," So, the general claim is supported."
9811,1, I upgraded the rating.
9812,3," \""\nThis is right. I am just surprised is has not been done before, since it requires only few lines of derivation."
9813,1,"\n\nTo sum up, I think that the general idea looks very natural and the results are supportive."
9814,1,\n\nSensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination.
9815,3, \n\nThe authors propose an algorithm which uses sequential Monte Carlo + autoencoders.
9816,2,I recommend the publication even if I am not impressed - (via shitmyreviewerssay
9817,2,"Recommendation: Publish elsewhere.
Comments: [none] "
9818,3, Is you Theorem 2 somehow an extension? Is Theorem 3 completely new?
9819,2,I showed this paper to my nurses and they agreed there was nothing new reported here.
9820,3,"""The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the \""model capacity\"" of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator."
9821,1,\\n\n* Section 3.3: Claim 1 is an interesting observation.
9822,3,"""The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \""standard\"" form and then into their correct morphological form,"
9823,1,". While they do show good performance,"
9824,3," Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro."""
9825,3,"   However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different."
9826,1,"""The paper presents some conceptually incremental improvements over the models in \u201cNeural Statistician\u201d and \u201cGenerative matching networks\u201d."
9827,1,"  \n\nWhile this is an interesting idea on its surface,"
9828,3, It would be favorable to empirically prove this by designing additional experiments.
9829,3," They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. "
9830,3,\n\n* The claim that the paper works with natural language should be toned down and clarified.
9831,3,"\n* Problem statement in section 3.1 should certainly be improved.[[CNT], [EMP-NEG], [SUG], [MIN]] Authors introduce rather heavy notation which is then used in a confusing way.[[CNT], [PNF-NEG], [CRT], [MIN]] For example, what is the top index in $s_t^{3-p}$ supposed to mean?"
9832,1," \n\nThe studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone-based learning."
9833,3," As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps."
9834,3,  Initial node embedding comes from both type and tokenized name information.
9835,3, Experiments are conducted using a LeNet-inspired architecture on MNIST and CIFAR10.
9836,3, \n\nReview - The paper attempts to contribute to the discussion about the exploding gradient problem by both introducing a metric for discussing this issue and by showing that current understanding of the exploding gradient problem may be incorrect. 
9837,3,"\n\nOther questions:\n- is \""n-gram\"" really the most appropriate term to use for the symbolic representation?"
9838,3," Do the various regions generate different parts of the final graph structure (i.e., focusing on only a subset of the nodes)?"
9839,3, I understood the reason not to compare with certain related work.
9840,2,It seems that the author is simply engaged in proprietary phrase coining â€“ advancing a new term for a well-researched phenomenon
9841,1,"""This is an interesting idea, and written clearly."
9842,3," You could use an NER for W/O-NE-Table and update the NE embeddings, and it should be as good as With-NE-Table model (and fairer to compare with too)."
9843,1," I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful"
9844,3,"""Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning."
9845,3,"""This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long)."
9846,3,"\n- In Eq 5, the denominator has z_y."
9847,1,\n\nThe analysis is elegant.
9848,3," On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me)"
9849,3," In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method."
9850,3, Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined.
9851,3,"""\n\n- 3.2: \""In general, if we wish to learn a model for X in which each latent variable ci affects some arbitrary subset Xi of the data (**where the Xi may overlap**), ...\"": Which is just like learning a Z for a labeled X but learning it in an unsupervised manner, i.e. the normal VAE, isn't it?"
9852,3,Section 4 extends\nanalysis to include SGD methods with momentum.
9853,3,"  however, seems to generalize much better for Turn Left, since it has seen the action during training (even though not the particular commands)"
9854,3, The core model is Variational Autoencoders (VAE) with an integrated visual attention mechanism that also generates the associated text.
9855,3, The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p.
9856,3, \n\nI have two comments on the experiment section:\n\n- Choice of experiments.
9857,2,"This is a heretofore unobserved result and is very interesting, but is not novel."
9858,3, The baselines are various simiplied versions of the proposed model.
9859,3," You could inject uncertainty in the captioning task for instance, e.g. consider that multiple version of each word e.g. dogA, dogB, docC which are alternatively used instead of dog with predefined substitution rates."
9860,3," In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?"
9861,3,"\n\nAs s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2."
9862,3," There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution."
9863,3,"\n- when evaluated on hand designed small maps, the agent doesn't perform very well (figure 6)."
9864,3, It's not clear which two baselines are depicted in 5b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baseline?
9865,1, \n\n====================\nClarity:  I found the paper clear and easy to understand. 
9866,1,"""The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network."
9867,3,\n\nOverall: The challenges proposed in the DuoRC dataset are interesting.
9868,3, Are the graph nodes pre-ordered? 
9869,3," To address this problem, the authors proposed an explicit grounding way to connect the words in a sentence and spatial regions in the images."
9870,3," On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used."
9871,3, The authors should measure the actual speedup also on a single GPU.
9872,3,The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima.
9873,3," They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size. "
9874,3," As long as both of them are identically distributed, then no discrepancy exists."
9875,3," The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training."
9876,3, \n\nThe paper shows that is possible to learn the between domains mapping using GAN only without a reconstruction (cyclic) loss
9877,3," \n\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.]."
9878,2,People have real issues and I am not at all sure that this one deserves attention.
9879,3, Second\nis what are the contributions to the state-of-the-art of the 2\nmethods introduced?
9880,3," Otherwise, the model can't learn anything."
9881,3," Finally, how performance is influenced by dimensionality P and L should also be clarified.\n\n"
9882,3,\n\n--Sub comment\nNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result.
9883,3, The only valid conclusion is that real and complex valued neural network cannot be directly compared using the same number of parameters.
9884,3," Specifically, they could analyze for a set of query words, what the most similar words are in the embeddings obtained from different subsections of the data."
9885,3,It is unclear if they are comparing to strong baselines.
9886,3, The paper should also discuss this method in Section 4.\
9887,1,"\n- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described."
9888,3," Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2."
9889,3,"\n\nOverall, I would say this is somewhat borderline, but I could be convinced to argue for acceptance based on the other reviews and the author response."
9890,3," Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs?"
9891,3," \n\nIn the words of one of the paper's own examples: \""It has a great atmosphere, with wonderful service.\"" :)\nStill, I wouldn't mind knowing a little more about what happened in the kitchen...\n\n"""
9892,3, Must it be trained for each new synthesis problem?
9893,3, Is this test loss the cross entropy?
9894,3," To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance."
9895,1," The theory part of the paper briefly reviews the relationship between learning rate, batch size, momentum coefficient, and the noise scale in stochastic gradient descent."
9896,3," Perhaps I am mistaken, but there seems to be n^2 spans in the document that one has to score."
9897,3,"""This paper studies empirical risk in deep neural networks."
9898,3,\ni)\tThe RW model is actually similar to an HMM.
9899,2,"This is madness, frankly, almost regardless of how it was implemented."
9900,3, Experiments on several real-world dataset reveal modest gains in comparison with the state of the art.
9901,1, The work is a useful addition to the communit
9902,3,".\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017)"
9903,3,"""This paper describes AdvGAN, a conditional GAN plus adversarial loss."
9904,3,"\n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016."""
9905,3,  What is the main objective of Eq. (7)?
9906,3," Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach"
9907,1,\n\nThe contributions of the paper are novel and significant.
9908,3,\n\n4. Could the authors indicate the range of values of \\lambda_{rec} and \\lambda_{nonrec} that were examined in the work?
9909,3, You show that it outperforms simulated annealing. Is this the state of the art?
9910,3," Subgoals are then used to learn different Q-value functions, one per subgoal."
9911,3,"""This paper proposes a multi-view semi-supervised method."
9912,3, Also the well-known original DBN paper has MNIST as main example (and main selling point) with close to 1% error.
9913,1," As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset."
9914,3, While a result has been stated for single-hidden ReLU networks.
9915,3,"""The paper introduces the notion of continuous convolutional neural networks."
9916,3,  \n\nMain comments:\nIt would strengthen the paper to also compare all these network learning based approaches to variational ones.
9917,3," Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm."
9918,3, It's nearest neighbours in the embedding space (i.e. semantically similar words) will also have high values in coordinate 99.
9919,3, but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere.
9920,3," not sure if it would help, but it might"
9921,3, 2) it works as described above but propagates the rechable region on a checkerboard only.
9922,1,"\n2) To my knowledge, the proposed approach is new"
9923,3,  Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique.
9924,3, Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process?
9925,3," Let's assume a 2d latent space, where each quadrant represents one MNIST digit (e.g. 1,2,3,4). You may observe a similar structure in this latent space yet still learn a good generator under even a standard 2d Gaussian prior. I guess my point is, a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it."
9926,1," Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML."
9927,3,"I\u2019m not an expert, but I assume there must be some similar idea in CNNs."
9928,3," \n\n4) Sec 3.5 Matching objects in successive frames using the Hungarian \nalgorithm is also well known, e.g. it is in the matlab function\nassignDetectionsToTracks ."
9929,3,"\n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model."
9930,3,\n4. Formulate complex analogue of Glorot weight normalization scheme
9931,3, They compare the method to EWC with the same architecture.
9932,3," For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work."
9933,3, I assume that the same is true for the evaluations of F on line 4 of Algorithm 1?
9934,3, though they do not harm the readability of the paper
9935,1,\n\nQuality:\nThe authors use formal concepts from information theory to underpin their basic idea of recovering latent factors and have spent a commendable amount of effort on clarifying different aspects on why these three measures are relevant.
9936,3, Several articles are wrongly placed and even some meaning is unclear.
9937,2,This would have been a question of high interest 10 years back
9938,3, The part about the actor-critic learning seems to lack many elements (whole architecture training?
9939,1," \nOverall, I like the idea, so I am leaning towards accepting the paper,"
9940,3,"\nIn the following, the authors explore the landscape of RNNs satisfying the necessary conditions."
9941,3,\n\n=================\n\nThe authors describe a new variant of a generative adversarial network (GAN) for generating images.
9942,3, I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view?
9943,3,"\n- I understand that there is no mixing in the test phase, perhaps it would be useful to recall it."""
9944,1, It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv!
9945,1,\n\nPros:\n1. The topic is interesting.
9946,1,"\nThe second motivation, w.r.t. IB seems interesting"
9947,3, The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards.
9948,3," Did the authors tried their approach to non-DA tasks, such as generating images, as often done with GANs?"
9949,3, If it is L_1 norm on the output coefficients the comparison is misleading.
9950,3," This goal is to achieve a similar effect to that of natural gradient, but with lighter computation."
9951,1,"  Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task."
9952,1, The experimental evaluation is now far more solid.
9953,3, \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these.
9954,3," At the cost of additional notation, this restriction could easily be lifted."
9955,3," However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions."
9956,3,\n\nThe paper is proposing a trick to train neural networks that is backed by strong arguments.
9957,3,\n\nCons:\n\n0. The whole paper just presented strategies and empirical results.
9958,1," Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks."
9959,2,The first problem is that the method - whatever it is and however it works - is insufficiently...
9960,3," However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states."
9961,3," Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods."
9962,3," -- the gradients should still pull the generator *towards* the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finer-grained details."
9963,1," They present experiments on several data sets, and observe that among all the techniques, random quantization can have a significant reduction of up to 32x in communication with minimal loss in accuracy."
9964,3,\n\nModel presented extends IW-GAN by using 3D convolution and also\nby supervising the generator using sample labels.
9965,3," \nRelated to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%."
9966,1,\n\nPros\n\nRelevant attempt to develop new predictive coding architectures
9967,3," It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds."""
9968,2,This result would be great if it were true
9969,2,I am sorry if I am missing something obvious here but this is not my area of work.
9970,3," In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model."
9971,3," \nFor the application of these ideas to spiking neurons including learning please see a recent paper:\nDen\u00e8ve, Sophie, Alireza Alemi, and Ralph Bourdoukan."
9972,1,\n\nBoth are useful contributions as long as deep wide Bayesian NNs are concerned.
9973,3, The outputs are evaluated by ROUGE-L and test perplexity.
9974,3, All one can conclude is that it performs slightly better than grid search with a small number of runs.
9975,3,"""Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage."
9976,3, The whole algorithm is tested on a toy problem with 3 repeats.
9977,3, This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.
9978,1, Experimental results on the IHDP dataset confirm the advantage of\nthe proposed approach.
9979,3, for which the reason could be a multitude of issues probably related to hyper-parameter tuning.
9980,3,"\n(\""The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\"
9981,3," Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact."
9982,2,The manuscript in the present form is not a review article but is rather a number of research papers stapled together.
9983,3,"\n\n[3] Huang, Gao, et al. \""Deep networks with stochastic depth.\"" European Conference on Computer Vision. Springer International Publishing, 2016.\n"""
9984,3," It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results."
9985,3," SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y' = argmin_y E(f(x), y)."
9986,3,\n- The authors say they use gameplay from no later than 11 minutes in the game to avoid the difficulties of increasing variance.
9987,3, Different models?
9988,3," Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0)."
9989,3,"""The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields."
9990,1, the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample.
9991,2,"I have read this paper several times through, and I have nothing to say in its defense."
9992,3,"  How is the structured update \""learned\""?"
9993,3," Under such conditions, Corollary 6 of MPCB also reads as s^n.."
9994,1,"""Summary\n========\n\nThe authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point."
9995,3,. The authors propose to do so by having a deep neural network similar to Kulkarni et al. (2016)'s Deep Successor Representation.
9996,3,"\n\n- In the study on reward sparsity, although a prediction horizon of 32 is less than the average steps needed to get to a rewarding state, a blind random walk might be enough to take the RL agent to a close-enough neighbourhood from which a greedy MC-based policy has a direct path to the goal."
9997,3,"\n- In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful."
9998,3,\n\nEqs.1-3: Why use recurrent formulation in both the graph propagation model and in the auto-regressive main loop (h_v -> h_v\u2019)?
9999,3,"  \n\nA second limitation of the work is the reliance on a \""true\"" set of disentangled factors.[[CNT], [null], [DIS], [MIN]] We generally want to learn learning disentangled representations in an unsupervised or semi-supervised manner, which means that we will in general not have access supervision data for the disentangled factors."
10000,3,"""The authors describe a mechanism for defending against adversarial learning attacks on classifiers."
10001,2,"hile it doesnt make for a flashy title (which the authors like more than British tabloids do), there is an alternative explanation"
10002,3," The Curse of Dimensionality for Local Kernel Machines. 2005."""
10003,3, The authors argue that GANs in fact generate the distributions with fairly low support.
10004,3,\n\n8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al 
10005,3, This only means that more understanding is needed as to how TR can be combined with SGD.
10006,3,"\n\nIt would be interesting to see the relation between the \""unified\"" gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework."
10007,2,(after having cited an author) one could call this name-dropping
10008,3,\n\nThis needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty.
10009,3," The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter."
10010,1, The evaluation shows that the approach performs better classifying MNIST digits than another approach.
10011,3," I understand that it is a metric to compare two images that is based on the mean-squared error so a very private image should have a low PSNR while a not private image should have a high PSNR, but I have no intuition about how small the PSNR should be to afford a useful amount of privacy. For instances, in nearly all of the images of Figures 21 and 22 I think it would be quite easy to guess the original images."
10012,3, This is not the fault of the present paper but an unfortunate tradition in the field.
10013,3," Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\"
10014,3,\n\nIt would be great if the authors could include the code that implements the model.
10015,3,"\n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA?"
10016,3,"\n\nSo at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution."
10017,1,"\n\nThe elimination module is interesting,"
10018,1,\n+ The idea is very interesting. 
10019,3,\n\nAdditional Comments:\n- Why in training you used logistic loss instead of the more common cross-entropy loss?
10020,1,"\n\nIn summary, I'm both excited about the dataset and new architecture,"
10021,3,"  \nIn the end of section 3, it mentioned that \""without normalization,\"" the method will not scale to an arbitrary number of objects."
10022,3,"  The game can be programmed to have an \u201cn\u201d lane highway, where n could reasonable go up to five to represent larger highways."
10023,3," Basically, I'd love to see the trends for how these types of tuning relate to each other over the whole populations: those trends could then be tested against experimental data (possibly in a future study)."""
10024,3,"  \n\nThe authors show how spike-based learning can be implemented with spiking neurons using such coding, and demonstrate the results on an MLP with one hidden layer applied to the temporal MNIST dataset, and to the Youtube-BB dataset."
10025,3, Currently a generic bound is employed which is not very insightful in my opinion.
10026,3,"- as stated in the related work section, effectively the same convnet+RNN architecture has been in common use for image captioning and other vision applications."
10027,3," \""One-hot\"" is the encoding of a categorical label."
10028,3,  It is more convincing to show the superiority of the proposed method than existing ones on the same ground.
10029,1,"\n\n1. The accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing, this also makes the search greedy , which could be suboptimal."
10030,3, It means that we multiplied by K the number of parameters in our model (K is the number of classes).
10031,2,"The team has generated the kind of gaudy, brobdignagian dataset that makes it such a curious and exciting time"
10032,1, The authors certainly make a convincing statement about the internal validity of the method.
10033,2,"The odds ratios in Table 2 are like the plains of Kansas: flat, flat flat!"
10034,3,"\nCouldn't comparisons be made, in some way, to other multitask work?"
10035,1,"Perhaps a model trained on the human-translated pairs from Task 1 of Multi30k? Obviously, outperforming such a model is not necessary for this approach to be interesting, but it would provide useful context on how well this is doing.\"
10036,3,\n+ Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches.
10037,3," Partial explanations are provided, again using results in compressive sensing theory."
10038,3,". On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT."
10039,3," The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS-MARL."
10040,3,"""This paper reviews the existing literature on attribute-based collaborative filtering. "
10041,3, It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.
10042,1, but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow.
10043,3," For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2."
10044,1,"\n\nWhile the learned representations are successful for the two main performance tasks discussed above,"
10045,3," However I have a few concerns:\n1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017)."
10046,3,"\n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network."
10047,3,\n- It seems like the performance of AE-k is increasing with increase of dimensionality of latent space for Fashion-MNIST.
10048,3,"The authors should look at several existing papers on stochastic trust region and stochastic quasi-Newton methods, e.g., papers from Katya Scheinberg (Lehigh) and Richard Byrd (Colorado)'s groups."
10049,3,\n3. the actual sources of variation are interpretable and explicit measurable quantities here.
10050,3, \nThe method assumes that it is possible to first execute 100 evaluations up to the total number of epochs.
10051,1," Nevertheless, the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of NNs."
10052,3," you should run them all for the same number of frames and then either compare (final score, cumulative score, #frames to human) or something else more fair/scientific."
10053,3, An open-source release would be ideal.
10054,3,"""The authors introduce a sequential/recurrent model for generation of small graphs."
10055,3, Why is nodes at different stages with the same initialization problematic?
10056,3,"\n  \u2022 [p6] \""Dosovitskiy & Koltun (2017) have not tested DFP on Atari games."
10057,3,"""There is a vast literature on structure learning for constructing neural networks (topologies, layers, learning rates, etc.) in an automatic fashion."
10058,3,\n\nThe first idea of the paper is to include time-remaining in the state.
10059,1,  The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting.
10060,3, \n\nThey focus on linear MLPs in the paper for computational simplicity.
10061,1, \n\nOriginality\n=========\nThe application seems original.
10062,3,"\n\nUnlike traditional regression, here we are not observing noisy realisations of the true target, since the algorithm is bootstrapping on non-stationary targets."
10063,3," To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet."
10064,3," But in general there may not be a 2D intrinsic property, or there is a higher dimensional hidden structure - so why not 3D or more? Related to this, why not using an objective that would result in a dynamics similar to a growing neural gas instead of an SOM?"
10065,3, The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices.
10066,3,\n- Could we look at the two distributions of inputs that each neuron tries to separate?
10067,3,\n\nI am also surprised about the baseline choice.
10068,3," For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts."
10069,3,"\n\nSecond, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network)."
10070,1," \n\nPros:\nThe paper is well written, the analysis interesting and the application of the Tucker2 framework sound."
10071,3, I think a much more complicated data would be more interesting.
10072,3," For instance, it only became clear at the end of the section that E was learned."
10073,3, \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE.
10074,3, What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted?
10075,1," In general I have a positive opinion about the paper, however, I\u2019d like to ask for some clarifications."
10076,3, This needs to be addressed.
10077,3,  The paper seems to claim that since overlapping architectures have higher expressivity that answers (a)
10078,3, \n\nComments:\n-\tThe paper mainly focuses on the soft sensor selection.
10079,1,"\n\nOverall, this seems to be a nice paper to me."
10080,1, \n- Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better.
10081,3,"\n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so."
10082,3," Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps."
10083,3," Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail."
10084,2,"If the author is comfortable with having his/her name on THIS, then I wont stand in the way of publication."
10085,3," However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims."
10086,3, Are there other alternatives in the literature?
10087,3,"   Then the input image could be rectified to a canonical orientation and scale, without needing equivariance."
10088,1, It is in line with current trends in the research community and is a good fit for ICLR.
10089,3,"""This paper applies the word pairs, instead to bag of words, to current RBM models"
10090,3,\n\nI'm also curious whether using a stochastic latent variable (Z) is necessary
10091,3, Would it be enough to match the DISTRIBUTION of the confidence? 
10092,3,  \n\nOverall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak.
10093,3, The approach is evaluated for several attacks on MNIST data.
10094,3,\n\nMinor/typos:\n- what is G(j|G\\j) in eq. (9)?
10095,3," The implementation of this seems straightforward, and can be done either via initialization or via pruning on the off-diagonals."
10096,3," Any optimal policy would therefore need to spend some time e in the danger state, on average."
10097,3," In addition, I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases."
10098,3, It appears to be the number of latent convolutional filters or channels generated by the state embedding network. 
10099,1, The proposed approach aims to overcome these drawbacks.
10100,3,\n\nMy major concerns are as follows.
10101,2,"The system described in the manuscript is inherently confusing. Despite re-reading, its basic elements did not make sense."
10102,3, Note that a similar strategy has been used in the recent past under the name of stability training.
10103,3,"""The paper proposes, under the GAN setting, mapping real data points back to the latent space via the \""generator reversal\"" procedure on a sample-by-sample basis (hence without the need of a shared recognition network) and then using this induced empirical distribution as the \""ideal\"" prior targeting which yet another GAN network might be trained to produce a better prior for the original GAN."
10104,3,\n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart.
10105,3, The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples.
10106,3," Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small."
10107,1, The experiments show that the proposed algorithm allows a synthesizer to do a better job of reliably finding a solution in a short amount of time (though the effect is somewhat small).
10108,2,Do we really need concepts and theories to discuss inequalities among young adults?
10109,1,"\n\nAs a side note, the k-NN MOA is central to for the evaluation of the proposed approach."
10110,3, Or did they invent the margin loss?
10111,2,This paper is an experiment to try and determine how badly a research paper can be but still be accepted
10112,3," Can this be related to the \""flat vs sharp\"" dilemma ?"
10113,3, (actor-critic on machine translation)\nMiao & Blunsom (2016) Language as a Latent Variable: Discrete Generative Models for Sentence Compression.
10114,1, I am curious about the efficiency of the method.
10115,1," \n\nSummary of the review: \n\nThe paper is well written, clear, tackles an interesting problem."
10116,3, Both RNNs are trained jointly and only the forward model is used at test time.
10117,3,"  https://en.wikipedia.org/wiki/Contrasting_and_categorization_of_emotions.\n\nOne of the most popular theories of emotion is the theory that there exist \u201cbasic\u201d emotions: Anger, Disgust, Fear, Happiness (enjoyment), Sadness and Surprise (Paul Ekman, cited by the authors)."
10118,3," In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN)."
10119,3," Taking into account the Lipschitz constraint and (non-) differentiability of optimal critic functions f are discussed in section 4 and Section 5 proposes a way to penalize candidate functions f that do not satisfy the Lipschitz condition using a tuning parameter lambda, ruling a trade-off between marginal fitting and gradient control."
10120,1,"This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5."
10121,1,\nGood results.
10122,1," The authors seem to understand the literature well, and not being an expert myself, I have the impression they are doing a fair job."
10123,3, \n\nThere is also a remark that G(A) tends to be modular when lambda is small which is useful.
10124,3, I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.
10125,3,"""Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention."
10126,3," Appendix A seems obvious but it cannot prove the validity of the assumption made in problem (2).[[CNT], [null], [CRT], [MAJ]] Based on previous works such as \u201cMulti-task Sparse Structure Learning with Gaussian Copula Models\u201d and \u201cLearning Sparse Task Relations in Multi-Task Learning\u201d, when the number of tasks is large, the task relation exhibits the sparse structure."
10127,3,\n\n4. The proposed method only considers the \u201c+-shape\u201d and \u201cx-shape\u201d sparse pattern.
10128,1,"\n\nEDIT: I have read the author's comments and am satisfied with their response. I believe the paper is suitable for publication in ICLR."""
10129,3,"\n\nThe proposed method has at its core a method for learning a parametric skill function (PSF) that takes as input a description of the initial state, goal state, parameters of the skill and outputs a sequence of actions (could be of varying length) which take the agent from initial state to goal state."
10130,3,"\n2. Based on mixture assumption, I suggest the author add one more comparison to other method,"
10131,3,". If the aim is to have realistic samples, a visual turing test is probably the best metric."
10132,3," [In general, I also recommend against using figure captions to describe the setup."
10133,3,  How should this be tuned so developers would want to use the tool?
10134,3,".\n- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted."
10135,3," Even if the logits are zero, the class would have a\nnon-zero probability and would receive gradients."
10136,1, I recommend acceptance to ICLR18.
10137,3, Some results are counterintuitive if the reader is not familiar with related works (e.g. the Zhang et al. 2016 achieves a lower acceleration with much lower ranks).
10138,1,\n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance.
10139,3, This context is conditioned upon to generate missing words.
10140,3,"""This paper focuses on the sub-problem of discovering previously unseen classes for open-world classification."
10141,3," With respect to KL-divergence, a G-test can be used (see https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback.E2.80.93Leibler_divergence)."
10142,2,I dont see how your approach has potential to shed light on a question that anyone might have.
10143,3,\n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves.
10144,3,"""The authors study cases where interpretation of deep learning predictions is extremely fragile."
10145,3, Maybe also rotating the codes with the singular vector matrix V or \\Sigma^{0.5} V?
10146,3,\n\nIt seems that you could have written a very different paper about model compression with CRFs that would have been very interesting and you could've have used many of the same experiments. It's unclear why SPENs are so important. 
10147,1," this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale."
10148,1,\n\n[after rebuttal: revised the score from 7 to 8].
10149,3," This is the case since for the generated realizations, the noise values are known."
10150,3," I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough."
10151,3,"This could be interesting for low power case, even if the \""effective compute\"" is larger than the baseline.\n"""
10152,3, Does the argument hold for general noise distributions ?
10153,2,"Starting from the title, the paper is a complete nonsense. [â€¦] . The paper is a long, verbose and unnecessary description of obvious or well-known stuff. [â€¦]. For the reasons above, rejecting the manuscript is a moral obligation."
10154,3,"\n- Section 4.3: \""Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update\"" is ambiguous: what is exactly meant by \""iteration\"" (and sometimes step elsewhere)?"
10155,3," It proposed a \""dynamic fixed point\"" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format."
10156,3, (How does it relate to graphs?
10157,3,"\n- The authors claim their method is \u201cvastly\u201d better at paraphrasing phrasal verbs than baselines, based on qualitative comparison."
10158,3," Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.\n\n"
10159,1, The paper finally provides an evaluation on the mini ImageNet problem without significantly improving on the MAML results on the same task.
10160,3, \n\n\nDetailed comments/questions:\n-            The use of Laplace approximation is (in the paper) motivated from a probabilistic/Bayes and uncertainty point-of-view.
10161,2,"For a section on thought, very little seems to have gone into it."
10162,1,.\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power.
10163,3," (It is obviously good to cite results from prior work, but then it would be more clear if the results are invoked as is without modifications.)"
10164,3," However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful."
10165,3, There is a gap that how this corollary implies generalization.
10166,2,"It is rather clear that the paper is incomplete and hastily submitted, as it contains no contribution number. This paper is NOT COMPLETE"
10167,3," I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values."
10168,3," The\npaper isn't presented in exactly these terms, but the idea is to consider a uniform distribution\nover programs and a zero-one likelihood for input-output examples (so observations of I/O examples\njust eliminate inconsistent programs)."
10169,3, I would think that generalizing such solution concepts to stochastic games and/or deep learning might be more interesting.
10170,3,\n(b)\tExperimental Results\n2.\tThe performance of the proposed method is not significantly better than other models in MT task.
10171,3, It is based on the two-stage method that object proposals are generated from the first stage with attention.
10172,3," For the reported results, Fitnets and Highway Networks did not use Batch Normalization (which is a powerful regularizer) while VANs and Resnets do."
10173,3," Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape?"
10174,3, Any insights or results on the optimization performance vs. the number of latent tensors?
10175,1, The paper is generally clearly written and represents a valuable contribution.
10176,3,We use the method\nof Tampuu et al. (2017) to alter the reward structure of Atari Pong so that whenever an agent scores a\npoint they receive a reward of 1 and the other player receives \u22122.
10177,3," Providing more structure to the text would also be useful vs. long, wordy paragraphs."
10178,1,"\nClarity: this paper is clear,"
10179,3,"\n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works."
10180,3," I understand that since the learner algorithm is an NN, this is not the case - but more explanation is necessary here - does your method also reduces the empirical possibility to get stuck in local minima?"
10181,2,"Once I penetrated the pigeon English, I found very little substance underneath.  (Um reviewer? I think you mean pidgin. Still hurts)"
10182,3,"\n- Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT."
10183,3, A von Mises- Fisher distribution policy is then assumed to be used by the expert to generate actions that guide the agent toward the subgoal.
10184,3,\n\niv) What is the average length of the answers in both ParaphraseRC and SelfRC?
10185,3, The visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class.
10186,3,"  \n\nAlthough the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks."
10187,3,They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation.
10188,3,\n3) Outperforms SkipThought in evals
10189,1, I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference).
10190,3, Are higher or lower\n  values better? Maybe highlight the best scores for each column.
10191,1,\n\n2) FID in real data. The numbers in Table 1 appear favorable to the projection model.
10192,1," This theory seems to be interesting and might have further potential in applications."""
10193,3,"  Also, it is quite important for the paper, I think it should be in the main part."
10194,3,"\n- \""One benefit of the ARAE framework is that it compresses the input to a\n  single code vector."
10195,1,\n\n+ Quality:\n- Simple method leading to great results on ImageNet!
10196,3," The idea is to add two terms to the loss, one representing quantization error, and the other representing the number of discrete values the quantization can support (or alternatively the number of bits used)."
10197,3,"  Sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselines,"
10198,1," This is a very practical idea, well-explained with a thorough set of experiments across three different tasks."
10199,1,"\n\nThis is a relatively new area to tackle, so while the experiments section could be strengthened, I think the ideas present in the paper are important and worth publishing."
10200,3, The paper shows how the densities at each nodes are computed (and normalised).
10201,1,\nIt seems that the authors have made an effort to accommodate reviewers' comments.
10202,3," how well can it be\nintegrated into a distributed workflow?), and included some comparisons with\nother recent recommended ways to increase batch size over time."
10203,3,\n+ [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks.
10204,3," Also, to be fair when discussion the results, the authors should say that simple concatenation outperforms the single sensor paradigm."
10205,3," \n\nIn the comparison to previous work, please explicitly mention the EMD algorithm, since it's used in the experiments."
10206,3," The authors also introduces \""Ensemble voting\"" facilitate exploitation"
10207,1,"\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem."""
10208,3," The Seq2seq architecture incorporates both intra-temporal and intra-decoder attention, and a pointer copying mechanism."
10209,3,Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere.
10210,3, Any observations about how it breaks down?
10211,3,\n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.
10212,1,\n2. Useful for object counting problem.
10213,3,"The authors should use the same hyper-parameters for all methods (Jaderberg, Zhang, Rank selection)."
10214,1,"\n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree."
10215,2,"In the end, I am not sure what is the thesis of the essay (or whether there is one) or how and why the parts are connected"
10216,2,"This casual tone is also borne out on the author's website, which is inappropriate if not offensive as a professional introduction."
10217,3,"""This paper considers the task of aspect sentiment classification, which entails categorizing texts with respect to the sentiment expressed concerning particular aspects (e.g., television resolution or price)."
10218,1,"It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models."
10219,3, This adds stochasticity as well so why and why not this work? 
10220,3, \n\nMy first concern is about the architecture description.
10221,3," For (2), the \""hard\"" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. "
10222,1," The experimental evaluation is complete and accurate. \n\n"""
10223,3," Did I miss this, or is it not explained anywhere?"
10224,3,"\n\n- Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet."
10225,3," If you trust the AIS estimate, then the result will be the actual KL divergence between the FFG and the true posterior."""
10226,2,This manuscript is a long drive through a countryside. A lot of stuff is irrelevant and pointless.' h/
10227,3," \n\nWeaknesses:\nThe paper gives the impression to be rushed, i.e. there are citations missing (page 3 and 6), the encoder model illustration is not as clear as it could be."
10228,3,"""This paper proposes a new convolutional network architecture, which is tested on three image classification tasks."
10229,3, It would be very helpful to describe a potential scenario where the proposed approach could be useful.
10230,1,\n\n1.  The result in Section 4.3 empirically showed that Trust Region Method could escape from sharp local minimum.
10231,3," It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau)."
10232,3, \n\nThe idea of the proposed method is related to the classic Dyna methods from Sutton.
10233,1," The authors did a good job of using different experiments (filtration number analysis, and teaching both the same architecture and a different architecture) to intuitively explain what their method actually does."
10234,3," Given the kind\nof general trend the authors seem to want to show here, I feel that a more\ntheoretic measure of problem hardness would be more appropriate here."
10235,1,"\n\nReview:\n\nThe paper reads well,"
10236,3,"n\n2. Strong human prior, network morphism IV is more general than skip connection, for example, a two column structure belongs to type IV."
10237,3,"\n\nComments:\n- The theoretical result (thm. 1) studies the case of full optimization, which is different than the proposed algorithm (running a fixed number of weight updates)."
10238,1, \n-  Open source codes.
10239,3,"\n\nFinally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently."
10240,3,"\n\nMinor:\n\nIn (5), what is the purpose of the -1 term in R_e?"
10241,3," \n- Why are Joulin 2016, Krause 2016 not relevant?"
10242,3,  It was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human readers.
10243,3,"\n1. GAN are sufficient to learn \u00ab\u00a0semantic mappings\u00a0\u00bb in an unsupervised way, if the considered networks are small enough\n2. Controlling the complexity of the network, i.e. the number of the layers, is crucial to come up with what is called \u00ab\u00a0semantic\u00a0\u00bb mappings when learning in an unsupervised way."
10244,3,"""This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window)."
10245,3,"\n\nSome typos:\n- Section 2.1:\nin the definition of G_t, the expectation is taken over p as well\nI_w and T_w should be a subset of S"
10246,3," It would have been nice to see a more systematic treatment of the specifics of the regularization needed to get grid cells. """
10247,3," The baselines chosen are 1). no embeddings 2). generic embeddings from english wiki, common crawl and combining data from previous and new domains."
10248,3," The paper\nsuggests always adding the I/O example that is least likely under this predictive distribution\n(i.e., the one that is most \""surprising\"")."
10249,3,\n\n- I think a bit more analysis is needed in section 4.2.
10250,3, This paper is mostly about linear networks.
10251,3,"""This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017)."
10252,3,"\n[4] M. L\u00e1zaro-Gredilla. Bayesian warped Gaussian processes.\n"""
10253,3,"\n4) If I understand dependency reinforcement embedding correctly, a RNN is trained for every trace. If so, is this scalable?"
10254,3, \n2. A cycle consistency loss that makes sure the content vector of transferred sentences and style vector of the original sentence should be able to reconstruct the original sentences.
10255,3,". Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models."
10256,3,"""The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams."
10257,1,"  Even tough a little bit ad-hoc, it seems promising based on the experiment results."
10258,3,"""The paper proposes training ``inference networks,'' which are neural network structured predictors. "
10259,1,"\n\nFirst of all, the paper is very well written and structured."
10260,3,"\n2. Pairwise Classification Network, (binary sigmoid, trained on pairs of images of same/different classes)"
10261,3,"""This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks."
10262,1," \n\nOverall, I find the paper important for furthering the understanding of fundamental RL algorithms."
10263,3,  This paper then compare this objective with the MMD distance between the samples A & B.
10264,3, More explanations are appreciated.
10265,2,The paper is an old fashioned one
10266,1," Next, the authors consider the problem of learning a time-unlimited policy from time-limited episodes."
10267,2,The value of this manuscript is not so much that this is novel research as it is a demonstration of opportunistic sampling and analysis
10268,3,"\n\n- The writing should be careful, e.g., \nvalue iteration is presented as a learning algorithm (which in my opinion is not)"
10269,3, Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices.
10270,3," This provides benefits already in the tabular case, since SRs naturally extend to domains with stochastic dynamics. "
10271,3,"   \n\nThe first part of the paper offers a strategy for constructing a product MDP out of an original MDP and the automaton associated with an LTL formula, and reminds us that we can learn within that restricted MDP."
10272,3, A domain name data set with two million instances is used for the experiments.
10273,3, Please refer to the paper \u201cDiscovering structure in multiple learning tasks: The TC algorithm\u201d published in ICML 1996.
10274,3," (Though the proposed system does have the advantage of only requiring a screenshot created using any software, rather than being restricted to a particular piece of software.)"
10275,3,\n+ It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses.
10276,2,The phrases I have so far avoided using in this review are 'lipstick on a pig' and 'bullshit baffles brains'
10277,3, \n\nSee also comment [*] later on the presentation.
10278,3,"  Hence, it remains unclear to what extend the achieved improvements are due to the proposed network design changes or the particular dataset they use for training."
10279,3,\n\nIt could be made clearer how significance is tested given the frequent usage of the term.
10280,3," If space allowed, an example of different ordering leads to the same graph will also help."
10281,3,\n\n(3) About the speedup: it could be imaged that the speedup from the usage of dilated CNN largely depends on the model architecture.
10282,3," In fact, you don't have any results demonstrating that the saddle-point approach is better than simpler alternatives."
10283,1, The paper is clearly motivated and authoritative in its conclusions
10284,3,"\n\nThe image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach."
10285,3," However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important."
10286,3," Because of this, two conditioned set ups are performed: 1) 25% of pixels are given before generation, and 2) 75% of pixels are given before generation."
10287,3,   Can this method be applied to lattices?
10288,2,The only conceivable contribution this paper can make is by providing the academic community with an alternative to counting sheep.
10289,1,"""\n-----UPDATE------\n\nHaving read the responses from the authors, and the other reviews, I am happy with my rating and maintain that this paper should be accepted."
10290,3," In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time."
10291,2,The paper is - and I mean this with no disrespect to the author- a sort of echidna or platypus of a paper.
10292,3,"""The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data."
10293,3,"  I think there are a lot of little details that don\u2019t seem that explicit to me.[[CNT], [null], [CRT], [MIN]]  How many seeds are run for each curve (are the results an average over multiple seeds)."
10294,3," The paper empirically evaluates this idea on Atari games with deep non-linear state representations, compared to state-of-the-art baselines."
10295,3," Also, it is always possible to simply add more paths to FractalNet if desired without increasing depth."
10296,3,"\n\nI don't see how the German verb \""orders\"" inflects with gender..."
10297,3, \n \n- What is the value of the learned syntax in section 5.2?
10298,3," The authors survey different methods from the literature,\npropose a novel one, and evaluate them on a set of benchmarks."
10299,3,"  To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations."
10300,3, \n\nIt would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.
10301,1," With some filling out, this could be a great paper."""
10302,3, I couldn\u2019t find it based on the results presented here
10303,1,  The authors present a very intriguing novel approach that  in a clear and coherent way. 
10304,3,"\n   a. End-to-end trained quantized networks have been studied in various previous works including stochastic neuron (Bengio et al 2013), quantization + fine tuning (Wu et al 2016 Quantized Convolutional Neural Networks for Mobile Devices), Binary connect (Courbariaux et al 2016) etc."
10305,3,"""The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks."
10306,3," Given that I don't even\nthink the representation of inputs and outputs is practical in general, I don't see what the \ncontribution is here."
10307,1,\n\n[Pros]\n- The paper is easy to follow.
10308,1,"\n\nAccording to the authors, the contributions are the following:\n1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to \""make sure\"" that the representation learned is used in the most efficient way."
10309,1," The visualizations are interesting and provide some general intuition, but they don't yield any clear novel insights that could be used in practice."
10310,3," Instead of considering the direct neighborhood of a vertex in the convolution step, a filter based on outgoing walks of increasing length is proposed."
10311,3,  Evaluations are performed on ImageNet and CIFAR-100.
10312,3,"  The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students."
10313,3," \n\nThe existence of amortization error is often ignored in the literature, but (as the authors point out) it is not negligible."
10314,3,"""This paper proposed a new optimization framework for semi-supervised learning based on derived inversion scheme for deep neural networks."
10315,3, There is nothing that prevents the model from using multiple dimensions to capture related structure of the data.
10316,3," \n\nOne other paper of note given that the authors train a MLP is an optimal teaching analysis of a perceptron: (Zhang, Ohannessian, Sen, Alfeld, & Zhu, 2016; NIPS).\n\n\n"""
10317,3, \na) How to set/learn the scaling parameter \\lambda_y and \\beta_y
10318,3, An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set.
10319,3," Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows\"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product)."
10320,1,\n\nI like the message conveyed in this paper.
10321,1, I support the acceptances of this paper.
10322,3, \n-- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions?
10323,3,"\n\n2. In order for the paper to be more self-contained, maybe list at least once the formula for \""rectifier net\"" (sth. like \""a^T max(0, wx + b) + c\"") ?"
10324,3,"\n\nI apologise for short and late review: I got access to the paper only after the original review deadline."""
10325,3," Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks,"
10326,3,  It's much more revealing to compare it to the empirical likelihoods of the words.
10327,3," Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications."
10328,3," Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C]. "
10329,1,  Synthetic and some simple real-world experiments demonstrate the effectiveness of the algorithm compared to random features given the fix number of bases.
10330,3," \n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\n"""
10331,3," In particular, articles are frequently missing from nouns."
10332,3,"\n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data. "
10333,3,  Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior?
10334,3,\n- DNN was used in Section 2 without being defined.
10335,3,"\n* Strictly speaking it is correct to refer to the individual nets in the ensembles as \""branches\"" and \""basic blocks."
10336,3,  It seems to me that the model trained on meaningful data should have a larger margin.
10337,1,  The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate.
10338,1, The ideas and formalism of the merge and partition operations are valuable contributions.
10339,3," A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions."
10340,3," I do not see why this should be formulated as a \""sequential\"" decision problem."
10341,3,"\n\nRegarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test."
10342,3,"\n\nOverall, I would like to see if the paper could have been stronger empirically."
10343,3," The assumption given in the introduction is that softmax would not yield such a representation, but nowhere in the paper this assumption is verified. "
10344,3," To accomplish this, they use a generator LSTM that takes in a sequence of random noise as well as a sequence of conditonal information and outputs a sequence."
10345,3,"On top of that, one can apply a trick similar to the one used in the linear case --that is,  construct the matrix representing the diffusion model by simply stacking samples of the SRs."
10346,3,"\n\n3. When choosing the orthogonal matrix, I think one obvious choice is to sample a matrix from the Stiefel manifold (the Q matrix of a random Gaussian). This baseline should be added in additional to H and B."
10347,3,"Specifically, compare the accuracy of a classifier trained on a noise-perturbed version of the real dataset to that of a classifier trained on a mix of real data and synthetic data generated by the model being evaluated."
10348,1," This has the advantage that the training process can be better parallelized, allowing for faster training if hundreds of GPUs are available for a short time."
10349,3, Results with linear and non-linear function approximation highlight the attributes of the method.
10350,3,"""The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized."
10351,3, It would be intersting to present some analysis regarding the gradient w.r.t. W.
10352,3," Secondly, more datasets including imagenet needs to be tested."
10353,3," Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes"
10354,3,\n\n- Is there a particular reason why the central points appears in both complementary kernels (+ and x)?
10355,1, The use of attention heat maps is interesting.
10356,3," This does not require any \""memory\"" because all necessary information is available to the network."
10357,3, A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper.
10358,3," Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here)."
10359,1,";\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory"
10360,3, \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format?
10361,2,Please correct the language and connet of teh COI part!
10362,3,"\n- It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups)."
10363,3, Am I missing something about the algorithm?
10364,1,"\n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights"
10365,3, How would you train such baselines?
10366,1, The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost.
10367,1, Extensive experiments demonstrate the usefulness of the approach.
10368,2,I would refrain from using enumerations in your paper and instead encourage you to think about the deep masculinism that comes with it
10369,3," They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action."
10370,3," DAuto does seem to offer more boost in domain pairs that are less similar. """
10371,3, How do this marginals help solving the NLI task?
10372,3, A plot demonstrating the tradeoff of \naccuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance.
10373,3,. It is hard to conclude which method is better based on a single real-world experiment
10374,3,"""The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning."
10375,3," I was also wondering about when 2 or more layers are block sparse, do these blocks overlap?"
10376,1, (The improvement on Zaslavsky's theorem is interesting.)
10377,3," Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective."
10378,3,"  Combining with an reconstruction objective and \""delete-and-copy\"" trick, it is able to cluster the data points into different groups and is shown to give competitive results on different benchmarks."
10379,3,"\n- VAN with the following altered equation: x_{l+1} = \\mathcal{F}(x_l) + (1-\\alpha)x_l, i.e. please remove the alpha in front of \\mathcal{F}."
10380,3, \n\nThe work is put in context and related to some previous relaxation approaches to sparsity.
10381,3," Indeed, Neal himself says that in the multi-output settings, all the outputs become uncorrelated."
10382,3, \n\nThe choosing of \\alpha_\\mu is generally large (10^4-10^5).
10383,3, AdvGAN is able to generate adversarial samples by running a forward pass on generator.
10384,1, \n\nSummary:\nThe goal of the technique to involve human interaction in generative processes is interesting.
10385,3," These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such."
10386,3,"\n\n\nReferences:\n[1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192).\n[2] Pad\u00f3, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199."
10387,3, You still propose to learn the proposal parameters using SMC but with lower number of particles?
10388,3,\n\n\nAbout the description of problem statement in Section 3:
10389,3,"\nFor the non-deterministic information, they have a residual predictor that uses a low-dimensional latent space. "
10390,3,"  They are thus not meaningless, but are a measure to model reaction instead of an independent process."
10391,1,\n\n*Clarity*\nThe paper is in general well written and easy to understand.
10392,3,"""This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T)."
10393,3," They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.)"
10394,2,"If the editor somehow accepts this paper, they risk permanent destroying the credibility of this journal and its editorial board"
10395,3," However, the authors provided no theoretical study on any of these aspects."
10396,1,\n\nPositives: \n- The paper is clearly written and easy to follow.
10397,3,  The performance of this approach is measured on several UCI datasets and compared with baselines.
10398,1,\n\nThis is a strong paper that presents a significant improvement in document summarization.
10399,1,\n\nClarity\n\nThe rationale is clear and the results are straightforward to interpret.
10400,1," From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference."
10401,2,Unless the authors performed some clever pagan ritual before euthanizing the animals I would use killed' instead of 'sacrificed'.
10402,1,\n\nPositives:\n\n(1) The approach is straightforward to implement and trains networks in a reasonable amount of time.
10403,1,"\n\nThe paper is written well,"
10404,3," Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent."
10405,2,Table 2 stunningly over-interprets some relatively small signals in the data.
10406,1," \n* At several points, the authors seek to make the work accessible by offering lay explanations for their more technical points."
10407,1," For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work"
10408,1," The paper is well explained, and it\u2019s also nice that the runtime is shown for each of the algorithm blocks."
10409,3," \n(a)In section 4, how to use formal method (Ledig et al., 2016) to enlarge the portrait from 64x64 to 512x512 is unclear."
10410,3,\u201d \u2014 Isn\u2019t this more than initialization?
10411,2,I would suggest either activating the spell-checker on Word or finding a way to keep your cat from walking on the keyboard.
10412,3, Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.
10413,3, but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away.
10414,3," 2) proposing a minor modification of the DTP algorithm at the output layer,"
10415,3,"""The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits."
10416,3, This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time.
10417,3,  This however should be quite important.
10418,3, To address this issue a `loss aggregation\u2019 is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used.
10419,3,"""The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community."
10420,3, In the experiments the authors generally show improved convergence over SVAE.
10421,3, There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future.
10422,3," \""This approach performs backpropagation through each step of gradient descent, permitting more stable training but also evidently more overfitting."
10423,3," So, I wish to see a section on testing with Resnet and GoogleNet."
10424,3, \n\nWhat is the exploration strategy in the experiments?
10425,1," given they are quite interesting.\n\n\n"""
10426,3,"  They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic)."
10427,2,"The title appears to me to be clickbait, and, as usual for clickbait, leads to disappointment"
10428,1,". The authors have started with right motivation and the initial section asks the right questions, however,"
10429,3," \n\n\n# Suggestions on readability\n\n* I have the feeling the paper inverts $\\alpha, \\beta$ from their use in Finn 2017 (step size for meta- vs task-training)."
10430,3, \n5. Can the authors show convergence of the teacher parameter \\theta?
10431,3,\n\n2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector?
10432,3, \n\n2. The idea in the approach seems novel (I have not found the similar training strategy) and they tested several UCI datasets with different fully connected network topologies and activation function settings.
10433,3, The method works by introducing a parameter associated with each weight to scale its learning rate based on the class label.
10434,3, In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n 
10435,3,"\n\nFor semi-supervised and active-learning results, please include error bars for the miniImagenet results."
10436,3,\n\nA few recent works have directly tested increasing batch sizes during\ntraining.
10437,1,\n\nResult (3) is more interesting as it is a new result.
10438,3,\n\nCons:\n- The model-based approach is disappointing compared to the model-free approach.
10439,3, However it would be nice to see the difference to other related methods more clearly.
10440,1,\n(+) The authors ablate and experiment on large scale datasets
10441,3," Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering."
10442,3,. Is the point that GloVe is a bad algorithm?
10443,3,\n\n- What is the objective of the problem in section 3
10444,1," The experiments in the paper demonstrated skip RNNs outperformed regular LSTMs and GRUs o thee addition, pixel MNIST and video action recognition tasks."
10445,3,"""This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks."
10446,3, \nAuthors propose a number of baseline models as well as metrics to assess the quality of \u201cdefogging\u201d.
10447,3, How does it compare to other methods?
10448,1, It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods\n3.
10449,3," On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy."
10450,1,\n\npros:\n\n- good use of intuition to guide algorithm choices
10451,3," Or where it attends varies across examples?\n\n10. Are you willing to release the code for reproducing the results?\n\nMinor comments:\n\nPage 1, \u201cexploit his own decision\u201d should be \u201cexploit its own decision"
10452,3,"""This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples."
10453,1,\n\nHere are the pros of this paper:\n1) Useful contribution in terms of using broader context for embedding a sentence.
10454,3," So I suggest that results for RL should be reported with and without intra-attention on both datasets, at least on the validation set."
10455,3,"\n\nSpecifically, the authors say: \""In our experiments, we use the result of \nminimising the variable corresponding to the output of the network, subject \nto the constraints of the linear approximation introduced by Ehlers (2017a)\""\nwhich sounds a bit like using linear programming relaxations, which is what\nthe approaches using branch and bound cited above use."
10456,3, I would prefer less material but better explained.
10457,1," Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model. "
10458,3," Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. \""two of the predicted titles for Die Hard have something to do with dying and being buried\"")."
10459,3," If no, I don't see how those curves compare."
10460,3," This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores."
10461,3, I wonder how does this approach compare with the proposed method.
10462,3,\n- The number of layers over which soft ordering was tested was fixed however. 
10463,1,"""The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only."
10464,3,\n\n- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.
10465,3," In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time."
10466,3," At each layer, the targets can be chosen using a variety of search algorithms such as beam search."
10467,1,\n\ncons:\nThe provided experiments are weak to demonstrate the effectiveness of the proposed method.
10468,3, One example is Section 4 in [Courbariaux et al. 2016].
10469,1,"""This should be the first work which introduces in the causal structure into the GAN, to solve the label dependency problem."
10470,1, Extensive experiments are performed to demonstrate the effectiveness of the proposed methods. 
10471,3,"  Regardless, the authors should report the micro-F1 in addition to the macro-F1."
10472,3,\n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well?
10473,3,\n\nCons:\nNot clear justification and motivations
10474,3, It would be very useful to know\nwhether the exact linear Gaussian model in the last layer proposed by the\nauthors has advantages with respect to a variational approximation on all the\nnetwork weights.
10475,3,\nI would only suggest to expand the experimental section with further (real) examples to strengthen the claim.
10476,3,"""This paper presents a model-based approach to variance reduction in policy gradient methods."
10477,3, \n\nThe paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model.
10478,3," \n\nMinor comments:\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint."
10479,3, The DeepLift explanations are almost as noisy as the ones obtained for Sensitivity Analysis (i.e. the gradient at the input point).
10480,3," The paper counts up these word-pair co-occurrences in a tensor, then applies a tensor decomposition and low-rank approximation method to produce word and preposition representations."
10481,3,\n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix
10482,3," Especially, the authors propose a visual Turing test to evaluate the synthesize quality of three generative models: WGAN-GP, DFC-VAE, and Pixel VAE."
10483,3," Related work is mentioned in 2.3, but a direct comparison in which the experimental settings are identical in the evaluation would have been helpful."
10484,3,It is not clear exactly when this will be useful. 
10485,1,"\n\nIn summary, the results and presented method are good, and eventually deserve publication."
10486,1, The approach is likely to be useful for other\nresearchers working on related problems.
10487,1,"  To summarize, the following are the pros of the paper:\n\n  - clarity and good presentation;"
10488,2,"Focus on limitations, because they are many!"
10489,3," Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014."
10490,3," However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training."
10491,3,"\n\nThe authors might discuss more how to extend their model to image recognition, or at least of other modalities as suggested."
10492,3," However, there is not a comparative evalution with these methods."
10493,3, \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders.
10494,3," Particularly with some clarity on the experiments, I would be willing to increase the score."
10495,1," \n\nThe paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid."
10496,3, The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method.
10497,3, This organization would make the writing much clearer.
10498,1, Figure 2 & 3 helps the readers understand the core algorithm.
10499,3,  Still I give it a try.
10500,3,"\n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines."
10501,3,\n\n6. Possibility to apply to natural images.
10502,3," In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category."
10503,3," \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc."
10504,1,  I think these contributions warrant publishing the paper at ICLR 2018.
10505,3, it will be good to test directly on real long-tailed datasets.
10506,3,"\n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? """
10507,3,(c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function\u2019s optimization
10508,3, That will show the generality of the proposed architecture.
10509,3," \n\nThe second contribution is partially valid,"
10510,3,"""In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes. "
10511,3,"\n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious"""
10512,3," I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of \""deep exploration\"" and you should be clear that your parameter noise does *not* address this issue."
10513,3,"  I would encourage the authors to release code that can\ndirectly generate Figure 2 and table 2.\n"""
10514,3," In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream."
10515,3, \n* Related work.
10516,3,"""This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN."
10517,3,\nii) The learned latent representation provides an interpretation of generated graph properties
10518,1," Given the amount of data 2*10**6 samples, this seems sufficient."
10519,3,\n\nminor comments:\n- on page five in the last paragraph there is a left parenthesis missing in the inline formula nabla dot w_t(x))^2.
10520,3," As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution"
10521,2,"This is like a project notebook. When Michelangelo finished the sistine chapel, did he also try to sell the scaffolding as a work of art?"
10522,3, These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.
10523,3," \nThey also evaluate the algorithms on the quality of the final policies for their approach, DQN, \nand  a supervised learning from demonstration approach ( LfD ) that requires expert actions."
10524,1," I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant."
10525,3, \n\nMutual information between the successive layers is decomposed as an entropy plus a conditional entropy term (eq 17).
10526,3,  It was not clear from this presentation how the human participants were rewarded for their performance.
10527,3,\n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains.
10528,3,"\n  \u2022 On page 4 and 5, when describing the gradient for the actor and critic, it would be better if these were given their own terminology, but if not, then use of the word respectively in each case would help."" "
10529,3," It can be expected, that after a change to these parts of the input, the model will decide differently, albeit coming to the same conclusion (in terms of predicted class membership), which reflects in the explanation map computed for the perturbed input."
10530,3, Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space.
10531,3,"""The paper proposes combining classification-specific neural networks with auto-encoders."
10532,1, \n+ ablation study / analysis of influence of parameters
10533,1," \n\nThe conclusion is speculative:\n\""Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it\nis key to suppress this subspace of shared positive directions, which can possibly be done through\nregularization of the objective function."
10534,1,"\n\nOverall, this seems like a natural and effective approach, and achieves good results."
10535,1, This could be of interest for a broader audience.
10536,3, It would be great to see the authors address this issue in a serious manner.
10537,3,"""This work re-evaluates complex-valued neural networks: complex weights, complex activation functions."
10538,3,"\n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? "
10539,3,"""The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices."
10540,2,The original study was published in PsycScience. This is just some work by a grad student. Reject. [on a failure to replicate a finding
10541,3,"""This paper introduces a new task that combines elements of instruction following\nand visual question answering: agents must accomplish particular tasks in an\ninteractive environment while providing one-word answers to questions about\nfeatures of the environment."
10542,3,"\n\n3. In the decoding phase, how does the system decide whether to query the DB?"
10543,1,"\n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new."
10544,2,"If the paper is accepted, I strongly recommend an English prof-reading."
10545,3, I would expect the resulting architecture to perform at least as well as variable action nets.
10546,3,"To be more informative they should be reported by number of epochs, in addition or not to percentage."
10547,3, The Bayesian neural networks are only\nBayesian in the last layer. 
10548,3, (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)
10549,1, Experiments are clearly described and results are significantly better compared to state of the art.
10550,3, They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features.
10551,3, Every category \nis defined by a specific combination of color and shape that is well recognized at test time.
10552,3," \nMy only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself."
10553,3,n\nI think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them.
10554,3, \n2. The technique is built on a lot of heuristics without theoretical consideration.
10555,3,  FashionMNIST and MNIST are similar in many ways.
10556,1,\n+ The idea to keep the decision boundary in the low-density region of the target domain makes sense
10557,3,"\n\nThe citation style in section 2.4 seems off.[[CNT], [PNF-NEG], [CRT], [MIN]] Also see [4] for a great description of how beam search is done in CTC."
10558,3,\n2) Could margin-based generalization bounds explain the superior generalization performance of the linear model trained on random vs. non-random data?
10559,3,"  I think that the paper would be made stronger and clearer if the questions I raised above are addressed prior to publication."""
10560,3,My recommendation is that you either rebrand the paper to be more about general methods for amortizing structured prediction inference using model compression or do more fine-grained experiments with SPENs that demonstrate empirical gains that leverage their flexible deep-network-based energy functions
10561,3,"\n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014."
10562,3, This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node.
10563,1," The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises:"
10564,3, \n\nThe entire dataset is based on 4 patients.
10565,3,\nThe AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space.
10566,3, The paragraph below Eq. 7 provides a possible explanation for the performance drop when $m$ is increased.
10567,3,\n\n- Adding results for m=3 to table 2 would bring further insight to the comparison.
10568,1, \n\nThe experimental results seem promising in the illustrative MNIST domain.
10569,3," Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions."
10570,3, This can be improved to help readers understand better.
10571,3,\n\nCons:\n\n1. This paper proposed a rather ad hoc proposal for training neural networks.
10572,1,"\nOverall, although the result are not very surprising, the approach is well justified and extensively tested."
10573,1,\n\nTherefore I would now support accepting this paper.
10574,1,\n\nQuality and significance:\nThe proposed methodology is simple and straightforward.
10575,3, The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.
10576,3," \n\n- Final section of 5.3, about the validation accuracy, and the accuracy on the test set after 50 epochs.[[CNT], [null], [DIS], [MIN]]  I am confused why these are different numbers.[[CNT], [null], [DIS], [GEN]]  Is it just because 50 epochs wasn't enough to reach convergence, while 300 seconds was?"
10577,1,"""The paper develops an efficient algorithm to solve the subproblem of the trust region method with an asymptotic linear convergence guarantee, and they demonstrate the performances of the trust region method incorporating their efficient solver in deep learning problems."
10578,3, The covariate-dependent\nembeddings are diagonal scalings of the shared embedding.
10579,3,  \n\nReachability: authors show that different ways of abstracting the state s into a vector encoding affect the performance of the system.
10580,3," but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs."
10581,3, How is it compared  the L_1 regularization vs. the proposal?
10582,3," The literature might not be totally clear about this, but it is very well discussed in a recent ICML paper: White 2017 [1]\n\nAnother way to pose and think about this problem is using the off-policy learning setting---perhaps best described in the Horde paper [2]."
10583,3,\n- Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image.
10584,3,"\nLe, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton."
10585,3,\n4. How is this work related to the extensive work in NLP in applying parsing to various tasks?
10586,3,"\n- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data."
10587,1," Given the high variability of deep RL, they have not convincingly shown it performs better."
10588,3," The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately."
10589,3,"  It has been pointed out before in various ways, however:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class)."
10590,1,"\""\n\nHere one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct."
10591,1, The writing is excellent.
10592,3,"\"" --> this does not seem very precise: under what policy is the 60-80% defined?"
10593,3,"\n-Use of ACOL and GAR is interesting, also the idea to make \""labeled\"" data from unlabelled ones by using data augmentation."
10594,3," These works go way back to the years 2005 - 2007, when deep learning was not called deep learning. "
10595,3," Currently, the improvement is only marginal,"
10596,3,"\n\nConclusion\nThis paper brings in an interesting idea, is it possible to cluster the unseen classes in an open-world classification scenario? "
10597,3,"\n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \""good\"" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective."
10598,3,"""The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al. (2015)."
10599,3, It looks as if the improvement comes from adding additional capacity to the model.
10600,2,I started reading this manuscript with much anticipation but my enthusiasm was short lived.
10601,3, But at which level?
10602,1," However, these assumptions are very constraining for multi-output classification and also interesting multi-output regressions."
10603,3, The given score will improve if the authors are able to address the stated issues.
10604,3," In addition to the observability, approximate Markov TFT (amTFT) methods are more processing-intense, since they fall back on a game's Q-function, as opposed to learned policies, making CCC a lightweight alternative."
10605,2,I stopped reading here.
10606,2,"From this conclusion, the premise does not follow."
10607,1,"\n\n- It seems that the model benefits from at least two separate effects - one is\n  the attention mechanism provided by the sigmoids, and the other is the\nstochasticity during training."
10608,3,"\n\nThe experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment."
10609,3,"Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons;"
10610,3,"\n- If the main contribution is the dataset, perhaps the claim that it is \""uniquely diverse\"" could be justified with some quantitative arguments / statistics, comparing to other datasets."
10611,3," The idea is interesting to me, but I think this paper still needs some improvements."
10612,3, It would help the reader to be more explicit here.
10613,3," Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black."
10614,3,"\n\n\nUnconditioned generation experiments:\nIn these experiments, sequential pixel-by-pixel MNIST generation is performed in which the proposed method did not help."
10615,3, See the Urmson et al. 2008 paper in the bibliography.) At the very least this technique should be a baseline.
10616,3,  The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.
10617,3, It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.
10618,3,"   This should be clarified, ideally with concrete examples."
10619,1," but at least show an improvement over more a naive approach,"
10620,3, While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features.
10621,3,\n\nThis paper instead proposes to learn a surrogate function for choosing which examples to select.
10622,3," One cannot hope to do image caption association prediction without capturing the image attributes...\n\n*,"
10623,3,"The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution."
10624,3," It would be better if the paper could point out the importance of NER for user utterances, and the fact that using the knowledge of which words are NEs in dialogue models could help in tasks where DB queries are necessary."
10625,3, How robust is the defense against samples generated by a different attack network?
10626,2,Someone has been foraging in theory and has managed to learn how to mangle simple concepts and hide them behind pretentious empty prose
10627,1,"""Paper is well written and clearly explained."
10628,3," The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \""crossover\"" network is presumably time-consuming."
10629,3, It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question. 
10630,3,"\n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point."
10631,3,"\n\nA bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful."
10632,3," The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence."
10633,3," The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent)."
10634,3, This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable.
10635,3,"n- for Figure 2a it would be clearer to normalize such that 1 is the best and 0 is the random policy, instead of 0 being score 0."
10636,3,  Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.
10637,3,\n\n2. The central contribution is extending the single step LP to a multi-step formulation.
10638,3,"\n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there."
10639,3," Given this, the presentation in the paper makes the idea look more novel than it is."
10640,3, The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks.
10641,3," Could this be a result of suboptimal, possibly compromised training?"
10642,3,"\n\nThe authors rightly say that one of the skills an autonomous car must have is the ability to change lanes, however this task is not one of the most difficult for autonomous vehicles to achieve and this ability has already been implemented in real vehicles."
10643,1,"  \n\nOn these games versus (their implementation of) DDQN, the results seem encouraging."
10644,2,The findings are not novel and the solution induces despair.
10645,1,"""The paper is clear and well written."
10646,3,"\nHowever, it would have been interesting to show the evolution of the learning rates (for every class) along the epochs and to correlate this evolution with the classes ratio or their separability or to analyse more in depths the properties of the obtained networks."
10647,1,"""Update (original review below):\nThe authors have addressed several of the reviewers' comments and improved the paper."
10648,3,"\n  2. Another crucial baseline is to train the Attend, Infer, Repeat model on the ShapeWorld images, and then take the latent state inferred at every step by that model, and use those features instead of the features described in Sec. 3.4"
10649,3,  \n\nSecondly: Discuss the connection with Deep GPs (Damianou and Lawrence 2013).
10650,3, Do you have any intuition for what kind of features or information the networks are capturing?
10651,3,"\n\n+++ Hyperparameters +++\n\nSince the initial values of \\lambda and \\eta' are new hyperparameters, include the values you chose for them, explain how you arrived at those values and plot the curve of how \\lambda evolves for at least some of the experiments."""
10652,3,"\n\nIn this formulation:\n\nThen R_ME has a high response if the node has saturated responses -1\u2019s or 1``s, as one desire such saturated responses, a should be negative."
10653,3, Existing methods are then compared based on how accurately they can produce the target action sequence based on the command input sequence.
10654,3, This can be computationally demanding.
10655,3, The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest.
10656,3,\n\nSome minor points that I wonder about:\n - The heuristic against repeating trigrams seems quite crude. 
10657,3," \nLet < Wk S, Wk Ej > be a weighted similarity between learner state features S and expert state features Ej\nand Ej\u2019 be the successor state features encountered by the expert."
10658,3,"\n\nHowever the new bound is an \""upper\"" bound of the worst-case performance which is very different from the conventional sampling based \""lower\"" bounds. "
10659,3,"\n\nDiscussion:\n\n+ The paper spends a lot of time justifying the proposed method by discussing the limits of the \""Improved training of Wasserstein GAN\"" from Gulrajani et al. (2017)."
10660,3,Spatial resolution*: 1) The analysis seems to be done with respect to DNN not to a  CNN. 
10661,2,I am not very excited about this. There must be better ways to spend your money. -Grant review result. Full review
10662,3," One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization. "
10663,3," Returns with higher confidence should be weighted higher, according to the confidence estimate around the value function estimate as a function of state? "
10664,1,"\n\nTo me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple."
10665,3," The authors state this question in the future direction, but it would make the paper more complete to consider it here.\n"""
10666,2,"author needs to slow down structure at the grammatical level, but more importantly, the process of research, reflection, and argumentation"
10667,1,\n\n== Evaluation \n\nI found this paper quite clear to read and enjoyed reading it.
10668,3,"\n\nWhile these simple tasks are useful for diagnostics, it is well-known that these tasks are simple and, as the author's suggest \""more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization-based policy algorithms.\"" "
10669,3," I am curious if a similar analysis could be applied to methods evaluated on the bAbI question-answering tasks (which can be represented as graphs, like the maze task) and possibly yield better initialization or optimization schemes that would remove the need for multiple random seeds."
10670,3, An experimental comparison is needed.
10671,3, Instead of reward.. \u201d Two sentences are disconnected and need to be rewritten.
10672,3, I won't repeat it here.
10673,3,  It would be helpful to understand how this approach avoids this issues;
10674,3, The plot looks like CommNet is still improving.
10675,3," That only happens if you perfectly sample each particle from the true posterior, conditioned on all future information."
10676,1," \n\nOverall this is a strong paper and I recommend to accept.\n \n\n\n"""
10677,3,\n13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images?
10678,3, The paper does not aim at learning causal structure from data (as clearly stated by the authors).
10679,3,"\n\nTo make the paper stronger and justify importance of the proposed architecture, I suggest authors to consider relaxing assumptions on the communication protocol to allow delayed and/or noisy communication (including gradients)."
10680,3,which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\
10681,3,"  It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret). "
10682,3,"\n\n- in page 2, \""in this figure\"": which figure is this referring to?"
10683,3,"\n- as an additional ingredient the authors also propose \""representation learning\"" by mapping x to some representation Phi(x)."
10684,3,"""Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks."
10685,3,"""- This paper shows an equivalence between proto value functions and successor representations."
10686,1,\n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs.
10687,3," It relies on the logistic regression model as the discriminator, and the dual formulation of logistic regression by Jaakkola and Haussler."
10688,3, \n\n1. Can the observations be used to explain more recent works?
10689,3," To this end, the authors combined a text reconstruction loss, an adversarial decoding loss, a cyclic consistency loss and a style discrepancy loss."
10690,3, A domain-level translation function is jointly optimized with an instance-level matching objective.
10691,3," I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic."
10692,3, Auto-regressive VAE is used for decoding.
10693,3," In The 10th International Conference on Autonomous Agents and Multiagent Systems: 2, 761--768."
10694,1, \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.
10695,2,Despite promising a very general result this manuscript unfortunately does not deliver (almost) any of the things listed in the abstract.
10696,3,\n4) Drops of the learning rates are scheduled to happen at certain times represented in terms of the number of training samples passed so far (not parameter updates).
10697,3," However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC."
10698,1," I agree the computational budget makes sense for cross data transfer,"
10699,3," \n\n* Information dynamics of learning process (Figures 3, 6, 7, 8) -> I am curious as to why you did not run the PIB for the same number of epochs as the SFNN?"
10700,3,"""This paper introduces a method to learn a policy on visually different but otherwise identical games."
10701,3, It would be useful to compare with a simple neural network baseline trained for K-way classification with standard backpropagation (though the UCI datasets may potentially be too small to achieve good performance).
10702,3, The method is evaluated on multiple experiments.
10703,3, It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort(given by the analytical solution)
10704,1,\n- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well
10705,3," How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning?"
10706,3,"\n3. Formulate complex batch normalization as a \""whitening\"" operation on complex domain"
10707,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
10708,1,n\nOriginality\nThis approach to finding important hidden units is novel
10709,3," \n\nPlease find my detailed comments/questions/suggestions below:\n\n1) IMO, the paper could have been written much better.[[CNT], [CLA-NEG], [SUG,CRT], [MIN]] At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}."
10710,1,"\n\nClarity:\nThe paper is well-written, the main ideas well-clarified."
10711,3,"\n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples."
10712,3, \n* Would the speedups be more dramatic for a larger dataset like Imagenet?
10713,3, This is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical arguments.
10714,3,"\n\n- Eq (9), this is done component-wise, i.e., Hadamard product, right?"
10715,3,"\nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\""Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\""\n \nwhich could have shed more light on the matter."
10716,3,\n2. Section 4: Are you using the soft unitary constraint in your experiments?
10717,2,The thanking of the Reviewers comments is both a waste of my time and the authors time.
10718,1, The paper is also clearly written and the theoretical result is accompanied by some supporting experiments.
10719,3," However, there should be some discussion and reporting of that aspect as well."
10720,3, You have no obligation to use this notation but it does make reading of your paper much easier on others in the community.
10721,3, \n\nQuality: Borderline.
10722,1,"\n\nThe idea is slightly novel, and the framework otherwise state-of-the-art."
10723,3," Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions."
10724,3,\n\nComments:\n\nThe model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i - g(x_i).
10725,3," Multiple data sets and network architectures are tested, and equally important, the effect of parameter settings is investigated."
10726,1,"\n\n====\n\nI appreciate the answers the authors added and I change the score to 6."""
10727,1," Nevertheless, it is well written and I think it is solid work with reasonable convincing experiments and good results."
10728,3,\n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation
10729,3, It would be useful to comment on that aspect.
10730,3,"\n\u2013 The claim \u201cthere is no need to use more powerful and complex classifier anymore\u201d is unsubstantiated, as the paper\u2019s approach still entails using a complex classifier (a FFNN) to learn an optimal intermediate representation."
10731,3,"\n\nWhat the authors propose is a simple idea,"
10732,3," In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion."
10733,3,"""The paper presents a new CNN architecture: CrescendoNet."
10734,3," This is supported by the literature on \""data cleaning\"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition."
10735,1,This seems to be a good direction.
10736,1,\n\nThe idea of employing ensemble of classifiers is smart and effective.
10737,2,Something is missing.
10738,1,\n\nThe paper is well-written and authors explain related work adequately.
10739,3,"\n\n* If I understand correctly, the only new part compared to Li & Malik (2016) is\nsection 3.5, where block-diagonal structure is imposed on the learned matrices."
10740,2,"Your footnote is unnecessary, and indeed confusing. Where did you get the word oftentimes from? We are in the 21st century now."
10741,1,.\n\nThis paper shows improvement over baselines.
10742,3,"\n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases."
10743,1," Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice."
10744,3, No comparisons to other gaze selection models or saliency models are given.
10745,3,...\n\n- The mathematical writing should be more rigorous;
10746,3,  This form of network compression has been worked on before.
10747,3, Interesting parallels with human cortical and hippocampal learning and memory are discussed.
10748,2,"While I personally enjoyed this contribution, I cannot escape the sense that this is much ado about very little, to paraphrase Shakespeare."
10749,3,"""This paper proposes MaskGAN, a GAN-based generative model of text based on\nthe idea of recovery from masked text."
10750,3," To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well."
10751,3," However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past)"
10752,2,Intellectually bankrupt
10753,3," Why a diagonal covariance has been estimated, and not a full covariance one?"
10754,3," The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks"
10755,3," Finally, the authors\nsay toward the end of Section 2 that \""A careful comparision shows that this\napproximation is precisely that which is implied by equation 4, as desired\"". "
10756,3, It is just an MC approximation.
10757,1,"\n\nOverall, the paper is well-written and explores the technical details of the presented approach."
10758,3, The paper proposes a Bayesian framework to address it with a Gaussian mixture model.
10759,3,\n\n9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W.
10760,3, The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class.
10761,2,I am concerned that the author is not getting the advice that she needs in order to produce a publishable paper
10762,3,"\n4) I wouldn't say that ResNets are Gated Neural Networks, as the branches are just summed up."
10763,3,\n- why wait until exactly 120 epochs for NTS-RProp before fine-tuning with actor-critic? 
10764,3, Is 0.22 in similarity high or low?
10765,3, In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?).
10766,3,  Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding.
10767,1,"""****\nI acknowledge the author's comments and improve my score to 7."
10768,3,"""This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks."
10769,3,.\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model?
10770,3, This figure could be moved into the paper's main body with some additional clarification.
10771,1,"""This paper considers the problem of self-normalizing models. This kind\nof approaches, such as NCE (Noise Contrastive Estimation) is very\npromising and important to provide efficient and large vocabulary\nlanguage models."
10772,3,"""Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space."
10773,3,"""In the paper titled \""Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\"", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks."
10774,3, \n- It would be interesting to see if the DA results with a kernel classifier are better (comparable to the state of the art)
10775,3,"?\n-\tCan the authors address the earlier comment on \u201chow to set thresholds for weights across different layers\u201d, by providing motivation for choice of penalty for each layer?"
10776,3," Here it appears from the description of Algorithm 1 that the transitions in the demonstration data have the same semantic as the interaction data, i.e., (s,a,r,s\u2019)."
10777,3,"\n\n\n- Minor errors:\n\nEq. (4), for consistency, should use the identity matrix for the covariance matrix definition."
10778,3, This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost.
10779,3,"  The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks."
10780,3,"\n\nIf my assessment of the situation is correct, I would like to ask you to repeat your experiments with the following two settings: \n\n- ResNet where after each block you multiply the result of the addition by 0.5, i.e. x_{l+1} = 0.5\\mathcal{F}(x_l) + 0.5x_l"
10781,3," However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are."
10782,3,"\n2) What is the exact heuristic in \""Text Styles\"" in section 3.1? Should be stated for replicability."""
10783,3," In particular, the authors tend to take shortcuts for some of their statements."
10784,3,\n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.
10785,3," which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).\n\n\n\n"""
10786,3," This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss."
10787,3," \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments."
10788,3,\n\n- What is the impact of the external word aligner quality?
10789,3,\nWhat are the underlying mathematical insights that lead towards selecting\nseparable convolutions?
10790,3,"\n\n \""3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance."
10791,3,\n\nThe depth of the TC block is determined by the sequence length.
10792,3,"\n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector?"
10793,3,"  For example, adding PWL to Theorems and Corollaries in Section 3.1 will help. "
10794,1," This seems to work quite well, and I speculate that it is because prepositions often function to indicate grammatical relations between different arguments, rather than being content-bearing words themselves."
10795,3,"? If it is set as hyper-parameter, how does the performance change concerning them? "
10796,2,"Written in parts like an experience track paper, minus the experience."
10797,1,\n\nPros:\nThe paper compares different classifiers on three datasets.
10798,3, The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN.
10799,3,"\nWhile this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue."
10800,3,"     Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered."
10801,3,\n\n- A lot of important experimental details are in the appendices and they differ among experiments.
10802,3, This will be the subject of future works.
10803,3," If the performance (hit rate or coverage) of this paper is near stoa methods, then such experimental results will make this paper much more solid"
10804,3,"\n- The details in appendix B are interesting, and I think they should really be a part of the main paper.[[CNT], [CNT], [APC], [MAJ]] That being said, the results in Section B.5, as the authors mention, are somewhat preliminary,"
10805,1, It is also impressive how much faster their model performs on tasks without sacrificing much performance.
10806,3, At least for the word translation task many of these common representation learning frameworks could have been easily evaluated.
10807,3,\n\nOne thing I hope the author could provide more clarification is the use of NER.
10808,3, Legend backwards?
10809,3,\n7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution?
10810,2,"The results are obvious. In fact, so obvious that perhaps no one has bothered to write them down..."
10811,1,". Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet."
10812,3," \nThe authors apply their different models on a small dataset (semeval 2014), they compare basic memory network implementations with their approaches."
10813,3," After generating both parts in parallel, they are combined using alpha blending to compose the final image."
10814,3, Stochastic gradient Riemannian Langevin dynamics on the probability simplex.
10815,3,"\n\n\n[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015."""
10816,3,\n\nIt would improve the paper to also discuss that the non-negativity constrained Tucker2 model may be subject to local minima solutions and have issues of non-uniqueness (i.e. rotational ambiguity).
10817,2,"The first author is a women. She should be in the kitchen, not writing papers' h/"
10818,3,"  Neuronal spike generation mechanism as an oversampling, noise-shaping a-to-d converter."
10819,1,"\nDespite its limited novelty, this is a very interesting and potentially impactful paper."
10820,1," A number of design decisions (such as instance normalization) seem to help yield better results,"
10821,3,"\n \nIn fact given that the proposed scheme applies in the batch case, it seems that other contenders that are very natural are applicable, including BFGS variants for the non-convex case (\n\nsee e.g. Li, D. H., & Fukushima, M. (2001)."
10822,2,To Review of the article was a challenge which I accepted. In fact I regret this decision
10823,1," Even though the significance of the work is apparent given the good results of the proposed neural network,"
10824,3,.\nThe key point is the definition of the transformations
10825,2,"Though less enthused about manuscripts novelty, this reviewer does admire the hardworking of your group."
10826,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
10827,3,"""This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length."
10828,1," While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment."
10829,3, How does num required epochs get impacted as we increase this class space?
10830,3," However, all compression methods such as pruning and quantization also have this concern."
10831,3,\n\nThis paper proposes a regularization strategy for autoencoders that is very\nsimilar to the adversarial autoencoder of Makhzani et al.
10832,3,"  The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works."
10833,1,"""** post-rebuttal revision **\n\nI thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time."
10834,3, Can you please explain if you use obfuscation when you report the final test performance too?
10835,3,"\n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n"""
10836,1," Yet, it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble."
10837,3,I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)?
10838,3,\n\nOptimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters.
10839,3, Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode.
10840,3," \n\nOriginality - The paper heavily builds upon prior work on hierarchical latent tree analysis and adds 'skip path' formulation to the architecture, however the structure learning is not performed end-to-end and in conjunction with the parameters."
10841,3,\n\nI am interested in the following experiment to see how useful it is to increase the batch size compared to fixed batch size settings.
10842,3," While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure."""
10843,3," \n\nFurther, the paper evaluates the language acquisition aspect via two zero-shot learning tasks -- ZS1) A setting consisting of previously seen concepts in unseen configurations ZS2) Contains new words that did not appear in the training phase."
10844,3,"\nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase."
10845,3, They found it improved the accuracy.
10846,3, The paper makes a useful contribution in demonstrating that convergence to efficient outcomes in social dilemmas without the need for agents to observe each other's actions.
10847,2,"I think the author could reduce the amount of equation to a proper level and describes the basic principle. [â€¦] To be honest, those equations make me a little headache"
10848,3, \n\nComments: \n1) Experiments are performed by restricting alternatives to also use a linear classifier for the discriminator.
10849,3, It would be interesting to see further elaboration of the pros and cons of such problem formulation.
10850,3,"\nUsing the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1)."
10851,3," However, the writing made it challenging and the experimental protocol raised some serious questions."
10852,3,"""This paper is proposing a new formulation for regularization of Wasserstein Generative Adversarial models (WGAN)."
10853,1,\n\nThe introduction covers relevant literature and nicely describes the motivation for later experiments.
10854,3, Did these hyper-parameters also include the sizes of the models? 
10855,2,The title is the closest Ive seen to manuscript clickbait
10856,1,\n* The paper is clearly written and well illustrated by figures and examples.
10857,3,"  Neural Networks, 2001. 14(6-7): p. 907-919."
10858,1," \n\nTo sum it up, it is a solid submission,"
10859,1," The motivation, notation, and method are clear."
10860,3,"\n\nCons:\n\n- Page 2- section 2- The reasoning that a deep-RL could not be more successful is not supported by any references and it is not convincing.[[CNT], [CNT], [CRT], [MAJ]]\n\n- Page 3- para 3 - mathematically the statement does not sound since the 2 expressions are exactly equivalent."
10861,3,  \n\n- Is it possible to compute eigenoptions online??
10862,3,"\nTo deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix."
10863,3,The author categories the existing works int four categories.
10864,3," but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance."
10865,1," That the same observation is made in such disparate brain areas (V1, EC) suggests that sparsity / efficiency might be quite universal constraints on the neural code."
10866,2,The supportive tone of this reviewâ€¦ took some effort.
10867,3,\n\nI also have the following questions I hope the authors could help me with:\n\n1. I failed to understand how Eqn (5). Could you please clarify.
10868,3,"After all, the L2 one is just an approximation of the entropic one."
10869,1,"\n-p14 left side, 4th cell up, \""Cross-AE\""-->\""ARAE\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nThis is a very nice paper with a clear idea (regularize discrete autoencoder using a flexible rather than a fixed prior), that makes good sense and is very clearly presented."
10870,3," The motivation for this control is to evaluate the impact of the adversarial loss, which is presented as the key conceptual contribution of the paper."
10871,1,"  \n\nIn general, the paper is well-written and the main ideas are clear."
10872,3,"\n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method."
10873,3, \n\nTheorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k\u2019=k..
10874,3,\n\n3. Clarification question: For the WSJ experiments was the model decoded without an LM?
10875,3,.\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].
10876,3," Cognitive Psychology, 71, 55-89.\n\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016). "
10877,3, This is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon.
10878,1, Overall action-dependent baseline outperform state-only versions.
10879,3,  This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space.
10880,1,"\n\nOverall, I feel the strengths of the paper outweight its weaknesses."
10881,3, The first GAN then uses the second GAN as prior to generate the z codes. 
10882,3, The approach is illustrated by numerical experiments.
10883,3," But if this is the case, wouldn't that imply that the quadratic approximation to the objective function is poor, and therefore that line 5 of algorithm 1 should shrink the trust region radius?"
10884,3, What were the nearest neighboring posters in the original VGG space? They should not be that bad too.
10885,3,\n\n---\n The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices).
10886,3,  It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values.
10887,3,"""The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor."
10888,3,"If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful."
10889,3,"""\nI think the first intuition is interesting."
10890,3, \n-\tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features.
10891,3," \n\nFor the computational learnability literature, complexity analysis for teaching has a subliterature devoted to it (analogous to the learning literature)."
10892,3," For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on."
10893,3, Sparse Gaussian processes using pseudo-inputs.
10894,2,Find your inner nerdâ€”it must be a big part of youâ€”bind and gap it and then dump it in the ocean tied to a large rock. -Referee 
10895,3, \n\nIn the last paragraph of Section 3 ``m = w^k-1'' This is a very big first layer.
10896,3, Some acknowledgement and discussion of this would be useful.
10897,3," The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are \""begin learnt\"", just from different datasets."
10898,3," \n\nSection 5.3:\n- What is the \""a : b :: c : d\"" notation?\n"""
10899,3," This paper also presents a method for learning embeddings specific for subgroups of the data, but based on hierarchical modeling."
10900,3,"\nHowever five of the words the authors retain: bored, annoyed, love, optimistic, and pensive are not in fact found in the PANAS-X scale:\n\nReference: The PANAS-X Scale: https://wiki.aalto.fi/download/attachments/50102838/PANAS-X-scale_spec.pdf Also the longer version that the authors cited: \nhttps://www2.psychology.uiowa.edu/faculty/clark/panas-x.pdf"
10901,3," For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization)"
10902,1,"\n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling."
10903,3,"\n\n[1] M. Defferrard, X. Bresson, and P. Vandergheynst."
10904,1,\n+ novel algorithm
10905,2,"Other papers are cited that were clearly not read carefully, resulting in some memorable howlers."
10906,3," That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates."
10907,1,"  The proposed workflow consists of the novel two-pass decomposition of a group of layers, and the fine-tuning of the remaining network."
10908,3," However, I found the empirical results to be a little underwhelming."
10909,1, \n\nClarity: Main ideas are clearly presented.
10910,1,\n+ Improved performance in speech recognition task.
10911,3, \n\nMy only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017).
10912,1,"\n\nFinally, the experimental part shows nice improvements "
10913,3," Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016."
10914,1," \n\nOverall, the paper is well-written, clear in its exposition and technically sound."
10915,2,I am concerned as it appears that participants in the current study were randomly assigned to one of three experimental conditions
10916,3," Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair."
10917,3," In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n"" "
10918,3,"""The authors propose a penalization term that enforces decorrelation between the dimensions of the representation "
10919,3," It seems some of the details are in Appendix-A.[[CNT], [PNF-NEU], [DIS], [MIN]] It would be better if authors move the important details of the technique and also some important experimental details to the main paper."
10920,1," Though [Gupta et al.] proposed an ego-centric neural memory in the RL context, the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory, whereas [Gupta et al.] designed the memory specifically for predicting free space."
10921,3,"""The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique."
10922,3, It would be nice to report the training loss to see if this is an optimization or a generalization problem.
10923,3, Only the state-reconstruction error is shown now.
10924,3," It thus seems challenging to use mini batch MC, how does the mini batch estimation compare to an estimation using the full dataset? "
10925,1," However, if one views the paper in a different light, namely showing some \u201cblind-spots\u201d of current conditional GAN approaches like lack of diversity, then it can be of much more interest to the broader ICLR community."
10926,3,n   what effect the monte-carlo approximation of the objective has on things.
10927,3, That this approach works well implies that amortization error cannot be ignored.
10928,3,"""The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version."
10929,3,"""Summary: the paper proposes a new insight to LSTM in which the core is an element-wise weighted sum."
10930,3,"\n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger."
10931,1," Table 1 is good,"
10932,3,"""The papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density function."
10933,1,  What is the takeaway from these two tables supposed to be?\n\n5) Part of the motivation for the work is said to be the increasing interest in inference networks: 
10934,3," Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a \""Gaussian\"", and thus the class probabilities are the reseponsibilities proportional to the class Gaussian."
10935,3, The paper should stress on this a bit more.
10936,3,"\nNevertheless, the additional experiments and clarifications are very welcome."
10937,3,"\n\nIn the experiments, does the \\varphi MLP explicitly enforce symmetry and identity or is it learned?"
10938,2,"The paper is in good shape, though no Sunday morning reading material."
10939,3,"\n\nSuggestions:\n- It would be great if authors can add more details of the multi-layer perceptron, used for predicting weights, in the paper."
10940,3, Do we get similar results?
10941,3," In practice this second solution is analogous to the first, but a general 'distractor' class\nis added."
10942,3,  I would be interested to see if layerwise\ninput skip connections (i.e. between each network layer L_i and the original input variable 'X') hastened the 'compression' stage of learning e.g. (i.e. the time during which the intermediate layers minimise MI with 'X').
10943,1,\n\nPros: \n(1) The paper is easy to follow.
10944,3,"\n\n(1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large."
10945,3," what happens if you just randomly pick one of the 10 crops for prediction?"""
10946,3,"The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts."
10947,3, \n\nThe justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me:
10948,3,"\n\nEVALUATION\n\nIs it really the case that no results are presented for the QA task, or am I\nmisreading one of the charts here?"
10949,3,"\nThis can be coherent with the problem of Domain Adaptation that can be NP-hard even under the co-variate shift assumption (Ben-David&Urner, ALT2012)."
10950,2,"The text is overly expansive, desultory, and often diaphanous, so that the raison d'Ãªtre of an overarching theoretical structure is neither pellucid nor convincing."
10951,3,"""Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH."
10952,1,"   Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results,"
10953,2,This needs to be standardized!!! It must be converted!!! Why should we expect some kind of relationship?? It needs to be justified!!!
10954,3, The battery can be \u201crecharged\u201d by moving to a \u201ccharge\u201d tile.
10955,3,"  It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V."
10956,3," \n\n1). In the \""Meta-learner\"" section 4.1, the authors talk about word features (u{_w_{i,j,k}},u{_w_{i,j',k}})."
10957,1,"\n\n[*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks."
10958,3," Examples include\n\n@article{wang-adelson-94,\n  author        = \""Wang,  J. Y. A. and Adelson, E. H.\"",\n  title         = {{Representing Moving Images with Layers}},\n  journal       = {{IEEE Transactions on Image Processing}},\n  year          = \""1994\"",\n  volume        = \""3(5)\"",\n  pages         = {625-638}\n}\nsee http://persci.mit.edu/pub_pdfs/wang_tr279.pdf\n\nand\n\n@article{frey-jojic-03,\n   author    = {Frey, B. J. and Jojic, N.},\n   title     = {{Transformation Invariant Clustering Using the EM Algorithm}},\n   journal   = {IEEE Trans Pattern Analysis and Machine Intelligence},\n   year      = {2003},\n   volume    = {25(1)},\n   pages     = {1-17}\n}\nwhere mask and appearances for each object of interest are learned. "
10959,1," \n\nThe adversarial loss helps significantly only with AMT fooling or realism of images, as expected because GANs produce sharp images rather than distributions, and is not very relevant for robot motion planning."
10960,1,"\nin the paper, and the approach is new as far as I know."
10961,1," \n\nOverall, the paper is well written and easy to follow."
10962,3," If all r_i have a norm < 1, then the terms that dominate will be those that consist of fewer r_i, resulting in a lower effective depth."
10963,1,"\n2. Regularizing the objective with KL div. seems promising, but expensive."
10964,3, It's not differentiable and requires sampling for training.
10965,3, Do other categories have the similar results?
10966,1,\n\nOverall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights.
10967,3, The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance.
10968,1," Please find my comments are below.\n\nOverall it is an interesting  but long paper,"
10969,1,"\n\nA few comments:\n\nThe paper is easy to read, and largely written well"
10970,3, A Monte Carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead prediction.
10971,3," While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all."
10972,3, They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\n
10973,3, \n2. The results are depicted with a latent space of 20 dimensions.
10974,3," For example, \n- Section Method, equation (4)"
10975,3, The literature is still healthy today.
10976,3," However, Corollary 3 is only a concentration bound on the gradient."
10977,3, Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning?
10978,3," I'm guessing it's because the RNNs don't \""work\"" in all environments with the same initialization (i.e., they either don't look like EC, or they don't obtain small errors in the navigation task)."
10979,3," on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network."
10980,3, Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more.
10981,3, The introduced parameters are not involved in the forward pass and hence cannot be updated by Backprop.
10982,3," The data is derived from an online repository of ~1500 Android apps,"
10983,3," Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum."
10984,3,"\n\n\nErrors:\n- page 2 last para: \""gives an concrete\"" -> \""gives a concrete\""\n- page 2 last para: \""matching\"" -> \""matched\""\nFigure 1: I think \""passage embedding h\"" and \""question embedding v\"" boxes should be switched.\n- page 7 3.3 first para: \""evidence fully\"" -> \""evidence to be fully\""."
10985,3,"""In this paper, the authors propose a new approach for learning underlying structure of visually distinct games."
10986,1,n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs.
10987,3, It is very probably more costful.
10988,1," The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy."
10989,3,"  From the experiments, we can see that the proposed method is effective. "
10990,3, What was the rational for choosing a vanilla RNN for the slave modules?
10991,1, Authors also propose a paraphrasing based data augmentation method which helps in improving the performance.
10992,3,"""This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks."
10993,2,The paper was so boring I fell asleep halfway through
10994,3,"\nGiven this architecture, the authors focus on characterizing the objective landscape of such a problem."
10995,3, Seems like there are two identical terms.
10996,3," Including explicit formulas would be very helpful, because, for example, it looks like when reported in table 1 the metrics are spatially averaged, yet I could not find an explicit notion of that."
10997,3,"  Also besides the number of updates reported in table, I think the wall-clock time for inference should also be reported, to demonstrate what the paper is trying to claim."
10998,2,The author has re-invented lukewarm water. This is all naive philologising.
10999,3," The authors propose the CP-S (stands for symmetric CP decomposition) approach which tackles such factorization in a \""batch\"" manner by considering small random subsets of the original tensor."
11000,3,"\n- In 3.1, Socher et al. do not use lstm"
11001,1,"""The paper seems to be significant since it integrates PGM inference with deep models."
11002,1,". Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea."
11003,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
11004,3,"There are also a number of items that could be added that I believe would strengthen the contribution and novelty, in particular:\n\nSome highly relevant references on (prosocial) reward shaping in social dilemmas are missing, such as Babes, Munoz de cote and Littman, 2008 and for the (iterated) prisoner's dilemma; Vassiliades and Christodoulou, 2010 which all provide important background material on the subject."
11005,3," Therefore, I am currently leaning towards rejecting this paper."
11006,3,"\n\n[1] Juan, C., Sutton, R. S., & Ram, A. Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces.\n\n[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[3] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2017). Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089."""
11007,3," \n\nIt can thus be expected, that the input- and class specific explanations change as well, to an also not so small extent."
11008,3, The skip connections are penalized by Lagrange multipliers that are gradually phased out during training.
11009,3," There are a few problems with the paper.\n\nFirstly, the title and abstract discuss \""modifying memories\"", but the content is only about a rotation operation."
11010,1," This paper does not bring completely new perspectives for the task, but the contribution is valuable to the community."
11011,3,"   Each object is described by a position, appearance feature and confidence of existence (presence)."
11012,3," At this moment, these identified class-specific core units are useful for neither reducing the size of the network, nor accelerating computation. """
11013,3," If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set."
11014,3, It seems that most safety constraints can be expressed via masking.
11015,1,"\n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications."
11016,3," It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \""[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search."
11017,2,"Unlawful, void and of no effect"
11018,3," The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem."
11019,1,"  \nI like the idea of the paper, in particular the design which enables calculating the number of steps to skip in advance."
11020,3,"""Summary\nThe paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated updates."
11021,3,"  \n\nThen a policy network is trained with deep Q learning whose architecture takes into account the objects in the scene, in an order agnostic way, and pairwise features are captured between pairs of objects, using similar layers as visual interaction nets."
11022,3, This should probably also be referenced.
11023,3,\n\nSignificance\nThe points of view defended in this paper can be a basis for founding a general theory for unsupervised learning of mappings between domains.
11024,1, \n--The experiments were well carried through and very thorough.
11025,3," \n\nExperiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain."
11026,1," Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn't leave the manifold of reasonable designs."
11027,3,"n\n9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)\n\n10."
11028,3,. The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger.
11029,1,"""This paper proposes a fast way to learn convolutional features that later can be used with any classifier."
11030,1," It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs."
11031,3," Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes."
11032,3, Is there a reason that these models have not been as successful in other domains?\
11033,3," It seems that the header \""data set\"" should be \""approach\""\nor something similar."
11034,2,"This is a potentially interesting problem. Yet, not all potentially interesting problems are useful, such as this one"
11035,1, The method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data.
11036,3, They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack
11037,3,"""The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent."
11038,3,"   Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity."
11039,3, Is its logit set to zero or -\\infty ?
11040,3,"\n\nReferences\n\n[1] Srivastava, Rupesh K., Klaus Greff, and J\u00fcrgen Schmidhuber."
11041,3,. The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried.
11042,3," Maybe more justifications are needed."""
11043,3,  The domain is video games.
11044,3,"\n\n3. The writing could be significantly improved, both at the grammatical level and the level of high level organization and presentation."
11045,3," Importantly, this does not entail that using a better regularization a similar RNN model can indeed learn such a representation."
11046,3, This will limit the applicability of the approach in most applications where fully connected networks are currently used.
11047,1,"\n\nOverall, the method is interesting and the dev set experiments were informative,"
11048,3,"  The combination of temporal logic formulas with reinforcement learning was developed previously in the literature, and the main contribution of this paper is for fast skill composition."
11049,3,"  arXiv preprint arXiv:1709.06560.\n\n[2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016)."
11050,3,but: 1/ you must provide baseline results with a well tuned phrase based mt system;
11051,3," In fact, there are obvious places where the exposition is excessively verbose, and there are clear opportunities to reduce the length of the submission."
11052,3,"""The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein\u2019s identity and control functionals."
11053,3,\n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise.
11054,3,  I find the analysis made by the authors to be very simplistic.
11055,3," \n\nIn equation 1, it wasn\u2019t obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation?"
11056,3,"\n\nIn addition to introducing a heuristic reward term, the authors propose to alter the Q-function\nto be specific to the subgoal."
11057,3, Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder.
11058,3," For this reason, I had to carefully re-read the critiqued paper to be able to assess the validity of the arguments made in this submission and to evaluate its merit from the point of view of the quality of the critique."
11059,3,". \n\nThe theoretical results are presented in a bitmap figure and only referred to in the text (not explained),"
11060,3, It makes use of the CP or rank-1 tensor decomposition to define the meaning of rank for a tensor.
11061,3,"  One would hope that, even if the search space is limited, the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same space."
11062,1, This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!
11063,3," \nBased on this definition, and assuming identifiability (i.e. uniqueness up to invariants), and for networks with Leaky ReLU activations,  the authors prove that if the number of mappings which preserve a degree of discrepancy (density preserving in the text) is small, then the  set of \u00ab\u00a0minimal\u00a0\u00bb mappings  of complexity C   that achieve the same degree of  discrepancy is also small. "
11064,3,"\n\nIn both these cases the results for PPO and TRPO vary pretty significantly from what we see here, and an important one to look at is the InvertedDoublePendulum-v1 task, which I would think PPO would get closer to 8000, and TRPO not get off the ground."
11065,3,"\nFinally, the authors use the DQN platform to learn a Q-value function using the learned  surrogate reward function that guides the agent to specific subgoals, depending on the situation."
11066,3,"  Specifically, By this way, the model could exploit the outputs of concept detection module to perform the actions and question answering as well jointly."
11067,3,"""The authors present a testing framework for deep RL methods in which difficulty can be controlled along a number of dimensions, including: reward delay, reward sparsity, episode length with terminating rewards, binary vs real rewards and perceptual complexity."
11068,3,\no\tWhat is the relation between the model learned at stage 2 (the centers c^i) and the model learnt by HDDA?
11069,3, The neural nets also compute node embeddings and graph embeddings and the embeddings of the current graph are used to compute the decisions at time step T.
11070,3," Do these all use the same minibatch, at each iteration?"
11071,3,"\nIdeally, the train and test urls would also be different in time."
11072,2,"There is a lot of terminology flung around such as 'false negatives', 'false positive', 'median'"
11073,3, This paper simply presents the results of different parameter k on testing set directly.
11074,3,"""This paper proposes a new machine comprehension model, which integrates several contributions like different embeddings for gate function and passage representation function, self-attention layers and highway network based fusion layers."
11075,3,"\n\n- The attacks are all based in L2, in the sense that the look for they measure perturbation in an L2 sense (as the paper evaluation does), while the defenses are all L_\\infty based (since the region classifier method samples from a hypercube, and PGD uses an L_\\infty perturbation limit)."
11076,3,"\n\nIf you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton\u2019s method on convex problems, by Yurii Nesterov."
11077,1,  The punchline of the paper is that the authors are able to achieve similar performance as \u201cfull ResNet training\u201d but with significantly reduced training time.
11078,3,"\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They\u2019re just different."""
11079,3,"\n-\tIts not clear enough what exactly is the \u2018PSNR\u2019 value which is used for the adversarial example detection, and what exactly is \u2018profile the PSNR of legitimate samples within each class\u2019."
11080,3," For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph."
11081,3, This reduced form of the LSTM is shown to perform comparably to \u201cfull\u201d LSTMs.
11082,1, New method proposed and shown to work well in one case.
11083,3," For example, \n- Section Method, equation (4)"
11084,3,\n  Putting these in a table along with the results would improve readability.
11085,3,"\n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n"""
11086,1,"\n\nSome comments:\n\n* Section 4: I found this argument extremely interesting.[[CNT], [null], [APC], [MAJ]] However, it\u2019s worth noting that your argument implies that you could get an O(1) SNR by averaging K noisy estimates of I_K."
11087,1,"\n\nMajor comments:\n\nThe paper is well written, and summarizes its contribution succinctly."
11088,1,"""This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering \""sparring\"" follow-the-regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4)."
11089,2,The work that this group does is a disgrace to science
11090,3,"\n- in terms of considering a monotonic alignment, Hori et al, \""Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM,"
11091,3," \nHere are my concerns:\n1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model."
11092,3," In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices."
11093,3,"\n\nNotes: I did not check the proofs of the theorems in detail. \n"""
11094,2,You have two many misprints
11095,2,"Unfortunately, the paper offers little more than vocally arguing. h/"
11096,3," If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear."
11097,3," Also, if the claim is that there are not deep learning survival analyses, please see, e.g. Jing and Smola."
11098,3, Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches.
11099,2,"It was mentioned that participants were recruited. In research, we dont recruit study participants"
11100,1," \n\nThe paper proposed a possibly interesting approach,"
11101,2,Are the 10 random reps chosen at random?
11102,2,I was somewhat disappointed after reading the MS; I was initially expecting some lighting hit but nothing really happened in the end.
11103,3,"\n\nIn the discussion of Table 1, it would be helpful to spell out the differences between the different Bary proj algorithms, since I would've expected EMD, Sinkhorn and Alg. 1 with R_e to all perform similarly."
11104,3, though I haven't checked all the details
11105,3,\n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.
11106,3, \n\nAuthors then empirically validate their model on several small-scale datasets where they compare their model with PSRNN and LSTM.
11107,3, So I will focus on the omniglot experiments.
11108,3,"  It uses support vector regression to show that a\nrelatively small number of samples of hyperparameters, architectures,\nand validation time series can lead to reasonable predictions of\neventual performance."
11109,2,I can see that the manuscript has archival value
11110,3," If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter."
11111,1, All of these are well known methods.
11112,3,\nCan the model recover richer structure that was imposed during data generation?
11113,1, And propose a challenge that addresses these issues and allows controlling different aspects of image variability.
11114,3, This work should be discussed and compared.
11115,3," One issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks, which is useful for learning from multiple tasks."
11116,3, There is also no evidence on whether adaptive learning on the fly is needed or not.
11117,2,This paper may sink without trace
11118,3,  The regression loss was also changed from l_1 to l_2.
11119,3," The paper shows that SGD can be understood using stochastic differential equations, where the noise scale is approximately aN/((1-m)B) (a = learning rate, N = size of training set, B = batch size, m = momentum). "
11120,3,"  This subset is chosen such that each command sequence corresponds to exactly one target action sequence, making it possible to apply standard seq2seq methods."
11121,3,\n\n3) p 4 eq 3 and sec 3.2 -- please justify *why* it makes sense to use\nthe concrete transform.
11122,1,\n\nThe experimental study is extensive.
11123,3," \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database."
11124,3," If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?"
11125,1, The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation.
11126,3," However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above."
11127,3,"  \""A survey on multi-view learning.\"" arXiv preprint arXiv:1304.5634 (2013)."
11128,3, The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset.
11129,3, but it would need a lot of work to address the issues raised above.
11130,3," Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible."
11131,3, Did the authors try such experiment?
11132,3,"""Summary: The paper proposed a two-dimensional approach to lifelong learning, in the context of multi-task learning."
11133,3," Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated."
11134,3,"\n\nTypos:\n- Equation 14: In the first term (target loss), theta should have an index t (I think).\n- Bottom of page 6: \""... and that as our validation set\"" (missing word).\n"""
11135,3,\n\nDoes the unit norm normalization used to construct the covariance disallow ARD input selection?
11136,3,\n[e] \u201cDo DRL-based navigation algorithms really 'learn to navigate'?
11137,3,  They do not cite the extensive literature on KB completion.
11138,3," The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one."
11139,1," \n\nThe paper is written clearly and the English is fine."""
11140,3,"""This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances."
11141,3,"  Probabilistic models of string distance have been common since Ristad and Yianlios in the late 90s, and there are proper probabilistic models that would work with your same dynamic programming algorithm, as well as improved models with some modest state splitting."
11142,3," Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this."
11143,3, The most interesting part is the Hermite polynomial expansion of the activation function.
11144,3,\n2. Formulate two complex-valued alternatives to ReLU and compare them
11145,3," As shown in [Szegedy et al 2014, \""Intriguing properties of neural networks\""] adding an extra linear transformation does not change the expressive power of the representation. "
11146,3,The experimental results show the benefits of this approach.
11147,3,\n\nAdvGAN is a simple and neat solution to for generating adversary samples.
11148,3, Does it have anything to do with existing issues in DRL?
11149,3, These issues may need a significant amount of effort to fix as I will elaborate more below.
11150,3, But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better).
11151,3,"""Summary:\n The paper presents an unsupervised method for detecting adversarial examples of neural networks."
11152,3,   Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.
11153,3, How will the method compare to EM algorithms and neural network based approaches? 
11154,3, I see this work as adding communication to improve the translation learning.
11155,3," When regularization of the network is not used during training, the trained RNNs no longer resemble the EC."
11156,3, It is also important to be able to benefit from off-policy data.
11157,3, And when data can be sparse. 
11158,3,"  \n(3) In algorithm 1, what exact method is used in determining if \\mu is converged or not?"
11159,1,.\n\nstrength\n\n* The paper is mostly clear and easy to follow
11160,3," The learning dynamics defined for the network results in specific update equations of the weights W (Eqn. 14), which combine elements of supervised learning and self-organizing maps (SOMs)."
11161,3," The goal is to capture hypernyms of some synsets, even if their occurrence is scarce on the training data."
11162,3,"\n\nIn section 2.2, how \\mu_i and \\sigma_i are computed?"
11163,3, This is nice
11164,3,".  However, the paper hasn\u2019t shown that this is necessarily possible assuming the hand-designed mockups aren\u2019t pixel-for-pixel matches with a screenshot that could be generated by the \u201cDSL code -> screenshot\u201d mapping that this system learns to invert."
11165,1,\nIt would be interesting to see whether the idea can be applied to more recent GAN models and still perform better
11166,3, Are these part of the encoder or decoder?
11167,3,"\nReferences:\net al.\nYI WU\n"""
11168,3,  How does such model choice affect the final performance?
11169,3," The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers."
11170,1,\n\n\nThe paper could have gone farther experimentally (or theoretically) in my opinion.
11171,3," Am I correct in understanding that this step depends only on Y, and that given Y,"
11172,3,\n1) why for the omniglot experiment the table reports the error results? 
11173,1,  \n\nThe idea is interesting and to my knowledge novel.
11174,3," The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings."
11175,1,"  Besides, the feed-forward approaches including [1] mentioned above are efficient and not too sensitive with respect to the network architectures."
11176,1, but close enough to be of interest and certainly indicates that their method is principled.
11177,3, \n-\tHow did you select the maximum number of epochs in Figure 5?
11178,3," \nThe first step, virtual observations, are used to provide stand ins for inputs and outputs of the GPN."
11179,3,"  For example, you explain how the algorithm for creating the backbone can use unsupervised data."
11180,3,"""This paper presents a method to search neural network architectures at the same time of training."
11181,3,"\n\nFurther in this direction, Section 4.1 claims \u201cZero partial derivatives are ignored to make the signal more clear"
11182,1," \n\nThe affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment."
11183,3,"Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU."
11184,3,"  They use a prior model proposed in Finn et al. 2016, make several incremental architectural improvements, and use an adversarial loss function instead of an L2 loss."
11185,3," In other words, how structured is this prior and does it really give us some of the interesting properties of deep neural networks, or is it mostly a cute mathematical trick?"
11186,3,"  The paper claims that the exponential has been noted to be ad-hoc, please provide a reference for this."
11187,1,\n\nThis is a well-written paper with interesting (and potentially useful) insights.
11188,3," Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level."
11189,2,"Publishing a weak paper can be a bit like kissing someone through a screen door - it might feel good for a moment, but what does it actually accomplish?"
11190,1,"\n\n-\tThe paper shows how state of the art relational networks, performing well on multiple relational tasks, fail to generalize to same-ness relationships,"
11191,3," Thus, if we can learn the SRs, we can extend the proposed approach to the nonlinear case"
11192,3,"""This paper presents a method for classifying Tumblr posts with associated images according to associated single emotion word hashtags."
11193,3," I still think the novelty, significance of the claims and protocol are still perhaps borderline for publication (though I'm leaning towards acceptance),"
11194,3,\nAnother thought on this: is it possible to integrate the trigram occurrence with summarization reward?
11195,1," \n\nPage 8:\n-\tThe results in general indicate that the method is much better than chance,"
11196,3," 2015.\n\n[2] Lee, Chen-Yu, et al."
11197,3,"  First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered."
11198,3," This method presents a family that can span the entire space, but the efficient parts of this family (which give the promised speedup) only span a tiny fraction of it, as they require only O(log(N)) params to specify an O(N^2) unitary matrix."
11199,3, 2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios);
11200,1," While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper."
11201,3," As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST)."
11202,3,". In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements"
11203,3, \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.
11204,1," The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively. "
11205,3,Certain gating in RNN is very useful but I don't know whether one can train any reasonable RNN with all multiplicative gates
11206,1," This is an interesting phenomenon and deservers further study, as currently doing the \u201cwrong\u201d things is better than doing the \u201cright\u201d thing."
11207,1," While\nresults are pretty good,"
