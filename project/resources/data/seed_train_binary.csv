,Class Index,Description
0,2,The authors have not bothered to learn the first thing about the theories they are hoping to refute with ill-designed experiments and muddled rationale
1,2,"Table 4 seems unnecessary given figure 8. Indeed, figure 8 also seems unnecessary

-R2 has been watching Tidying Up"
2,1," Especially it borrows the cyclic loss from the image style transfer, which provides a reasonable regularization to the text style transfer model."
3,1,"n\nThe paper, however, misses comparison against important work from the literature that is very relevant to their task \u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA."
4,1,\n\nOverall the paper is well written.
5,2,"How does [redacted theory] explain your results? Or really, your underwhelming results?"
6,2,"They include practical examples and code samples, and (in a slightly bizarre move) source code will be made available on acceptance."
7,2,there is no evidence to show this is the first paper to propose the idea
8,1, Right now the results are not very convincing.
9,2,Why exactly this task? I can think of a zillion other cognitive tasks
10,2,"eviewer : 'The project can hardly be described as high risk/high gain' 
Reviewer : 'The project is clearly high risk/high gain"
11,2,The authors should completely revise it after completing the study and by following common scientific standards
12,1," Could this submission show some fine-tune experiments?"""
13,1, \n2. A cycle consistency loss that makes sure the content vector of transferred sentences and style vector of the original sentence should be able to reconstruct the original sentences.
14,2,default settings?? huh???
15,1,"\n- It would interesting to compare this approach with a conditional training pipeline that sequentially adds branches, keeping the previous branches fixed."
16,2,This is a long and tedious manuscript 
17,1," \n\nWhile the experiments are compelling,"
18,1,\n\nReview:\nThe manuscript describes the proposed algorithm in great detail and the description is easy to follow.
19,1,"\n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016)"
20,2,Often sounds like a precocious high-school student who is trying to show off how clever he is
21,1," Many terms are not defined or defined after being introduced (e.g. CIGAR, MF, BQMQ)."
22,1," This way, finer discretization does not increase the dimension of \\x (nor its approximation), but rather improves the resolution."
23,1, (b) To find the value that best matches a key at the decoder stage?
24,2,"Reviewer : The conclusions are supported by the results and discussion.
Reviewer : The conclusions are not supported by the result"
25,2,The supportive tone of this review… took some effort.
26,2,This manuscript is a long drive through a countryside. A lot of stuff is irrelevant and pointless.' h/
27,2,The results look like a smorgasbord of data
28,2,I dont believe in simulations
29,2,I doubt that many readers will read anything beyond the abstract if the article remains in its present form
30,2,Perhaps I have just read these papers at monthly intervals and the author had bad luck.
31,2,here is no point in [..] the statistical overkill in the submission. It simply should not have happened. It should never happen again
32,1,"\n\nI have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them."
33,1," For (2), the \""hard\"" functions has a better parameterization, and the gap between 3-layer and 2-layer is proved bigger. "
34,1, But there are a few key issues that are not clearly addressed and the experimental results are not convincing.
35,2,"presumptuous, ignorant, ill-conceived and downright dangerous"
36,2,"The text is marred by conceptual messiness, careless reasoning, sloppy phrasing and  inadequate acquaintance with the relevant literature"
37,1,"\n- The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN."
38,1, \n\nPros:\n* Important problem
39,1," Because of different benchmarks, it is not clear whether the performance improvements are due to technical improvements or sub-optimal parameters/training for the baseline methods."
40,2,"eviewer 2 method acting You will notice that my comments are a bit jumbled, this is somewhat a reflection of the need for improved organisation of the MS in general"
41,1," This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation."
42,2,"The data are weak, based on very small samples, not analyzed properly, and based on measurements  in the wrong medium. I see little value."
43,1,"""Summary:\nThis paper proposes a new approach to tackle the problem of prediction under\nthe shift in design, which consists of the shift in policy (conditional\ndistribution of treatment given features) and the shift in domain (marginal \ndistribution of features)."
44,1," The paper essentially introduces a method to use off-policy data, which is of course important,"
45,2,The interchangeably use of evaluate and validate is a concern as it is not clear if authors know the difference between these 'verbs' 
46,1," However, no detailed information is given in the paper."
47,2,Black-box modeling exercise using a hodge-podge of data tied together with a poorly-defined model
48,2,"I urge the authors to not publish this article anywhere, as it will impede the progress of scientific understanding"
49,2,I am personally offended that the authors believed that this study had a reasonable chance of being accepted to a serious journal
50,2,"Can you explain this part a bit further, but without going into detail."
51,2,"I cannot figure out what point you are making and why you have written this article. This version cannot be saved, so start again with a much clearer vision of what you are trying to say, and the point you are making."
52,2,I suggest you consult a competent statistical advisor.
53,2,Table 2 stunningly over-interprets some relatively small signals in the data.
54,2,In order to be able to publish this manuscript it need [sic] to be rewritten in the form of a scientific article
55,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
56,1,\nThis idea however is difficult to be applied to deep learning with a large amount of data.
57,2,Please use something in parenthesis so your reader doesnt have to take a break from reviewing your work to Google it
58,1,"  To the reviewer, It seems \u201clife-long learning\u201d is the same as \u201conline learning\u201d.[[CNT], [CLA-NEG], [CRT], [MIN]]  However, the whole paper does not define what \u201clife-long learning\u201d is.[[CNT], [CLA-NEG], [CRT], [MIN]]\nThe limited budget scheme is well established in the literature.[[CNT], [CNT], [APC], [MAJ]] \n1. J. Hu, H. Yang, I. King, M. R. Lyu, and A. M.-C. So. Kernelized online imbalanced learning with fixed budgets."
59,2,This looks like a very early draft
60,1," Also, the experiments takes a fixed 20K iterations for training, and the convergence status (e.g. whether the accumulated gradient has stabilized the policy) is not clear."
61,2,You were either in a rush or you do not care too much about getting your paper accepted
62,1," \n\n- As pointed out by the authors, the idea of this formulation and doubly SGD is not new. "
63,2,"By the end of the paper, I was left with the impression that the lid had been lifted off a big can of..."
64,2,The authors have gained some attention in the community: most of this is due to the wrong reasons
65,2,Pleasepleaseplease with sugar on top remove sentences that sound like Hegelian light bulb moments while not meaning anything
66,1,"  First,  a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow."
67,1," Of course, that involves O(K^2) or O(K^3) computation, which is a weakness."
68,1," The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement."
69,2,The authors need to add a level of puzzlement to their interpretations
70,1, The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy.
71,2,I am concerned as it appears that participants in the current study were randomly assigned to one of three experimental conditions
72,2,It is always rather pleasant to recommend that a paper be rejected.
73,1,"""- Good work on developing VAEs for few-shot learning."
74,1,\n\nSignificance\nThis paper makes incremental improvements and would be of moderate interest to\nthe machine learning community.
75,1, It proposes many heuristics to use the object feature and attention weights to find the correct count.
76,1,"\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem."""
77,2,"Like Dr Whos Tardis, the inside of the paper is bigger than the outside. - (H/"
78,2,"The paper descends into nonsense, never to return, on line 44."
79,2,Well Written. 
80,1," For example, have another RNN read the assertions and somehow integrate that."
81,1,  The inclusion of proof summaries in the main text would strengthen this aspect of the paper.
82,2,"But fundamentally, why did you bother?"
83,2,I would suggest that you do some homework and redirect this work to an actual new and novel and mechanistic work and test it against data
84,1," \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels."
85,1,\n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper.
86,2,Ive never read anything like it &amp; I do not mean it as a compliment
87,2,"If the author is comfortable with having his/her name on THIS, then I wont stand in the way of publication."
88,1," When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000]."
89,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
90,2,I dont think this study would add anything to either theory or practice
91,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
92,1, The wording here was confusing.
93,1,\n- No comparisons with prior work are provided.
94,1," \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible."
95,2,Ive never read anything like it &amp; I do not mean it as a compliment
96,2,The beekeeping example also fails to convince. There is no discussion of bees' actual lived experience.
97,2,The presentation is of a standard that I would reject from an undergraduate student
98,1, Extensive experiments demonstrate the usefulness of the approach.
99,2,The authors should refer to the super interesting article on this topic in Wikipedia.
100,1, The analyses are interesting and done well.
101,1," This method presents a family that can span the entire space, but the efficient parts of this family (which give the promised speedup) only span a tiny fraction of it, as they require only O(log(N)) params to specify an O(N^2) unitary matrix."
102,1," \n\nAfter rebuttal:\nThe writing of the paper greatly improved, still missing insights (see comments below)."
103,2,"Yes measurements were made, but why, besides a teaching exercise, remains obscure."
104,2,The length of this review is occasioned by the density of error and misconceived arguments in this manuscript.
105,1," I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples."
106,2,A classic instance of reinvention of the square wheel.
107,2,"This paper is under-referenced, conceptually impoverished, and poorly written."
108,2,"The authors are perpetuating misguided generalizations in the face of substantial
experimental data to the contrary"
109,1," \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance."
110,1,\n\n3. Some details are not clear.
111,2,"Not now, not ever. - Review of an NSF grant submission. This was the entire review"
112,2,The regression analysis is rubbish. Let's see what happens when you do this properly.
113,2,"Exhaustive pages of statistical and meta-analytic data and results which seem rather redundant are presented, about 60+ pages of minute analyses of each possible particle of data."
114,1,"\n- Sec 1: \""abilities not its representation\"" -> comma before \""not\""."
115,1,"\n* Problem statement in section 3.1 should certainly be improved.[[CNT], [EMP-NEG], [SUG], [MIN]] Authors introduce rather heavy notation which is then used in a confusing way.[[CNT], [PNF-NEG], [CRT], [MIN]] For example, what is the top index in $s_t^{3-p}$ supposed to mean?"
116,2,Are we modelling an astronomical object here or an abstraction?
117,1," Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST."
118,2,"This paper reads like a womans diary, not like a scientific piece of work"
119,2,"It is difficult to see the merits of this proposal, and it is doubtful whether the author can contribute anything to this area of research"
120,1," \n* The block quotes in the introduction may be quite important for points later in the paper, but summarizing the points of these quotes may be a better use of space. The authors more successfully did this in paragraph 2 of the introduction. \n* All long descriptions of the appendix should be carefully revisited and possibly removed due to page length considerations."
121,1," I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al. and Maddison et al. work)."
122,1, I enjoyed reading the paper which is in general clearly written.
123,1, Usually it would be defined as $\\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$
124,2,This sentence is so hard to digest it gave me reflux
125,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean
126,2,The conclusion drawn by the authors seems self explanatory and does not require any validation through the presenter work
127,2,"This manuscript uses half of the available pages, and fails to explain what has been done, how and..."
128,1,"\n\nWhile this paper is as far as I can tell novel in how it does what it does,"
129,1," It is unclear what exactly helps, in which case, and why.\u2028\n\n3."
130,1," The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM."
131,1," Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks,"
132,1,\n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs
133,1,\n\nIt is rather unclear why changing the learning rate affects the performance of the model and it is would have been interesting to discuss.
134,1, \nThe experimental setting is also unclear.
135,1,\n\nThe reviewer is favorable in rising the rating towards acceptance if points 1 and 2 will be fixed.
136,2,I am not getting much support for your paper. I hope that privately at least you did not have great confidence in it.
137,1, (b) there are serious writing issues.
138,2,I started reading this manuscript with much anticipation but my enthusiasm was short lived.
139,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
140,1,\n\n3. Why comparing to A3C+ which is not necessarily better than A3C in final performance?
141,1," Vanilla GAN is know to be hard to train and there has been many variants recently that overcome some of those difficulties and its mode collapse problem. \n"""
142,2,"It is unclear how this would advance the field beyond providing additional, previously unknown information"
143,2,It is always rather pleasant to recommend that a paper be rejected.
144,1,\n - p.2: You use pretrained GloVe vectors that you do not update.
145,2,I find the title and the main premise of the abstract confusing and illogical. [key concept X] has the logic of a Monty Python sketch
146,2,"This casual tone is also borne out on the author's website, which is inappropriate if not offensive as a professional introduction."
147,2,"I have read this paper several times through, and I have nothing to say in its defense."
148,2,The writing and data presentation are so bad that I had to leave work and go home early and then spend time to wonder what life is about.
149,2,"The fact that the question of this paper has never been asked should, on balance, count against the paper"
150,2,"I cannot make out signs of independent thinking, work beyond the state of the art, or anything groundbreaking"
151,1," A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization."""
152,1," The ideas are interesting,"
153,2,This is a rather pedestrian treatment of a popular and well-reported topic
154,1,"\n\nThe problem considered by the paper is interesting,"
155,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
156,1," \n\nOn top of this, I do not enjoy the style the paper is written in, the language is convoluted."
157,1,"""This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution."
158,2,"There is an over-reliance on sophisticated statistics at the expense of good old fashioned, scientific thought"
159,2,The authors criticize the approach of [citation of X] before introducing their own. Seems somewhat hypocritical to me.
160,2,Someone has been foraging in theory and has managed to learn how to mangle simple concepts and hide them behind pretentious empty prose
161,1, The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area.
162,1, It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions.
163,2,"If published, uneducated and misinformed statements like this would jeopardize the credibility of this journal"
164,2,"You do not use the empirical data for the analysis, but the empirical data uses you"
165,2,"A great deal of effort has been expended here, but to what end?"
166,1, However I would encourage authors to rephrase their claim of emergent translation (the title is misleading) as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages (bridged by the image) and hence there is no autonomous emergence of translation out of need.
167,1, Only on CIFAR100 the proposed approach is much better than other approaches.
168,1," In evaluation time, they insert these cells between layers of a network comparable in size to known networks."
169,1,  It is not clear what implantation of GAN they are using?.
170,2,Is this really a discovery or just the confirmation of math?
171,2,What is a systematic review? I have never heard of an unsystematic review.
172,1, The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015).
173,2,The current manuscript is deceptive and I do not recommend its publication.
174,1," To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (\""x-space\"" of the GAN), or varied the magnitude of filters or filter planes."
175,2,"I am not sure why there is a full section about limitations, this in itself says a lot about the study"
176,2,It is a collection of good ideas that do not add up to or advance to any meaningful conclusion
177,1, In general it was an OK paper and there are many to be improved.\n\n+ Novelty seems minor.
178,2,Preliminary and intriguing results that should be published elsewhere.
179,2,"The results are obvious. In fact, so obvious that perhaps no one has bothered to write them down..."
180,2,"It was mentioned that participants were recruited. In research, we dont recruit study participants"
181,1," \nAuthors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification."
182,1,"""The paper is well written and clear"
183,1,"  The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks."
184,2,"Bad language, weird sentences, half true statements and even nonsense statements continue throughout the draft, I refuse to review more of this draft until these language issues get fixed properly."
185,2,I did not attempt to understand Figure 2 because there was no motivation given for it
186,2,"For a section on thought, very little seems to have gone into it."
187,2,"When the reader is finished struggling through all the methods and results, he/she is left wondering whether it was worth the time."
188,1," In the experiments, authors only stated that \u201cwe fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph\u201d, and then the graph is used to train existing models as the input of the graph."
189,1,"""The paper considers a problem of adversarial examples applied to the deep neural networks."
190,1, \n\nThere are many baselines missing.
191,2,Not good.
192,2,Your research paper is not motivated. There is no motivation behind your algorithm. Your results lack motivation
193,1,\n They do not match up to the Zhang paper (I have tried to find the matching accuracies there).
194,1," The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\"
195,1," Unlike the generative distribution sampling of GANs, the method provides an interesting compositional scheme, where the low frequencies are regressed and the high frequencies are obtained by \""copying\"" patches from the training set."
196,1, I would argue that there is a lot of evidence for local inhibitory connection in the cortex.
197,1," For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation."
198,2,The manuscript shall be rejected in its current form if submitted for a journal
199,2,"Lots of work, effort, but no real science."
200,2,"The question is not very clear, the analysis is not very thorough, and the conclusions are rather trivial or even self-contradictory"
201,2,"In fact, your hypotheses are not all that complex are they? Nor is Figure 1; nest-ce pas? ,moi"
202,1," It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves."
203,2,"X and Y are both tools that knotheads can use to move 
science backwards"
204,1,\n\nPros:\n- Good literature review.
205,1,"  The current 82.1% accuracy is nice to see,"
206,2,"it is impossible for the reader to keep track of the hundreds (thousands?) of acronyms in the paper. If there ever is an award for most acronyms in a paper, this one would win it hands down.  h/"
207,1,\n\nThis is a timely and interesting topic.
208,1,"\nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank. "
209,2,The manuscript in the present form is not a review article but is rather a number of research papers stapled together.
210,2,This paper introduces tools to answer questions which it does not seem many people are interested in
211,2,To my mind the paper is similar to the medieval debate concerning angels dancing on pinheads. I do not think the paper should be published
212,2,This is a paper struggling not to die
213,1,"""This paper suggests a simple yet effective approach for learning with weak supervision."
214,2,Very very sloppy
215,1, What are the runtimes?
216,2,I would suggest either activating the spell-checker on Word or finding a way to keep your cat from walking on the keyboard.
217,2,I fail to see the contribution either to physics or social science
218,1,"  The idea of using class label to adjust each weight\u2019s learning rate is interesting and somewhat novel,"
219,1," The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper."
220,1, \n- What is the effect of network hyper-parameters?
221,1,"\nMoreover, it would be interesting to show if this class-based learning rates changes the convergence of the model or if the early stopping occurs earlier etc...\n\n"""
222,2,"While I personally enjoyed this contribution, I cannot escape the sense that this is much ado about very little, to paraphrase Shakespeare."
223,2,"This paper reads like a womans diary, not like a scientific piece of work"
224,2,The peaceful atmosphere between Christmas and New Year was transiently disrupted by reading this...
225,2,"Sprinkled here and there are some things, also taken from his readings, that are more or less correct. But it is all very confused."
226,1,"\nTypo: \nIn Session 3 Line 7, there is a missing reference."
227,2,"This is nonsense, It is disingenuous to say the least &amp; The authors should stop pretending their method is useful h/"
228,2,I dont know what the heck the red highlighted text is supposed to mean and left it as is.
229,2,The derivation is correct but too simple! The paper is therefore not suitable for a general readership.
230,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
231,1,"Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions."
232,2,"This paper is conceptually unclear, and the causal argument/hypotheses are muddled. In short, it is a mess. I stopped reading after page 7"
233,1,  Where is this method most applicable and where is it not applicable?
234,1," A KB is usually a pretty static entity, and things are added to it at a slow pace."
235,2,"These theoretical results have not been verified experimentally. Therefore, they are wrong"
236,2,"Most of the work is methodological rather than scientific, making it somewhat boring."
237,1, It would be nice to discuss computational cost as well.
238,2,this may eventually be a cited paper.
239,2,This kind of prose simply borders on cruelty against the reader. And the conclusion is the intellectual equivalent of bubblegum.
240,2,"If you arent going to make to the effort to understand, then you should just give up"
241,2,"The turn of phrase is just a vague rhetorical equivocation that is meant to add a semblance of an intellectual air to the text, a rhetorical attempt at being in fashion, but it dissolves only in empty intellectualism."
242,1," \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is)."
243,2,"Can you explain this part a bit further, but without going into detail"
244,2,"In the end, I am not sure what is the thesis of the essay (or whether there is one) or how and why the parts are connected"
245,1,"\n\nAlthough the manuscript has many positive aspects to it,"
246,2,The authors are treating what might be called a boutique version within a model that is already considered a boutique model' by many.
247,1,"  However, while results on convex hull task are good,"
248,1," The point made in the text between \""Where\"" and \""overseas\"" is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words \""where\"" and \""in\"" both commonly expressing a location."
249,2,"Why dont you just send copies of this to the two people in the world who care about it, and forget the publication route?"
250,2,"The experiments are reasonable, but they fail the fundamental test of good science"
251,1, I found the formulation of the \\alpha to be non-intuitive and confusing at times.
252,2,"...to show their similarities or differences, as well as, advantages or disadvantages by your own letters.."
253,2,The paper could be considered for acceptance given a rewrite of the paper and change in the title and abstract.
254,2,"If philosophy of science was not taught in the authors doctoral program, (s)he needs to go back and dope slap his/her major professor"
255,1,\n* There are thorough discussion with related works
256,1,\n+ Improved performance in speech recognition task.
257,1, so that the recurrent neural networks with attention could capture the learning signal to avoid the repetition issue and the heuristic function in the test time can be removed.
258,2,Too mathematically inclined.
259,2,This work is antithetical to the spirit of [XX research] and will impede potentially important developments.
260,2,"If I never see another piece of writing started with these lines, it will still be too soon."
261,2,"Nothing about the paper is objectively concerning, but I am left unconvinced"
262,2,"It is at best of little value and, in the worst case, irrelevant and offensive"
263,1," However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator."
264,1,"  It would be good to explicitly state that (boldface) s is a vector of scores s_u (or score vectors, in case of multiple edge types) for all u in V."
265,2,I showed this paper to my nurses and they agreed there was nothing new reported here.
266,1,. It is a term to minimize a risk not a parameter
267,1,"\n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree."
268,2,"Im disappointed to say, given the outstanding pedigree of the authors of this paper, that it adds nothing to earlier research."
269,1, Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.
270,1," The given examples seem to exhibit certain kind of mode collapse, i.e. different examples have similar wording from a very limited vocabulary.[[CNT], [EMP-NEG], [CRT], [MIN]] It is possible that the generator just learned to overfit the sentiment classifier, so that the classifier thought the transferred sentences have the desired sentiment, but the transferred sentences may lack variations and hence lacks practical use."
271,1,\n\nMany of the ideas presented are novel.
272,1," Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label)."
273,1,\n\nCons\n-------\n\nNone
274,1,"  \n\n2. The proposed idea is very well motivated, and the proposed model seems correct."
275,2,"Unfortunately, I cannot recommend this paper for publication because its contents violate the laws of physics"
276,1,"  I noticed the following issues:\n\n1) The learning task is based on error patterns, but it's not clear to me what exactly that means from a software development standpoint."
277,2,"My disappointment when finding flawed analyses, conclusions, and terminology [..] was therefore substantial"
278,1," \n\nBecause of this, the paper needs to be updated and cleaned up before it can be properly reviewed."
279,2,The biggest value of this paper lies in the fact that its pages are bound together.
280,1,"\n\nAs mentioned in earlier comments, please reword / clarify your use of \""activation function\""."
281,2,"And finally, the references are a mess."
282,2,The most striking—and troubling—feature of this manuscript is that it contains so many exceedingly strange assumptions.
283,2,The authors seem to be reinventing the wheel and a flat tire to go along with it
284,2,The presentation resembles a fishing expedition with claims made beyond the performance of the technology
285,1, And an extensive literature of theoretical results.
286,2,Not earth shatteringly original but it is hard to be so in this field.
287,1,"  \n\nIf convenient, could the authors comment on a similarly motivated paper under review at iclr 2018:\nVARIANCE-BASED GRADIENT COMPRESSION FOR EFFICIENT DISTRIBUTED DEEP LEARNING"
288,2,"While an interesting concept, in its current form, the approach taken is fundamentally inadequate and flawed for almost all use cases."
289,1," It would make a lot of sense to use the same loss as the evaluation metric (not to mention the properties of PCA)."""
290,1,"""Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field."
291,2,"Unfortunately the paper itself is full of so many fundamental misunderstandings, I don't even know where to begin in criticizing it"
292,1, Are the assumptions in Theorem 2 reasonable?
293,2,"There is no research methodology, no data, no model, no significant analysis and no conclusions which arise from the study"
294,2,Perhaps what I have written is deflating
295,1," In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable."
296,2,"The first and last paragraphs are weak and read in a style unlike the rest of the paper, like poor advertising copy."
297,1, \nand 2) the distributional discrepancy between the re-weighted source domain and\nthe target domain.
298,2,To put it bluntly: The last thing I want is a reviewer rejecting my papers with reference to an ideological paper like this one
299,2,It seems like you are torturing the data until the model converges.
300,1, Hence I'm not fully convinced that this model indeed works as claimed.
301,1," \n- Since there are existing methods to generate images from a textual description (e.g. Zhang ICCV 2017, \""StackGAN\""), Fig. 10 merits a comparison to those."
302,1, What is the bases for these parameter choices?
303,2,The results are as weak as a wet noodle
304,2,"This paper is baffling.[…] In particular, I have not looked at all at section 2 of the paper"
305,2,"While I think there is good reason to have performed this work, it does make for unexciting reading."
306,2,This work is not an easy topic. 
307,2,"It is rather clear that the paper is incomplete and hastily submitted, as it contains no contribution number. This paper is NOT COMPLETE"
308,2,"I believe that the authors have done scientific work, but in the current form of the paper it is impossible to judge it."
309,1," It also showed good online A/B test performance, which indicates that this approach has been tested in real world."
310,2,"Despite the apparently impeccable arrangement of the essay, with headings and sub-headings, the progression of argument is not always transparent, often hindered by otiose wording. 

"
311,1," --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly."
312,1, I am curious about the efficiency of the method.
313,2,Please reject it completely and then block the authors email ID so they cant use the online system in the future.
314,1,"   There is some text about a variant of CTC, but it does not explain very clearly what was done or what the motivation was."
315,2,insights that rarely amount to very much beyond the bleeding obvious. [..] the problem with jargon is that any idiot can have a go
316,1," Another example, \""are obtained using the GloVe vector, not using PPMI\"" - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work).\u2028\n\n4."
317,2,This is a terrible sentence. It has so many different things in it and not enough punctuation
318,1," However, this is not the case for the former (see, e.g., Comon et al., 2008 as cited in this paper)."
319,1," All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION."""
320,2,The rest of the Introduction is just as badly done as the first paragraph so I will not continue
321,1," \n\nIt is not clear why the authors have decided to use out-dated 5-layer \""LeNet\""  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures)."
322,1,"  \n2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted."
323,1,\n\nWeaknesses:\n- Figure 2: the plots are too small.
324,1," I also think that the authors should not discuss the general framework and rather focus on \""data teaching\"", which is the only focus of the current paper."
325,2,"If this was taken from a successfully defended thesis, as it appears to have been, then he should not have been awarded a PhD"
326,1,"\nHowever, performance results seem to be competitive and that's the reader may\nbe eager for insights."
327,2," found the use of the evolutionary theory problematic. This is a highly contested theory, with major flaws"
328,2,Thats not how science is done.
329,1, Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.
330,1,"None of this work is cited, which I find inexcusable.\u2028\n\n2. "
331,1,"\n\nOverall, I think the technical contributions of the paper are quite limited, and the experiments are not well enough described for publication."
332,1," \nFrom these 100 evaluations (with different hyperparameters / architectures), the final performance y_T is collected."
333,1,"\n- The results show only one form of comparison, and the results have confidence intervals that overlap with at least one competing method in all tasks."""
334,2,"While the problem is a very important one for modern society, the topic and lessons are not of broad interest"
335,1," This is in my view is how implementations of episodic tasks with a timeout should be done and is implemented this way is classic RL frameworks (e.g., RL glue)."
336,1," As is, I don't really see how this motivation has anything to do with getting things out of a KB."
337,1,"  For example, the Cai et al. paper from ICLR 2017 is not considered"
338,2,"Publishable, but why?"
339,1,.\n\n2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem.
340,1,\nI do not know which message the paper tries to get across here.
341,2,"Alarming errors, even if they could be easily corrected, gave a sense of carelessness that makes me hesitant to recommend it for revisions"
342,1,\n - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD).
343,1,In the context here this didn\u2019t seem a particularly relevant addition to the paper. 
344,1,\n****\n\nSummary:\nThe authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations.
345,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
346,1,"""The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper."
347,1, This is a very well-accepted method actually used in real-world autonomous cars.
348,1,"\n\n\n#######################################################\n\nPost-rebuttal review\n---------------------------\n\nGiven the details the authors provided to my review, I decided to adjust my score."
349,1," I Firstly, the paper introduces a large sketch dataset that future papers can rely on."
350,1, By the way to me results presented in figure 5 are not enough to claim that the agent trained on random map is implementing a purely reactive wall-following strategy.
351,1, And quantify spatial tuning somehow (a natural method would be to use the sparsity measures sometimes applied to neural data to quantify how selective the spatial profile is to one or a few specific locations).
352,2,It is not clear whether important new insights will be gleaned - it cannot be clear until the final product is reviewed
353,1,"""The paper extends the idea of eigenoptions, recently proposed by Machado et al. to domains with stochastic transitions and where state features are learned."
354,2,"The paper is in good shape, though no Sunday morning reading material."
355,1, While the RWA was an interesting idea
356,2,The authors should at least try to read some of the previous work in the field before attempting to solve our problems.
357,1, Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow.
358,2,"I also do not feel that the lead PI is qualified to undertake this work, [..]  she needs to be academically successful first"
359,2,2. [Paper Strengths]: None
360,2,I found the entire premise of the work to be utterly theoretically bankrupt
361,1,"\n\nThere are also quite a few correctness errors in the paper, and the polish of the plots and language needs work, as outlined below. "
362,2,The PI is an excellent networker but that he does not have sufficient required scientific expertise and capacity to successfully execute the project
363,1, The experimental results show that the propped model outperforms tree-lstm using external parsers.
364,2,"Table 4 seems unnecessary given figure 8. Indeed, figure 8 also seems unnecessary."
365,1,\nThe latent variables plus a one-hot-encoding representation of the relation is used to reconstruct the input entities.
366,2,Since you submitted the paper to a scientific journal: where is the science?
367,1,"""Summary:\nThis paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion"
368,1, It seems right and valid.
369,1,  \n\nThe third observation seems less useful to me.
370,1," I am not saying that there is none, but I do not see how the presented experimental results show evidence for this."
371,2,"author needs to slow down structure at the grammatical level, but more importantly, the process of research, reflection, and argumentation"
372,1, It is the first to study constructing minimal training sets for NPI given a black-box oracle.
373,1, It seems to perform well in practice as shown in the experimental section.
374,1," As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution."
375,2,"Your footnote is unnecessary, and indeed confusing. Where did you get the word oftentimes from? We are in the 21st century now."
376,2,With the appropriate revisions these results could provide a very limited contribution to the field.
377,1,  This paper then compare this objective with the MMD distance between the samples A & B.
378,2,"Since the manuscript is so lacking in all aspects, I wont bother going through it in detail."
379,1,\nThere is however a lack of technical novelty or insight in the models themselves.
380,2,proposal language is frightfully unclear.
381,2,"I know you want to use hormones to study physiological changes, but humans are more than hormones"
382,2,This paper may sink without trace' h/
383,1,\nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN.
384,1," Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?\n\n"
385,1," That is because almost all tasks require good representations for all words, not just prepositions."
386,2,"The manuscript is too long for what the authors have to say. However, additional text is required as outlined below."
387,2,The approach taken is fundamentally inadequate and flawed for almost all use cases
388,2,"If this topic were not dear to my heart, I would perhaps have struggled to follow your logic."
389,1,\n\nPoints against the paper:\n- Methodological advances are limited / unmotivated choice of model
390,2,This study is weak. not innovation h/
391,2,"The authors use a log transformation, which is statistical machination, intended to deceive -.."
392,2,"Unlawful, void and of no effect"
393,1," Because of this unknown, I could not understand the experiment setup and data formatting!"
394,1, \n\nTypos / Details: \n- The range of the coefficient of determination is from 0 to 1.
395,2,"The study rationale is unclear, the exact research question to be addressed poorly delineated and the overall motivation for the study poorly linked to the literature summary and aims of the study"
396,2,"The paper is silent on the kinds of issues that occupy yards of library shelf-space, and goes well beyond what is warranted by the data"
397,1, But unfortunately doesn\u2019t show any results even qualitative like generated samples for other  work on next frame video prediction.
398,1,"""The idea is clearly stated (but lacks some details) and I enjoyed reading the paper."
399,1," The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence."
400,2,"I would propose skipping the entire first page, which is bewildering and distracting"
401,1,\nHave you tried on other tasks?
402,1,"\n\nAs is, I cannot recommend acceptance given the current experiments and lack of theoretical results."
403,2,"I recommend acceptance, provided the editors are willing to stretch the standards for publication a..."
404,1,"  The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same."
405,2,What is a systematic review? I have never heard of an unsystematic review.
406,1,"  Even tough a little bit ad-hoc, it seems promising based on the experiment results."
407,2,"This sounds nice, but in fact it is vague. There are many instances of this kind of nice-sounding vagueness."
408,1," In particular, what are the state space and transitions?"
409,2,The authors should refer to the super interesting article on this topic in Wikipedia.
410,1, The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis. 
411,1," However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides."
412,1, Any of these references would have made a lot of sense.
413,1," \n\nExperiments don't vary the attack much to understand how robust the method is."""
414,2,Why chase a gene in this ridiculous organism?
415,1,"""** post-rebuttal revision **\n\nI thank the authors for running the baseline experiments, especially for running the TwinNet to learn an agreement between two RNNs going forward in time."
416,2,...It's impossible to judge the claims of the authors although there is real reason to believe that the claims may pan out...
417,2,"Right now, there is zero rationale for the study and zero reason to read the study."
418,1,"\n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well."
419,2,I would advise the authors to go back to the drawing board and consider exactly what this paper is to do and do this well
420,1, \n\nQuality: The empirical results (including a video of an actual robotic arm system performing the task) looks good.
421,2,"I cannot, for the life of me, figure out why this paper was written."
422,1,"  The game can be programmed to have an \u201cn\u201d lane highway, where n could reasonable go up to five to represent larger highways."
423,1, I would suggest to:\n1. Compare more clearly setups where you fix the hidden size.
424,1, Much explanation is needed in the author reply in order to clear these questions.
425,1,"""The paper is interesting,"
426,2,"The research questions are vague, which makes them uninteresting"
427,2,"Findings are presented in an anecdotal, descriptive style that I cant interpret."
428,1," However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features."
429,2,I want to vomit; I cant believe this paper was submitted.
430,1,\n+ The approach is capable of theoretically handling all linked information to an entity as additional information to the link structure
431,1," In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets."
432,1,It would be\n  interesting to see what would the method learn if the number of layers was explicitly set to be\n  large and an identity layer was put as one of the option.
433,1,  This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.
434,1,n\n- Originality:\nLooking at RNN from an iterative refinement point of view seems novel.
435,1,  \n \nShould provide a citation for DRQN
436,2,I would like to urge the authors to improve the language description to make the manuscript much clear and logic.
437,2,Use of words like performativity (is that a word?)
438,1,\nThe paper is NOT well organized and so the technical novelty of the method is unclear.
439,1," For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context."
440,2,"The paper brings to mind the Mark Twain quote: 'I didnt have time to write a short letter, so I wrote a long one instead.'"
441,1,\nScheme A consists of training a high precision teacher jointly with a low precision student.
442,1, Might this also suffer from non-convergence issues like you argue SVAE does?
443,2,I'm a bonehead so maybe I missed this?
444,1, Even the definition of external vs. internal environment (section 4) was unclear which is used a few times later.
445,1,\n\n\nMinor issues - \n* Use one style for introducing and defining terms either use italics or single quotes.
446,2,"Unfortunately, for the reader (and it almost seems that for the writers as well), there is no insight or conclusions coming out of their comparison."
447,1, More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm.
448,1, This would have been useful to study in itself.
449,1, My impression is hence that the only possible outcome is\n\nrejection.
450,2,This paper is absolutely ridiculous. It shouldn't be published anywhere and the author should not be encouraged to revise
451,2,Various statements seem to be sweeping and inaccurate generalizations with little robust...
452,1, The experimental results seem solid and seem to support the authors' claims.
453,2,"The study is poorly conceived, inadequately conducted and the conclusions made by the authors do not necessarily follow from the results"
454,2,Nothing new or ground breaking is discussed in this tutorial
455,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock.
456,1,DGA detection concerns the (automatic) distinction of actual and artificially generated domain names.
457,2,There are far too many typos and grammatical mistakes. I started to correct these but got annoyed.
458,2,The abstract is intriguing but confusing
459,2,The theoretical section is unreadable for anyone unfamiliar with the language of relevant French theorists[paper about *German* theorists
460,1," Without understanding this first result, it\u2019s difficult to decide to what extent the rest of the paper\u2019s results are to be believed."
461,1," which isnt very common in most meta-learning papers today, so it\u2019s encouraging that the method works in that regime.\n"
462,2,"Given the way the paper was structured, I felt I had to read most of it."
463,1, Presumably this statement needs to be made while also keeping mind the number of importance samples.
464,2,"Given the pedigree of the senior author, I would have expected better"
465,1,\n\n\nDetailed comments:\n\n- I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear.
466,2,I gave up pointing out all the mistakes because even the author apparently doesn't consider this manuscript a good scientific presentation
467,1," There are no theoretical results regarding this question in the paper, and the empirical justification is also lacking."
468,2,This work is stuck in the past. The referee would rather talk about the future - some of the senior co-authors were the future once!
469,2,"In any aspect that this paper is different from XXX et al., (20XX), it shouldnt be"
470,2,The title is the closest Ive seen to manuscript clickbait
471,2,This is depressing! So much work with so little science -..
472,2,This whole paper is wildly speculative and needlessly convoluted.
473,1, \n\nLemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria.
474,1,\n\nOriginality:\n\npros:\n-The paper while simple sheds some light on important problem with the prior distribution used in GAN.
475,2,"It looks to be more of a chance for the authors to promote a product using a poorly constructed, non-replicable pilot study"
476,2,"And finally comes the conclusion, which is the intellectual equivalent of bubblegum."
477,2,The take home message has to be extracted with significant labor from a punishing set of figures with multiple bar graphs
478,2,"Although it doesnt make for a flashy title (which the authors like more than British tabloids do), there is an alternative interpretation"
479,1, but the presentation is severely lacking.
480,2,You have two many misprints
481,1," As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear."
482,1,"\n\nFinally, the experimental part shows nice improvements "
483,2,This paper is desperate. Please reject it completely and then block the author's email ID so they can't use the online system in the future.
484,2,"I would suggest activating the spellchecker on Word, or keeping the cat from walking on your keyboard"
485,2,"Indeed, the article as a whole shows a lack of sophistication and nuance that is diagnostic of the particular genre of thought pieces"
486,2,"High was my expectation, and so much deeper was my disappointment"
487,2,"If this was taken from a successfully defended thesis, as it appears to have been, then he should not have been awarded a PhD"
488,1,"\u201d Let\u2019s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons?"
489,2,"Not sure how to say this diplomatically, but the manuscript is really dull"
490,2,We regret that some of the remarks made by Referee 1 were not edited before being sent to you.
491,2,"Moreover, it is unclear whether the effect is sufficiently important to warrant replication."
492,1, The experimental results are very good and give strong support for the proposed normalization.
493,1,". As I understand, MCTS was not used in this experiment."
494,1, It is shown empirically that the constrained update does not diverge on Baird\u2019s counter example and improves performance in a grid world domain and cart pole over DQN.
495,1," See more detailed points below in Weaknesses.[[CNT], [null], [DIS], [GEN]]\n\n**Strengths**\nI like the high-level motivation of the work, that one needs to understand and establish that language or semantics can help learn better representations for images. "
496,1," If it wasn\u2019t, then the comparison is unfair, as the results for CP-ALS are drastically underestimated."
497,2,I dont believe in simulations
498,2,People have real issues and I am not at all sure that this one deserves attention.
499,2,"Its like you sat around and dreamed up ideas, except theyre just castles in the air"
500,2,Clearly you can tell that I have become distracted by the style of reporting results. I have in fact given up trying to understand it
501,1, The invariance introduced here does not seem to be related to any real world phenomenon. 
502,1," If that is the case, could the authors describe this a bit further?"""
503,1," The \u201cpipeline\u201d is never well defined, only implicitly in p.7 top, and then it is hard to relate the various figures/tables to bottom line results (having the labels wrong does not help that)."
504,1, The numerical experiments show that using CCC strategy leads to an increase in the proportion of efficient equilibrium outcomes.
505,1,"  That and other recent work have provided some systematic evaluations of complex-valued networks, and shown their utility in a number of cases. "
506,2,"They have addressed most of the reviewer comments, although their responses to a few of them remind..."
507,1, This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.
508,1,\n\n+ Paper is well written and easy to follow.
509,1," Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results - and I do not see evidence for a sophisticated latent representation learned by the network."
510,1, The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network.
511,2,It was as if they walked in with a blank sheet of paper.
512,2,The lead author of this study has an apparent history of convincing otherwise well-respected scholars to be unwitting co-authors on his poor excuses for academic papers
513,2,The MS is like a group of old wise men sitting around mulling over the terrible state of the world
514,1, The application is straightforward and thus technical novelty of this paper is limited.
515,2,A classic instance of reinvention of the square wheel.
516,2,"You know that this table is unreadable – at least one of the authors of this paper must have said this is unreadable, lets put in a series of bar plots with the numbers written on the bar. I agree with that person."
517,1, The main motivation seems to be that it is easier to optimize.
518,1," Although the ideas are interest and technically sound, and the proposed algorithms are demonstrated to outperform several baselines in various machine learning tasks,"
519,2,"I hesitated to suggest this manuscript to be rejected, but I handled to the (naive?) idea that this actual pumpkin can turn into a fair-looking princess."
520,2,Hypothesized is such an ugly word
521,1, \n\n****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful.
522,1,"\n\nI think the paper does a fairly good job at doing what it does,"
523,2,"From this conclusion, the premise does not follow."
524,1,\n- The idea is novel and impactful if its evaluated properly and consistently.
525,1,\n2) Terms used in the paper are not defined/explained.
526,1," However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is."
527,1,"\n\nAs such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference."
528,2,"Given the dodgy sampling technique, one could and should wonder how useful it is to include a variable with so little explained variance"
529,2,"It was hard to bite myself through, but in the back I found some meat."
530,2,"Mr. Beard is very courageous to give references- if someone did look at another book, I am sure they would stop reading Mr. Beard'-Feynma"
531,1,\n\nClarifications:\n- See the above mentioned clarification issues in 'major weaknesses'. 
532,1, It seems as a more natural way to do it.
533,2,Based on theoretical considerations I dont see a reason to perform these experiments.
534,2,I apologize for this but frankly some parts read like a report of a high-school student on a scientific experiment.
535,2,"The abstract says absolutely nothing, and I mean this literally and not as a judgement for the content of the paper."
536,2,The authors appear to be blissfully unaware ... [reviewer 6. Out of 7. Seven.
537,1, A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result.
538,1," There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method. "
539,1,"\n\nMinors:\nThere are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3."""
540,2,"Let me expand, using an analogy. "
541,1," The \""parallel ordering\"" terminology also seems to be arbitrary..."
542,1," Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution."
543,1," \nOverall, I like the paper."
544,2,"The paper is overlong, very verbose and contains unnecessary repetition."
545,2,The figures are dishonest and not all that useful.
546,2,I dont see how your approach has potential to shed light on a question that anyone might have.
547,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock.
548,1, \n\nStrengths:\n- The paper is very well written.
549,1,\n\nPros:\n-- Efficient model
550,1, \n\nThere is also a toy example created to show that this approach works well compared to the RNN based approaches.
551,1,This would give a stronger sense of the kind of wins that are possible in this framework
552,2,The overall tenor is disturbingly glossy and pretentious given the gravity of the topic
553,2,This paper is to science as astrology is to astrophysics' h/
554,1, The UPS optimizer by itself is not new.
555,2,"I stopped reading the subsequent data reports carefully, because I no longer had confidence that they would be accurate"
556,2,An alternative to counting sheep.
557,2,I now turn to my best guess about what the authors might be doing
558,2,This manuscript is not publishable in a reasonable sense
559,2,Details of how important these effects are are missing. Ref.75 is entertaining but inadequate in this respect.
560,1,"""--------------\nSummary and Evaluation:\n--------------\nThis work present a novel multi-agent reference game designed to train monolingual agents to perform translation between their respective languages -- all without parallel corpora."
561,2,Perpetuating the recognition of this in the face of irrefutable evidence is dilettantish and scientifically unacceptable
562,1," They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments."
563,2,"This paper is so bad I cannot even reject it! 
"
564,1,\n- The method lacks details (see Questions above)
565,1, An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. 
566,2,An article like this is just a waste of peer-reviewing resources
567,2,The figure is a mystery to me. Where did all this sound energy go? It defies the basic laws of physics.
568,2,You have two many misprints
569,2,There are FAR too many analyses and results. The reader is swamped.  Its simply not possible to take it all in. It needs to be pruned.
570,1," Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016)."
571,1, \n- how many new data points are finally added into the training data set?
572,1," It is well written, the idea is well articulated and presented."
573,2,"Im really sorry about this reviewer. If youd like, I can get you a new one. - Edito"
574,2,This article reads like the work of a reasonably competent undergraduate.
575,1," \n\nThis is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition."
576,1," My impression is that most papers on NLI use much larger vocabs, no?"
577,2,"his is the complete review, not an excerpt: Good topic, but i dont get an idea of the results."
578,2,This strikes me as the worst kind of postmodern legerdemain
579,2,I would rather read a meta-analysis
580,2,"I sent your MS to three referees in the hopes of finding someone who might like it a little.  Sadly, I failed"
581,1, I'm not sure what key insights can be taken away from this.
582,1,  That limits the value.
583,2,Didnt like this one 
584,1," Even though LCW performs better than others in this circumstance,"
585,1," I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art."
586,2,The title of the submission is misleading and it should be renamed 'A Personal Diary'
587,1,"""The authors define a novel method for creating a pair of models, a student and a teacher model, that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to people."
588,1,"""This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal."
589,2,"It is very lengthy, full of mistakes, irrelevant information, and completely fails to attract..."
590,1,"""****\nI acknowledge the author's comments and improve my score to 7."
591,2,"The result does improve the state-of-the-art, but it is not strong enough for acceptance"
592,2,This inclusion criteria is not needed because to be in a master program they would be legal adults. Please remove and adjust everywhere
593,1,\n\nPros:\n\n1. This paper proposes a simple and intuitive approach for training neural networks.
594,1,"""The manuscript introduces the sensor transformation attention networks, a generic neural architecture able to learn the attention that must be payed to different input channels (sensors) depending on the relative quality of each sensor with respect to the others."
595,2,"I think the author could reduce the amount of equation to a proper level and describes the basic principle. […] To be honest, those equations make me a little headache"
596,1, though it's not clear from the paper that the approach is a substantial improvement over previous work.
597,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
598,1," As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful."
599,2,This is a confusing paper
600,1, The paper is missing a clear analysis of NGM's limitations...
601,2,"Fig 3e is fanciful, verging on silly"
602,1,.\nI raised my score to 7.
603,2,"I'm really sorry about this reviewer. If you'd like, I can get you a new one. - Edito"
604,1,"""The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension."
605,1, \n\n2. The authors proposed a simple method to ground the language on visual input.
606,1,\n\nThe paper mentions that the approach is \u201cunsupervised\u201d. 
607,1, It is not clear what is the source of the other cancer control case.
608,2,I would suggest the authors to have some native English speaking to go through it
609,2, DONT SEE WHATS SO HARD I CLEARLY SAID TURN RUEAT
610,1," In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers?"
611,2,The findings are not novel and the solution induces despair.
612,1,"\n\n1. The accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing, this also makes the search greedy , which could be suboptimal."
613,2,"I haven't read the word audacity yet in commenting on my reviews, so that is a first. 

- Reviewers response to a response to their revie"
614,1," Also, the experimental results look quite promising"
615,1,"   The proposed network predicts a 2D mask image, where local maxima  correspond to object locations, and values of the maxima correspond to presence values."
616,1," Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016."
617,2,"Probably switching to Bayesian stats will be too difficult for many scientists in the less intelligent fields (e.g., psych)"
618,2,"Recommendation: Publish elsewhere.
Comments: [none] "
619,1, Both these contributions are important in the effectiveness of the overall algorithm.
620,1," Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough."
621,2,This is an interesting paper but it is not relevant
622,1,"""# Summary and Assessment\n\nThe paper addresses an important issue\u2013that of making learning of recurrent networks tractable for sequence lengths well beyond 1\u2019000s of time steps."
623,1,"\n-\tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me."
624,2,The conclusions are still not consistent with the (lack of) findings.
625,2,It (paper) has a kind of self-help quality to it
626,1,"  \n\nA major point of concern is that they do not use the public dataset proposed in Finn et al. 2016, but use their own (smaller) dataset."
627,2,Line 181: on each of the two trees. What does the term two trees refer to?
628,2,For the sake of time I have listed only a few (thirteen!) of the most glaring errors
629,2,"My summary assessment of the paper is as follows:
Introduction: Poor
Background: Poor
Methods: Poor
Results: Poor
Discussion: Poor
Conclusions: Poor 
Overall assessment: Poor
Recommendation: Reject
Further Comments: Why was this submitted to a journal?"
630,2,"Weak, poor experimental design, no analysis possible, carelessly written, poorly thought through"
631,1, This is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen.
632,2,This needs some rephrasing—its loaded with the assumption that there is a real world
633,2,This paper still reads in places more like alternative fan-fiction than scholarship.
634,1,"\n\nProx tables (see also the new edition of Bauschke and Combettes' book): P. L. Combettes and J.-C. Pesquet, \""Proximal splitting methods in signal processing,\"" in: Fixed-Point Algorithms for Inverse Problems in Science and Engineering (2011) http://www4.ncsu.edu/~pcombet/prox.pdf"
635,2,"Ultimately, the results are just a set of observations."
636,2,The only conceivable contribution this paper can make is by providing the academic community with an alternative to counting sheep.
637,1," It argues that because there should be an optimal noise scale (which maximizes test performance), the batch size should be taken proportional to the learning rate, as well as the training set size, and proportional to 1/(1-m)."
638,1, The good aspect of this paper is that it has some performance improvements.
639,2,"By now, there are over 1,000 [articles on this topic], but these authors have not read a single one."
640,1," The paper is aiming to solve a practical problem, and has done some solid research work to validate that."
641,1,"""The key contribution of the paper is a new method for nonlinear dimensionality reduction."
642,2,You make the reader feel uneducated
643,2,This reads like a pretty good MA level seminar paper but comes nowhere near the intellectual status required for publication in journal X
644,2,I am constructing this review more in a stream of conscious thought than a systematic assessment.
645,2,This article is on an interesting topic. Unfortunately there is no more positive to say about this manuscript.
646,2,"I am sure that you would find what you expect, but I question its value as research"
647,2,I concur with the reviewers that the methods are insufficient and thus the conclusion are totally...
648,2,"However, the applicant seems to have run out of steam before he developed a detailed plan and completed the proposal. This is unfortunate"
649,1,"\n3. In Eq. (3), \\tilde{D} is not defined."
650,2,"No new insights, no important question addressed, no problem solved. -.."
651,1,"  \n\nIn general, the paper is well-written and the main ideas are clear."
652,2,"I was really looking forward to reading this manuscript, however this enthusiasm soon waned."
653,1," \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules."
654,1,"""The quality of this paper is good."
655,1,"  Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper."
656,2,I was somewhat disappointed after reading the MS; I was initially expecting some lighting hit but nothing really happened in the end.
657,1,"\n- Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi."
658,1," Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means."
659,1,"\n\nPros:\n\n-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time\n\n-Good empirical evaluation"
660,1,\n\nContributions:\n- The paper proposes a cheaper activation and validates it with an MNIST experiment.
661,2,The results section is not great (boring).
662,1, \n\n- The related work section seems light.
663,2,The following paragraph will strike many of your readers as shrill. They will stop reading the article and throw it into the fire
664,1, \nComparison of Maximum Likelihood and GAN-based training of Real NVPs\nhttps://arxiv.org/pdf/1705.05263.pdf
665,2,This would seem to constitute the very minimum basic scientific requirement for attempting to publish a body of (unoriginal) data
666,2,"Focus on limitations, because they are many!"
667,1," However, I\u2019m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized."
668,1,\n\nQuality:\nThe proposed approach is well motivated and the experiments show the limits of applicability range of the technique.
669,2,"papers such as this, lacking any knowledge of astronomy and physics, should never be published in any scientific journal with referees"
670,2,A blizzard of extraneous data external information should be culled.
671,2,eviewer 2: 'THOU SHALL AWAIT MY JUDGEMENT... FOR SIX  EARTHLY YEARS
672,2,The results look like a smorgasbord of data
673,1,"\n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental."
674,2,"Frankly, I read the manuscript about two weeks ago and don't remember the context, nor did I cross-walk one part of the essay with another to validate the thought."
675,2,An effort as I might imagine necessary to make this paper work (beyond flagging the insanity) would require a Swiftian talent for irony
676,2,"The paper descends into nonsense, never to return, on line 44."
677,2,Ah - now I see a glimpse of promise in this paper - Five pages into the documen
678,2,This review is without a doubt the single most difficult one I have had to write so far.
679,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
680,2,"On my opinion, the approach the authors are using is trivial and the problem they are solving is made up."
681,1, I am raising my score a bit higher.
682,1,"It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models."
683,1, This may significantly change the conclusions drawn from\nthe experiments.
684,1," \nb.\tRelated to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers?"
685,2,"I invite the authors to provide compelling, coherent, and quantitative data."
686,2,The last two sentences of the summary greatly exaggerate the value of this paper and the usefulness of its conclusions.
687,2,This is all science done by wishful thinking. -..
688,2,There is no need to test these hypotheses. They have been tested a long time ago. It is in all...
689,1,"\n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n"""
690,2,The proposal is largely descriptive and mostly a fishing expedition. It will be great if they catch some interesting or unexpected fish
691,2,"The paper is overlong, very verbose and contains unnecessary repetition. - (via shitmyreviewerssay)"
692,1, The clustering by weight (4.1.) is nice and convincing that the model learns something useful.
693,2,"Once I penetrated the pigeon English, I found very little substance underneath.  (Um reviewer? I think you mean pidgin. Still hurts)"
694,2,Arrestingly Pedestrian is both insulting and a great band name
695,2,The manuscript in the present form is not a review article but is rather a number of research papers stapled together.
696,1, \n\nPros:\n- Investigating the ability of distributed representation in encoding input structured is in general interesting.
697,1,"  but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications."""
698,2,However I deplore the fact that this paper has been created at all
699,1,". I therefore recommend that the paper be accepted.\n\n"""
700,1,""".\n\n\nMinor comments:\n- page 1: The reference list could also include  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00934 and  https://arxiv.org/abs/1510.02777\n- "
701,2,It is hard to imagine researchers of this caliber being unaware of a half-century the field X
702,1," \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc."
703,2,This research is not worth a hill of beans
704,2,The peaceful atmosphere between Christmas and New Year was transiently disrupted by reading this...
705,2,I would drop the first clause in the paper title. It does nothing other than degrade the scientific integrity of the work
706,1," This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations."
707,1,"""The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network."
708,1," Overall, the reported functionality is nice,"
709,1,"\n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction."
710,2,"As far as this reviewer can tell, what the authors present is not so much a model as it is an equation."
711,2,Reviewer : I do not have any additional comments. I can let this manuscript pass.
712,1,"\n\nApart from these comments, I found the paper interesting especially for the big amount fo comparisons carried out."
713,2,This is rubbish!!!
714,1, The experimental results also look promising.
715,2,This theoretical framework is dark and depressing. But it fits your study well
716,2,"The scales appear to be not divided into quartiles as stated, but rather are based on the distribution of the data (into quartiles)."
717,1,"\n\nIn addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10"
718,1, \n\nCons/questions:\n1. The motivation of the model choice of q is not clear.
719,1," However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all."
720,1, \n\nSignificance \n\nThe approach outlined in this paper may spawn a new research direction.
721,1,"  \n\nSmall comments\n---\nThere is a typo in Figure 4 -- \""Howerver\"" should be \""However\"""
722,1," but this manuscript should be improved prior to publication."""
723,1,"""This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image."
724,1, I would think this should be less-than-or-equal
725,2,I stopped reading here.
726,1,"  The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). """
727,1, These solutions achieve zero squared-loss.
728,2,Presented paper has 13 pages and 26 adequate references. The paper seems to be very interesting.
729,2,This is a pointless paper. It offers neither interesting new data nor cogent explanation
730,1," \nFor example: \u201cThe effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification \u201c"
731,2,"I shall not comment beyond the end of the methods section, and shall comment selectively rather than exhaustively (which would indeed be exhausting)"
732,2,"You aimed for the bare minimum, and missed!"
733,1, \n- It is not clear to me where the baseline results come from.
734,1, The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies.
735,1, Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently.
736,2,and I'll refer to you as the author despite the effort you have put into ensuring that I know who you are...
737,2,"it reads like someone who searched around the literature without much authentic understanding of social science, methodology or statistics"
738,2,Being first is not sufficient. I could be first to do a backflip off a building with no net but that doesnt make it a good idea
739,1," This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions."
740,2,…this paper is extremely lengthy and tedious with respect to its importance and relevance.
741,1," For example, the authors\u2019 first contribution is hidden among the text presentation of section 2.[[CNT], [null], [CRT], [MIN]] \n* The paper relies heavily on the supplement to make their central points.[[CNT], [null], [DIS], [MIN]] \n* It is nearly double the recommended page length with a nearly 30 page supplement"
742,1, \n\n- The paper frequently overclaims.
743,1, Therefore I recommend acceptance for it.
744,2,the sthanthard of writing is impercable
745,1, The authors report the highest measured acceleration of VGG16 using low-rank approximation techniques (6.2x vs 5x previously) with a similar accuracy drop (1.2% vs 1.0% previously).
746,1," The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible."
747,2,Are you kidding?
748,1," This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline."
749,1, I suspect it might be speaker identification.
750,2,It is essentially an opinion piece that editorializes shamelessly about the superior methods of a recent paper in the first person
751,1, The proposed model combines some of the strengths of factorization machines and of polynomial regression.
752,2,This left me somewhere between scratching my head and pulling my hair out
753,1,"\nThe acknowledgements should not be included here either. \n\n"""
754,1, The experiments with Baird's and CartPole were both convincing as preliminary evidence that this could be effective. 
755,2,This paper makes no contribution
756,2,The authors are amateurs
757,1, \n\nMinor points:\n- Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)}
758,2,"The system described in the manuscript is inherently confusing. Despite re-reading, its basic elements did not make sense."
759,1,  Table 1 probably reports 100 * R^2? Please fix the description.
760,2,The stimulus is artificially produced and therefore irrelevant.
761,2,"Many questions on the text, for example, cause embarrassment in understanding the text"
762,2,"Theories can be meaningful when they can explain phenomenon. If theories exist for themselves, they may be play of language."
763,2,This is starting to feel like a book report
764,1,\n - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct.
765,1,"  Moreover, is this D_S same as the style classifier used in the metric?"
766,1,"\nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n"""
767,2,"The paper is okay, its not wow but its hard to reject it."
768,2,The authors conclusions not only contradict their own data but also the laws of thermodynamics
769,2,The author gives the impression of having had some mathematical training.
770,2,Neither research nor science
771,2,"You aimed for the bare minimum, and missed!"
772,2,There is so much that is wrong with this paper that it is difficult to know where to start
773,1,\n- It is not clear to me what the baselines actually are or how I can found more info on those.
774,1,"""This paper studies a dual formulation of an adversarial loss based on an upper-bound of the logistic loss."
775,2,I dont know what to do with this.
776,1,  \n\n%%% After Author's response %%%\na. My mistake. Perhaps it should be clarified in the text that u are the weights.
777,1," In all three cases, the proposed solution outperforms the baselines on larger problem instances. """
778,1,\n\nDetails:\n- p. 4 please do not qualify KL as a distance metric 
779,1,.\n\nweakness\n\n* The graph in p.3 don't show the architecture of the network clearly.
780,1,". It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered,e.g. does the proposed sparse learning method converge at the same rate as the others?"
781,1, Have you compared with using random embeddings for the named entities?
782,1," Other than that, I don't get much more insight from the theoretical result."
783,1,\n\nI think the work has several important issues:\n\n1.
784,2,You know nothing about [general topic of the paper]. Cite [five irrelevant citations from same scholar].
785,1," Furthermore, given its simplicity, I would expect a comparison against scheduled sampling."
786,2,"While the authors have no legal obligation to cite these unpublished results, they are probably morally obliged to consider them"
787,1,"\n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case."
788,2,This paper is indicative of a degenerative research paradigm' h/
789,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock. -Referee 
790,1,\n\nSection 2 motivates the suggested linear scaling using previous SGD analysis\nfrom Smith and Le (2017).
791,2,"When the reader is finished struggling through all the methods and results, he/she is left wondering whether it was worth the time."
792,2,This manuscript achieves the dubious distinction of being conceptually stillborn
793,2,Your proposed method should be compared with another method that introduced in a prestigious paper...
794,1," \n\nPerhaps the authors have just done a good job of laying the groundwork, but the dual-based approach proposed in section 3.1 seems quite natural."
795,2,"This is a pretty trivial study, sample size is suspiciously high, and a tiny effect of 5% percent (who cares if it's significant)"
796,2,"I have read this MS twice, which given the grammatical howlers in the Abstract would appear to be more times than it has been read by the authors"
797,1," After reading the authors responses and consulting the differences between the versions of the paper, my review remains the same."
798,1," \n\nIn the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions."
799,2,this piece…shows no real understanding of history at all
800,1, The claim of competitive performance needs better justification according to the presentation of the F1 scores.
801,1, In which cases the former did correctly but the latter didn\u2019t?
802,1, \n\n2. Baselines are not necessarily sufficient
803,2,"I appreciated how the author seemingly had in mind that a goodly percentage of the readership are not native speakers, so anything too academic or erudite might be lost on them"
804,2,"As it is often the case with conceptual developments, most of it doesnt make a lot of sense, in the..."
805,2,The authors have not bothered to learn the first thing about the theories they are hoping to refute with ill-designed experiments and muddled rationale.
806,1," So, I'd really urge the authors to extend this evaluation."
807,2,this may eventually be a cited paper.
808,1,".\n\n(2) I think it would have been stronger if the authors would have provided some empirical results validating their claims. \n\n"""
809,2,I am afraid this manuscript may contribute not so much towards the fields advancement as much as toward its eventual demise.
810,1," This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K."
811,2,"The word asses should read assess 
"
812,1,"\n\n          Limited experiments \n\n"""
813,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
814,2,..incoherent babble of unsubstantiated overstatement.
815,1,  The same issue is found in Figure 2.
816,1," \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work."
817,1," While this might be (known) intuition that people rely on when learning deep generative models, it is important to be able to test it, as this paper does."
818,2,"The authors spelling of coordinate, while technically correct, is arcane and annoying."
819,1, It is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forgetting.
820,1," This seems to work quite well, and I speculate that it is because prepositions often function to indicate grammatical relations between different arguments, rather than being content-bearing words themselves."
821,2,The original study was published in PsycScience. This [failed replication] is just some work by a grad student. Reject.
822,2,I find this submission confusingly written. The aims and objectives seem rather muddled. References are rather sparse.
823,1, Why the error rate reported here is higher than that in the original paper?
824,2,"There is a lot of terminology flung around such as 'false negatives', 'false positive', 'median'"
825,2,This submission looks more like an advertising booklet rather than a research paper.
826,1," \n\nOverall: I like the idea this paper proposes,"
827,2,The stimuli are impossible to compute.
828,2,The analyses are too statistic - submissio
829,2,It feels a little bit like someone wanting to run a series of statistics.
830,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
831,1,"For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it})."
832,1, A nice application\nof the separable principle to GCN.
833,1,"\n\n\n## Quality\n\nOverall, only single training runs from a random initialization are used."
834,1,\n\nI like this paper.
835,2,"Though the objective of the paper pretends to be ambitious, there are a significant number of problems that limit the study's usefulness."
836,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
837,2,Why chase a gene in this ridiculous organism?
838,2,"Statistical analysis. It is a bit strange for me that authors have used Python for statistical analysis instead of using SPSS or MATLAB as usual in the field. Please, explain."
839,1, \n\nExperiments -- why/how would you have distorted test data?
840,1," In practice this second solution is analogous to the first, but a general 'distractor' class\nis added."
841,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean
842,1," I still think the work is somewhat incremental, but they have done a good job of exploring the idea (which is nice)."
843,1,"\n\nFirst, the writing is far from clear. There are typos and errors all over at an unacceptable level."
844,1,"\n\nOverall, this work is providing useful experimental insights, clearly motivating\nfurther study.\n"""
845,2,The manuscript is a collection of fragmented and disconnected descriptive observations.
846,1, Below are some less important comments.\n\nSec 5.1: great results!
847,2,"We invited 18 reviewers and after quite a long time, only one reviewer had agreed. That review is now..."
848,2,"Written in parts like an experience track paper, minus the experience."
849,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -Edito"
850,2,I find the title and the main premise of the abstract confusing and illogical. [key concept X] has the logic of a Monty Python sketch
851,1," Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement."
852,1, This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015).
853,1,"""This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. "
854,1,Does it well describe the new space?
855,1,"\n - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold.\n"""
856,2,I am concerned that the author is not getting the advice that she needs in order to produce a publishable paper
857,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
858,1,"\n\nOverall the paper is well-structured and related work covers the relevant papers,"
859,1,"\n\nTo make the paper better, more empirical results are needed."
860,2,Figure 3: I haven't been this confused since Cardi B's 2019 Met Gala outfit
861,2,(although I admit that here is a possibility they might turn out to be correct in that they are guessing right)
862,2,This result would be great if it were true
863,2,It appears that publication in any form would be premature at this time.
864,2,English need to be corrected by an english speaker
865,1,"  After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry-picked, and it will be good to include more NLP tasks."""
866,2,I started to review this but could not get much past the abstract
867,1,\n\n\nREVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments.
868,1," \n\n To demonstrate the \""mesh/graph generation\"" capability truly, the authors need to experiment on novel topology generation."
869,2,"The underlying science here is quite interesting, but the presentation does its best to disguise it"
870,2,"It appears the authors has generated reports in a hurry, compiled, and presented as an article"
871,1," \n- It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters?"
872,2,This looks like a work of pure fantasy.
873,1, This figure only shows that training shallower networks is more effective than training the deeper networks on GPU.
874,1,  This work has a well-established motivation: traditional attention for target-dependent sentiment classification cannot model the interaction between target term and context words when making predictions.
875,2,"This is clearly a submission that needs to be shredded, burned, and the ashes buried in multiple..."
876,2,The writing is often arrestingly pedestrian' :ArrestinglyPedestrian!
877,1," Still, I think the contribution in that part is a: sentiment-psychologically inspired analysis of the Thumbrl data set."
878,2,I think time will show that inheritance (section 1.5.3) is a terrible idea.
879,2,This book has more mistakes than a hound has fleas
880,2,The value of this manuscript is not so much that this is novel research as it is a demonstration of opportunistic sampling and analysis
881,2,I find this a smart-looking house built on a weak and shifty foundation
882,1, \n\nCons:\nThere are a few key technical issues that are not clearly addressed.
883,1,\n\nExperiments suggest that it can result in better structured predictors than training models directly via backpropagation gradient descent. 
884,2,You haven't reflect all relevant studies.
885,2,My greatest criticism of the paper is the tendency of the authors to make an argument and then immediately contradict themselves
886,1,"  This  somehow allows to non-parametrically infer from the data the \""shape\"" of the activation functions needed for a specific problem."
887,2,"XXX is inserted purely for fashion, adds nothing, and reflects the authors belief/wish that fashionable papers regardless of logic or content have a larger chance of acceptance. Our community needs to combat this sort of unreflexive pseudo-scholarship"
888,2,It is worth saying that I am not convinced that you contribute to the evidence base in this paper.
889,1,"\n\nWhile I can definitely imagine that your network architecture is well-designed and a good choice for image classification tasks,"
890,2,Among the topics to be subject to the integrity of attention needs to be work related to the subject of innovation does not
891,2,"I guess this proposal could be interesting, if youre interested in this obscure sect of biology"
892,2,This is from a methods perspective rather unacceptable in these days of voodoo science.
893,2,The results of the study are adequately justified.
894,1," However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear."
895,1, More things could have been considered.
896,1," If my understanding is correct, the LTMN is trained to predict the baseline solver's output."
897,2,"Though less enthused about manuscripts novelty, this reviewer does admire the hard work of your group."
898,2,I felt like I was reading a horror movie
899,2,"I was originally very excited to review this paper, since such a bridge would span uncharted lands – here be dragons!"
900,1,"  They are also fairly specific, for example \u201csurprise\u201d is sudden reaction to something unexpected, which is it exactly the same as seeing a flower on your car and expressing \u201cwhat a nice surprise."
901,1,"\n\nIn the ImageNet classifier family prediction, how different are the various families from each other?"
902,1, This is especially true as a more challenging benchmark\ncould be created very easily by simply scaling up the image.
903,2,To me the question is uninteresting
904,2,default settings?? huh???
905,1,"""This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network."
906,1," Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation."
907,2,The presentation of the paper is difficult to follow for a hard scientist and it sometimes reads as if it were machine generated
908,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor."
909,1," Moreover, the empirical comparisons are only conducted on MNIST."
910,2,Uninteresting. Unpublishable. Reject.
911,2,The whole premise just reverted the biosensor field back 20yrs
912,1," In a similar vein, you should be clear about the connections to Lipton et al 2016 as another method for approximate Bayesian posteriors in DQN."
913,2,"The team has generated the kind of gaudy, brobdignagian dataset that makes it such a curious and exciting time"
914,1," Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al."
915,1,"  Experiments are conducted on the task of image classification for a couple well-known DNN architectures (VGG and Resnet) to show a speedup of runtime in testing, significant compression of the network, and minimal degradation in performance."
916,2,"your figures are provided at 72 dpi, which is fine for childrens games, but professional quality images require 300 dpi"
917,2,The manuscript reads much like an unrevised masters level paper.
918,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
919,1," Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used."
920,1,"\nFor example, the existing method and proposed method seems to be mixed in Section 2."
921,2,I can see that the manuscript has archival value
922,2,"Unfortunatelly [sic], the paper is very shallow. The results are at the level of a naive master thesis"
923,2,'It is not even wrong' - Wolfgang Pauli commenting on the manuscript of a junior colleagu
924,1," \n\n1, the derivation of the update of \\alpha relies on the expectation formulation."
925,1, They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks.
926,2,"The odds ratios in Table 2 are like the plains of Kansas: flat, flat flat!"
927,1,  \n\nWeak points\n---\n- I have a critical question for clarification in the experiments. 
928,1, How long should be the training to ensure a good and stable convergence of the method?
929,1,Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels.
930,2,"This is madness, frankly, almost regardless of how it was implemented."
931,2,The final section illustrates the papers poverty. We are given the sort of banality that civil servants write for their masters speeches
932,2,"The paper is overlong, very verbose and contains unnecessary repetition."
933,1, but I have the following complaints:\n\n(1)  I have the same question as the other reviewer.
934,2,The writing is often arrestingly pedestrian
935,2,The paper is - and I mean this with no disrespect to the author- a sort of echidna or platypus of a paper.
936,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
937,2,"I am not sure why there is a full section about limitations, this in itself says a lot about the study"
938,2,"This study, an original work and repetition of earlier feature of these studies is similar. Writing language is quite a lot of errors"
939,1, \nA subgoal is defined as a linear transformation of the distance traveled by an agent during a transition.
940,2,"Not now, not ever"
941,2,"While this study represents a substantial amount of work, it is not all that clear why the work was..."
942,2,Nobody in their right mind would ever suggest such a model
943,2,"Indeed, by the end of the paper, the reader is left with a feeling of so what now?"
944,1,"   The idea of extracting policies corresponding to individual automaton states and making them into options seems novel,"
945,2,I am not very excited about this. There must be better ways to spend your money. -Grant review result. Full review
946,1,\n* The paper is clearly written and well illustrated by figures and examples.
947,2,It [the paper] has a kind of self-help quality to it
948,2,The concluding 'takes a village' sentence is also a bit unoriginal.
949,2,This would have been a question of high interest 10 years back
950,1, The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach.
951,1,\n\nSection 5.1:\n- I don't agree with the authors that the topics in Table 3 are interpretable.
952,1," By the proposed representation, the authors are able to apply image classification methods (supervised or unsupervised) to subgraph classification."
953,2,"the correlations are not that strong (e.g., p = .022)"
954,2,Pity about the main thesis. - First Sentence of the Revie
955,1,"Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply."
956,1,"\n- Possibly inflated results reported for Hyperband experiment"""
957,1,\n* Some typos\n    - page 4: some duplicate words in discriminative embedding session\n    - page 4: auxliary -> auxiliary\n    - page 7: tescting -> testing\n\n
958,2,"I am generally very happy to provide extensive comments on manuscripts, but this submission was an absolute waste of my time."
959,1," Moreover, a very simple baseline to assess the LSTM component of the model, would be to produce a graph by sampling links repeatedly from the latent space of node embeddings."
960,1," The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound."
961,2,"The figure is really not needed. Were I to be shown this by a psychologist, I suspect I would hire a replacement"
962,2,Reject – More holes than my grandads string vest!
963,2,I find the author's writing to be very undergraduate-like.
964,1," This is in part due to the fact that the paper is relatively short, and would benefit from more detail."
965,2,"Many of the most serious errors are more or less just copied out from what he has read, so it is hard to know how to deal with such cases"
966,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. - Edito"
967,1, The authors should add more justification for the where/how these representations will be useful.
968,2,"The 'Theoretical Analysis' section is trivial and requires no analysis, as any sensible schoolkid can identify its solution"
969,2,It amounts to story-telling ! 
970,1," However, I am not convinced that this is novel enough for publication at ICLR."
971,2,"If the editor somehow accepts this paper, they risk permanent destroying the credibility of this journal and its editorial board"
972,1, however it is poorly written and do not contribute much in terms of novelty of the approach.
973,2,I started to review this but could not get much past the abstract.
974,2,These snotty kids in quantum information
975,1," The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance."""
976,1, \nI think that the paper should be submitted to a journal or conference in the application domain where it would be a better fit.
977,1, Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated.
978,2,"If participants were recruited from a university, I imagine they would usually be 18-22 years old. Why does your sample range from 18 to 63? Im a bit lost here."
979,1,"\n\n\nMinor:\n\n- no legend for Fig. 1\n\n-notes -> noted\n\nhave focused\n\n\n\n\n"""
980,2,"The whole paper reminds me of a paper of a couple of years ago, which I didnt like."
981,1," Intuitively, the diagram shown in Figure 4 works well for 3 classes in dimension 2."
982,1, I think I might have missed something here.
983,1, Sketching some such scenarios would help the reader understand why the issue is practically important.
984,1,"\u201d - what does \u201cthe accuracy improvement is smaller than 0.1%\u201d mean?"""
985,1,"""The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves."
986,2,The introduction seems pointless. It offers some odd views
987,2,I can see that the manuscript has archival value
988,1,".\n* While many loosely-related works were surveyed, it is not clear why literally none of them were compared."
989,2,The conclusion is something of a shaggy dog.
990,2,The problem is epitomized by almost every word choice in the title (vi
991,2,"The paper is also unnecessarily sprawling, verbose, and heavy on extended descriptive exposition of other peoples views."
992,1,"  In European Conference on Computer Vision, pp. 297\u2013312, 2014a."
993,1,\n\nPros:\n- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs
994,1," I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge."
995,2,"entire review:] 'Research method is very important; however, the reviewer cannot accept a paper without hypothesis, validity, reliability"
996,1,"n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays."
997,2,The original study was published in PsycScience. This is just some work by a grad student. Reject. [on a failure to replicate a finding
998,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
999,1, \n\nThe main novelty of this work are 1-balancing mechanism for the replay memory.
1000,2,"To be frank, it was boring to read once one got past the beginning. This is rather ironic given the paper is about humour."
1001,2,Having read through this a couple of times I have ended up feeling rather depressed.
1002,1,\n\n\nThere are several issues with the paper and I cannot recommend acceptance of the paper in the current state.
1003,1,\nThe improvement w.r.t. other methods seems marginal.
1004,2,I do not want to see the paper again.
1005,1,"\n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical)."
1006,2,"…somewhat anachronistic–this study would have been really interesting 10-15 years ago, but not it seems quite out of date."
1007,2,The lack of theory is painful at times
1008,1," Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work!"
1009,2,I now have had a chance to look at this paper. I think it is a bit of a joke.
1010,2,"This paper reads like a womans diary, not like a scientific piece of work"
1011,1, \n\nExperimental results seem promising but I wasn\u2019t fully convinced of its conclusions.
1012,2,"If the results are correct they cannot be new, if they are new they cannot possibly be correct. It is hard to say which is the case"
1013,2,"So overall we do not recommend a resubmission, but can let you try if you insist

- The Journal that accepted the resubmission"
1014,2,Have you no command of the English language?
1015,2,"I am concerned that the survey data for this report were collected in 2005, 15 years ago."
1016,2,"I think this article is carefully argued, but at no moment is it stated that Heidegger was a Nazi."
1017,1,\n3) Treatment of related work is lacking.
1018,2,"I suspect it may represent the 'off-cuts' of some other scholarly project, scraped together from the workship floor to make up another publication"
1019,1," While the idea of pivot based common representation learning for language pairs with no parallel data is not new, adding the communication aspect as an additional supervision is novel."
1020,2,This sentence would... put off anyone not being paid to read it
1021,2,This study has the same problem with focusing on a garbage-can group that is not uniformly sampled and has no evolutionary cohesion.
1022,2,"This paper reads like a womans diary, not like a scientific piece of work"
1023,1,  VAE is very power latent variable model which also not being compared against.
1024,2,"I read the first 6 pages of the paper. […] Sorry, I cant finish reading this paper. I certainly will not recommend anyone else."
1025,2,"This is a disaster. I could continue, but you see my point."
1026,2,Pacific oysters and carpenters ants are not 'animals'. Correct this word.
1027,1, Such representation style is highly discouraging and brings about un-necessary readability difficulties.
1028,1," The idea of structured matrices in this context is not new, but the diagonal block structure appears to be. "
1029,2,I am concerned that so many trees died for this to be the result.
1030,2,"I also added back in the two lines on p.16 that youd inexplicably deleted. If theres a reason for their deletion, let me know.

-Journal editor, after telling me to reduce the word coun"
1031,2,this is where the real problems start
1032,2,"The title appears to me to be clickbait, and, as usual for clickbait, leads to disappointment"
1033,1,\n2. Cheap soft unitary constraint
1034,2,The proposal is largely descriptive and mostly a fishing expedition. It will be great if they catch some interesting or unexpected fish
1035,1, \n\nThis paper is a very difficult for me to assign a final rating.
1036,2,You should consider consulting a competent statistical adviser.
1037,2,"There is essentially nothing unexpected, although the central observation of [..] is an unexpectedly large and important effect"
1038,1," \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing."
1039,2,The writing is not at school level
1040,1, The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN.
1041,1,"""1) This paper proposes a method for learning the sentence representations with sentences dependencies information."
1042,1,"""The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \""strong\"" network)."
1043,1," Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance."
1044,1," The results show promising results,"
1045,2,Note that you failed to provide to provide the contribution number in the acknowledgements. The paper is NOT COMPLETE!
1046,1," I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from \""slope\"" to \""gradient norm\""."
1047,2,"The question in the first paragraph is not functional in my opinion, as are the words inner workings"
1048,1," However, it is unclear that overall training time can be reduced with the help of this technique."
1049,2,Unfortunately I was hoping for more.
1050,2,None of the following comments on the original manuscript has been correctly reflected or answered [1st round review
1051,1, There are no comparisons to other possibilities.
1052,1,"\nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN."
1053,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
1054,1," \n\n\nCons - \n* The main narrative is lost in the text, leaving a reader unsure of the authors main points and contributions as they read."
1055,1," \n\n- The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc."
1056,1,"\n\nThe significance of this work would still be quite strong if, as the author's suggest, these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable parameters. "
1057,2,"As such, this paper is not suitable for publication in a peer reviewed scientific journal of any sort"
1058,1," The experiments performed show that the proposed approach, after\nhyper-parameter tuning, significantly outperforms the epsilon-greedy\nexploration approaches such as DDQN."
1059,1,\n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs.
1060,2,Please correct the language and connet of teh COI part!
1061,1,"\n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference."
1062,2,"I disagree with many aspects of this paper, but sometimes it is best to give the authors as much rope as they want."
1063,1,\n\nThis seems to be a nice treatment of distribution to distribution regression with neural networks.
1064,2,The supportive tone of this review… took some effort. 
1065,2,Didnt like this one
1066,1, \n\nLearning both inverse and forward models is very effective.
1067,2,Despite promising a very general result this manuscript unfortunately does not deliver (almost) any of the things listed in the abstract.
1068,1,. The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features.
1069,1,"""Quality\n\nThis is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging."
1070,1,"  Graph structure is determined by program abstract syntax tree (AST) and next-token edges, as well as variable/function name identity, assignment and other deterministic semantic relations."
1071,1," \n\n+ At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup."
1072,1,\n(6) MC methods can outperform TD methods with more complex and high dimensional perceptual inputs.
1073,2,The team is very experienced. It [the paper] felt a bit less self referential than they often are
1074,1,\n\nPaper Strengths:\n* The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet-BC L = 118 k = 35 e = 3).
1075,2,It is difficult from this reviewers perspective to even call these studies.
1076,1, It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior.
1077,1,"  They also do not report results on unseen objects, when occlusions are present, and on human motion video prediction, unlike the other papers."
1078,2,This needs to be standardized!!! It must be converted!!! Why should we expect some kind of relationship?? It needs to be justified!!!
1079,1," Please explain the difference."""
1080,1, Other AnonReviewer also point out some similar work.
1081,2,It reads like papers often do when they are written in LaTeX. Reject.
1082,2,This manuscript is obviously not suitable for publication in high impact factor journals.
1083,2,Your abstract wouldnt have made me want to read it had I not been a reviewer.
1084,2,Are the 10 random reps chosen at random?
1085,1,.\n\nI enjoyed reading this paper.
1086,2,Are you trying to be funny? Dont.
1087,1,\nExperiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables.
1088,1,"  Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized."
1089,2,I am impatient with this vague assertion.
1090,1,"\u00a0\u00bb \n\n\u00ab\u00a0a predictive state is defined as\u2026 , where\u2026  is a vector of features of future observations and ...  is a vector of\nfeatures of historical observations."
1091,2,The sum total is frustration with what I can only call cavalier treatment of promising material.
1092,2,"Even if other readers found it comprehensible, there is not even a proposed path to make [this model] into a modeling system."
1093,1," On the other hand, although re-weighting methods are\nunbiased, they suffer from the drawbacks of high variance and unknown optimal\nweights."
1094,1, The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising.
1095,2,This paper does not contain information that could make a scientific proposal.
1096,1, I would expect the resulting architecture to perform at least as well as variable action nets.
1097,2,I can see that the manuscript has archival value
1098,2,This was one of the least interesting papers that I have read in quite some time.
1099,1, Movielens comes to mind.
1100,2,I should be happy I dont have to spend time reviewing this dreadful paper; but I'm depressed at such bad science
1101,1, Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset.
1102,1, The paper is well written and the experiments are interesting.
1103,1,". All data are then labelled as \""original\"" or \""transformed by ...(specific transformation)\"". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels."
1104,1,"\ne.\tIt is not completely clear to me that batch-normalization takes care of the scale constant (if so, then why did XNOR-NET needed an additional scale constant?),perhaps this should be further clarified."
1105,1,"\n\n- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?"
1106,2,"presumptuous, ignorant and downright dangerous."
1107,2,"That omission is a standard feature of articles from the fourth author's lab. Although it normally doesn't bite him, it does in this case."
1108,1,"The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work."
1109,2,"ntire review, not an excerpt: I do not trust the data or the underlying thesis."
1110,2,I wasn't sure which problem the author is solving &amp; vice verse it wasn't clear what problem the solution is intended to solve or explorer
1111,1,\n\nThe cherry on the sundae are the experimental results.
1112,1, Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so!
1113,2,This paper does not offer a revolutionary breakthrough
1114,2,"There are numerous problems with this paper, starting with the title."
1115,2,"The first author is a women. She should be in the kitchen, not writing papers' h/"
1116,2,I feel the results are insufficiently counter-intuitive to warrant publication in [fancy journal]
1117,2,The sthanthard of writing is impercable
1118,1, One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance.
1119,1,".\n- Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*.\n- Missing word in first sentence of S4.1: ... reason __ the ..."""
1120,2,"Large parts of the manuscript read now more like a Master thesis than a scientific paper. I hope that the more experienced co-authors - if there are any - can help with this aspect of style. 
"
1121,2,This would seem to constitute the very minimum basic scientific requirement for attempting to publish...
1122,1,"""This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors."
1123,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
1124,2,I have difficulties identifying the aim and added value of the present study to this topic.
1125,1," In particular it demonstrates adversarial training of a recurrent generator for an ICU monitoring multidimensional time series, proposes to evaluate such models by the performance (on real data) of supervised classifiers trained on the synthetic data (\""TSTR\""), and empirically analyzes the privacy implications of training and using such a model."
1126,2,Authors rarely followed the advices
1127,1,"""The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption."
1128,2,It is safe to say that the authors want to see their paper published more than I really want to...
1129,2,"Suggesting much but saying nothing of import, the sort of balderdash that is often compared to the waste of male ruminants."
1130,1,\n\nOther questions and comments:\nThe ablation shows 0.7 improvement on EM with mixed objective.
1131,2,I wish I could explain what is the purpose of the manuscript.
1132,1," If so, the evaluation setting of dividing data into three *random* sets of training, validation, and test, in 5.3 doesn't seem to be the right and most appropriate choice."
1133,1," The sparse coding models see something similar: without sparsity constraints, the image representations do not resemble those seen in V1, but with sparsity, the learned representations match V1 quite well."
1134,1, Is there a way to support this experimentally?
1135,1,\n\n\nPro:\n-\tA novel idea of producing natural adversary examples with a GAN
1136,2,To me the question is uninteresting
1137,2,"Did all 5 authors say,Yes, this is a piece of work I am proud to have my name on?"
1138,1, The paper cites many previous approaches to this but does not compare against any of them.
1139,1," These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017),"
1140,2,I don't see much science in this manuscript.
1141,1, The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.
1142,2,The examples are stale – the method is not exciting. There is nothing much here.
1143,1, the paper leaves something to be desired in quality and clarity.
1144,1,"\nBut I do not catch the details of the user study, e.g., the number of users."
1145,1,"""I find this paper not suitable for ICLR."
1146,1,"  Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise."
1147,1,\n\n- Page 4 p_i\u2019s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly).
1148,1," For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline."
1149,2,The first sentence is unfortunate
1150,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
1151,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
1152,2,They claim on page 9 a significant improvement of their method. This is stupid!
1153,1, Both these models are built off of an existing model on SQuAD \u2013 the Bidirectional Attention Flow (BiDAF) model.
1154,2,..incoherent babble of unsubstantiated overstatement.
1155,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
1156,1," Some of the conclusions could be further clarified with additional experiments (e.g., Sec 3.6 \u2018while the reason that RMS also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the ImageNet dataset\u2019).\n"""
1157,1,\n\nIt's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations.
1158,2,"I am concerned that the survey data for this report were collected in 2005, 15 years ago."
1159,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
1160,1,"\n\nThe idea of explicitly accounting for the boundedness of clinical scores is interesting,"
1161,2,"The authors merely used somewhat
bigger guns than previous studies and generated nothing but more smoke."
1162,1,"  Additionally, this approach allows a significant reduction of training time it seems.\n\n"""
1163,2,The paper was so boring I fell asleep halfway through
1164,1,"\n-\tThe experiments show that essentially, the latent defenders are stronger than the input defender in most cases."
1165,2,"In summary, the conclusions of the manuscript are on one side well known results achieved in a complicated way without demonstrating the advantage of proceeding so, and on the other side unfounded speculations based on simplistic reasoning and irrelevant setup."
1166,2,"I recommend acceptance, provided the editors are willing to stretch the standards for publication a bit h/"
1167,2,"The following discussion seems to ignore this major flaw, which turns mere arm-waving into Olympic-level calisthenics"
1168,2,It would be charitable to call this a comparison of apples and oranges. Its more like steak and...
1169,1,\nI think the evaluation could be improved by using malware URLs that were obtained during a larger time window.
1170,2,This paper is written.
1171,2,The standard of writing (including spelling and grammar) is also satisfactory.
1172,2,"In the interest of being helpful, my suggestion is that the authors go back and review what is involved in the scientific method"
1173,1,"\n\nThe proposed method is novel and the incorporated ideas are quite interesting (e.g., discriminating real from fake random walks, generating random walks from node embeddings and LSTMs)."
1174,1," Addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage."
1175,1,"\nThe paper first proposes an approach based on a formulation of deep networks with (non-parameterized, non-gated) skip connections with an equality constraint that effectively removes the skip connections when satisfied."
1176,2,The presentation is of a standard that I would reject from an undergraduate student
1177,1,It also adopts an interesting multicodebook approach for encoding than binary embeddings.
1178,2,This paper adds nothing to the existing knowledge of the subject 
1179,2,"This was a well-written study that upon my first reading appeared flawless. - R1, who recommended to rejec"
1180,1," Specifically, the idea is to use the structure of the PGM to perform efficient inference."
1181,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
1182,1," \nThe experimental results seem promising, but the presentation can be improved."
1183,2,"I believe that there are important questions in this area, questions that have intellectual merit, but the PI has not found any…"
1184,1, And propose a challenge that addresses these issues and allows controlling different aspects of image variability.
1185,2,I think the N-mixture modeling should be abandoned: it is clear that they [the authors] do not understand this class of models
1186,1," If y* is simply fixed somewhere in the model, then I'm worried that it may cause mode collapse (i.e. the encoder always output similar values), and one possible bad consequence is that y1_i of different sentences in the source domain may have very similar values."
1187,2,There are not enough headings.
1188,2,The discussion is inappropriate and the new content is generally poorly written
1189,1, This is very much different from large part of the cited classic active vision literature.
1190,1," If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited."
1191,2,The way the study is framed here and in the main body comes off as straw-mannish.
1192,1,"\n\n- The form of equation (6) was very nice, and you could also point out the connection with backward Euler for finite-difference methods."
1193,2,This needs to be standardized!!! It must be converted!!! Why should we expect some kind of relationship?? It needs to be justified!!!
1194,2,You have two many misprints
1195,2,The paper failed to make the reviewers more than semi-excited 
1196,1,"\n - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them."
1197,2,It is not hard to develop this method…I could write code for this on a rainy afternoon.
1198,1,"\n\nIt would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix."
1199,2,The English language ranks this manuscript among the top 5 worst manuscripts I have ever reviewed
1200,1," The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here."
1201,2,"I recommend to drop these assertions and predictions, and just characterize the effects. Unfortunately this revised scope is more limited."
1202,2,This is an area ripe for future research. Recommendation: Rejec
1203,1," \n - Fig 2: It's weird that only the +dict (left) model learns to connect \""In\"" and \""where\""."
1204,1,"\n\nAlso, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR. """
1205,1," The term \""posterior value\"" sounds ambiguous"
1206,2,"This is an outline of a paper, and not a fully-thought, well-organized, thoroughly-discussed paper. [..."
1207,1,"\n\nTo sum up, I can not recommend the paper to acceptance,"
1208,2,"The text is overly expansive, desultory, and often diaphanous, so that the raison d'être of an overarching theoretical structure is neither pellucid nor convincing."
1209,1, The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information.
1210,2,(after having cited an author) one could call this name-dropping
1211,2,This left me somewhere between scratching my head and pulling my hair out
1212,1,"\nGood cover of relevant work in sec 3.[[CNT], [CNT], [APC], [MAJ]]\n\nCons\nThe paper emphasis on the fact the their modeling multi-modal time series distributions, which is almost the case for most of the video sequence data."
1213,1," Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST."
1214,1, They obtain state of the art results on most of the datasets.
1215,2,The writing and presentation are so bad that I had to go home early and spend time wondering what life is about.
1216,1, However one of the problems of this paper is clarity.
1217,1," However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions."
1218,2,"Unfortunately, the paper offers little more than vocally arguing. h/"
1219,2,"It is a bit strange for me that authors have used Python for statistical analysis instead of SPSS or MATLAB as usual. Please, explain"
1220,1, To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way.
1221,1,\n\n* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI.
1222,1,"\n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it\u2019s not clear from the text and experiments whether it actually was necessary."
1223,2,An alternative to counting sheep
1224,1,\n\nThe paper has significant writing issues.
1225,2,Figure 1. What does Min in panel c stand for?
1226,1, It uses the image and its attributes.
1227,1,  This is would be worthwhile to appreciate the benefit of the proposed approach.
1228,1,"\n\nConcerning the text, some questions/suggestions:\n- Abstract, line 1: I suppose \""In the Chinese society...\""--- are there many Chinese societies?"
1229,2,"There is too much detail provided which is not relevant to the paper, or any for that matter."
1230,2,I think the audience will eat him alive. But I want to be there to hear it.
1231,2,"this study would have been really interesting 10-15 years ago, but not it seems quite out of date."
1232,1, \n2. The discussion comparing the related work/baseline methods is insightful.
1233,1,\n\nThe paper starts off strong.
1234,1,\n- Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance.
1235,1,\n\n\nMy Comments:\n\nThis paper is a direct application of adversarial learning to the task of reading comprehension.
1236,2,"It appears the authors has generated reports in a hurry, compiled, and presented as an article."
1237,2,"I urge the authors to not publish this article anywhere, as it will impede the progress of scientific understanding."
1238,1,\n* Figure 1 that introduces them contains typos.
1239,1," As a matter of fact, all images shown (including those in the appendix) are blurred versions of the original images, except of one single image: Fig. 4 last row, 2nd image (and that is not commented on)."
1240,2,This manuscript was neither enjoyable nor informative to read
1241,1,"\n\n7. Figure 3 is missing the sub-labels (a), (b), (c), (d)."
1242,1," This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets."
1243,2,I certainly agree [with your point] here but please do not justify it with the selectively chosen and largely incorrect arguments above.
1244,2,"Finally, I have substantial criticisms, in addition to the disappointment of seeing hoary old..."
1245,1,"  It makes main points\n1) ``standard GAN'' is an ill-defined term that may refer to two different learning criteria, with different properties"
1246,2,"Bad, very bad."
1247,2,"The Vygotsky reference is a straight-up drive-by citation, adding nothing except whatever luster he adds to the authors claims."
1248,2,"n a review of a neuroimaging methods paper: By 'sex' you mean gender, and not sexual behavior?"
1249,1,\n\nI expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggested.
1250,2,This paper does not leave me satisfied
1251,1,  The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation.
1252,1,\nCon:\n1. Not entirely convincing that it should work better than already existing methods.
1253,1,"""This is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning, evolutionary computation and reinforcement learning in a novel way."
1254,2,My major concern to accept this work-in-progress paper is that these findings are not super interesting to readers in my opinion.
1255,1," With some filling out, this could be a great paper."""
1256,1, In what we call the Pong Player\u2019s Dilemma (PPD) when an agent \nscores they gain a reward of 1 but the partner receives a reward of \u22122.
1257,2,"The fact that something has not been studies is not, in itself, a reason why it should be studied."
1258,1,"\n\n\nAfter revision:\nSome of my comments were addressed, and some were not."
1259,2,You have put in a lot of effort answering a question that should have never been asked
1260,1," For example, I assume GRU is gated recurrent unit, but this isn't stated."
1261,2,This is a perfect example of the worst kind of research in social psychology
1262,1,Then a lemma by Kalai and Vempala can be used.
1263,1,\n\nThe second observation is much less clear to me.
1264,1," In spirit, these simulations are similar to those in the original paper by M. Egorov."
1265,1," For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI)"
1266,2,Some self citations may be easily taken out without harming the paper -..
1267,2,These snotty kids in quantum information
1268,1,\n\nI thought the little 2-mode MOG was a nice example of the premise of the model.
1269,1," \nNo new method is being proposed, only existing methods are applied directly to the task."
1270,1," \nFor the application of these ideas to spiking neurons including learning please see a recent paper:\nDen\u00e8ve, Sophie, Alireza Alemi, and Ralph Bourdoukan."
1271,2,The reported mean of 7.7 is misleading because it appears that close to half of your participants are scoring below that mean 
1272,1," I feel if comparing L=2 vs L=3 is hard, the authors should be more up front about that in the introduction/abstract"
1273,1," While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure."""
1274,2,"The enormous and overwhelming -not to say pathetic- admiration, even adoration of the author for her subject is barely acceptable"
1275,1, \n3. The experiments are toyish and not convincing.
1276,2,"I really dont like to be harsh in my reviews, but…"
1277,1, The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation.
1278,1," The experimental evaluation is complete and accurate. \n\n"""
1279,2,It would be wholly inappropriate to randomize living people to an intervention.
1280,1," In its current form, I am borderline but leaning towards rejecting this paper."
1281,1,\n\nStrengths:\nThe model and the mixed objective is well-motivated and clearly explained.\nNear state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard).
1282,1,"""The paper addresses the problem of tensor decomposition which is relevant and interesting."
1283,1," The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against."
1284,2,I would advise the authors to go back to the drawing board and consider exactly what this paper is trying to do and do this well
1285,2,If you want to solve a puzzle you could just do sudoku.
1286,1,\n- Skipping behavior can be controlled via an auxiliary loss term
1287,1, The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator.
1288,2,Appears to be on the perimeter of meaningful investigation.
1289,1," Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation."
1290,2,Publishing this manuscript would disembowel the credibility of this journal
1291,1,"\n\nI think this paper does not provide compelling pieces of evidence of unexpected pathological behavior in the previous paper, and also does not provide any insight of how to improve upon and address the obvious limitations of previous work."
1292,2,It is not clear what the author wants to accomplish. - Reviewer 
1293,2,This is not a paper. This is part of … something. It cannot be reviewed and should be rejected right...
1294,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock. -Referee 
1295,2,The manuscript embarrassingly fails in addressing the declared aims.
1296,2,I hope the authors can learn from this exercise on what is expected to craft a publishable paper. -Edito
1297,2,It reads like papers often do when they are written in LaTeX. Reject.
1298,1, but the hierarchical approach seems to have more advantages and seems a more straightforward solution.
1299,2,I want to vomit; I cant believe this paper was submitted.
1300,2,However the paper has several fundamental flaws that give it little or no value as a thoughtful piece of research
1301,1," What most impressed me, however, was the literature review."
1302,1," As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime."
1303,1,\n- The basic observation wrt the behavior of AT is clearly communicated.
1304,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
1305,1,\n- A wide range of experiments are conducted to demonstrate performance of the proposed method.
1306,2,The use of the word surgeries to mean surgical procedures is a linguistic atrocity
1307,1, I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms.
1308,2,"In the course of this paper many stones were thrown but, unfortunately, no birds were harmed."
1309,2,The proposal is also poorly written and unfocused with only brief moments of meritorious thinking.
1310,1," As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution."
1311,1, But I was surprised that the authors didn\u2019t use it in the second experiment (reverse dictionary).
1312,1,"""This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention."
1313,1,  This is a minor modification.
1314,1,"\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed."
1315,2,revise this paper at your own risk 
1316,1,"\n\nThis paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission."
1317,2,This looks like a very early draft
1318,2,Some papers are a pleasure to read. This is not one of them.
1319,1,  The same applies to figure 5. 
1320,1,"\n- figures readability can be improved."""
1321,2,The main problem is that it does not fit the intuition about what should work.
1322,2,is bisexuals referring to the entire sample or those who identify solely as bisexual?
1323,2,STRENGTHS: none.
1324,2,"Written in parts like an experience track paper, minus the experience"
1325,1,"\n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process."
1326,2,NO'  (h/
1327,2,"here is a lot of terminology flung around such as false negatives, false positive and median, first quartile, third quartile"
1328,1,"\n2) though the non-saturating variant (see Eq. 3) of ``standard GAN'' may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN''). "
1329,1, The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting.
1330,2,It seems that the author is simply engaged in proprietary phrase coining – advancing a new term for a well-researched phenomenon
1331,2,The thanking of the Reviewers comments is both a waste of my time and the authors time.
1332,2,DO NOT have your heroes set the villain on FIRE. Its actually a war crime and…makes the heroes look like major arses.
1333,2,"Overall, I think this manuscript is a waste of time."
1334,1," For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper."
1335,2,What is this muck?
1336,2,"Do we need to clarify the meaning of, all?"
1337,1,"\n- when an agent is trained on fixed number of random map, its performance on random unseen maps doesn't increase with the number of training maps beyond ~100 maps. (figure 5)."
1338,2,The chapter is too scholarly and too casual
1339,1,"  If it had better (e.g. ROC curve) evaluation and some modeling novelty, I would rate it higher still."
1340,1,"\n\nI found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective."
1341,1,\n- the results are not remarkable.
1342,2,"I could not find any passage in the MS that would explain to me what is the exact novel idea, proposal, argument, or hypothesis"
1343,1,"At other points the authors default to an chronological narrative that can be useful at times (i.e. page 9), but here seems to distract from their overall narrative."
1344,1," The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input)."
1345,1, The resulting model is illustrated on a few goal-oriented dialog tasks.
1346,1, The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016).
1347,1,"  Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST)."
1348,1,\n- Experiments on three different tasks indicating the potential of the proposed technique.
1349,2,The findings are not novel and the solution induces despair.
1350,2,"There is no research methodology, no data, no model, no significant analysis and no conclusions which arise from the study"
1351,2,"Quite frankly, it seems that the technical language is used more to frighten the reader than facilitate his task"
1352,1,"\n-p14 left side, 4th cell up, \""Cross-AE\""-->\""ARAE\""[[CNT], [CLA-NEG], [CRT], [MIN]]\n\nThis is a very nice paper with a clear idea (regularize discrete autoencoder using a flexible rather than a fixed prior), that makes good sense and is very clearly presented."
1353,1," The authors make a useful comparison between these bounds in Section 3 showing that none is dominating the others, but still analyzing their properties in terms of structural properties of the weight matrices."
1354,1," Given the limited substantive changes to the main text, I did not deeply re-read the text of the paper beyond the introduction."""
1355,1," Also, if any visualization (over the chart) can be provided, that\u2019d be helpful to understand what is going on. \n"""
1356,1,"\n\nThe paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing."
1357,2,The title showed promise...
1358,1, The paper is very well written.
1359,1, The idea deserves a publication.
1360,1,\n\nThe idea seems promising
1361,2,One wonders whether the analysis was an exercise in using a cannon to open an unlocked door
1362,2,This paper could be considered for acceptance given a rewrite of the paper and a change of the title and abstract.
1363,2,"The paper comes with proofs, but – at a first glance – they seem to be more cute than useful."
1364,1,"  \n\nWhile the idea of integrating the spelling error probability into the search for completions is a sound one, the specific details of the model being pursued feel very ad hoc, which diminishes the ultimate impact of these results."
1365,2,The conclusion is something of a shaggy dog.
1366,2,There are so many things wrong with this manuscript that I do not know where to begin
1367,1," I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues."
1368,2,I felt like I was reading a horror movie
1369,2,Lots of hand waving in this Discussion
1370,2,Ive never read anything like it &amp; I do not mean it as a compliment
1371,2,I dont see how your approach has potential to shed light on a question that anyone might have.
1372,2,We regret that some of the remarks made by Referee 1 were not edited before being sent to you. -Editorial Assistant on behalf of Edito
1373,2,It is shocking to read how statistics are being misused just for the sake of being able to write something.
1374,2,Im just bored as a reader.
1375,1," I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper."
1376,1,  \n2. TR seems to lose generalization more gracefully than SGD when batch size is increased.
1377,1,"\"" Why would it overfit more? Simply because training was more stable? Couldn't you prevent overfitting by regularizing more?\n\n* "
1378,1,\n\n A simple nearest neighborhood and a likelihood model is built to assign a picture to an object class
1379,1, It is then claimed that the generated images show that the network has learned good latent representations.
1380,2,The rest of this review operates from the assumption that this paper is a sincere attempt at scientific evidence and argument.
1381,2,The language is so inaccessible that I can't make up my mind whether they're trying to hide something or actually think this is good writing
1382,1," This is nice to know but I think does not cross the acceptance threshold.\n\n"""
1383,2,"The manuscript makes 3 claims: The 1st we've known for years, the 2nd for decades, the 3rd for centuries."
1384,2,Farcical
1385,2,The paper is - and I mean this with no disrespect to the author- a sort of echidna or platypus of a paper.
1386,2,"The paper is definitely exploratory, but probably not of interest to people other than the author."
1387,2,This looks like a very early draft
1388,2,I doubt that its worth publishing a handful of graphs that few would find surprising and that anyone...
1389,1," The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated."
1390,2,This paper is desperate. Please reject it completely and then block the author's email ID so they can't use the online system in the future.
1391,2,Sprinkled here and there are some things that are more or less correct. But it is all very confused.
1392,2,The sentence is not only overly complex but is a string of theory-jargon that purports to be a dog but is rather a dog skeleton with half the bones missing h/
1393,1, \nAs a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive.
1394,2,"Authors wanted to make title catchy but actually finished trivial title, a la Daily mail"
1395,2,"Overally speaking, the manuscript is well written."
1396,1, The proposed approach seems to be of interest and to produce interesting results.
1397,2,They show they can account for 20% of the variance. No wonder. The usually accepted level is 50% to...
1398,1," The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions."
1399,1," but they do show some value for the technique in the area of domain-specific coreference.\n\n"""
1400,1,\n\nSome of the results are quite entertaining indeed.
1401,2,This is depressing. So much work with so little science
1402,1," So it is not clear if the proposed algorithm is better for training the classifier, which may be of more interest."
1403,1," However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau."
1404,2,"I nearly said reject, but then I recalled that I have a hangover and am feeling grumpy"
1405,2,This is (Im sorry) utter nonsense.
1406,1, So I think that the motivation behind introducing this specific difference should be clear.
1407,2,"This is such a promising topic, but I was very disappointed in this paper combining a substantial amount of author talent."
1408,2,"An exercise in feature manipulation, of the brainless kind"
1409,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock. -Referee 
1410,1,"\n\nMinor points:\n- Typo in Eq 10\n- Typo on page 6 (/cite instead of \\cite)"""
1411,2,This paper is written.
1412,1,"\n\nSpecifically, the authors say: \""In our experiments, we use the result of \nminimising the variable corresponding to the output of the network, subject \nto the constraints of the linear approximation introduced by Ehlers (2017a)\""\nwhich sounds a bit like using linear programming relaxations, which is what\nthe approaches using branch and bound cited above use."
1413,1," Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words."
1414,2,The authors last name sounds Spanish. I didn't read the manuscript because I'm sure it's full of bad English
1415,1,"""This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization."
1416,2,"hD project paper submitted one year after successful defense: 

Student should get a new PhD project"
1417,2,No respectable biochemist would be seen in the same county as these data.
1418,2,"hile it doesnt make for a flashy title (which the authors like more than British tabloids do), there is an alternative explanation"
1419,2,The author should abandon the premise that his work can be considered research.
1420,2,The paper is ill-informed and poorly argued. It is not suitable in my view for this or any other...
1421,2,"In a revised form, it would not look out of place in a scholarly journal."
1422,2,"It would take a great deal of time to sort this out. From what little I can glean from the scientific nature of this submission, addressing the comments listed above will not yield a submission suitable for XXXX"
1423,1, -- finding a more efficient search path would be an important next step.
1424,2,My argument against it as a full paper is that it makes an incremental contribution to the state of the the science.
1425,2,The Discussion section of the paper is neither informative nor enlightening and is certainly theoretically questionable
1426,2,"The data as presented is not very convincing, even to a believer."
1427,1," The answers to the critiques referenced in the \n paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience."
1428,1,   Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations.
1429,2,eview of a PhD project paper submitted one year after successful defense: Student should get a new PhD project
1430,2,"The author does not exhibit adequate acquaintance with the subject, the scholarship on it, the structure of logical argument, or English."
1431,2,I am personally offended that the authors believed that this study had a reasonable chance of being accepted to a serious scientific journal.
1432,2,It is more of a blog post than a research article
1433,1,Is it possible that a different choice of hyperparameters can change the model ranking
1434,2,"This piece offers nothing new…is poorly written, analytically weak and repeatedly inaccurate. h/"
1435,1," If Lipton's method would be expensive to apply to large-scale\nsettings such as the Atari games, the authors could also compare with that\nmethod in smaller and simpler problems."
1436,1,"""I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution."
1437,2,They show they can account for 20% of the variance. No wonder. The usually accepted level is 50% to be useful
1438,1, And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).\
1439,2,Such a prestigious journal can surely make better use of its limited space.
1440,2,I can't possibly imagine what led the authors to believe that their paper was remotely interesting enough to submit for publication
1441,1, This is not a professional ML paper looks like.
1442,1,  However there are several weaknesses to the paper (or maybe just things I didn\u2019t understand).
1443,2,"The paper is grossly over referenced, and reads a little like a student trying to impress a supervisor that a lot has been read"
1444,1,\n- I would not call Reg a regularization term since it is not shrinking the coefficients
1445,1,"  In the former case, the statement \u201cx is uniformly sampled from X\u201d does not make sense because X is practically infinite."
1446,1," Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al."
1447,2,The writing and presentation are so bad that I had to go home early and spend time wondering what life is about
1448,2,This work is stuck in the past. The referee would rather talk about the future - some of the senior co-authors were the future once!
1449,2,Find your inner nerd—it must be a big part of you—bind and gap it and then dump it in the ocean tied to a large rock.
1450,2,all I can say is that the author is blissfully unaware of what a standard is – in linguistic terms – and does not have the linguistic competence to describe it
1451,1, I am raising the score.
1452,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
1453,2,The paper suffers from its desire to be accessible and directly impactful
1454,1," The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on."
1455,1," In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations)."
1456,1, This can be improved to help readers understand better.
1457,2,"The authors presents a flurry of statistics, but they do not explain why or how those are relevant to the study"
1458,1,\nThere is not much that's technically new in the paper-- at least not much that's really understandable.
1459,1," \n\nFour simple, shallow neural nets are built as query, write, value, and policy networks."
1460,1,  A minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to one.
1461,1," But I don't find it justified anywhere why \""leave 99.7% of i, j pairs unpenalized\"" is sth."
1462,2,"The color rainbow is pretty, but largely useless."
1463,1,"\n\nFor this reason, I will give the score marginally below the acceptance threshold now."
1464,1,\n- The other experiments are lacking important details.
1465,1,"  \n\nCons: \nReferences to \""CaffeNet\""  and \""LeNet\"" (even though the latter is well-known) are missing."
1466,2,There are so many things wrong with this manuscript that I do not know where to begin
1467,2,To justify these conclusions and spur the authors onto the better things they are undoubtedly capable of I append some details.
1468,1," I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained."
1469,1,?\n* The term A(Z) in the objective function can be more clearly described
1470,2,This section gives the impression that you'll throw a handful of darts at a target and see what you happen to hit
1471,2,This literature review is nothing more than a merry dance around the books
1472,2,The investigator is in the top 50% of his field
1473,1,. Novel combinations of GAN formulations (non-saturating with\ngradient penalties) are evaluated to disentangle the effects of formulation\nchanges.
1474,1,\n+ The approach is a novel twist on an existing method for learning from noisy data.
1475,1, I think a language modeling benchmark and/or a larger scale question answering dataset should be considered.
1476,2,I do not have the background to assess the accuracy or to detect errors in the equations but I do not agree with this
1477,2,This is a sin of omission!
1478,2,"Starting from the title, the paper is a complete nonsense. […] . The paper is a long, verbose and unnecessary description of obvious or well-known stuff. […]. For the reasons above, rejecting the manuscript is a moral obligation."
1479,2,A failing course paper written by an undergrad
1480,2,Figure 6. This figure is silly.
1481,1," However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English."
1482,2,Was the white noise random?
1483,2,"So the paper, which I was initially excited to read, ended up just making me mad."
1484,2,I found every single reading of every theorist mentioned in this article seriously wanting
1485,1,"\n\nRelated works:\n- For your consideration: is multi-task survival analysis effectively a competing risks model, except that these models also estimate risk after the first competing event (i.e. in a competing risks model the rates for other events simply go to 0 or near-zero)? Please discuss."
1486,1, I would suggest rewording title/abstract/intro
1487,1,\n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without.
1488,2,There is so much that is wrong with this paper that it is difficult to know where to start
1489,1, The latter is not recommended since the authors use double quotes in the abstract to express that the exploding gradient problem is not solved.
1490,1," \n* In the text, figure 4 (which is in the supplement) is referenced before figure 3 (which is in the text)."
1491,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
1492,2,"Im sorry, this topic is just not very interesting."
1493,2,The author should abandon the premise that his work can be considered research
1494,2,I have rarely read a more blown-up and annoying paper in the last couple of years than this hot-air balloon manuscript
1495,2,I really want to like this study. ? Crap. It's me. I'm stopping you
1496,1," For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks."
1497,2,The work that this group does is a disgrace to science
1498,1," While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods."
1499,1,\nYou should clearly divide the existing study and your work.
1500,2,The statistical analyses were not correct. Actually they were so confused that I lost all confidence in the analyses and data presentation
1501,2,"The fact that the question of this paper has never been asked should, on balance, count against the paper."
1502,2,"There is potentially an interesting essay to be written about [x], but this one isnt it"
1503,1,"\n- In table 2, it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm (0.3%) is \u201cgood enough\u201d."
1504,2,My first concern is that I dont get it.
1505,2,I also thing that the English of the manuscript need further polishing.
1506,2,"You aimed for the bare minimum, and missed!"
1507,1," The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that \u201cDQNs  are susceptible to periodically repeating mistakes\u201d."
1508,1,\n\nThe experimental study is extensive.
1509,1, I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper.
1510,1," \n\nThe authors do mention experiments on page 8, but confess that some of the results are somewhat underwhelming."
1511,2,"Moreover, it is very difficult to see the actual contribution this manuscript will have"
1512,2,"I have 3 main objections to this paper: it is self-contradictory, it's functionally obsolete, and it's been submitted to the wrong journal"
1513,2,nless the authors performed some pagan ritual before euthanizing the animals I would use killed (or euthanized) instead of sacrificed
1514,2,My greatest criticism of the paper is the tendency of the authors to make an argument and then immediately contradict themselves
1515,2,I recommend the publication even if I am not impressed
1516,1,\n* The motivation of using feature histograms as embedding is not clear
1517,2,Nobody in their right mind would ever suggest such a model
1518,1, Evaluation metrics include repeated latency to the goal and comparison to the shortest route.
1519,1,\n\nPaper Strengths:\n- An incremental yet interesting advance in geometric CNNs.
1520,1," Would be interesting to see the time/accuracy frontier."""
1521,2,"Not now, not ever."
1522,1," I would suggest to either really add multi-hidden-layer results (which is not really doable in a conference revision), or state multi-layer work as outlook."
1523,1," I would recommend at least summarizing the main findings of Appendix A in the main text.[[CNT], [null], [SUG], [MIN]]\n\n* A relevant missing citation: Turner and Sahani\u2019s \u201cTwo problems with variational expectation maximisation for time-series models\u201d (http://www.gatsby.ucl.ac.uk/~maneesh/papers/turner-sahani-2010-ildn.pdf)."
1524,1," For this reason, I vote for a borderline accept."
1525,2,The authors should discard the data and collect it again properly.
1526,2,This study does not go much deeper than a sunday morning kitchen table calculation
1527,2,As a service to the authors I have decided to try to convey a sense of the extreme nature of the problems encumbering this submission
1528,1,  It was not clear from this presentation how the human participants were rewarded for their performance.
1529,2,Uninteresting. Unpublishable. Reject.
1530,1,".\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens."
1531,1,\n\nThe filters by themselves seem trivial and as such do not offer much novelty.
1532,2,I recommend the publication even if I am not impressed
1533,1,"""This paper presents an image-to-image cross domain translation framework based on generative adversarial networks."
1534,2,"This is a heretofore unobserved result and is very interesting, but is not novel."
1535,2,"Words are used inappropriately—I count, for example, 13 instances of 'unique', but it is used correctly only once."
1536,1,"\n- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. \""To re (label), or not to re (label).\"" HCOMP 2014."
1537,1," This has already been successfully applied in multiple domains eg. in computer vision (Krizhevsky et al, NIPS 2011), NLP (Bahdanau et al 2014), image retrieval (Krizhevsky et al. ESANN 2011) etc, and also studied comprehensively in autoencoding literature."
1538,2,"The biggest problem with this manuscript, which has nearly sucked the will to live out of me, is the terrible writing style"
1539,1, It would be good to justify (empirically) the proposed reward function.
1540,1," Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \""sub-human\"" games you might hope.)"
1541,1," Aside\nfrom the WGAN critic needing to be optimized for more steps, couldn't the\nremaining components be trained jointly, with a weighted sum of terms for the\nencoder?"
1542,1, \n\nCons: \n- All experiments use simulated workers; this is probably common but still not very convincing.
1543,1,"\n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive."
1544,1,\n\nPros:\n1. The required time for architecture searching is significantly reduced.
1545,1,  It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values.
1546,1, \n\nIf the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that.
1547,2,Your model is a black hole from which no light escapes
1548,1,"""The authors propose using piecewise linear activation functions with contraints to make it continous."
1549,2,"Also, the manuscript is very tedious and reads like an unedited thesis chapter."
1550,2,"Line 156-160; this is the only correct, sensible and interesting finding of the paper."
1551,2,The experimental design is a bit funny
1552,1,\n\nPros/cons\nPros\n-Adresses an important problem in representation learning
1553,1, Rest of presented work is more or less standard.
1554,2,Studies undertaken in such a manner as presented here degrade all science by giving the semblance of legitimacy to illegitimate work.
1555,2,"I have read this paper several times through, and I have nothing to say in its defense."
1556,1," Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used."
1557,2,"For a section on thought, very little seems to have gone into it."
1558,1, The presented algorithm sketch-rnn seems novel and significantly different from prior work.
1559,1,"  Unsurprisingly (since the EEN noise does not seem to be conditioned on the input), the baseline deterministic model performs quite well. "
1560,1, Similar experiments should be conducted on switchboard and wsj because librespeech is several times larger than switchboard and wsj.
1561,1," \n\nSince a sequence of similar linear systems have to be solved could a preconditioner be gradually be solved and updated from previous iterations, using for example a BFGS approximation of the Hessian or other similar technique."
1562,1," Also, to be fair when discussion the results, the authors should say that simple concatenation outperforms the single sensor paradigm."
1563,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
1564,2,the sthanthard of writing is impercable
1565,1, The reported tables seem to ignore a lot of the relevant information
1566,1," The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement)."
1567,2,To Review of the article was a challenge which I accepted. In fact I regret this decision
1568,2,"This will never work: Negative reviews of famous, ground-breaking papers...."
1569,2,The author is tilting at windmills.
1570,1,"\n\nIn figure 4a, x-axis should be \""number of landmarks\""."
1571,1," Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation."
1572,2,You need to learn how to think inside the box and stop smoking whatever it is you're smoking
1573,1,\n\nThe unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks.
1574,1," \n- I do not understand the purpose of \""input injection\"" nor where it is used in the paper. "
1575,2,The article could benefit from a good linguistic editing in order for it to be better sound and...
1576,2,I recommend the publication even if I am not impressed
1577,2,"So, I guess the [XX] theory was dead on arrival when it was proposed."
1578,1, Many equations were unclear to me for similar reasons to the point I decided to only skim those parts.
1579,2,"leftover is a noun, like old pizza. left over is the verb you want."
1580,1,"\n\u2013 The claim \u201cthere is no need to use more powerful and complex classifier anymore\u201d is unsubstantiated, as the paper\u2019s approach still entails using a complex classifier (a FFNN) to learn an optimal intermediate representation."
1581,1,\nThere are also quite a few misspellings. 
1582,2,"This is a potentially interesting problem. Yet, not all potentially interesting problems are useful, such as this one"
1583,1,"  For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here."
1584,2,Intellectually bankrupt
1585,2,"So, what is the point of this?"
1586,1,\n\nThe novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance.
1587,2,"This, of course, is disingenuous if not unethical"
1588,2,Well written. (entire review
1589,2,I do not believe or trust the data presented or the underlying thesis.
1590,2,You need a comma here. Do they not have commas at your institution or do they just cost a lot?
1591,2,"While the authors do pick a good problem, thats where the quality of the paper ends for me."
1592,1," The shown samples from model looks extremely, low quality and really hard to see the authors interpretations of it."
1593,1,\n-Which epsilon did you use for evaluation of DDQN in the experiments?
1594,1," \n- Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices. """
1595,2,This work is only a maths game.
1596,2,To improve this you need to be more robust on all fronts etc.
1597,1, It may also be interesting to consider class-specific representations that are more general than just the class label.
1598,1," \nSince the system is presented without any comparisons to alternatives for any of the individual components, it doesn't really shed any light on the significance of the various modeling decisions that were made."
1599,2,"I have 3 main objections to this paper: it is self-contradictory, it's functionally obsolete, and it's been submitted to the wrong journal"
1600,2,"This is an interesting manuscript, not because of its results, but because of its complete ignorance of due scientific process"
1601,1," The authors furthermore introduce a simplification of the setting, i.e. that nothing changes in a scene during saccadic exploration, which is rather unusual for active vision problems. "
1602,1," The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g. equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral."
1603,2,There are not enough headings.
1604,2,This paper contains neither theory nor research.
1605,1," The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example."
1606,1," It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix."
1607,2,I would very much have liked to read the article promised in the abstract.
1608,2,The author has re-invented lukewarm water. This is all naive philologising.
1609,1," For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks?"
1610,1,"  Some of the notation is confusing here \u2014 for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case."
1611,2,"Publication of this paper will not advance our knowledge in any shape of form, it will just result in other researchers pointing out how bad this study actually is"
1612,1,\n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem.
1613,1,\n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written.
1614,1," The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory."
1615,1,"\n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights"
1616,1,"\n\nThere are many good ideas and experiments in this paper and I would strongly encourage the authors to resubmit this work to a future conference, making sure to reorganize the paper to adhere to the relevant formatting guidelines."""
1617,1, The paper can be understood with no problem.
1618,1,"\n\n3. In section 3.1, when presenting random variables (U_1, ..., U_d), I find the word \""Bernourlli\"" a bit misleading because typically people would expect U_i to take values from {0, 1} whereas here you assume {-1, +1}."
1619,2,"For a section on thought, very little seems to have gone into it."
1620,2,The phrases I have so far avoided using in this review are 'lipstick on a pig' and 'bullshit baffles brains'
1621,2,It is clear that this manuscript will not win a beauty contest.
1622,1,"\nIn conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier,"
1623,1,"""The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing."
1624,2,This technique will never work.
1625,2,Proposition 7 was so fundamentally wrong that I saw no point in reading beyond it
1626,1, The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described).
1627,1," \n\nOverall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders."
1628,2,Something is missing.
1629,2,"Through streamlining, more meaningful references and restriction to the essentials, the manuscript may still be saved"
1630,2,This work amounts to a form of methodological perfectionism. Perfectionism can be the enemy of the possible in science.
1631,1," What is missing from this picture is when a blind walk cannot reach such a state, e.g. when a narrow corridor is present in the environment."
1632,1," The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art."
1633,1,\n - Why does ROUGE-L seem to work better than ROUGE-1 or ROUGE-2 as the reward?
1634,2,"The following discussion seems to ignore this major flaw, which turns mere arm-waving into Olympic-level calisthenics"
1635,1,".\n\nThe experimental part is satisfactory, and seems to be done in a decent manner."
1636,1, This is an important and useful problem in robotics and other\napplications.
1637,1,"""This paper proposes a method to learn a control policy from both interactions with an environment and demonstrations."
1638,1,"I therefore cannot recommend this paper for publication."""
1639,1," \n\n* It would also be good to cite the following related work, which closely ties into the model of Eslami 2016, and is prior work: \n\nEfficient inference in occlusion-aware generative models of images,\nJonathan Huang, Kevin Murphy."
1640,1,"""The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks."
1641,2,In the experimental part there is a lack of scientific by taking over results from reference literature.
1642,1,\n\nEvaluation: \nSignificance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value.
1643,2,[The methods section] reads more as if these explanations are put in to guide the authors themselves
1644,2,It reads as if the author were giving a lecture and wandered off point to tell an interesting story.
1645,2,The paper is neither exciting nor harmful.
1646,1,"""In recent years there have been many notable successes in deep reinforcement learning."
1647,1," \n- Despite the author\u2019s expectations that their representations will be \u2018widely used\u2019, I am struggling to think of cases where they would be useful, outside of the very specific tasks involving prepositions that they use."
1648,2,"The authors merely used somewhat
bigger guns than previous studies and generated nothing but more smoke."
1649,2,This paper may sink without trace
1650,1," Given that I don't even\nthink the representation of inputs and outputs is practical in general, I don't see what the \ncontribution is here."
1651,1,\n* the paper is clearly written and easy to understand
1652,1," Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community."
1653,1,\n\npro:\n- Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work\n- Easy to read and follow
1654,1," The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task."
1655,2,Did you have a seizure while writing this sentence? Because I feel like I had one while reading it.
1656,2,A different paper would be a good paper
1657,2,The candidate has not demonstrated to me the required knowledge in any of the starred areas
1658,2,"Written in parts like an experience track paper, minus the experience"
1659,1,"""The paper seems to be significant since it integrates PGM inference with deep models."
1660,1, This would enable a better test of the generalization capabilities in what is essentially a continuously changing environment.
1661,1," Also, cluster-to-cluster might not fit well."
1662,2,The work that this group does is a disgrace to science
1663,2,I just dont get the point of this
1664,1,"\n\nIn summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation."
1665,1, Do these correspond with some known models?
1666,1,"  The paper presents significant, novel work in a straightforward, clear and engaging way."
1667,2,Table 5 is a beast. I have no idea what it is trying to say because it terrifies me
1668,2,"This is like a project notebook. When Michelangelo finished the sistine chapel, did he also try to sell the scaffolding as a work of art?"
1669,2,The writer of the manuscript is utterly ridiculous and appears to believe they will solve poverty through radio astronomy
1670,1," Even assuming this is a meaningful task, surely the natural baseline would be to treat these phrasal verbs as non-compositional (e.g. extend the vocab with words like \u201csparked_off\u201d)  and train Word2Vec."
1671,1," This paper also proposes adaptive dropconnect, which extends dropconnect (Wan et al., 2013) by using an adaptive algorithm for masking network topology."
1672,1,"\\n\n\nOverall, I like the paper, I like the algorithm and I think it is a valuable contribution."
1673,2,You have put in a lot of effort answering a question that should have never been asked -..
1674,1, The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful.
1675,2,My recommendation for the authors is therefore to shelve the manuscript
1676,1," In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network."
1677,1," As far I understand, all of the original work policy gradients involved stochastic policies."
1678,1," Nevertheless, it is well written and I think it is solid work with reasonable convincing experiments and good results."
1679,1," Command of related work is ok,"
1680,1, While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement.
1681,2,Startlingly naive and jejeune. Obviously a poorly tailored master's thesis.
1682,1," Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly."
1683,1," Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. "
1684,1," While the motivation of the paper makes sense, the model is not properly justified, and I learned very little after reading the paper."
1685,1," If not, then C is not proportional the identity matrix, as claimed in section 5.3."
1686,1," For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion."
1687,1,"  the second part was not clear to me, and the last part does not seem very useful."
1688,2,(although I admit that here is a possibility they might turn out to be correct in that they are guessing right)
1689,2,What were you thinking?
1690,1,"  Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data."
1691,1, Extensive experiments are performed to demonstrate the effectiveness of the proposed methods. 
1692,1,\n\n\n - The paper proposes a (I believe) novel method to obtain visual explanations.
1693,2,This paper is very weak
1694,1," For example, you could imagine that in a morphologically rich language, this method would work well to learn the representation of certain morphemes such as case endings or verbal conjugation."""
1695,2,They arbitrarily rule out models with interactions but without corresponding main effects. Etc.
1696,2,Its complicated to understand what the objective of the study even is
1697,2,"While I wont advocate strongly for its publication, I also would not object to its publication in the journal"
1698,1," While the authors list 7 contributions, the current version of the text is a challenge to read and makes it challenging to distill an overarching theme or narrative to these contributions."
1699,1," Also, do the simulated speedup results in the appendix account for potentially stopping a new best configuration, or do they simply count how much computational time is saved, without looking at performance? The latter would of course be extremely misleading and should be fixed."
1700,1,\n- Intensive experiments to validate the performance.
1701,1," \n\nThe studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone-based learning."
1702,1, The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work.
1703,1," But, it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment."
1704,1,"\n\nNegatives: \n- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation)."
1705,2,This paper is baffling
1706,2,"Publishing a weak paper can be a bit like kissing someone through a screen door - it might feel good for a moment, but what does it actually accomplish?"
1707,2,The paper raises the suspicion that the author has not been trained as a historian
1708,2,At least I say it to your face and sign my name
1709,2,"Concerning the discussion, again the merits of the work are downplayed to a point that its almost..."
1710,1," If these are the generated images, then some reconstruction is done by the network, fine, but also not unsurprising as the network was told to do so by the used objective function."
1711,1, Low-Rank Doubly Stochastic Matrix Decomposition for Cluster Analysis.
1712,2,Intermediary steps and the apologetics for [topic x] derived from an ahistorical cult and its author.
1713,2,"Limited scholarship, flawed design, and faulty logic engender no enthusiasm whatsoever"
1714,1," The reconstruction of unseen images is claimed central but as far as I could see, Figures 2, 3, and 4 are not even referred to in the text, nor is there any objective measure discussed."
1715,1,"\n\n[4] A Maas, Z Xie, D Jurafsky, A Ng, Lexicon-Free Conversational Speech Recognition with Neural Networks, 2015"
1716,2,"I cannot imagine any physician or scientist who has a job, who will have the time to read this paper."
1717,2,"Overall, I dont quite get what the authors think theyve accomplished."
1718,2,"The accompanying simulation movie is neither interesting, nor particularly beautiful."
1719,2,"Thats not possible to do without mind-reading, and theres nothing in the Method section about mind-reading methods"
1720,2,"Since the paper is mathematically empty &amp; provides no new ideas or findings, I see no reason to publish it"
1721,2,"The title of the paper indicates that the study was an RCT, but was it?"
1722,2,"The research claims these phenomena are understudied, as though there is some amount of study they should endure."
1723,1," Figure 4 could have answered this question, however it is not clear from the paper whether the CP-ALS procedure was followed by fine-tuning or not."
1724,1,\n\nMinor points: Fig.1 conveys not that much information.
1725,2,So many electrons worked so very very hard on this paper
1726,1, Were these values different?
1727,1,\n\nCons:\nLacking in theoretical analysis or significant experimental results.
1728,1,"""The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the \""fourier feature\"" corresponding to the kernel at a set of randomly sampled quadrature points."
1729,1, It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy.
1730,2,"Most part of methodology is useless, most of paragraphs are inrelevant to the main topics"
1731,1,"""Make SVM great again with Siamese kernel for few-shot learning "
1732,1,. How does the greedy step affect training and decoding?
1733,2,"There is no novelty in the work, the approach or the results and the implications in this context are quite possibly irrelevant."
1734,1,\n\n- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc?
1735,1,"\n\nAll of which is true as far as it goes, but I think it\u2019s a bit of a distraction."
1736,2,The map on the article is entirely unscientific. The data itself is also very dubious.
1737,1,\n\nPros:\n(+) The paper is well written and the method is well explained
1738,1,"\n\nThe main technical difference of the present work compared from the  main prior work (Vendrov, 2015) is that in addition to mean vector representation they use here also a variance component."
1739,2,"The authors are permitted to believe what they want to, but the data did not support important implications."
1740,2,"The paper is badly written, and poorly organized. In its current form, the paper cannot be accepted. The paper is poorly written or poorly thought-out. I think the paper is poorly written and most of its statements are wrongly and poorly motivated."
1741,1,\n\npros:\n(1) The idea is introduced clearly and rather straightforward.
1742,2,Someone has been foraging in theory and has managed to learn how to mangle simple concepts and hide them behind pretentious empty prose
1743,1," Moreover, I would be very curious about ways to better integrate causality and generative models, that don\u2019t focus only on the label space."
1744,2,"I would suggest activating the spellchecker on Word, or keeping the cat from walking on your keyboard"
1745,2,"Cite newer, relevant references, especially those published by X 2012, and X 2008. Best wishes, Dr. X, Associate Editor"
1746,2,The scientific contribution of this paper - if there is any at all - is at best hopelessly insignificant.
1747,1,"\n\nThroughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear."
1748,1, Also some at least quantifiable (if not benchmarked) outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sections.
1749,1, The behaviors of linear networks and practical (deep and nonlinear) networks are very different.
1750,1, \n\nThe experiments are complete and the writing is good.
1751,2,My first concern is that I dont get it. - (via shitmyreviewerssay
1752,2,"The Gettysburg Address was only 272 words, and was one of the most powerful speeches in history. This paper, on the other hand, is over 8,000 words and says absolutely nothing at all."
1753,1, This latter technique is similar to randomized coordinate descent.
1754,2,"The research adds meaningful to the literature, how?"
1755,1,"\nOther comments and remarks:\nThe meaning of the following sentence is not clear, it probably should be rephrased: \u201cWe observed that if the network is trained in the restored dense form, the training result can be more stable because of its smoother convex."
1756,1," \n\nThe paper is written clearly and the English is fine."""
1757,1, But I can not at all give a good evaluation given the current experimental results (unless substantial new evidence which make me evaluate these results differently is provided in a discussion).
1758,1,\n- Figure 4 is impossible to read in print.
1759,2,So in summary: the paper is oblique to the entire current literature and it fails to relate to relevant investigations
1760,2,It is clear that the author has read way too much and understood way too little.
1761,2,This sort of presentation issue continued through the first few sections (after which I was reading with decreasing attention
1762,2,Was this an undergraduate class assignment?
1763,1," \n\nThis paper has a lot of content, but not all of it appears to be relevant to the authors\u2019 central points."
1764,2,Is this a joke?
1765,2,The english languish should be improved
1766,2,"Looking at the general poor quality of the paper, Im surprised by the list of the co-authors"
1767,2,It was agonizing for this reviewer to read a total of nine pages describing the overall methods.
1768,2,"Is Tartarstan a magical land where they make the worlds tartar sauce? If not, I can only assume the author is referring to Tatarstan"
1769,1, \n- I did not see Table 1 referenced in the text.
1770,2,"I'm not inclined to suggest acceptance because I haven't enough elements to do so.

Significance: 7/10
Soundness: 8/10
Presentation: 9/1"
1771,1, but perhaps a bit preliminary.
1772,1," \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting."
1773,2,This paper goes to significant amounts of trouble to accomplish something that is already well established but in a less elegant manner
1774,2,Asterisk rather than Asterix
1775,2,This paper has the same relevance as a paper in astronomy which places the earth as the centre of the universe
1776,1," A more complete reference is \""handbook of weighted automata\"" by Droste."
1777,2,rampant and unsupported speculation
1778,1," \n\nThe contribution in terms of the theory of using random features to perform kernel ridge regression is novel, and interesting."
1779,1,"""This is a well written paper on a compelling topic: how to train \""an automated teacher\"" to use intuitive strategies  that would also apply to humans."
1780,2,The paper is presented as a rather undigestible and tortuous collection of disparate results
1781,2,he authors report results from pages 16-26. This section reflects what I would brutally call 'death by figures
1782,2,The rest of this review operates from the assumption that this paper is a sincere attempt at scientific evidence and argument.
1783,2,This was one of the least interesting papers that I have read in quite some time.
1784,1,  Cons:   A very natural and simple solution that is fairly obvious.
1785,2,Future work: The authors personal research agenda is irrelevant here.
1786,1," In  section 3, the shifted version, \\delta, is abruptly proposed only based on \""results presented in 4.1\"" could improve learning."
1787,1, \n\nPros:\n+ The results are very pleasing visually.
1788,2,"First, unless my statistics is failing me, a less than 1.0 SD is not significant."
1789,2,"If the paper is accepted, I strongly recommend an English prof-reading."
1790,2,Simply conducting the same analyses in a different data set does not equate novelty or impact to the...
1791,1,"\n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,"
1792,1," Based on the review taxonomy, the authors presents a mixed objective which aims for bretter clustering performance."
1793,2,"Ugh. Read a book. This is grossly oversimplified, and not an appropriate statement for a scientific..."
1794,1, Experiments on German/English and Chinese/English show gains over other reinforcement learning methods.
1795,1," And it is concluded in the final three sentences of the paper that the presented network \""can infer effective latent representations for images of other objects\"" (i.e., of objects that have not been used for training); and further, that \""in this regards, the network is better than most existing algorithms [...]\""."
1796,1,\n\n*Clarity*\nThe paper is in general well written and easy to understand.
1797,2,N/A. (Full review text)
1798,2,editor] I will make a final determination without ... review (which will assuredly be positive unless you go all Trump on someone).
1799,2,"Other papers are cited that were clearly not read carefully, resulting in some memorable howlers."
1800,1, The authors show that UA results in gains on several of the games.
1801,2,"Although no ground-shaking breakthroughs are made, it is worthy of publication"
1802,1, Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning.
1803,1,"\n\nSo, the paper is relevant and well presented."
1804,1,\n2. It provides useful insights of model behaviors which are attractive to a large group of people in the community.
1805,2,"I read it again, this time squashed between two large people on the delayed flight home, and still enjoyed reading it"
1806,2,The regression analysis is rubbish. Let's see what happens when you do this properly.
1807,1," It is not surprising that algorithms that take this into account do better, as your examples and experiments illustrate."
1808,1, The model is trained on an empirical distribution whose points are sampled from the true distribution.
1809,1,\n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability.
1810,1, \nThe authors use deep Q learning from Mnih et al 2015 to learn their optimal policy.
1811,2,"There may be a good paper crying to be let out of this manuscript but if so, it is hard to know."
1812,1,"\n\nFirst of all, the paper is very well written and structured."
1813,2,"It is not clear what percentage of the eligible people are not applying due to being lazy, inertia, or principles, which makes it hard to gauge the size of the potential market. [The people that the reviewer is referring to are immigrants"
1814,1, \n\nReview summary:\n\nI think this paper is interesting.
1815,1, The idea is interesting and novel that PACT has not been applied to compressing networks in the past.
1816,2,This paper is fluently written and meticulously researched. I do not recommend it for publication.
1817,2,"The so-called XX test is incompetent, irrelevant, immaterial and without any foundation whatsoever in the established literature"
1818,1,"\n\nThe motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \\x, then one encounters the curse of dimensionality as one obtains finer and finer discretization."
1819,1,"\n\n- Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful."
1820,2,I have received one highly negative review and wondered whether it could be overcome by other reviews. I conclude that the answer is no
1821,1, While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.
1822,1," In Paragraph \u201cMaximum Likelihood\u201d, page 2, the formalization of the studied problem is unclear."
1823,2,"If the author is comfortable having his/her name on this paper, then I won't stand in the way of its publication"
1824,2,This code sample cannot be adequately described without the use of strong language.
1825,1,"\n-\tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best."
1826,1,\n- Distilling may hurt performance ( Figure 2.d)
1827,1,"\n\nThere is addition experimental data reported which I didn't find very conclusive nor relevant to the analysis, particularly the attention heat map and the effect of apples and texture."
1828,1, This opens nice perpectives for better and faster inference.
1829,2,The supportive tone of this review… took some effort.
1830,1,g \n-\tThe method enables creation of adversarial examples for block box classifiers
1831,1,"""[ =========================== REVISION ===============================================================]\nI am satisfied with the answers to my questions. "
1832,1," However, differentiating TreeQN also amounts to back-propagating through a \""single\"" trajectory in the tree that gives the maximum Q-value. "
1833,1,"\n\nThe paper concludes with \""Overall the complex-valued neural networks do not perform as well as expected"
1834,2,There are many stylistics phrases to tidy which you can find yourself
1835,2,I am sympathetic to what the author is trying to do here - Reviewer 
1836,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
1837,2,[H] did a few sloppy experiments and attached his name to the great pioneering work by [P]. He should not be mentioned.
1838,1,"  In term of impact, this is a theoretical paper looking at optimizing a sandbox problem where the results may be one day applicable to the real autonomous driving case."
1839,1,"\n\nI liked Section 3, however while it is true that all methods differ in the way they\ndo the filtering, they also differ in the way the input graph is represented\n(use of the adjacency or not)."
1840,2,I also do not feel that the lead PI is qualified to undertake this work…she needs to be academically successful first.
1841,2,"This paper was authored by a keen blogger and his colleagues. The paper perhaps reflects this informal approach, to its detriment"
1842,1, The authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new RL algorithms.
1843,2,…defining general populations for studies might help the reader and help the authors avoid their meaningless generalities.
1844,2,Have you no command of the English language?
1845,1,"  \n\nAnd note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better."
1846,2,What do the authors mean by one standard error?
1847,1," This seems rather limiting, can you comment on that?"
1848,2,Several books that are cited but there is no evidence that they have ever been studied and...
1849,1, but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere.
1850,1, It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span.
1851,1," Somewhat unsatisfying, longer-term prediction results into weaker game play."
1852,1," What is less trivial is estimating (4) for various activations, and that seems to the major methodological contribution."
1853,2,Was the white noise random?
1854,2,I have reviewed many manuscripts in my career and I have never seen so many repeated mistakes of this...
1855,2,"It is difficult to imagine any paper overtaking this one for lack of imagination, logic, or data—it is beyond redemption"
1856,2,"Overall, the superheated sense of justification and self-advertising on display here are simply..."
1857,2,I dont believe in simulations
1858,1," \""On the state of the art of evaluation in neural language models."
1859,1," It seems the novelty here is mainly the empirical exploration (will return to this later), and numerical integration for various activation functions."
1860,1," \n\nNotice the following suspiciously similar paragraphs from the two papers:\n\nFrom \""MAINTAINING COOPERATION IN COMPLEX SOCIAL DILEMMAS USING DEEP REINFORCEMENT LEARNING\"":\nWe also look at an environment where strategies must be learned from raw pixels. "
1861,1, This gives an image that is similar to the original but with features that caused the classification of the disease removed.
1862,1, \n- writing of the paper
1863,2,"This article describes [XY], a research project investigating methods for game design. The paper is poorly written [full review"
1864,1,"  The proposed workflow consists of the novel two-pass decomposition of a group of layers, and the fine-tuning of the remaining network."
1865,1,\n2.) Model generalization to data-sequences longer than the training set.
1866,2,"It is not clear how the of data presented in Figures 1, 2, 4, 5, 6, 7, 8, S1, and S2 was quantified and analyzed"
1867,2,Line 306. The sentence follows a bit of a Yoda-esque grammar
1868,2,This article reads like the work of a reasonably competent undergraduate.
1869,2,Do we really need concepts and theories to discuss inequalities among young adults?
1870,2,"Not a well prepared application, full of mistakes &amp; lacking some necessary preliminary data. Not at all fundable."
1871,2,"Nothing changes in our collective research programs as a result of this work. Still, the work needs to appear somewhere in the literature"
1872,2,"As I understand it, you already have good evidence (youve said as much on Facebook). There's no reason to leave this as an open question."
1873,1,  Experiments include both artificial data and real data.
1874,1,\n\n- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations).
1875,1," If no, I don't see how those curves compare."
1876,2,All these data are given in the author's Ph.D thesis. Does this material need to be published?
1877,1,\n- the datasets are not described properly.
1878,2,"However, the applicant seems to have run out of steam before he developed a detailed plan and completed the proposal. This is unfortunate"
1879,2,I now turn to my best guess about what the authors might be doing.
1880,2,"The quality of the data need to improve substantially. The data shown are not compelling, lack clarity, and do not support the conclusions."
1881,2,The proposal is poorly written and unfocused with only brief moments of meritorious thinking
1882,1,\n\nThe toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets.
1883,2,I found the use of the evolutionary theory problematic. This is a highly contested theory and the authors do not attend to the major flaws
1884,1," \n\nDespite mentioning matrix completion in the introduction and comparing to work of Ge et. al., the paper does not seem to have strong implications for matrix completion."
1885,2,"You will see that Reviewer 2 has slightly missed the point, so please dont pay too much attention to their comments in your revision. -E"
1886,2,"This is a nicely done paper but the sample is small, the measures uninformative and the findings have only very weak relevance to policy."
1887,1, the examples of knowledge storage from bAbI in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple.
1888,1,"\n\n-dealing with partial observability (work on decentralized POMDPs by Peshkin et al, 2000, Bernstein, Amato, etc.)"
1889,1," \n\nWhile the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016)"
1890,1, \n\nArchitecture\n- MFSC are log Filterbanks ...
1891,2,It reads as if the author were giving a lecture and wandered off point to tell an interesting story.
1892,1, It combines several insights into a nice narrative about infinite Bayesian deep networks.
1893,2,"Your content is stellar and your writing is excellent. However, consider hiring an editor for subsequent revisions - The entire revie"
1894,2,Your discipline doesnt exist
1895,2,…this is a antique approach to a modern problem
1896,2,"Who are and where did they come from? That is, why was this obsucre and arbitrary method chosen?"
1897,1, \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting.
1898,2,The paper does have some moderately interesting results. But...nothing took me by surprise or wonder
1899,1," There are technical issues with what is presented, with some seemingly factual errors."
1900,1, The MNIST explanations help a lot.
1901,2,"In the interest of being helpful, my suggestion is that the authors go back and review what is involved in the scientific method"
1902,2,This made the paper very long and may bore the reader
1903,2,I find myself disagreeing with most of this papers conclusions
1904,1, \n- It looks like it is not finished.
1905,1," I therefore recommend not to accept this paper in its current form."""
1906,1, Are the differences between SQDML/RAML and ML significant?
1907,1,"\n\n- one would expect more information to strictly improve performance,\n  but the results are a bit mixed (perhaps due to convergence to local optima and both actor and critic being learned at same time, \n  or the Gaussian assumptions, etc.)."
1908,2,This is a antique approach to a modern characterizationproblem
1909,2,"This is a misleading paper, badly executed and negligently written"
1910,2,I dont understand thermodynamics.
1911,1, This only means that more understanding is needed as to how TR can be combined with SGD.
1912,2,I have rarely read a more blown-up and annoying paper in the last couple of years than this hot-air balloon manuscript
1913,2,"A good deal of effort has been expended here, but to what end? - Reviewer "
1914,2,I am not convinced if any clear real value of this research.
1915,2,"[REDACTED]'s talks are popular, but then so are Ke$ha concerts."
1916,2,"Please respond to the Reviewer 2's comments, who suggested Rejection of the paper'. Reviewer 2 Comments to the authors: 'None."
1917,1," While the idea makes sense,"
1918,1," In particular, they aim at generating a complete set that fully specifies the behavior of the oracle."
1919,1,  This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.
1920,1,"\n\nOverall, I would argue that this paper is a clear accept."""
1921,1," but I doubt if a pragmatic ReLU network user will learn anything by reading this paper."""
1922,1," If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems."
1923,1,.\n* The description of the 2-D histogram on p.4 is not clear.
1924,1,  Reasonable baselines.
1925,1,"\n\n[1] Juan, C., Sutton, R. S., & Ram, A. Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces.\n\n[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[3] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2017). Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089."""
1926,2,"Generally, some extra clarification might be nice to improve clarity…"
1927,2,SURELY THE AUTHORS CAN COME UP WITH A BETTER REFERENCE THAN WIKIPEDIA TO SUPPORT THEIR POINT HERE!
1928,2,"This is a disaster. I could continue, but you see my point."
1929,2,This abstract is based on using intelligent transportation systems (whatever that is). This aim is not fulfilled by the paper
1930,1, Is q(t|x) in Fig 1 a typo?
1931,2,The manuscript presents the resits [sic] of a meta-analysis which seems to be based on very abundant but very poor primary data
1932,2,This paper is about combining inflexible specifications in a flexible way.
1933,2,The paper is an old fashioned one
1934,2,I am sorry if I am missing something obvious here but this is not my area of work.
1935,2,"The authors use a log transformation, which is statistical machination, intended to deceive"
1936,1,"\nUnlike the most common form of imitation learning or behavioral cloning, the authors \nformulate their solution in the case where the expert\u2019s state trajectory is observable, \nbut the expert\u2019s actions are not."
1937,2,At first I thought this was a practical joke h/
1938,1,"\n\nSection 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the \u201cbaseline\u201d Random Forest classifier."
1939,1," Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly."
1940,2,This ridiculous comment that a powerful computer algebra systems was required on page 6 is absurd.
1941,1," For one example, we\u2019re told that \u201cPreposition selection [is] a major area of study in both syntactic and semantic computational linguistics\u201d, but at best it\u2019s quite a specialized niche."
1942,1," As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain."
1943,2,"This paper must be rejected, because the work it describes is clearly impossible."
1944,2,The abstract is ok in the context of a weak manuscript
1945,1," \n\nWhile the authors do attempt to accomplish a lot in these nearly 16 pages of text, the authors' main points and overall narrative gets lost due to the writing that is a bit jumbled at times and that relies heavily on the supplement."
1946,2,It reads more like a diagnosis confirmed by a set of examples
1947,1,\n- A new structure for weight matrices that provides good performance while reducing matrix storage requirements and speeding up forward and backward passes.
1948,2,This reads more like a shopping list than an academic paper.
1949,2,I want to vomit; I cant believe this paper was submitted.
1950,1,"\n\n=-=-=-= Response to the authors\n\nDuring the initial reviewing period, I was unable to distill the significance of the authors\u2019 contributions from the current literature in large part due to the nature of the writing style."
1951,2,I am very enthusiastic about the topic of the article and the applied research design to answer the research questions.
1952,1, What would happen if shapes different than random squared patterns were used at test time?
1953,1, \n\nOne challenge in assessing the experimental claims is that practical neural networks are nonsmooth; the quadratic model developed from the hessian is only valid very locally.
1954,1," (with mistakes in an equation), however, it does not contribute much in terms of novelty or new ideas."
1955,1," Also, I see the advantage of referring only to the \""good\"" quantiles when needed."
1956,1," \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n"""
1957,2,Black-box modeling exercise using a hodge-podge of data tied together with a poorly-defined model
1958,2,So much trash belonging to the worst school of Bedlam literature — since Mr. Melville seems not so much unable to learn as disdainful of learning the craft of an artist. [original review of Moby Dick
1959,2,"Not all that compelling an idea to me, regardless of whether it is true"
1960,1,  \n\nBoth extensions are reasonable to me.
1961,1,"As a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c."""
1962,1, These patterns often appear in biological datasets. 
1963,2,Jargon-riddled assertions are made as though the meagre data substantiate the claims.
1964,2,"Adequate, if not particularly appealing."
1965,2,"In a nutshell, please cut out all the hype, show some integrity, and write a balanced paper."
1966,1," That connection might be worth discussing because removing the regularizing (i.e., metabolic cost) constraint from your RNNS makes them learn representations that differ from the ones seen in EC."
1967,1," However, the method is not properly motivated."
1968,2,"The text is overly expansive, desultory, and often diaphanous, so that the raison d'être of an overarching theoretical structure is neither pellucid nor convincing."
1969,2,"Though less enthused about manuscripts novelty, this reviewer does admire the hardworking of your group."
1970,2,"This is a very difficult paper to review, and difficult - even painful - to read"
1971,2,Despite all my efforts I failed to understand what the actual focus of this paper is
1972,2,This literature review is nothing more than a merry dance around the books
1973,1,"""MARS is suggested to combine multiple adversaries with different roles."
1974,1,\nI also could find the details on how figure 1 was produced
1975,1," For example, \""In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance."
1976,2,"presumptuous, ignorant and downright dangerous"
1977,2,"Sorry for our long silence, due to some perplexity on our side at reading your manuscript."
1978,2,"First, the paper is for a large part incomprehensible"
1979,2,"Did all 5 authors say,Yes, this is a piece of work I am proud to have my name on? - Twitter"
1980,1,  Can this be formalized?
1981,2,I basically stumbled across every second sentence
1982,2,I think the audience will eat him alive. But I want to be there to hear it.
1983,1, This is obviously not enough.
1984,1," How is the accuracy measured, e.g. with respect to the number of objects mentioned in the sentence, the accuracy of the attribute values, the deviation from the ground truth sentence (if so, what is the evaluation metric)? "
1985,2,Data is presented as though it were reliable observation when it could equally well be described as unwarranted slander
1986,1, The paper would be much improved if it was generally toned down.
1987,1," Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated."
1988,2,This is not a paper. This is part of … something. It cannot be reviewed and should be rejected right...
1989,1, but well below current best single models of around 76-78 F1.
1990,2,The reviewer's comments read 'fuck you'  (handwritten on a strip of paper scotch-taped to the editors letter
1991,1, but not precisely described.
1992,1,\n\nWhat concerns me is the extension to multiple layers.
1993,2,"I believe that there are important questions in this area, questions that have intellectual merit, but the PI has not found any…"
1994,2,This manuscript is only theoretical in the sense that the methods are proposed but not adequately tested
1995,2,They present an irresponsibly unbalanced literature review of the issues at hand and design an experiment that is ill suited to address the stated aims of the paper' 1/
1996,1, This should be supported by some quantitative results.
1997,2,"I nearly said reject, but then I recalled that I have a hangover and am feeling grumpy"
1998,2,"My disappointment when finding flawed analyses, conclusions, and terminology [..] was therefore substantial"
1999,1,\n\nClarity:\nThe paper is generally well-written and easy to read.
2000,1," Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work."
2001,2,"Why dont you just send copies of this to the two people in the world who care about it, and forget the publication route?"
2002,1, This paper instead designed a new boosting method which puts large weights on the category with large error in this round.
2003,1,\n- Limited applicability to settings where >> 100 configurations can be run fully
2004,1," Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains."
2005,2,"...But it would take me a long time to try and figure this out, so I am approving publication  "
2006,2,"Overally speaking, the manuscript is well written."
2007,1,"  Therefore, how to choose this parameter is essential."
2008,1,\nThe convergence constraint procedure from Table 4 is not clear.
2009,1, The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets 
2010,1,"\nI would also request the paper not to casually mention the 7x speedup that can be found in the appendix, without quantifying this.[[CNT], [null], [SUG], [MIN]] This is only possible for a large number of 40 Hyperband iterations, and in the interesting cases of the first few iterations speedups are very small."
2011,1,"    In other text learning task (e.g., [1]) SPL showed improved performance."
2012,2,Add statistical support! - Review of NIH grant with biostatistician as a co-investigator
2013,1,\n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al).
2014,1," \n\n- The analysis of section 4.1 is interesting, it was insightful and to the best of my knowledge novel."
2015,1, The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results.
2016,1,"\n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale."
2017,1," Moreover, is CIFAR 10 experiments conclusive enough. """
2018,2,"Frankly, she knows nothing about X or Y and should stick with what she does know."
2019,1,"\nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark."
2020,2,The authors offer no credible theoretical reason why we should care about this result.
2021,2,This is a very weak paper trying to bend things to build a relationship that does not exist
2022,1," It boils down to these simple commonly known facts about deep RL agents:\n- When evaluated outside of its training distribution, it might not generalized very well (figure 4/6)"
2023,2,"Currently, the impression is that this was simply another piece of research/ consultation."
2024,1," The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations)."
2025,2,Why do you have so many tables? Did you go to Ikea?
2026,2,The introduction is so confusing and purely written that I gave up.
2027,1," Assuming that the result carries over to ConvNets, I find this result to be very interesting."
2028,2,The first problem is that the method - whatever it is and however it works - is insufficiently...
2029,1," Also, I feel algorithm 1 is spurious given that it merely switch by systems."
2030,1,"\n\n5.  The experiments on adversarial robustness and face verification seems more interesting to me,"
2031,2,"It is difficult to imagine any paper overtaking this one for lack of imagination, logic, or data—it is beyond redemption"
2032,1," The empirical evaluations, analysis and comparisons to existing methods are well executed."
2033,2,I now turn to my best guess about what the authors might be doing.
2034,2,"'Putting it unfairly perhaps, and I am sure this is not what is meant, but one ungenerous reading of the proposal is..."
2035,2,[XX] is just as interesting to us as the new dress bought by Kim Kardashian
2036,2,"Measurements were made, but why, besides a teaching exercise, remains obscure"
2037,1, Just because a network performs averaging of different paths and individual paths perform worse than sets of paths doesn't imply that ensembling as a mechanism is in fact the cause of the performance of the entire architecture.
2038,2,This would be an embarrassment even if submitted as an undergraduate class paper' h/
2039,2,The description of sampling technique is too long and boring!
2040,1," It should be noted that all three reviewers pointed out the length of the paper as a weakness of the paper, and that in the most recent draft, the authors made the main text of the paper longer."
2041,2,Please also consider making the submission looking less like an advertising booklet rather than a research paper
2042,1,"""The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks \u2014 i.e., the existence of a single perturbation which causes a network to misclassify most inputs."
2043,1, But then they just concept net to augment text.
2044,1, \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot).
2045,2,"lthough the benefits of unprofessional reviews include gifs, laughts &amp; shared gasps of horror, this important new paper by  and  lays out the drawbacks:  In the interest of scholarly rigour we will quote from the 'highlights"
2046,2,I have received one highly negative review and wondered whether it could be overcome by other reviews. I conclude that the answer is no
2047,2,"Your piece is very well written and researched. I think the is interesting, but not particularly..."
2048,2,"There is hardly any paragraph (even in the abstract) that is not messy, disorganized, confusing, that does not contain mistakes (some are quite embarrassing), redundancies, abusive shortcuts or discussions that sound absurd."
2049,2,I recommend the publication even if I am not impressed - (via shitmyreviewerssay
2050,1,"\n* In Equation 2, should there be a balancing parameter for the reconstruction loss?"
2051,1,  I found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compelling.
2052,1," The fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is, to my mind, not at all shown here."
2053,2,"The correlations are not that strong (e.g., p = .022)"
2054,2,I would have preferred to read a meta-analysis
2055,1," In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?"""
2056,1,  I myself am very curious about what would happen and would love to see this exchange catalyzed.
2057,2,This last version is riddled with redundant information and circular phrases.
2058,1,  There are several places where it is not immediately clear why a certain block of text is included (i.e. the proof outlines on pages 8 and 10). 
2059,1,\nThe key novel idea is to learn a pairwise similarity function using the examples from the known classes to apply to examples of unknown classes.
2060,1," \n\nThe adversarial loss helps significantly only with AMT fooling or realism of images, as expected because GANs produce sharp images rather than distributions, and is not very relevant for robot motion planning."
2061,1,"  In Advances in Neural Information Processing Systems, pp. 3844\u20133852, 2016."
2062,1,\n3. another explanation about the weights as the rescaling to matrix A needs to further clarified.
2063,2,"eviewer 1: 'I really enjoyed the introduction'.

Editor: 'In line with Reviewer 1, the introduction doesn't do a good job..."
2064,1," Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine."
2065,2,"Publication of this paper will not advance our knowledge in any shape or form, it will just result in other researchers pointing out how bad this study actually is"
2066,1," \n\nIn addition, as the encoder-decoder structure gradually becomes the standard choice of sequence prediction, I would suggest the authors to add the sum of parameters into model ablation for reference."
2067,2,Based on these my recommendation would be that the present manuscript cannot be published in this or any other journal
2068,1, It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem.
2069,2,the sthanthard of writing is impercable
2070,1, The theorem stated in the paper seems to provide an interesting link between SR and the Laplacian.
2071,2,"The authors showed the differences at scale 3, and so what?"
2072,2,"The manuscript is poorly written although it can be follow with lots of typos, and a review by a native English speaker is clearly needed"
2073,2,A few points fall into the category of So What' h/
2074,1, The authors also propose a way to determine the target rank of each layer given the target overall acceleration.
2075,1,"\nIs there a specific reason for doing so?\n\n"""
2076,2,Is this not the most frightening finding of all?
2077,1,It would be a good idea to put the test results directly on Fig. 4 as it does not ease reading currently (and postpone ResNet-56 in the appendix).
2078,1, \n* The citation style of Authors (YEAR) at times leads to awkward sentence parsing.
2079,1," Furthermore, the paper is nearly double the recommended page length and has a nearly 30 page supplement."
2080,2,Maybe a scientific journal is the wrong outlet for your data
2081,2,"The ideas seem to be a combination of wishful thinking, poor chemical insight and limited understanding of the techniques involved"
2082,2,The work that this group does is a disgrace to science
2083,1,\n\n* Fig 6: What does 'clean gradients' mean?
2084,1,   Please provide numerical support.
2085,1,\n3. trains variatns of a forward model f on the hidden states of the various learned agents.
2086,1," The paper makes a nice contribution to the details of deep neural networks with ReLUs,"
2087,1, In this setting the behavior policy can have terminations and episodes in the classic sense (perhaps due to time outs).
2088,2,This [sentence] construction should be reserved for police procedurals and bad Mafia movies.
2089,1," It would be better to show how much \u201celimination\u201d and \u201csubtraction\u201d effect the final performance, besides the effect of subtraction gate.\n\n2)"
2090,1, It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee.
2091,1,\n\nSection 4 fails to mention that its use of performance prediction for early stopping follows exactly that of Domhan et al (2015) and that this is not a contribution of this paper; this feels a bit disingenious and should be fixed.
2092,1, Does it lead to better stability to choose one or the other?
2093,2,Usually climate studies do not show a good method for the proposed research. This is one of them.
2094,1, because (a) an important baseline is missing
2095,2,We note that you may well have completed an interesting study but the manuscript in its current form does not convey that
2096,2,"I am not aware of a published citation that justifies these calculations, but I have really not felt the need for such a citation given the straightforward computation"
2097,1, \n\n2) The inclusion of remaining-time as a part of the agent's observations and resulting improvement in time-limited domains is somewhat obvious.
2098,2,This paper is to science as astrology is to astronomy h/
2099,1, but not huge.
2100,1, but for a combination of reasons I think it's more like a workshop-track paper.
2101,2,"This paper is definitely not suitable to PNAS, or, for that matter to any other journal."
2102,2,The authors should opt for a less pretentious title
2103,1,I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow.
2104,1,"though experiments are limited\n"""
2105,2,The project is lack of interest.
2106,2,This paper is too difficult for a journal on Astrophysical Fluid Dynamics
2107,1,\n\n== Detailed suggestions ==\n\n1) The authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with ReLU activation functions* so that the current context of the result is more transparent.
2108,1,"  However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR."
2109,2,"Limited scholarship, flawed design, and faulty logic engenders no enthusiasm whatsoever."
2110,1, it lacks in two aspects.
2111,2,This code sample cannot be adequately described without the use of strong language.
2112,2,The orgnization and writing of the paper need to improve. There are some grammar errors need to correct.
2113,1, \n\n\n2) Pros:\n+ Novel idea that makes sense for learning a more robust representation for predicting the future and prevent only local temporal correlations learned.
2114,2,"Rev 1: The paper is generally well written (the English is good)
Rev 2: A proof reading by a mother tongue would improve readability."
2115,1, The fonts are too small for the numbers and the legends.
2116,1," Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis."
2117,1," \n* Given that many figures have several subfigures, the authors should consider using a package that will denote subfigures with letters."
2118,2,This paper is an experiment to try and determine how badly a research paper can be but still be accepted
2119,1, The paper contain many interesting contributions
2120,1,  Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases.
2121,1,"Pros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\n"
2122,1,\n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental.
2123,2,"There is a very interesting story to be conveyed in this paper, but it is hidden behind layers of mud, obscured by poor writing"
2124,1, They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations.
2125,2,It just doesnt make sense.
2126,1," This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning."
2127,1,"\n\nSection 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments. h is never discussed after section 2.2."
2128,1," First, it only considers a single task for which GANs are very popular. "
2129,1," (Though the proposed system does have the advantage of only requiring a screenshot created using any software, rather than being restricted to a particular piece of software.)"
2130,2,This paper is so devoid of content it should be rejected outright.
2131,1," \n\nThe \u201csoft ordering\u201d approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task."
2132,1, But seems none of the 4 loss functions incorporates this constraint.
2133,1,"  However, the construction seems artificial and these functions don't seem to be visually very complex."
2134,2,That gives a ridiculous demonstration where authors forgot science and reinvent history (…) the...
2135,2,"The research team cooked up a great filet mignon. Instead of mushrooms and an exquisite French sauce, we got American ketchup."
2136,2,"Thus, there is nothing new in the manuscript that warrants publication in Science or most other journals"
2137,2,"The authors spelling of coordinate, while technically correct, is arcane and annoying."
2138,1,"\n\n## Quality/science of experiments\nThe experimental results have been updated, and the performance of the baseline now seems much more reasonable."
2139,1,"\n\nI do want to note my other concerns:\n\nI suspect the theoretical results obtained here are somewhat restricted to the least-squares, autoencoder loss."
2140,2,'The only redeeming aspect of this manuscript is that it is so poorly written that it fails to convey the incorrect conclusions drawn here' [from the most recent episode of  on the review process
2141,2,The investigator is in the top 50% of his field
2142,1,\n\nI don't think any of the experiments reported actually refute any of the original paper's claim.
2143,1, They also show experimentally that penalized gradients stabilize the learning process.
2144,2,"I am, frankly, underwhelmed by the revisions. Most of the responses sound smooth, but really just written to avoid serious additional work"
2145,2,This paper is fluently written and meticulously researched. I do not recommend it for publication.
2146,1,\n\nDespite the rich literature of this recent topic the related work\nsection is rather convincing.
2147,1," The discussion is largely based on a sequence of experiments, some of which are interesting and insightful."
2148,2,I dont believe in simulations
2149,2,Where is research?
2150,2,I have followed the work of this group for the last few years and it is usually well polished. That is not the case for the present paper.
2151,1,"\n\n2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence."
2152,1,\n- I find figure 1 (c) somewhat confusing.
2153,2,This result would be great if it were true
2154,1,"  In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained."
2155,1,"""This paper presents an analysis of LSTMS showing that they have a from where the memory cell contents at each step is a weighted combination of the \u201ccontent update\u201d values computed at each time step."
2156,2,I don't think that means what you think it means
2157,2,I would refrain from using enumerations in your paper and instead encourage you to think about the deep masculinism that comes with it
2158,2,Ah - now I see a glimpse of promise in this paper - Five pages into the documen
2159,2,You're really funny.
2160,2,"For a section on thought, very little seems to have gone into it."
2161,2,I want to vomit; I cant believe this paper was submitted.
2162,2,"I appreciated how the author assumed that a goodly percentage of
the readership aren't native speakers, so anything academic would be lost"
2163,1,  It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem).
2164,1, \n\n+ Experiment not strong to support the idea.
2165,2,The arguments in the paper are compelling but not convincing
2166,1," In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works."
2167,1, Seems a bit marginal to me.
2168,2,Reading this made my brain melt. Very true though.
2169,2,Unless the authors performed some clever pagan ritual before euthanizing the animals I would use killed' instead of 'sacrificed'.
2170,1,"\n\ncons:\n\n- no mention of time costs, except that for more samples, S > 1, for\n taylor approximation, it can be very expensive."
2171,1,.\n- You claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experiments.
2172,1, \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names
2173,2,I really wanted to like this study
2174,1, Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community.
2175,1,\nKSS seems to need the user study.
2176,2,(on a computer simulation paper) Did you use software?
2177,1,\n- My only concern about the novelty of the paper is that the idea of using CYK chart-based mechanism is already explored in Le and Zuidema (2015).
2178,1, I support the acceptances of this paper.
2179,2,"There are also other points that have to be addressed, but I do not think it is makes sense to go into further detail."
2180,2,"I understand Wikipedia is not the best source of information, however…based on the information from Wikipedia, your hypothesis breaks down"
2181,1, \n\nClarity:\nThe paper is easy to read.
2182,2,One gets the feeling that the references were included for the sake of having something to refer to.
2183,2,This proposal left me cold - In response to a grant proposa
2184,1," \n\nPros:\n1, The paper is well presented and is easy to follow."
