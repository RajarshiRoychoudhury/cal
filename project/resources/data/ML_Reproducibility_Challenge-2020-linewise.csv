,id,Description,forum_id
0,0,**Reproducibility summary:** The report does not have the mandatory reproducibility summary.,4hm5ufX69jo
1,1,"**Scope of reproducibility:** In Section 3, the authors state that the aim of the section is ""to examine whether self-attention layers in practice do actually learn to operate like convolutional layers when trained on standard image classification tasks.""",4hm5ufX69jo
2,2,**Code:** It's unclear whether the authors used the original authors' code (which is publicly available) or not,4hm5ufX69jo
3,3,"The authors say they ""closely follow the official implementation for reproducing the three embedding schemes and attention mechanisms"" (line 98) but do not discuss how their implementation is similar to or different from the official implementation",4hm5ufX69jo
4,4,"The authors' codebase is clean and organized, but lacks documentation.",4hm5ufX69jo
5,5,**Communication with the original authors**: The authors did not discuss whether they communicated with the original authors,4hm5ufX69jo
6,6,**Hyperparameter search:** The authors did not conduct a hyperparameter search.,4hm5ufX69jo
7,7,**Ablation study**: The authors did not conduct any ablation studies.,4hm5ufX69jo
8,8,**Discussion on results:** The authors reproduced the original paper's CIFAR-10 experiments and reported similar findings,4hm5ufX69jo
9,9,The authors' discussion of their results is on the shorter side (2 paragraphs in Section 3.3),4hm5ufX69jo
10,10,**Recommendations for reproducibility:** The authors did not provide recommendations for reproducibility.,4hm5ufX69jo
11,11,**Results beyond the paper**: This report contains several results beyond the original paper,4hm5ufX69jo
12,12,"In Section 4, the authors discuss two recent works that attempt to replace convolutions with self-attention",4hm5ufX69jo
13,13,"While I found the introduced works interesting, the section lacks discussion of how these works relates to the original paper",4hm5ufX69jo
14,14,"In Section 5, the authors propose a new attention operation named Hierarchical Attention (HA) and demonstrate its effectiveness against normal Self-Attention (SA)",4hm5ufX69jo
15,15,"As the authors emphasize, this operation improves accuracy while using substantially less number of parameters.",4hm5ufX69jo
16,16,"**Overall organization and clarity:** Overall, the report was organized and clearly written",4hm5ufX69jo
17,17,"However, the authors did not include the mandatory reproducibility summary, and did not discuss their experience reproducing the paper (e.g",4hm5ufX69jo
18,18,"methodology, computational requirements, what was easy/difficult, communication with the original authors)",4hm5ufX69jo
19,19,"While the authors presented results beyond the scope of the original work, the report needs more work on the reproducibility front",4hm5ufX69jo
20,20,I also suggest keeping the citation style consistent and using appropriate labels and references to reference tables and figures.,4hm5ufX69jo
21,21,***Reproducibility Summary***,4hm5ufX69jo
22,22,"Unfortunately the authors have not included the mandatory first page reproducibility summary, and following the reviewer guidelines the submission is liable to desk rejection.",4hm5ufX69jo
23,23,***Scope of reproducibility***,4hm5ufX69jo
24,24,The paper does not state clearly the scope of the reproducibility,4hm5ufX69jo
25,25,"The authors reimplement the main experiment in the paper on Cifar, for which they obtain similar results",4hm5ufX69jo
26,26,"They also reproduce a similar experiment where they visualize the attention maps, however their attention maps differ significantly from those shown in the paper and there is no in-depth analysis of them.",4hm5ufX69jo
27,27,***Code***,4hm5ufX69jo
28,28,The authors have implemented their own code to reproduce the paper,4hm5ufX69jo
29,29,"Unfortunately, their code is attached but is not de-anonymized.",4hm5ufX69jo
30,30,***Communication with original authors***,4hm5ufX69jo
31,31,It is not clearly stated in the paper whether the authors of the reproducibility report contacted the original authors of the paper (or at least I couldn't find this information easily),4hm5ufX69jo
32,32,This information could have been readily available if the authors had included a reproducibility summary.,4hm5ufX69jo
33,33,***Hyperparameter search and ablation study***,4hm5ufX69jo
34,34,"The authors conducted limited hyperparameter searches and ablation studies, but the experiments in the original paper are limited in this regard too and therefore it does not seem highly relevant in the evaluation of this report.",4hm5ufX69jo
35,35,***Discussion on results***,4hm5ufX69jo
36,36,The authors do not discuss the results in depth,4hm5ufX69jo
37,37,Most of the conclusions are limited to re-stating the results obtained without properly discussing the differences between their results and the ones found in the original paper,4hm5ufX69jo
38,38,"This can be seen in the discussion for the filter results, for example.",4hm5ufX69jo
39,39,***Recommendations for reproducibility***,4hm5ufX69jo
40,40,The authors do not directly provide recommendations to improve the reproducibility of the original paper.,4hm5ufX69jo
41,41,**Results beyond the paper***,4hm5ufX69jo
42,42,The authors put the focus of this report on a new type of attention they propose,4hm5ufX69jo
43,43,"Unfortunately, this is at the expense of the quality of the rest of the report, and as part of the reproducibility challenge the paper is does not focus enough on reproducing the original paper.",4hm5ufX69jo
44,44,***Overall clarity***,4hm5ufX69jo
45,45,"The article could be better organized, providing first a reproducibility summary.",4hm5ufX69jo
46,46,***Overall rating***,4hm5ufX69jo
47,47,"I argue for the rejection of this article based on the omission of the mandatory reproducibility summary, the lack of clarity and the focus on the results beyond the paper but not on reproducing the results of the original paper.",4hm5ufX69jo
48,48,The authors do not provide reproducibility summary and the manuscript does not match the expected template,4hm5ufX69jo
49,49,There is no discussion about the scope of reproducibility nor any discussion over what was easy/hard,4hm5ufX69jo
50,50,Missing evaluations from the appendix section of the paper.,4hm5ufX69jo
51,51,"Overall, authors are able to replicate the experiments reported in the original paper",4hm5ufX69jo
52,52,They also show additional experiments with other choice of attention-based networks which is interesting,4hm5ufX69jo
53,53,Current analysis lacks visualizations for 'centers of attention' which is important,4hm5ufX69jo
54,54,Why is the #params in table 1 and 2 same ? Is the accuracy(paper) in table 2 correct comparison ?,4hm5ufX69jo
55,55,"It is commendable that along with reproducing results, the authors propose hierarchical attention operation and evaluate it's performance",4hm5ufX69jo
56,56,They show that it resolves the computational memory requirement as intended,4hm5ufX69jo
57,57,"However, keeping the scope of this venue in consideration the paper does not match the expected format and lacks detailed analysis.",4hm5ufX69jo
58,58,*Problem statement: ,PRXM8-O9PKd
59,59,"The paper clearly states the reproducing details, together with the detailed results and difficulties.",PRXM8-O9PKd
60,60,*Presentation:,PRXM8-O9PKd
61,61,The paper is well-organized and well-written,PRXM8-O9PKd
62,62,*Communication with original authors,PRXM8-O9PKd
63,63,The authors had some communication with the original authors.,PRXM8-O9PKd
64,64,*Code:,PRXM8-O9PKd
65,65,The code is well-organized and can be reproduced,PRXM8-O9PKd
66,66,I have tested the code on my side and all components work well,PRXM8-O9PKd
67,67,*Recommendations for reproducibility,PRXM8-O9PKd
68,68,The authors provided useful comments for reproducing the original paper,PRXM8-O9PKd
69,69,I have read the code and found those comments are consistent with the provided codes,PRXM8-O9PKd
70,70,*A few concerns,PRXM8-O9PKd
71,71,**The tense is not consistent in the whole paper,PRXM8-O9PKd
72,72,"In some sections, the author used past tense while the present tense is used in some other sections",PRXM8-O9PKd
73,73,"** It will be even better if the authors can provide a simple illustration on the two major algorithms, like Fig.3 and Fig.4 in the original paper",PRXM8-O9PKd
74,74,The figures and simple explanations would help readers to follow your report.,PRXM8-O9PKd
75,75,The authors developed code for the original paper from scratch and communicated with the original authors for detail,PRXM8-O9PKd
76,76,"They provide a in-depth, easily readable, well organized report and analysis",PRXM8-O9PKd
77,77,They include extensive HPO,PRXM8-O9PKd
78,78,They give valuable recommendations for future work,PRXM8-O9PKd
79,79,"Unfortunately, code will only be submitted only after the review process.",PRXM8-O9PKd
80,80,__Summary__,ysFCiXtCOj
81,81,"The original paper (OP) proposes a new activation function, snake",ysFCiXtCOj
82,82,"Snake is claimed to be usable with datasets both with and without cyclicity, on the contrary to previous activations",ysFCiXtCOj
83,83,"Experiments include low-dimension regression to illustrate the limitations of conventional activations, and applications of snake and baselines to timeseries to illustrate their ability (or not) to deal with cyclic data",ysFCiXtCOj
84,84,Snake is also evaluated on common benchmarks (CIFAR10) to show that it performs well compared to baselines even when the dataset is not cyclic.,ysFCiXtCOj
85,85,This reproducibility report (RR) reproduces all the experiments of the OP and confirms the OP's findings.,ysFCiXtCOj
86,86,"The RR also extends the study to two new settings (image generation and sentiment analysis), both non-cyclic, in which the performance of snake is studied.",ysFCiXtCOj
87,87,__Positive points__,ysFCiXtCOj
88,88,"- The OP's code was not publicly available and the authors recoded all of the experiments themselves, which needs to be commended.",ysFCiXtCOj
89,89,"- Similarly, the authors optimized hyperparameters to make the implementations work despite the fact that they were missing in the original paper.",ysFCiXtCOj
90,90,- The authors performed additional experiments with elaborate models,ysFCiXtCOj
91,91,"Both the feedforward and recurrent architectures that were used were not tested in the original paper, which demonstrates the applicability of the original idea to new settings and  adds value to the original idea from the OP.",ysFCiXtCOj
92,92,- I appreciated the exhaustive replication of the several experiments that this paper contained.,ysFCiXtCOj
93,93,"- The RR authors contacted of the OP authors to get additional information on the points of the paper that lacked clarity, and used this information to replicate the OP results.",ysFCiXtCOj
94,94,"- The format of the reproducibility report was respected, and the report is overall clear",ysFCiXtCOj
95,95,The authors clearly state that the results are reproducible,ysFCiXtCOj
96,96,The report is overall well written and concise.,ysFCiXtCOj
97,97,__Negative points__,ysFCiXtCOj
98,98,"- The general experiments are reproduced, but I find that a finer analysis is absent from the RR",ysFCiXtCOj
99,99,"The results are reproduced through a figure, but this figure is often not referred to",ysFCiXtCOj
100,100,A small conclusion about the reproducibility of the specific experiment is systematically absent,ysFCiXtCOj
101,101,"No quantified metric on the closeness to the original performance is given, or commented, even though the paper gave quantified results for the financial experiment (OP's Tab.2)",ysFCiXtCOj
102,102,"For instance, when the performance is illustrated through a graph, I would have expected a comment on the closeness between the accuracy of the OP's model and the RR's model.",ysFCiXtCOj
103,103,- I find that the report lacked precision,ysFCiXtCOj
104,104,"For instance: (1) I am not sure which optimizer was used for most of the experiments, or what initialization; Sec 4.1 is an example of this",ysFCiXtCOj
105,105,"(2) p.6, l.146: ""orders of magnitude"": how many? Can you give an estimate of the time in each case? (3) p.7, l.159: can you be more precise than ""reasonably realistic samples""? Are there metrics that exist that could quantify by a number the quality of the images produced? (4) p.8, l.163: took much longer: can you be more precise? etc.",ysFCiXtCOj
106,106,"- I find that overall, the report does not take a step back w.r.t the experiments it makes",ysFCiXtCOj
107,107,"As a consequence, the RR did not reflect on some of the differences, notably Fig.3 (the snake network starts to diverge, on the contrary to OP's Fig",ysFCiXtCOj
108,108,4) and Fig.5 (the snake network does not fit the training points): how can these differences be explained? ,ysFCiXtCOj
109,109,"- The RR mentions having to find / guess some hyperparameters (Sec 3.3), but does not comment on the value that was ultimately found and used in the report, or the range of hyperparameters that was tested",ysFCiXtCOj
110,110,"It is true that the code is available on Github, but an explicit reference to it would help lift the doubt on certain implementation details",ysFCiXtCOj
111,111,"For now, the reader does not gain the knowledge missing from the OP by reading the RR.",ysFCiXtCOj
112,112,"- It was often difficult to separate the RR's conclusions from the OP's (for instance, it is confusing on a first read if the last paragraph of p.5 contains conclusions from the RR's authors or the OP's authors, or even if the experiment itself was in the OP (since the experiment does not appear in the main paper, but in the appendix)).",ysFCiXtCOj
113,113,===,ysFCiXtCOj
114,114,"Overall, I believe the report could be improved by introducing each experiment better, providing the implementation details, and giving a conclusion on the reproducibility of each experiment",ysFCiXtCOj
115,115,"Adding numerical results indicative of reproducibility (was the reproduced end accuracy of the model within X% of the OP's results?) for each experiment would help ground the report, and help it discuss in more details the state of reproducibility of the OP.",ysFCiXtCOj
116,116,===,ysFCiXtCOj
117,117,__Additional comments__,ysFCiXtCOj
118,118,"- Some figures are not referenced (ex: Fig.2, Fig.3, Fig.4,)",ysFCiXtCOj
119,119,This is problematic because this reinforces the idea that the experiment was run in the RR without an analysis of its results and its significance,ysFCiXtCOj
120,120,How can you tell that the results were indeed replicated?,ysFCiXtCOj
121,121,- The RR mentions that reimplementing the initialization was not obvious from the paper and that one of the OP's authors helped,ysFCiXtCOj
122,122,An additional comment about what was the problem / what was the solution found (or at least a pointer to the RR's code) would be useful.,ysFCiXtCOj
123,123,- Regarding the discussion of Fig.8: it seems to me that the difference between the RNN and the MLP could simply be due to a regularization effect: the RNN also seems to learn the right lower frequency,ysFCiXtCOj
124,124,"- The authors mention in the reproducibility summary that they reimplemented everything from scratch, though looking at the code, it appears that some implementations (of the LaProp optimizer or of the base code for the GAN) were based on existing codebases",ysFCiXtCOj
125,125,"While obviously the authors are not expected to recode *everything* from scratch, it would be more transparent to indicate that some parts of the code were built on top of existing code in the RR",ysFCiXtCOj
126,126,"- A mention of the software used (Pytorch for the neural networks) would have been useful, as all libraries do not have necessarily the same default hyperparameters",ysFCiXtCOj
127,127,"Similarly, the authors mention in Sec",ysFCiXtCOj
128,128,"3.5 that many experiments could be run locally: more details on the local hardware (which CPU for instance, a minima an indication on the quality of the ""local"" computer that was used) would be welcome.",ysFCiXtCOj
129,129,- The RR mentions that making the parameter  a learnable was not properly explained in the OP,ysFCiXtCOj
130,130,"Here too, a pointer to the solution found would be appropriate.",ysFCiXtCOj
131,131,- The introduction is extremely close to the one in the OP,ysFCiXtCOj
132,132,"I would suggest reformulating the introduction more, or possibly shorten it, to avoid repeating the OP.",ysFCiXtCOj
133,133,"- The figures were overall well made and close in design to the original paper, which helps when comparing results",ysFCiXtCOj
134,134,"Possible improvements could be to respect the color attribution for different models (Fig.2) and making sure that the scale of two plots coincide to make comparisons easier (Fig.10 top left and bottom left, Fig.13 for instance).",ysFCiXtCOj
135,135,The paper is well written and clearly structured.,ysFCiXtCOj
136,136,It is a valuable confirmation of the original paper.,ysFCiXtCOj
137,137,I only have a few formatting criticisms:,ysFCiXtCOj
138,138,"sin, cos, and snake should not be italic, but rather \sin or {\rm sin",ysFCiXtCOj
139,139,"I think, „4e -4“ should be relaced by 0.0004 or 4\cdot 10^{-4}",ysFCiXtCOj
140,140,"In Section 4.4 „Effect of \alpha$, use \boldmath{\alpha}",ysFCiXtCOj
141,141,"The presented report is well organized, systematic, clear and concise with most of the major experimental results reproduced and reported",ysFCiXtCOj
142,142,"To begin with, authors have clearly mentioned the key claims to be investigated in the scope of reproducibility with the focus on the properties of  the activation functions and their behaviour in the experiments reported in the original paper",ysFCiXtCOj
143,143,It has also been proposed to conduct additional experiments beyond what is already covered in the source paper,ysFCiXtCOj
144,144,"These includes DCGAN for handwritten digit generation and LSTM model for sentiment analysis, each modified to include the proposed snake activation function",ysFCiXtCOj
145,145,"It is however to be noted that when result plots for different cases are not reported in the same graph, it helps to report them on the same scale, wherever possible (e.g",ysFCiXtCOj
146,146,"Figure 12 and 13), for easier comparison.",ysFCiXtCOj
147,147,"Due to the unavailability of the source code from the author's of original paper, the current authors, in consultation with the original authors, have done a good job of replicating the results, which is matching qualitatively to a great extent with the ones reported in the original paper",ysFCiXtCOj
148,148,"While the architectural details were available in the original text, the hyper-parameters and any other information missing were assumed and arrived at with trial and error.",ysFCiXtCOj
149,149,Most of the results from the main content of the original paper has been reported and are found to be qualitatively matching,ysFCiXtCOj
150,150,Figure 1 has demonstrated the inability of learning periodicity outside of the training region and thus substantiates the claim 1 of the scope of reproducibility,ysFCiXtCOj
151,151,These reporting also matches with the corresponding figure in the original work for the widely used activation functions - ReLU and Tanh.,ysFCiXtCOj
152,152,"To substantiate claim 2 which states that the proposed snake activation function captures periodicity outside of the training samples and also maintains favourable optimization properties such as loss reduction comparable to or better than ReLU activation, corresponding results from original work has also been reproduced",ysFCiXtCOj
153,153,"Although the frequency of the sin wave used in the experiment appears to be different, it is helpful in demonstrating that even though periodicity is observed, significant deviations can still occur at far away places from the training samples",ysFCiXtCOj
154,154,"Nevertheless, differences in implementations can be a source of difference.",ysFCiXtCOj
155,155,"In addition to the demonstrations on synthetic datasets discussed earlier, the authors have further reproduced results on real-world/application oriented datasets that includes performance reporting of ResNet18 on CIFAR-10 dataset, atmospheric temperature prediction in Minamitorishima island (Japan), patient body temperature prediction and Wilshere 5000 index prediction",ysFCiXtCOj
156,156,All these reported results corroborates the points in the original paper across varying and diverse datasets.,ysFCiXtCOj
157,157,"Finally, the authors have reported results for a few experiments from the Appendix section such as the one showing the effect of various value of $a$",ysFCiXtCOj
158,158,It is also admirable that they have reported results and discussed implications of use of snake activation function in other scenarios such as training DCGANs and LSTMs,ysFCiXtCOj
159,159,"While the snake activation was shown to have reasonable and comparable training speed and test performances as evidenced by Figure 2 and 4, the extra experiments on non-toy datasets helped in highlighting the relatively slow training speed.",ysFCiXtCOj
160,160,"Thus, overall, the authors have tried to cover extensive set of experiments demonstrating pros and cons of the proposed snake activation function and have reproduced the results qualitatively to a great extent",ysFCiXtCOj
161,161,The difficulty in reproducing some of the work such as comparison with RNN on regressing a Simple Periodic function where the details were missing and the data points had to be inferred from the graphs have been mentioned,ysFCiXtCOj
162,162,"Similarly, it is also suggested to clarify the implementation details for showing variance correction using ResNet101 on CIFAR-10 for better reproducibility.",ysFCiXtCOj
163,163,"In this report, the authors reproduce the experiment parts of the paper: ""Can Gradient Clipping Mitigate Label Noise?"" by Aditya Krishna Menon, Ankit Singh Rawat, Sashank J",TM_SgwWJA23
164,164,"Reddi, Sanjiv Kumar.",TM_SgwWJA23
165,165,The main contributions of this report are: ,TM_SgwWJA23
166,166,1,TM_SgwWJA23
167,167,The authors reproduce experiments of the original paper to prove that Partially Huberised Loss does have label noise robustness.,TM_SgwWJA23
168,168,2,TM_SgwWJA23
169,169,The authors report corrected hyperparameters that were reported incorrectly in the original paper.,TM_SgwWJA23
170,170,3,TM_SgwWJA23
171,171,"Although it's not so much different from what the original paper insists, the authors provide results that are different from the original paper.",TM_SgwWJA23
172,172,4,TM_SgwWJA23
173,173,The authors clarify that the ResNet-50 architecture used in the original paper differs from the architecture of He et al,TM_SgwWJA23
174,174,2015 (this choice was not made clear in the original paper) and explains why this modification is necessary.,TM_SgwWJA23
175,175,Minor issue:,TM_SgwWJA23
176,176,"In experiments using Long & Servedio dataset, there is no explanation for why is the accuracy becomes lower when the corruption rate ρ is lower.",TM_SgwWJA23
177,177,Verdict:,TM_SgwWJA23
178,178,The report contains some solid experiments and additional information and corrections not in the original paper,TM_SgwWJA23
179,179,I can see researchers trying to build upon the original paper benefitting from reading this reproducibility report,TM_SgwWJA23
180,180,"For this reason, I recommend the report be accepted.",TM_SgwWJA23
181,181,The report aims to reproduce a paper on so-called Partially Huberised losses,TM_SgwWJA23
182,182,These losses are used to mitigate the label noise,TM_SgwWJA23
183,183,The report replicates all the experiments from the original paper and makes insightful discoveries.,TM_SgwWJA23
184,184,The report is well-written and easy to follow,TM_SgwWJA23
185,185,The original paper is summarized and all the experiments are described in details,TM_SgwWJA23
186,186,The report also mentions the communication with the authors of the original paper which revealed some typos or mistakes in the original paper.,TM_SgwWJA23
187,187,"The original paper was reproduced from scratch with a different framework (PyTorch, instead of Tensorflow)",TM_SgwWJA23
188,188,This is good way to test the reproducability.,TM_SgwWJA23
189,189,Suggestions for improvements:,TM_SgwWJA23
190,190,"- The very first section ""Scope of Reproducability"" should discuss, the scope",TM_SgwWJA23
191,191,What the report reproduces and what it does not.,TM_SgwWJA23
192,192,- The hyper-parameter search is the same as in the original paper,TM_SgwWJA23
193,193,The report does not go an extra mile to search wider space,TM_SgwWJA23
194,194,"Nevertheless, I understand that such experiments may be time consuming and require a lot of computational resources.",TM_SgwWJA23
195,195,"- It is always desirable to include extra datasets, extra experiments, more architectures to more thoroughly test the claims of the original paper.",TM_SgwWJA23
196,196,- Section 4.2.2 mentions that the reproduced experiments used the mixed precision for training,TM_SgwWJA23
197,197,The report should clarify if this is different from the original paper and if it may influence the results.,TM_SgwWJA23
198,198,"Overall, this is a good report",TM_SgwWJA23
199,199,I recommend accept.,TM_SgwWJA23
200,200,The report aims to reproduce the results of the paper 'Can gradient clipping mitigate label noise?' The report gives a summary of how the reproduction is conducted and briefly introduce the original paper,TM_SgwWJA23
201,201,The paper also gives the details including the hyper-parameters and computational infrastructures,TM_SgwWJA23
202,202,The authors provides the reproducing codes with detailed documentation.,TM_SgwWJA23
203,203,The given report examines the reproducibilty of a deep denoising model SADNet for natural images based on spatial-adaptive residual blocks,yiAI9QN9nYt
204,204,"The model was proposed in the paper ""Spatial-Adaptive Network for Single Image Denoising"" by Chang et al",yiAI9QN9nYt
205,205,The model was reimplemented by the authors of the report using the residual spatial-adaptive block and the context block from Chang et al.,yiAI9QN9nYt
206,206,  ,yiAI9QN9nYt
207,207,**Reproducibility Summary:**,yiAI9QN9nYt
208,208,The report contains a reproducibility summary as required by the template.,yiAI9QN9nYt
209,209,**Scope of Reproducibility:**,yiAI9QN9nYt
210,210,The authors of the report clearly state the scope of their reproducibility experiments.,yiAI9QN9nYt
211,211,**Code:**,yiAI9QN9nYt
212,212,The code was provided,yiAI9QN9nYt
213,213,The authors reimplemented the training loop and the model and reused some code from the original repository (the residual spatial-adaptive block and the context block),yiAI9QN9nYt
214,214,The authors use the dataloader provided with the dataset,yiAI9QN9nYt
215,215,"Overall, the code is well written but some comments and doc-strings would highly improve the readability",yiAI9QN9nYt
216,216,"Unfortunately, I was not able to run the code due to CUDA incompatibilities.",yiAI9QN9nYt
217,217,**Communication with Original Authors:**,yiAI9QN9nYt
218,218,The authors of the report state that no communication with Chang et al,yiAI9QN9nYt
219,219,was necessary to reproduce the results.,yiAI9QN9nYt
220,220,**Hyperparameter Search:**,yiAI9QN9nYt
221,221,The authors use the hyperparameter settings stated in the paper,yiAI9QN9nYt
222,222,No further hyperparameter sweeps were performed,yiAI9QN9nYt
223,223,Given the long training time of three days this choice seems reasonable,yiAI9QN9nYt
224,224,"However, the authors changed the initialization scheme to the Kaiming initializer (instead of Xavier uniform initializer)",yiAI9QN9nYt
225,225,	    ,yiAI9QN9nYt
226,226,**Ablation Study:**,yiAI9QN9nYt
227,227,No ablation study was carried out.,yiAI9QN9nYt
228,228,**Discussion on Results:**,yiAI9QN9nYt
229,229,The authors were able to reproduce most of the results from the original paper,yiAI9QN9nYt
230,230,The PSNR/SSIM values closely match the values reported by Chang et al,yiAI9QN9nYt
231,231,"However, visually the results shown in Figure 5 are qualitatively worse than the original results",yiAI9QN9nYt
232,232,Also runtimes are compared with the runtime stated in the original work (using different hardware) which is not appropriate,yiAI9QN9nYt
233,233,Running the original code on the same hardware would have been a more meaningful way of comparing runtimes.,yiAI9QN9nYt
234,234,**Recommendations for Reproducibility:**,yiAI9QN9nYt
235,235,No recommendations made.,yiAI9QN9nYt
236,236,**Results Beyond the Paper:**,yiAI9QN9nYt
237,237,No results beyond the original work reported.,yiAI9QN9nYt
238,238,**Overall Organization and Clarity:**,yiAI9QN9nYt
239,239,The report is well-written and well-organized,yiAI9QN9nYt
240,240,The authors should add references for the first sentence of the introduction,yiAI9QN9nYt
241,241,Some small comments on the writing can be found at the end of the review.,yiAI9QN9nYt
242,242,**Summary of Review:**,yiAI9QN9nYt
243,243,The report at hand successfully reproduced the original paper,yiAI9QN9nYt
244,244,The authors are very clear about their experimental setup and the problems that they faced during reproduction,yiAI9QN9nYt
245,245,"However, there are some minor problems in the discussion",yiAI9QN9nYt
246,246,"Nonetheless, I recommend to accept the paper to the ML Reproducibility Challenge 2020.",yiAI9QN9nYt
247,247,**Minor Remarks:**,yiAI9QN9nYt
248,248,- line 90: recieves -> receive,yiAI9QN9nYt
249,249,- line 114-115: 1e-4 vs 10^8,yiAI9QN9nYt
250,250,- caption Figure 2 and Figure 3: validation dataset should not be capitalized,yiAI9QN9nYt
251,251,- Equation (1): the \delta in front of m_i is missing,yiAI9QN9nYt
252,252,- Equation (2): set-notation is not appropriate here: \delta p^s and \delta m^s should form a tuple instead of a set,yiAI9QN9nYt
253,253,"However, this notation is also used in the original work.",yiAI9QN9nYt
254,254,"- line 144: \sigma = {30, 50, 70}: here \in should be used instead of ""=""",yiAI9QN9nYt
255,255,> Overall approach to reproduce the results and the adjoining paper are quite clear.,yiAI9QN9nYt
256,256,> The reproducibility summary is provided.,yiAI9QN9nYt
257,257,> It seemed easy to reproduce the results from the paper although the authors had to resort to default hyperparameters as these were not readily available,yiAI9QN9nYt
258,258,Their results still tallied with that of the original paper showing that the model architecture is robust to change in hyperparameter settings,yiAI9QN9nYt
259,259,> It is not clear if the authors rewrote the original PyTorch code.,yiAI9QN9nYt
260,260,"> Even though no communication with the original authors was made, some areas to tidy up in the code are discussed by the authors.",yiAI9QN9nYt
261,261,The paper seeks to reproduce the results of the paper titled “VCNet: A Robust Approach to Blind Image Inpainting”,RKAKTnLzb8q
262,262,"Due to computational constraints, some hyperparameters like the batch size (and consequently others as well) were modified",RKAKTnLzb8q
263,263,"For the same reason, only a subset of the original training data was used for training the models",RKAKTnLzb8q
264,264,"Due to the above reasons, the quantitative results are somewhat different from the original paper",RKAKTnLzb8q
265,265,"Despite such (statistically significant) differences, it seems reasonable to deduce that the core claims of the original paper are found to hold",RKAKTnLzb8q
266,266,"Overall, the reproducibility study is reasonable, and the changes necessitated by the lack of constraints do not substantially take away from the study conducted and reported herein",RKAKTnLzb8q
267,267,The study qualitatively substantiates the main claims of the original work and presents a novel ablation study that investigates the robustness aspect of the original model,RKAKTnLzb8q
268,268,This study will be useful to the audience of this challenge as well as the wider AI/ ML community interested in the original work,RKAKTnLzb8q
269,269,I have some concerns regarding the stated scope and the alignment of the study design with the stated scope but perhaps it can be addressed in a revision,RKAKTnLzb8q
270,270,"Similarly, the discussion on the results and the clarity should be improved",RKAKTnLzb8q
271,271,"Overall, my recommendation is a borderline acceptance of the paper if the authors address the shortcomings listed below",RKAKTnLzb8q
272,272,"In the following, an evaluation of this paper on the metrics suggested by the RC 2020 challenge is presented:",RKAKTnLzb8q
273,273,Reproducibility Summary: ,RKAKTnLzb8q
274,274,A 1-page summary is provided and adheres to the style guidelines,RKAKTnLzb8q
275,275,The major findings have been reported in the summary.,RKAKTnLzb8q
276,276,Scope of Reproducibility: ,RKAKTnLzb8q
277,277,The authors have provided a brief and clear summarization of the problem statement and the proposed approach,RKAKTnLzb8q
278,278,"However, more care should have been taken in stating the questions sought to be answered and aligning the study design to answering them.",RKAKTnLzb8q
279,279,(a) Q.1 – mask prediction – is evaluated,RKAKTnLzb8q
280,280,(b) Q.2 – quantitative results – instead should be inpainting quality as measured by PSNR and SSIM metrics,RKAKTnLzb8q
281,281,(c) Q.3 – whether it is possible to generate realistic inpainting results – is too vague,RKAKTnLzb8q
282,282,(d) Q.4 – whether PCN resolves the degrading inpainting performance – is not tested as an independent claim,RKAKTnLzb8q
283,283,"(e) Q.5 – ablation studies mentioned in the paper – design choices (MPN, PCN ($\rho$), semantic consistency term, etc",RKAKTnLzb8q
284,284,is not tested,RKAKTnLzb8q
285,285,"Instead, different use cases – generalization to unseen noise, raindrop removal, and, face swapping are investigated.",RKAKTnLzb8q
286,286,Code: ,RKAKTnLzb8q
287,287,The code for the original paper is not publicly available,RKAKTnLzb8q
288,288,The authors have implemented the code from scratch in PyTorch,RKAKTnLzb8q
289,289,"As mentioned in the report, the original paper contains detailed description of the architecture making the process easier but still some parts needed more clarifications.",RKAKTnLzb8q
290,290,Communication with original authors: ,RKAKTnLzb8q
291,291,The authors mention that even though the original work is quite descriptive they had to reach out to the authors to clarify certain architectural details,RKAKTnLzb8q
292,292,The original authors gave access to the private repository and the concerning points were clarified.,RKAKTnLzb8q
293,293,Hyperparameter search: ,RKAKTnLzb8q
294,294,The authors of the report had limited access to computational resources,RKAKTnLzb8q
295,295,All the experiments were conducted on a single RTX 2080Ti,RKAKTnLzb8q
296,296,"Due to the limited GPU memory, the maximum batch size that could be used for training was 4, which is different from the original proposed work",RKAKTnLzb8q
297,297,"To compensate for the smaller batch size, the authors have finetuned the learning rate and the number of iterations accordingly",RKAKTnLzb8q
298,298,This setting should be useful to audience with similar constraints on the availability of resources.,RKAKTnLzb8q
299,299,"The report also highlights key details that were crucial to reproducing the results but were missing in the original paper, for example, weight initialization and the architecture details of the discriminator",RKAKTnLzb8q
300,300,All the combinations of hyperparameter settings used in the experiments are provided in Table 1 of the report.,RKAKTnLzb8q
301,301,Ablation study: ,RKAKTnLzb8q
302,302,"The authors claim to conduct an ablation study, but I am not sure if it can be called such",RKAKTnLzb8q
303,303,Ablation refers to studying the impact of modeling design choices on performance by carefully ablating (removing) parts of the model,RKAKTnLzb8q
304,304,"Authors instead study the downstream used of the model – (a) unseen noise sources (including raindrops) during test, and, (b) the face swapping task",RKAKTnLzb8q
305,305,The first task is only demonstrated qualitatively (like in the original paper) while the authors are unable to replicate the results on the second task,RKAKTnLzb8q
306,306,"However, neither is the second task properly defined in the original paper nor in this study",RKAKTnLzb8q
307,307,"In addition to not conducting any independent ablation studies, the authors did not even replicate the ablation studies conducted in the original work (MPN, for example).",RKAKTnLzb8q
308,308,Discussion on results: ,RKAKTnLzb8q
309,309,The authors clearly state aspects that were easy/ difficult to reproduce and the steps they took to address the difficulties,RKAKTnLzb8q
310,310,This work focused on reproducing the following results (different batch size and hyperparameters; smaller training data):,RKAKTnLzb8q
311,311,Mask Prediction: The quantitative results (BCE metric) on the FFHQ dataset are off (worse) by about 10% relative error while the results on the Places2 dataset are off (worse) by about 73%,RKAKTnLzb8q
312,312,Qualitative results (examples presented) show that the predicted masks are close to the ground truth masks,RKAKTnLzb8q
313,313,This is not reflected in the quantitative evaluation and is not adequately explained.,RKAKTnLzb8q
314,314,"Inpainting quality: The results on the FFHQ and Places2 datasets on the PSNR and SSIM metrics have a relative error of (18%, 0.4%) and (4.8%, 1.2%)",RKAKTnLzb8q
315,315,"Since the SSIM metric is almost identical to the original (despite the differences in the training set up), the differences do not seem to be perceptually relevant, and it can be reasonably claimed that the performance from the original work was reproduced",RKAKTnLzb8q
316,316,It is unclear if the above differences are due to insufficient hyperparameter tuning or due to the limited data used for training.,RKAKTnLzb8q
317,317,Recommendations for reproducibility:  ,RKAKTnLzb8q
318,318,none,RKAKTnLzb8q
319,319,Overall clarity and organization: ,RKAKTnLzb8q
320,320,The report's overall clarity and structure is adequate,RKAKTnLzb8q
321,321,There are a few typos and awkward grammatical constructs that can be improved,RKAKTnLzb8q
322,322,"The report should be carefully checked for such errors (e.g., typos in line 81 and 86; the mask is denoted using M not N).",RKAKTnLzb8q
323,323,Readability can be improved further by incorporating the more important equations in the methodology section,RKAKTnLzb8q
324,324, In line 127 the authors claim that the original paper does not mention about alpha blending which is not true (refer page 5 of the original paper),RKAKTnLzb8q
325,325,The report should be carefully checked for syntactic error (typos in line 81 and 86; the mask is denoted using M not N).,RKAKTnLzb8q
326,326,The given report examines the reproducibility of VCNet model for blind image inpainting of natural images,RKAKTnLzb8q
327,327,"The model was proposed in the paper ""VCNet: A Robust Approach to Blind Image Inpainting"" by Wang et al",RKAKTnLzb8q
328,328,and it consists of two parts: a mask prediction network that tries to classify the input image into corrupted and non-corrupted regions and an inpainting network that performes the reconstruction,RKAKTnLzb8q
329,329,Furthermore a discriminator network is used for training with an adversarial loss.,RKAKTnLzb8q
330,330,**Reproducibility Summary:**,RKAKTnLzb8q
331,331,The report contains a reproducibility summary as required by the template.,RKAKTnLzb8q
332,332,**Scope of Reproducibility:**,RKAKTnLzb8q
333,333,The authors of the report clearly state the scope of their reproducibility experiments.,RKAKTnLzb8q
334,334,**Code:**,RKAKTnLzb8q
335,335,The model was implemented from scratch and the code was submitted,RKAKTnLzb8q
336,336,It is very well written but some doc-strings would highly improve the readability,RKAKTnLzb8q
337,337,"Although the training weights were provided, I was not able to re-run the model without downloading the very large datasets",RKAKTnLzb8q
338,338,A small demo would have been useful to verify the implementation.,RKAKTnLzb8q
339,339,**Communication with Original Authors:**,RKAKTnLzb8q
340,340,The authors state that they were in contact with the original authors since the beginning of the challenge and were given access to the private repository to double-check their reproduced code.,RKAKTnLzb8q
341,341,**Hyperparameter Search:**,RKAKTnLzb8q
342,342,Most of the hyperparameters were set to the values reported in the original work are used,RKAKTnLzb8q
343,343,"However, the authors decreased the batch size and adapted the learning rate due to compuatational limitation.",RKAKTnLzb8q
344,344,**Ablation Study:**,RKAKTnLzb8q
345,345,The ablation studies from the original work were reproduced,RKAKTnLzb8q
346,346,"The authors succeeded to reproduce the raindrop removal experiments and the experiments using different noise sources, but fail to reproduce the face-swapping experiments",RKAKTnLzb8q
347,347,"However, the authors spent a lot of effort to reproduce the face-swapping experiments.",RKAKTnLzb8q
348,348,**Discussion on Results:**,RKAKTnLzb8q
349,349,The results are discussed in an appropriate way,RKAKTnLzb8q
350,350,"The report contains visual results as well as quantitative results using the metrics from the original work (binary cross-entropy-loss, PSNR, and SSIM)",RKAKTnLzb8q
351,351,The authors are sincere that they were not able to reproduce the face-swapping experiments from the paper.,RKAKTnLzb8q
352,352,**Recommendations for Reproducibility:**,RKAKTnLzb8q
353,353,"The authors point out that some implementation details are missing in the original paper (input format, discriminator architecture and weight initialization).",RKAKTnLzb8q
354,354,**Results Beyond the Paper:**,RKAKTnLzb8q
355,355,No results beyond the paper are reported.,RKAKTnLzb8q
356,356,**Overall Organization and Clarity:**,RKAKTnLzb8q
357,357,The description of the experiments is well-written and the report is overall well-organized,RKAKTnLzb8q
358,358,"However, there are some issues in the section that describes the methodology, i.e., Section 3:",RKAKTnLzb8q
359,359,1,RKAKTnLzb8q
360,360,The authors use GT to denote the ground truth image which can be very miss-leading,RKAKTnLzb8q
361,361,I would recommend to use G or O (the latter one was used in the original work),RKAKTnLzb8q
362,362,2,RKAKTnLzb8q
363,363,What is confidence-driven mask smoothing? The term is also not mentioned in the original work,RKAKTnLzb8q
364,364,A brief explanation would be useful.,RKAKTnLzb8q
365,365,3,RKAKTnLzb8q
366,366,In Equation 5: the variables V_{GT}^l and V_O^l are not defined,RKAKTnLzb8q
367,367,"I suppose they are used to denote some feature maps in the l-th layer? However, this is just a guess.",RKAKTnLzb8q
368,368,4.The last two loss-terms L_{mrf} and L_{adv} are not further specified,RKAKTnLzb8q
369,369,The original work mentions that the WGAN-GP objective is used as adversarial term and the ID-MRF loss is used as texture consistency term,RKAKTnLzb8q
370,370,It would recommend to add these explanations to the report.,RKAKTnLzb8q
371,371,**Summary of Review:**,RKAKTnLzb8q
372,372,The authors did a great job to reproduce the results from the original work and I recommend to accept the paper to the ML Reproducibility Challenge 2020 after the issues in the above section have been adressed,RKAKTnLzb8q
373,373,I will improve my score as soon as these issues are fixed.,RKAKTnLzb8q
374,374,**Minor Remarks:**,RKAKTnLzb8q
375,375,- Equation (5): period should be placed after the equation (not in front of it),RKAKTnLzb8q
376,376,"- l 115: ""with gaining of 0.02"": I am not familiar with the term ""gaining""",RKAKTnLzb8q
377,377,Is it used to refer to the standard deviation/variance?,RKAKTnLzb8q
378,378,"Figure 3 and 5: wrong wording: ""(5) The masks applied confidence-driven smoothing""",RKAKTnLzb8q
379,379,This study attempted to verify the claims of the Reformer paper:,3s8Y7dHYkN-
380,380,1,3s8Y7dHYkN-
381,381,LSH based attention achieves higher speed for long sequences: reproduced,3s8Y7dHYkN-
382,382,2,3s8Y7dHYkN-
383,383,Reformer has low memory footprint than baseline: reproduced,3s8Y7dHYkN-
384,384,3,3s8Y7dHYkN-
385,385,Reversible layers and shared key queries achieve similar performance with baseline: not reproduced,3s8Y7dHYkN-
386,386,4,3s8Y7dHYkN-
387,387,LSH based attention achieves similar performance as full attention: reproduced,3s8Y7dHYkN-
388,388,"The authors reported both training and validation losses for 1, 3 and 4",3s8Y7dHYkN-
389,389,Claim 2 was supported by Figure 7.,3s8Y7dHYkN-
390,390,The study also pointed out that the reformer may increase training/inference time due to increased rounds of hashing to match the baseline performance.,3s8Y7dHYkN-
391,391,General Remarks,3s8Y7dHYkN-
392,392,The fact that the authors failed to anonymize the report by mentioning their names and also by referencing a public Github repo,3s8Y7dHYkN-
393,393," That had no effect on reviewing their work, so I don't consider it a problem",3s8Y7dHYkN-
394,394,"I think they did overall a good work reproducing the result and providing a practical ablation study, given the scale of the experiments",3s8Y7dHYkN-
395,395,The authors did follow the template and included the summary page correctly.,3s8Y7dHYkN-
396,396,Positives,3s8Y7dHYkN-
397,397,The paper breaks down the scope of the report into verifying 7 claims of the paper,3s8Y7dHYkN-
398,398,I also liked the glossary that they introduced in 3.1,3s8Y7dHYkN-
399,399,Overall the authors managed to reproduce many claims,3s8Y7dHYkN-
400,400,"Some claims were not reproduced, because of several engineering technicalities that the original paper introduced.",3s8Y7dHYkN-
401,401,This was an insightful observation,3s8Y7dHYkN-
402,402,"""In our experimentation we observe that Reformer enables training of deep models with long sequences on a single GPU,",3s8Y7dHYkN-
403,403,supporting the democratization of machine learning,3s8Y7dHYkN-
404,404,Practitioners interested in using Reformer should note however,3s8Y7dHYkN-
405,405,that we also observed a non-negligible increase in time complexity due to the trade-off of memory for compute in the,3s8Y7dHYkN-
406,406,Reversible Residual Layers and the trade-off of time for increased accuracy as the number of hashes used increases.,3s8Y7dHYkN-
407,407,"This behaviour is also observed elsewhere, e.g",3s8Y7dHYkN-
408,408,Katharopoulos et al,3s8Y7dHYkN-
409,409,[2020] and Tay et al,3s8Y7dHYkN-
410,410,[2020],3s8Y7dHYkN-
411,411,"""",3s8Y7dHYkN-
412,412,Need to be fixed,3s8Y7dHYkN-
413,413,There are too many footnotes on websites,3s8Y7dHYkN-
414,414,They should be moved to the references section,3s8Y7dHYkN-
415,415,It will make reading easier,3s8Y7dHYkN-
416,416,"In fact, it might be more appropriate to create an appendix since the websites contain a lot of information that is important for understanding the report.",3s8Y7dHYkN-
417,417,The mini Ablation study on how to reproduce table 1 is very useful and points a weakness of the paper.,3s8Y7dHYkN-
418,418,The 4.2 section is confusing and needs to be rewritten,3s8Y7dHYkN-
419,419,The authors make a reference to Github,3s8Y7dHYkN-
420,420,"This sentence ""The first half of the sequence is masked from the loss function, so the goal for the model is to learn that midway through the sequence it has to repeat 0w"" doesn't make sense to me.",3s8Y7dHYkN-
421,421,Table 1 the column names need to be top-aligned,3s8Y7dHYkN-
422,422,# Reproducibility Summary ,hq3TxQK5cox
423,423,Main summary not clear,hq3TxQK5cox
424,424,"Is it the objective of the original paper, or the objective of the reproduction? The rest of the summary is clear.",hq3TxQK5cox
425,425,# Scope of reproducibility,hq3TxQK5cox
426,426,"They reproduce results from original paper, and test with additional dimensionality reduction algorithms as well as on additional datasets.",hq3TxQK5cox
427,427,# Code,hq3TxQK5cox
428,428,Was extended with additional DR algorithms and re-implemented in PyTorch,hq3TxQK5cox
429,429,It is not clearly mentioned however whether the algorithms were re-implemented solely based on information found in the paper or if the code has been used as a reference.,hq3TxQK5cox
430,430,# Communication with original authors,hq3TxQK5cox
431,431,"No major communication, only to assert the dependencies needed to run the original cod.",hq3TxQK5cox
432,432,# Hyperparameter Search,hq3TxQK5cox
433,433,The hyperparameters are described but there is no mention of hyperparameter optimization.,hq3TxQK5cox
434,434,# Ablation Study,hq3TxQK5cox
435,435,The results are tested on additional DR algorithms,hq3TxQK5cox
436,436,I would consider this as an ablation study since it verifies the effect of one of the components of the whole procedure.,hq3TxQK5cox
437,437,# Discussion on results,hq3TxQK5cox
438,438,The results are well described and compared with original work,hq3TxQK5cox
439,439,The results section is more difficult to read than the rest of the report,hq3TxQK5cox
440,440,"The figures should be better described, it is very difficile to make sense of Figure 5 in particular",hq3TxQK5cox
441,441,# Recommendations for reproducibility,hq3TxQK5cox
442,442,None,hq3TxQK5cox
443,443,# Results beyond the paper :,hq3TxQK5cox
444,444,The method is tested with different DR and on 3 additional datasets,hq3TxQK5cox
445,445,The comparison on the additional datasets is interesting because these have no low-dimensionality manifolds and thus should be trickier for the DR methods.,hq3TxQK5cox
446,446,# Overall organization and clarity,hq3TxQK5cox
447,447,The paper is very clear and especially well written with the exception of the results section,hq3TxQK5cox
448,448,The figures lack explanation,hq3TxQK5cox
449,449,See minor comments for grammatical errors.,hq3TxQK5cox
450,450,# Comments,hq3TxQK5cox
451,451,Section 3: Methodology,hq3TxQK5cox
452,452,It should be made clear whether the re-implementation has been attempted with only looking at information available in the paper or if they used the available code-base as a reference,hq3TxQK5cox
453,453,This makes an important difference for the reproduction as looking at the code-base could lead to re-implementation of important tricks that were not mentioned in the paper,hq3TxQK5cox
454,454,It would be good to cite the work behind the linear and non-linear models listed in 3.1.1 and 3.1.2.,hq3TxQK5cox
455,455,I don’t understand the role of the digression,hq3TxQK5cox
456,456,I feel it states in different words what has already been said implicitly in the prior sections,hq3TxQK5cox
457,457,Footnotes 14 and 15: I was clueless about what I should do to make the comparisons,hq3TxQK5cox
458,458,Ideally these results should be integrated in the paper and presented clearly.,hq3TxQK5cox
459,459,# Minor comments,hq3TxQK5cox
460,460,Figure 1’s caption: I don’t understand the sentence: 'the difference between point 1 and 3 is explained each data point can be seen as an individual cluster,hq3TxQK5cox
461,461,',hq3TxQK5cox
462,462,Section 3.5: we have ran -> we have run,hq3TxQK5cox
463,463,Section 3.5: All experiments are ran -> All experiments were run,hq3TxQK5cox
464,464,Footnote 13: be only using -> by only using,hq3TxQK5cox
465,465,Section 5.1: set up was the obtaining > set up was obtaining,hq3TxQK5cox
466,466,Section 5.2: Since we are aim -> Since we were aiming,hq3TxQK5cox
467,467,### SUMMARY,hq3TxQK5cox
468,468,The paper claims that: ,hq3TxQK5cox
469,469,"- The common data exploration workflow (of learning low dimensional representations, identifying features which help examine differences across clusters to determine what they represent as they correlate to an unobserved concept of interest) is treated as an interpretable machine learning problem where:",hq3TxQK5cox
470,470,    - Global Counterfactual Explanations (GCE's) ensure pair-wise explanations for all points within a cluster ,hq3TxQK5cox
471,471,"    - Transitive Global Translations (TGT's) generalize the above compressed sensing solution to find the complete set of explanations are both symmetrical and transitive among all groups simultaneously and empirically demonstrate the same with the following datasets: synthetic, UCI (Iris, Boston Housing and Heart Diseases data) as well as single-cell RNA data with adequate correctness and coverage",hq3TxQK5cox
472,472,- TGT's identify explanations that accurately explain models while being relatively sparse and reportedly match underlying patterns in the data.  ,hq3TxQK5cox
473,473,The submitted report addresses the above claims as follows:,hq3TxQK5cox
474,474,"- _Re-execution_ of existing code along with re-written code variants (upgraded to TF 2.x, Pytorch and without external dependencies such as on scvis) on all the above mentioned datasets establishes correctness, coverage, and sparsity, thus verifying both claims",hq3TxQK5cox
475,475,"- Additionally, experimentation with other linear and non-linear dimensionality reduction algorithms (truncated SVD, sparse PCA, Gaussian variational autoencoder, kernel PCA, manifold dimensionality reduction algorithms like isomap and local linear embedding) on the following additional datasets -(seeds, Wine and Glass",hq3TxQK5cox
476,476,"dataset excluding single-cell RNA) - explored along with dynamic scaling of data in latent spaces for the purpose of achieving a certain amount of variance in order to test applicability to differing data structures, uncovers the following limitations:",hq3TxQK5cox
477,477,    - Constrained variable freedom interferes in manifold mapping where matrix based explanations/explanations beyond translations (such as with rotation/scaling) may be necessary,hq3TxQK5cox
478,478,"    - Structure of the data produces different type of clusters and hence, structure, shape, method of cluster annotation and variance in the latent space affects algorithmic performance",hq3TxQK5cox
479,479,     - Highly non-linear dimensionality reduction algorithms perform worse in terms of explainability (probably due to sparsity).,hq3TxQK5cox
480,480,### MERITS,hq3TxQK5cox
481,481,"The additional experimentation is rather impressive and the report reflects an intuitive understanding of concepts such as coverage, correctness, and counterfactual explanations.",hq3TxQK5cox
482,482,### MINOR CORRECTIONS,hq3TxQK5cox
483,483,"- In Methodology, ""Exprimentation was done on a Macbook""; **Correction:** ""_Experimentation_ was done on a Macbook""",hq3TxQK5cox
484,484,"- In Section 1 - Introduction, for the argument regarding algorithmic decisions that involve grouping of data, the reviewer recommends improving the context of the statement by clarifying settings (for instance,  the original paper discusses naturally arising grouping under the context of encoders or decoders) or by supporting this claim through citations.",hq3TxQK5cox
485,485,"- In Section 2 - Scope of Reproducibility, ""do do not use the model""; **Correction:** ""_do_ not use the model""",hq3TxQK5cox
486,486,"- In Section 3.1.1 - Linear Methods, ""SPCA reduce the dimentionality in a linear fashion""; **Correction:** ""SPCA reduce the _dimensionality_ in a linear fashion"" ",hq3TxQK5cox
487,487,"- In Section 3.1.2 - Non-Linear Methods , ""While Isomap could be seen as a extension of KPCA, LLE can be understood as a combination of PCAs ran on local neighborhoods.""; **Correction:** ""While Isomap could be seen as _an_ extension of KPCA, LLE can be understood as a combination of PCAs _run_ on local neighborhoods""",hq3TxQK5cox
488,488,"- In Section 3.3 - Datasets, ""sigmoidial Kernel-PCA procedure""; **Correction:** ""_sigmoidal_ Kernel-PCA procedure""",hq3TxQK5cox
489,489,"- In Section 3.5 - Experimental setup and computational requirements, ""Exprimentation was done on a Macbook""; **Correction:** ""_Experimentation_ was done on a Macbook""",hq3TxQK5cox
490,490,"- In Section 5 - Discussion, ""latent space will yield different different mappings""; **Correction:** ""latent space will yield _different_ mappings or inconsistently different mappings""",hq3TxQK5cox
491,491,"- In Section 5.1 - What was easy, ""set up was the obtaining the right version""; **Correction:** ""set up was _obtaining_ the right version""",hq3TxQK5cox
492,492,"- Optionally, formatting of references can be enhanced: Explaining groups of points in low-dimensional representations -> Explaining Groups of Points in Low-Dimensional Representations.",hq3TxQK5cox
493,493,"- In general, the reviewer is of the opinion that the report could be structured in a more organized fashion:",hq3TxQK5cox
494,494,"    - In Section 5.1 - What was easy, it's recommended to move the discussions pertaining to ""the hardest part"" to Section 5.2.",hq3TxQK5cox
495,495,    - Reduce the redundancy within the paper,hq3TxQK5cox
496,496,"For example, footnote 3 and footnote 1 essentially the same",hq3TxQK5cox
497,497,"    - In Figure 1, it might be helpful to reuse the author's notations of ````````````` $R_{initial}$, $R_{target}$ representations and  $X_{initial}$, $X_{target}$ preimages etc",hq3TxQK5cox
498,498,"    - For consistency, move all links to footnote",hq3TxQK5cox
499,499,(For example: link in Section 3.5),hq3TxQK5cox
500,500,    - Reduce the back and forth between the report and paper,hq3TxQK5cox
501,501,(For example: restate equation 9 in Section 3.4),hq3TxQK5cox
502,502,### RECOMMENDATIONS,hq3TxQK5cox
503,503,- Please anonymize your submission,hq3TxQK5cox
504,504,"Also, note that your summary **must fit** in the first page",hq3TxQK5cox
505,505,- The original authors of the paper consider a **similarity metric** across explanations which has not been examined in this reproducibility report,hq3TxQK5cox
506,506,Kindly address the same.,hq3TxQK5cox
507,507,"- Optionally, you could consider discussing the best set of hyperparameters from the experiments that resulted in reported results",hq3TxQK5cox
508,508,"- Optionally, you could also demonstrate causal structures of the data and the inability of the [Plumb et al model](https://openreview.net/forum?id=MFj70_2-eY1) to capture the same (specifically with differently structured data, higher dimensional data or varying cluster variance, shape etc) to further strengthen your argument regarding shortcomings - similar to Table 1 of original paper",hq3TxQK5cox
509,509,"- As per the [Machine Learning Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf), you could include statistics of the datasets in a tabular form and a tabulation of important results in the README file on [your github repository](https://github.com/giguru/fact-ai).",hq3TxQK5cox
510,510,"- As far as the creative insight on compressions is concerned, this space has been fairly explored before, [even in the context of compressed sensing](https://ermongroup.github.io/blog/uae/)",hq3TxQK5cox
511,511,"I hence recommend moving the discussion to applications or perhaps reviewing this discussion in context of a specific dataset or application like, in the case of [complementary biological representations](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02021-3)",hq3TxQK5cox
512,512,"- For explanations beyond translations (x′=Mx+δ), the reviewer *appreciates the effort taken in this new direction* but critically, the argument of ""incorporating rotation as an explanation that comes at the cost of explainability"" sounds delicate",hq3TxQK5cox
513,513,The authors have chosen to reproduce results related to 'Rigging the Lottery: Making All Tickets Winners',riCIeP6LzEE
514,514,The code was based on that of the authors of the original paper,riCIeP6LzEE
515,515,Results are consistent with the original ones for similar testbenches,riCIeP6LzEE
516,516,"However, tuning hyperprameters turned out to be challenging.",riCIeP6LzEE
517,517,"The reproducibility report is very well written, the problem is first formulated, the methodology is clearly presented, and the results are well described",riCIeP6LzEE
518,518,"The authors also indicate communications with those of the original paper, which is good to see.",riCIeP6LzEE
519,519,"Overall, this report is of very high quality and impact",riCIeP6LzEE
520,520,The authors reproduce the original paper from scratch,riCIeP6LzEE
521,521,The authors perform hyperparam sensitivity study,riCIeP6LzEE
522,522,The authors also perform extensive ablation experiments and uncover an interesting finding on the dependency of initialization,riCIeP6LzEE
523,523,"This project checks all components needed for a good reproducibility report, and I believe this is worthy of a journal submission",riCIeP6LzEE
524,524,I would like to thank the authors for their hard work!,riCIeP6LzEE
525,525,* Reproducibility Summary,riCIeP6LzEE
526,526,"  The report contains a well-written, concrete reproducibility summary",riCIeP6LzEE
527,527,The summary outlines the scope of the paper to reproduce the RigL algorithm from scratch by re-implementing the methodology on PyTorch,riCIeP6LzEE
528,528,"The summary also concisely highlights the major findings, where the report gets within 0.1% of the reported values in the original paper on CIFAR10.",riCIeP6LzEE
529,529,* Scope of reproducibility,riCIeP6LzEE
530,530,"  The report investigates several central claims from the original paper, Riga",riCIeP6LzEE
531,531,"The report contains an investigation of the sensitivity of hyperparameters, model ablation, and choice of initialization too.",riCIeP6LzEE
532,532,* Code: whether reproduced from scratch or re-used author repository.,riCIeP6LzEE
533,533,"  Authors re-implement the code of RigL from scratch in Pytorch, which was original written in Tensorflow",riCIeP6LzEE
534,534,"This makes the report extremely strong, as it helps to robustly validate the core claims of the original paper",riCIeP6LzEE
535,535,The authors provide their code in the supplementary material,riCIeP6LzEE
536,536,"It is also very much appreciated that the authors plan to release the training plots, which would be a strong contribution towards the understanding of RigL.",riCIeP6LzEE
537,537,* Communication with original authors,riCIeP6LzEE
538,538,"  Authors of the report communicated with the original authors successfully, who helped the authors clarify implementation and evaluation details.",riCIeP6LzEE
539,539,* Hyperparameter Search,riCIeP6LzEE
540,540,  Authors tune the hyperparameters with Optuna and carefully examine the impact of each hyperparam chosen in the original paper.,riCIeP6LzEE
541,541,* Ablation Study,riCIeP6LzEE
542,542,  The report goes a step further to perform an ablation study to investigate the impact of ERK initialization for a given target parameter count and training budget,riCIeP6LzEE
543,543,"The paper also performs experiments on redistribution, which is shown to help RigL with random initialization but not ERK",riCIeP6LzEE
544,544,This is a very interesting finding!,riCIeP6LzEE
545,545,* Discussion on results,riCIeP6LzEE
546,546,  The report contains ample discussion on the results,riCIeP6LzEE
547,547,"Primarily, they find the central claim of the original paper holds true",riCIeP6LzEE
548,548,RigL is also found to be fairly robust to the choice of hyperparameters,riCIeP6LzEE
549,549,"The report finds further evidence that the choice of initialization has a much greater impact on the final performance, and proposes interesting future directions for research.",riCIeP6LzEE
550,550,* Recommendations for reproducibility,riCIeP6LzEE
551,551,  The authors highly commend the original paper on their state of reproducibility and thank the original paper authors for their communication.,riCIeP6LzEE
552,552,* Overall organization and clarity,riCIeP6LzEE
553,553,  The paper is well organized and well written.,riCIeP6LzEE
554,554,This paper can be accepted without any modifications,UkIQrHoru_J
555,555,# Reproducibility Summary,UkIQrHoru_J
556,556,"The summary is provided, although given that the results indicate ALBERT's findings were not reproduced, I think that the ""what was difficult"" section could have been written with more details",UkIQrHoru_J
557,557,"In other words, if the findings of the paper are refuted there should be substantial effort demonstrating good faith reimplementation and/or reasons why the original findings are wrong",UkIQrHoru_J
558,558,It's also unclear whether the original findings were factually incorrect — they reported an incorrect number — or were overreaching in their claims.,UkIQrHoru_J
559,559,# Scope of Reproducibility,UkIQrHoru_J
560,560,The scope is two-fold:,UkIQrHoru_J
561,561,- Reproduce the ALBERT model from a pre-trained checkpoint.,UkIQrHoru_J
562,562,- Compare ALBERT performance to other baselines using more in-depth analysis.,UkIQrHoru_J
563,563,# Communication,UkIQrHoru_J
564,564,No communication was mentioned,UkIQrHoru_J
565,565,EDIT: The reproduction authors do explain why they chose not to communicate.,UkIQrHoru_J
566,566,# Hyperparameter Search,UkIQrHoru_J
567,567,I think this section could use substantial improvement,UkIQrHoru_J
568,568,"Fine-tuning tends to require very careful hyperparameter choice in order to prevent issues such as ""catastrophic forgetting""",UkIQrHoru_J
569,569,"Also, it seems the authors use off-the-shelf fine-tuned models and did not perform their own fine-tuning",UkIQrHoru_J
570,570,"This is fine, but much more due diligence should be done to explain how the checkpoints they use were created.",UkIQrHoru_J
571,571,# Ablation Study,UkIQrHoru_J
572,572,The reproduction authors did not do any re-training with different hyperparameters or other ablation,UkIQrHoru_J
573,573,Although they did run a new set of evaluation.,UkIQrHoru_J
574,574,# Discussion,UkIQrHoru_J
575,575,I think this paper would benefit from richer discussion,UkIQrHoru_J
576,576,"Specifically, it would be interesting to explain in detail why the evaluation methods used were chosen.",UkIQrHoru_J
577,577,# Recommendations,UkIQrHoru_J
578,578,The reproduction authors did not provide a recommendation to the original authors.,UkIQrHoru_J
579,579,# Results beyond the paper,UkIQrHoru_J
580,580,The reproduction authors were ambitious and set out to report many results and findings not in the original work,UkIQrHoru_J
581,581,"The line of inquiry is interesting — perhaps leaderboards like GLUE do not reflect all important model properties, such as fairness and robustness.",UkIQrHoru_J
582,582,# Overall Organization and Clarity,UkIQrHoru_J
583,583,Several parts of the paper were not clear,UkIQrHoru_J
584,584,"Even if this is a reproducibility paper, the authors should still summarize in detail the paper they are reproducing and any other techniques (such as TextAttack)",UkIQrHoru_J
585,585,"This would help, but other parts of the writing were a bit ambiguous",UkIQrHoru_J
586,586,For instance:,UkIQrHoru_J
587,587,"> However, ALBERT (10min 30s) took noticeably longer to produce the embeddings compared to BERT (8min 49s).",UkIQrHoru_J
588,588,"Does this mean that UMAP took 10m30s, or that ALBERT took 10m30s to output the embeddings used for visualization?",UkIQrHoru_J
589,589,"In addition, it would be helpful for the reader if the plots and figure were embedded more naturally in the text",UkIQrHoru_J
590,590,"It can be jarring when a crucial table takes a full page, and perhaps it could be broken into smaller sections and distributed throughout the text adjacent to its mentions.",UkIQrHoru_J
591,591,"This paper reported on a comparative study of BERT and ALBERT for self-supervised learning in the context of language representations, concluding that the authors were unable to reproduce empirical evidence for the claims presented by the selected paper",UkIQrHoru_J
592,592,"However, in their view ""communication with the original authors did not seem appropriate, as this study does not seek to fully re-implement the original paper."" I find that this is an unfair approach to the selected paper, as in order to act politically and respectfully among our scholarly community, contacting the first/corresponding author would have been the right first action here",UkIQrHoru_J
593,593,"As concluded in https://www.aclweb.org/anthology/W16-6110.pdf , https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5998676/ , and https://www.aclweb.org/anthology/R19-1089.pdf, for, successful reproducibility in natural language processing requires more than having access to data and code, and in general, contacting the authors leads to obtaining the same main findings.",UkIQrHoru_J
594,594,Authors replicated the work of Saxena et al,VFAwCMdWY7
595,595,[2020] on multi-hop question answering over knowledge graphs using KB embeddings,VFAwCMdWY7
596,596,Glad to see the core work was reproducible,VFAwCMdWY7
597,597,The added experiments on the use of other recent KB embeddings and use of transformer architectures for question embeddings were helpful to highlight the impact of other methods on the overall framework,VFAwCMdWY7
598,598,The absence of sufficient documentation and unavailability of hyperparameter values made the task difficult,VFAwCMdWY7
599,599,The relation matching experiment was not possible to be replicated due to similar issues,VFAwCMdWY7
600,600,"Overall, the work appeared to be solid and would be beneficial to the community",VFAwCMdWY7
601,601,  ,VFAwCMdWY7
602,602,The report reran the open-sourced codes from the original paper and managed to reproduce most of the results presented in the original paper,VFAwCMdWY7
603,603,The authors also experimented with different question encoder and achieved an improvement over the original results.,VFAwCMdWY7
604,604,"The authors also cleaned up the codes, added comments, and provided command line functions",VFAwCMdWY7
605,605,The paper also reported hardware requirement and experiment run time.,VFAwCMdWY7
606,606,It's glad to see that the authors try additional models beyond the original implementation,VFAwCMdWY7
607,607,"However, the authors mostly reused the default hyper-parameters, as well as the open-sourced codes from the original paper",VFAwCMdWY7
608,608,"This challenge recommends authors to either re-implement the original codes, or conduct hyper-parameter sweep",VFAwCMdWY7
609,609,This paper failed to follow the instructions carefully,VFAwCMdWY7
610,610, ,VFAwCMdWY7
611,611,To improve the report:,VFAwCMdWY7
612,612,"Could you please provide more details about the ""relation matching"" module?  It's claimed that this is the key issue that causes 22.5% drop in performance in MetaQA KG-Full 3 hop questions, but it's not discussed in the report at all",VFAwCMdWY7
613,613,I think briefly introducing this module will make this report stand alone and help reader understand why it causes a huge drop in performance.,VFAwCMdWY7
614,614,"In this report, the authors investigate the ACL 2020 paper by Saxena et al",VFAwCMdWY7
615,615,on knowledge base embeddings,VFAwCMdWY7
616,616,Reproducibility Summary : Major findings are included in the summary.,VFAwCMdWY7
617,617,Scope of reproducibility - Scope was clearly delineated and adhered to.,VFAwCMdWY7
618,618,"Code: Reused author repository, but made some changes to improve the code (enhance modularity) and make it easy to swap in pertained models for the question and knowledge graph embedding modules.",VFAwCMdWY7
619,619,Communication with original authors: There was communication with the first author of the paper (virtual meetings),VFAwCMdWY7
620,620,"Efforts were made to try to address the parts of the paper that could not be reproduced, though the relation matching part of the code wasn't used which plays a part in the gap.",VFAwCMdWY7
621,621,Hyperparameter Search: No hyper parameters sweep,VFAwCMdWY7
622,622,Used default values,VFAwCMdWY7
623,623,However they did tune hyper-params in their extended experiments,VFAwCMdWY7
624,624,Things were especially difficult as little information about hyperparam values was provided in the paper and by the authors.,VFAwCMdWY7
625,625,Ablation Study: Did do ablations over substituting the knowledge graph and question embedding modules.,VFAwCMdWY7
626,626,Discussion on results: Descriptions on what was easy and difficult were provided,VFAwCMdWY7
627,627,"The scope of the reproduction was to reproduce results in the original paper which they were able to get close on one dataset (MetaQA) but remain far apart on the second (WebQSP) dataset, 11.7 percent absolute despite correspondence with the authors (although the reason is lack of relation matching)",VFAwCMdWY7
628,628,The authors did extra work to test different knowledge graph embedding models and different transformer models for question embeddings,VFAwCMdWY7
629,629,They found improvements by using TuckER and Sentence Bert respectively.,VFAwCMdWY7
630,630,Overall organization and clarity: Paper is clear and organized and easy to read,VFAwCMdWY7
631,631,"It feels complete to me as well, showing what can be reproduced and what cannot - although more discussions on where the divergence for QebQSP could've been provided and the experiments without the relation matching make it unclear how close reproduction was performed for WebQSP.",VFAwCMdWY7
632,632,"The submission did a good job in communicating with the authors, and because of that it has identified a missing component (a JSON meta-data generator) and a discrepancy between code and paper on the neural network design",4tyWL6P08yY
633,633,This enabled the submission to reproduce the running pipeline of the model mostly based on the original authors' public code.,4tyWL6P08yY
634,634,"However, the submission did not report any numerical results that can be used to judge the reproducibility of the original paper",4tyWL6P08yY
635,635,It only quantitatively states that the produced results seem good with high confidence scores,4tyWL6P08yY
636,636,"The purpose of the reproducibility is to verify such numerical results, therefore the submission is a clear rejection.",4tyWL6P08yY
637,637,"That said, I believe if given more time, the submission could become a good reproduction of the original paper with clear numerical results, and perhaps even some additional ablation study.",4tyWL6P08yY
638,638,This reproducibility report is about TabStructNet which was published in ECCV 2020,4tyWL6P08yY
639,639,"Authors of this report provide summary of report, scope of reproducibility and communicated with original author of the TabStructNet.",4tyWL6P08yY
640,640,"Basically, there are available codes from the original authors, so this report tried to discover missing points in the original paper",4tyWL6P08yY
641,641,"Also, they experimented with missing modules such as the post-processing module, JSON generation module, and FPN",4tyWL6P08yY
642,642,"Therefore, it is important to perform plenty of “hyperparameter search” and “ablation” to analyze them.",4tyWL6P08yY
643,643,"However, ablation studies and hyperparameter search are very limited in this report",4tyWL6P08yY
644,644,"Also, discussion on results is not enough",4tyWL6P08yY
645,645,They just provide several examples and short conclusions.,4tyWL6P08yY
646,646,"Line4, there is a typo: EECV 2020 → ECCV 2020",4tyWL6P08yY
647,647,"The paper reproduces TabStructNet, an approach to automatically translate tables to XML format",4tyWL6P08yY
648,648,The authors of the submission make use the provided code repository of the original authors and document the required changes and extensions to get the code running as well as available divergences of the implementation with respect to the paper.,4tyWL6P08yY
649,649,"However, the paper does not reproduce a representative set of results of the original paper, as only exemplary inputs and outputs of the model are presented",4tyWL6P08yY
650,650,"While it provides useful information to the reader, as the available code repository is analysed in terms of functional reusability, the paper misses to analyse the quality of the implementation on original or representative datasets with original metrics",4tyWL6P08yY
651,651,There is also no ablation study present in the submission,4tyWL6P08yY
652,652,The authors have chosen to reproduce results related to Re-Hamiltonian Generative Networks,Zszk4rXgesL
653,653,"The code was built from scratch in Pytorch, based on the description of the original paper",Zszk4rXgesL
654,654,"Results are consistent with the original ones for similar testbenches, but show sub-optimal behavior on new data",Zszk4rXgesL
655,655,It is also found that results are highly dependent on hyper-parameter tuning.,Zszk4rXgesL
656,656,"The reproducibility report is very well written, the problem is first formulated, the methodology is clearly presented, and the results are well described",Zszk4rXgesL
657,657,"The authors also indicate communications with those of the original paper, which is good to see.",Zszk4rXgesL
658,658,"The report clearly summarizes the problem statement of the original paper ""Hamiltonian Generative Networks"" (HGN) as scope of reproducibility: learn a Hamiltonian dynamics from a image sequence",Zszk4rXgesL
659,659,"As no source code was available, the authors reimplemented the program for the experiments from scratch based on the description in the original paper and questions about implementation details answered by the original authors.",Zszk4rXgesL
660,660,The code is available on GitHub as a Python program based on Pytorch,Zszk4rXgesL
661,661,It is properly documented and cleanly written,Zszk4rXgesL
662,662,All dynamical systems from the original paper have been implemented and the experiments replicated,Zszk4rXgesL
663,663,"Results are comparable with the original paper in autoencoder mode, where training happens with the full input sequence",Zszk4rXgesL
664,664,"But using only the first five frames for training the HGN leads to significantly worse results for two systems (mass-spring and pendulum), but not for the other two (two-body and three-body)",Zszk4rXgesL
665,665,"This difference is not discussed, only the average over all four systems",Zszk4rXgesL
666,666,"I suspect that this is caused by the choice of the Lagrage multiplier, as the authors did not obtain sufficient information about the automatic optimization of this hyperparameter.",Zszk4rXgesL
667,667,"Additionally, the authors present successful results using other integrators and extra dynamical systems",Zszk4rXgesL
668,668,They also show that the calculation of the derivatives for the Hamiltonian equations of motion by backpropagation can be replaced by a network which learns them directly.,Zszk4rXgesL
669,669,The discussion clearly indicates the state of reproducibility and summarizes the easy and difficult parts of that task,Zszk4rXgesL
670,670,"There were no explicit recommendations for better reproducibility, but the authors describe, what information missing in the original paper could have been helpful.",Zszk4rXgesL
671,671,The reproducibility report is well written,Zszk4rXgesL
672,672,I recommend to add a discussion about the differences of reproducibility between the dynamical systems and to make an explicit recommendation to the original authors for improving reproducibility.,Zszk4rXgesL
673,673,"Arguably, learning how physical systems work is one of the key unsolved problems in machine learning",Zszk4rXgesL
674,674,The present report investigates an attempt to solve such a problem with significant results but one that doesn't reveal a lot about it's inner workings,Zszk4rXgesL
675,675,"The author(s) do a great job in reproducing, replicating, and presenting the challenges",Zszk4rXgesL
676,676,The report also communicates the core problem very well and their contact with the authors of the original paper is transparent,Zszk4rXgesL
677,677,What would improve the paper:,Zszk4rXgesL
678,678,- testing hyperparameter sensitivity would be useful,Zszk4rXgesL
679,679,using exactly as the ones indicated is good for replication but not enough to understand the scope of the sensitivity of the model,Zszk4rXgesL
680,680,"- testing extra environments is a great addition, and I think it would also benefit a lot from understanding failing cases a bit deeper",Zszk4rXgesL
681,681,It seems the authors are aware of possible improvements and some of them can be standalone work by themselves.,Zszk4rXgesL
682,682,====,Zszk4rXgesL
683,683,trivial typos:,Zszk4rXgesL
684,684,line 45 an —> and,Zszk4rXgesL
685,685,line 214 or —> our,Zszk4rXgesL
686,686,The reproducibility report of the Double Hard Debias:Tailoring Word Embeddings for Gender,FoazbSYjXoc
687,687,Bias Mitigation (Wang et al) clearly identifies the three claims made by the author and sets the motivation to reproduce these claims,FoazbSYjXoc
688,688, The methodology used is also clearly outlined and there was communication with the original authors to ensure that experiments were performed without deviation from the original,FoazbSYjXoc
689,689,The results obtained on reproduction match two of the three claims made by the original authors,FoazbSYjXoc
690,690,The reproducibility reports also attempts to find reasons why one experiment did not have identical results reported on the original papers but these hypotheses were not validated.,FoazbSYjXoc
691,691,Authors have presented a well-organized reproducibility report,FoazbSYjXoc
692,692,"Scope of reproducibility was categorized into clear and independent claims, which were representative of and covers the extent and novelty of the original source paper being reproduced",FoazbSYjXoc
693,693,"To verify the original results, claims were further mapped to the corresponding qualitative and quantitative findings of the source work, thereby making the subsequent sections easy to follow",FoazbSYjXoc
694,694,"Not all the results in the source paper has been attempted to be replicated, however, the reasons to exclude them have also been explicitly mentioned (either computational constraint or the results substantiating the claims which are also corroborated by other presented results).",FoazbSYjXoc
695,695,Authors have used the source code made public by the authors of the original paper,FoazbSYjXoc
696,696,"While the codes were modified to fit in a pipeline to run them, as mentioned by the authors, they also expanded the set of WEAT scores to cover all the metrics reported in the source paper",FoazbSYjXoc
697,697,"Communication with original authors were also established, particularly regarding the deviations of results from the source paper",FoazbSYjXoc
698,698,"Although the differences could not be resolved, discussions on reasons for the same is also briefly included along with the observation that the results qualitatively holds, albeit to a lesser extent",FoazbSYjXoc
699,699,"For example, substantiating claim 2, the gender classification scores reported in Table 2 of the reproducibility report for top K gendered words were observed to be less when compared with Glove embeddings but were still greater than the reported values in the original work",FoazbSYjXoc
700,700,"Similarly, to substantiate claim 3, original and reproduced performances on word analogy and concept categorization benchmark datasets are presented in Table 3 of the reproducibility report and small differences are observed on concept categorization dataset.",FoazbSYjXoc
701,701,"Although it is mentioned as well as noted that the results specific to the claims in the scope of reproducibility will be reported, it is observed that Figure 2 of the original work pertaining to claim 1 in the reproducibility report regarding identification of word-frequency direction(s) among word embeddings' principal components is not included in the report",FoazbSYjXoc
702,702,Of further note is the omission of reproducibility of scores and results pertaining to Hard (Debiased) Glove embeddings in tabular results i.e,FoazbSYjXoc
703,703,"Table 1,2 and 3",FoazbSYjXoc
704,704,This is in contrast with the outlined scope of reproducibility where it was explicitly mentioned that the current work's focus is on comparing the Double-Hard Debiased embeddings against the Hard Debiased embeddings,FoazbSYjXoc
705,705,"Thus, this doesn't necessarily reproduces the results qualitatively, particularly when there are significant differences",FoazbSYjXoc
706,706,"For example, claim 2 in the reproducibility report is regarding reduced gender-bias in moving from Hard Debias to Double-Hard Debias and the same is also claimed in the original paper",FoazbSYjXoc
707,707,"In the original paper, this is demonstrated through clustering accuracy/gender classification scores",FoazbSYjXoc
708,708,"In the reproducibility report, while Double-Hard Debias results have been reported and are found to be worse when compared to source paper, reporting the Hard Debias results would have substantiated or refuted the gender bias reduction in qualitative sense (although still differing) when moving towards Double-Hard Debiasing.",FoazbSYjXoc
709,709,"Furthermore, the results, associated discussions particularly when calling out observed discrepancies/differences needs to be highlighted properly",FoazbSYjXoc
710,710,"For example in Table 3 in the reproducibility report, while differences in AP and Battig categorization tests have been highlighted and reported, differences for other tests, some of which are even more pronounced, are still marked as matching.",FoazbSYjXoc
711,711,"Finally, discussion on results and difficulty in reproducibility can be made more thorough, specifically, set of choices that could be made and lead to difference in results",FoazbSYjXoc
712,712,"For example, for results pertaining to Table 3 where classification scores on top K gendered words are reported, the original work mentions identifying such words based on cosine similarity with gender direction in the original GloVe embedding space",FoazbSYjXoc
713,713,"Similarly, it is mentioned that random seeds could also be one of the factors causing differences in the results",FoazbSYjXoc
714,714,"Discussion and documentation on the gender direction used, range of random seeds used (given that experiments in the scope were not too computationally expensive) are further needed to cover the extent of reproducibility.",FoazbSYjXoc
715,715,"In this report, the authors investigate the ACL 2020 paper by Saxena et al",FoazbSYjXoc
716,716,on knowledge base embeddings,FoazbSYjXoc
717,717,Reproducibility Summary : Major findings are included in the summary.,FoazbSYjXoc
718,718,Scope of reproducibility - Scope was clearly delineated and adhered to.,FoazbSYjXoc
719,719,"Code: Reused author repository, but made some changes to improve the code (enhance modularity) and make it easy to swap in pertained models for the question-answer module.",FoazbSYjXoc
720,720,Communication with original authors: There was communication with the first author of the paper (virtual meetings),FoazbSYjXoc
721,721,"Efforts were made to try to address the parts of the paper that could not be reproduced, though the relation matching part of the code wasn't used which plays a part in the gap.",FoazbSYjXoc
722,722,Hyperparameter Search: No hyper parameters sweep,FoazbSYjXoc
723,723,Used default values,FoazbSYjXoc
724,724,However they did tune hyper-params in their extended experiments,FoazbSYjXoc
725,725,Things were especially difficult as little information about hyperparam values was provided in the paper and by the authors.,FoazbSYjXoc
726,726,Ablation Study: No ablations,FoazbSYjXoc
727,727,Discussion on results: Descriptions on what was easy and difficult were provided,FoazbSYjXoc
728,728,"The scope of the reproduction was to reproduce results in the original paper which they were able to get close on one dataset (MetaQA) but remain far apart on the second (WebQSP) dataset, 11.7 percent absolute despite correspondence with the authors (although the reason is lack of relation matching due ",FoazbSYjXoc
729,729,The authors did extra work to test different knowledge graph embedding models and different transformer models for question embeddings,FoazbSYjXoc
730,730,They found improvements by using TuckER and Sentence Bert resprecively.,FoazbSYjXoc
731,731,Overall organization and clarity: Paper is clear and organized and easy to read,FoazbSYjXoc
732,732,"It feels complete to me as well, showing what can be reproduced and what cannot - although more discussions on where the divergence for QebQSP could've been provided and the experiments without the relation matching make it unclear how close reproduction was performed for WebQSP.",FoazbSYjXoc
733,733,- Reproducibility Summary: the paper includes the reproducibility summary on the first page,LI1n_od-aEq
734,734,"The summary is clear, well written, and major findings (i.e",LI1n_od-aEq
735,735,successful replication within a good margin) are incorporated in the summary.,LI1n_od-aEq
736,736,- Scope of reproducibility: it is well described: specifically the authors investigate the roles of different BERT layers in the context of Reading Comprehension based Question Answering (RCQA),LI1n_od-aEq
737,737,The authors list all the reproduced/verified claims from the original paper.,LI1n_od-aEq
738,738,- Code: the authors re-implement the original approach since the official code is not available,LI1n_od-aEq
739,739,The code folder contains a well-documented readme file and requirements are specified.,LI1n_od-aEq
740,740,- Communication with original authors: the authors had email interactions with the original authors,LI1n_od-aEq
741,741,The questions and answers are included in the supplementary material.,LI1n_od-aEq
742,742,"- Hyperparameter Search: the original authors did not provide any code, thus the authors could not re-use it for hyperparameter search",LI1n_od-aEq
743,743,"The authors report the selected hyperparameters, but they do not mention whether they did a grid search on the hyperparameter space.",LI1n_od-aEq
744,744,"- Ablation Study: there is no ablation study, but it does not make sense to have it.",LI1n_od-aEq
745,745,"- Discussion on results: the authors obtain F1 scores that are comparable to those in the original paper, but their model slightly underperforms with respect to the original one",LI1n_od-aEq
746,746,The authors do not mention why the results are different and what are the plausible causes,LI1n_od-aEq
747,747,"Apart from this, the experimental results are well presented and compared with the original results",LI1n_od-aEq
748,748,The authors describe easy parts and challenges in reproducing the original paper.,LI1n_od-aEq
749,749,"- Recommendations for reproducibility: the authors do not explicitly mention any recommendations for the original authors, but they do describe missing details that were important for the reproducibility.",LI1n_od-aEq
750,750,- Results beyond the paper: the authors present some results beyond the original paper: they implemented Integrated Gradients (IG) in a different way and extended the original analysis of Jensen-Shannon Divergence heatmaps with different cut-offs.,LI1n_od-aEq
751,751,- Overall organization and clarity: the paper is clear and well organized,LI1n_od-aEq
752,752,I list some typos in the following:,LI1n_od-aEq
753,753,    - Line 45: t-SNE -> please expand the acronym at least when first mentioned;,LI1n_od-aEq
754,754,    - Line 81: don’t -> do not,LI1n_od-aEq
755,755,    - Line 149: we represent show,LI1n_od-aEq
756,756,Overall evaluation:,LI1n_od-aEq
757,757,Pros:,LI1n_od-aEq
758,758,- The paper is clear and well written;,LI1n_od-aEq
759,759,- The authors could reproduce (within a decent margin) the results of the original paper;,LI1n_od-aEq
760,760,- The authors performed some extra analyses beyond the original paper.,LI1n_od-aEq
761,761,Cons:,LI1n_od-aEq
762,762,- Some experimental results could have been explained with more details.,LI1n_od-aEq
763,763,"The reproduced work of ""towards interpreting BERT for reading comprehension QA"" is well organized and clearly explained",LI1n_od-aEq
764,764,The authors rebuilt the experiment from scratch due to the availability of the original paper code,LI1n_od-aEq
765,765,The implementation and scope align with the original paper,LI1n_od-aEq
766,766,"A large part of the results matched, and the difference was explained which was mostly due to computation limit and sample difference",LI1n_od-aEq
767,767,"Overall, I'd recommend accepting.",LI1n_od-aEq
768,768,Thank you for this carefully written paper that focused on  interpreting BERT for reading comprehension questions and answers (Q&A),LI1n_od-aEq
769,769,"It was successful in reproducing the main findings of its chosen paper, without an originally released codebase to build on",LI1n_od-aEq
770,770,"Obtaining this outcome, required, as expected regular interaction with the first author of the chosen paper",LI1n_od-aEq
771,771,"My major comments or suggested improvements relate to documenting the  reading comprehension Q&A; the authors excelled in reporting on the solution (processing methods, evaluation methods, processing outcomes), and even had findings beyond the chosen paper",LI1n_od-aEq
772,772,"However, the problem of  reading comprehension Q&A, its relevant literature, and implications to reading comprehension QA practice were not addressed",LI1n_od-aEq
773,773,"More over, I would have expected to see a statement relating human subject ethics in the paper, although its data originated from a previously openly released corpus",LI1n_od-aEq
774,774,This contradiction between the (currently weak) domain substance and (currently outstanding) computing contributions made me rate the paper as Marginally above acceptance threshold.,LI1n_od-aEq
775,775,The paper is a valuable check of the original paper.,IU5y7hIIZqS
776,776,A few corrections in formatting and language are needed.,IU5y7hIIZqS
777,777,"span should not be italic, but rather {\rm span",IU5y7hIIZqS
778,778,A blank is missing in „fit(more“,IU5y7hIIZqS
779,779,„The number of experiments performed where realtively [sic] less“ -> „The number of experiments was relatively small“,IU5y7hIIZqS
780,780,„table[2]“ -> „Table 2“,IU5y7hIIZqS
781,781,„approximators.[table[5]]“ -> „approximators (see Table 5).“,IU5y7hIIZqS
782,782,"I think, „5e-5“ should be replaced by 5\cdot 10^{-5}",IU5y7hIIZqS
783,783,"Reproducibility Summary : The report includes this summary as the first page, which contains major findings",IU5y7hIIZqS
784,784,"Scope of reproducibility: The report concisely and clearly states the scope, and follows it",IU5y7hIIZqS
785,785,"Code: As mentioned in the report that the original code was closed sourced, the authors reproduced the code for several sets of experiments",IU5y7hIIZqS
786,786,"Communication with original authors: The authors of this report contacted the original authors for multiple queries, and the original authors replied to these queries",IU5y7hIIZqS
787,787,"However, I am not sure whether the original authors have evaluated the results in this report",IU5y7hIIZqS
788,788,Hyperparameter Search: It seems that the authors of the report mostly used from the hyperparameters in the original paper.,IU5y7hIIZqS
789,789,Ablation Study: The ablation study is not comprehensive,IU5y7hIIZqS
790,790,"Discussion on results: The report discusses the state of reproducibility of the original paper, and mentions the easy parts and difficult parts",IU5y7hIIZqS
791,791,"More specifically, for the case with linear function approximation, several results in the original paper could not be reproduced.",IU5y7hIIZqS
792,792,Recommendations for reproducibility: It seems the report does not discuss on how the original paper can improve its reproducibility,IU5y7hIIZqS
793,793,"Results beyond the paper: The report tries to show the results in Table [5] using a different evaluation technique, but it seems that the new results are not complete.",IU5y7hIIZqS
794,794,"Overall organization and clarity: The organization is ok, but there are several grammatical issues, e.g., several periods are missing.",IU5y7hIIZqS
795,795,**Reproducibility summary:** The summary is overall clear and well summarizes the authors' reproducibility effort,0Z5rHjU2mfX
796,796,**Scope of reproducibility:** The authors state four concrete claims they aim to verify,0Z5rHjU2mfX
797,797,"However, Claim 2 (""The deliberately texture biased model g also reduces cross bias"") was confusingly stated, as *g* is a ""biased"" model from a bias characterizing model class, that is designed to suffer from more bias",0Z5rHjU2mfX
798,798,"I believe the authors meant to say ""using *g* to train *f* helps reduce cross bias.""",0Z5rHjU2mfX
799,799,**Code:** The authors used the original authors' code with minor modifications,0Z5rHjU2mfX
800,800,The authors made their version of the code available with appropriate documentation.,0Z5rHjU2mfX
801,801,"**Communication with the original authors**: The authors communicated with the original authors and received clarifications on the HEX, RuBi, and Learned-Mixin implementations, and WNIDs for constructing the 9-Class ImageNet dataset",0Z5rHjU2mfX
802,802,"However, I wish the authors would have done additional communication with the original authors on the following fronts: (1) Even after receiving clarifications on HEX and Learned-Mixin, the authors write that they were unable to compare them to ReBias",0Z5rHjU2mfX
803,803,"It would have been nice if the authors did further communications with the original authors to get HEX and Learned-Mixin (as well as StylisedImageNet, another work compared in the paper although the authors do not discuss it) working",0Z5rHjU2mfX
804,804,(2) The authors discuss that they were unable to train ReBias on 9-Class ImageNet,0Z5rHjU2mfX
805,805,"As this is the main dataset of the paper, I wish the authors had reached out to the original authors to resolve the difficulties.",0Z5rHjU2mfX
806,806,**Hyperparameter search:** The authors didn't conduct a hyperparameter search,0Z5rHjU2mfX
807,807,"However, they were forced to reduce the batch size from 128 to 16 due to their limited computational resources, and tried other learning rates and step sizes to counteract the smaller batch size.",0Z5rHjU2mfX
808,808,**Ablation study**: The authors didn't conduct any ablation studies.,0Z5rHjU2mfX
809,809,"**Discussion on results:** On Biased MNIST, I'm confused by the way the authors presented their results because their results are not within 1% of the original results in the paper",0Z5rHjU2mfX
810,810,I suggest the authors to double check this claim and include a side-by-side comparison with the original results,0Z5rHjU2mfX
811,811,"On 9-Class ImageNet, the authors state that they succesfully trained the Vanilla and Biased model, while they failed to train the ReBias model due to exploding gradients",0Z5rHjU2mfX
812,812,Still I wish the authors had included their results in the report so that the readers can get a sense of how different the results are,0Z5rHjU2mfX
813,813,**Recommendations for reproducibility:** The authors don't provide explicit recommendations to the original authors,0Z5rHjU2mfX
814,814,"However, their descriptions of the difficulties they ran into (e.g",0Z5rHjU2mfX
815,815,"insufficient explanation of the implementation of prior works, insufficient explanation of the construction of 9-Class ImageNet, gradient explosion problem) may help the original authors improve the reproducibility of the work.",0Z5rHjU2mfX
816,816,"**Results beyond the paper**: For Biased MNIST, the authors conduct edadditional experiments with $\rho=0.98, 0.95, 0.9, 0.85$ and make an interesting observation that the Vanilla model beats ReBias for lower $\rho$, providing an enlarged picture",0Z5rHjU2mfX
817,817,"Furthermore, they posed two very interesting questions (bottom of page 6) although the authors did not attempt to answer them.",0Z5rHjU2mfX
818,818,"**Overall organization and clarity:** Overall, the report was organized and clearly written",0Z5rHjU2mfX
819,819,"However, there are several typos and grammatical errors that I hope the authors address in their revision.",0Z5rHjU2mfX
820,820,**Minor comments:** (1) Figure 1 was helpful in understanding $\rho$ in the Biased MNIST experimental setup,0Z5rHjU2mfX
821,821,(2) Tables 1-2 are unnecessary as Tables 3–4 subsume them,0Z5rHjU2mfX
822,822,(3) Figure 2 was not very helpful in understanding the change in Vanilla/ReBias performance because the two lines are almost always on top of each other,0Z5rHjU2mfX
823,823,"(4) The authors made a typo in the learning objective for g in Algorithms 1, 3, and 4",0Z5rHjU2mfX
824,824,"**Summary**: Overall, the authors made good effort to reproduce the original work and provided additional insights which I appreciate",0Z5rHjU2mfX
825,825,The authors successfully reproduced Biased MNIST results and failed to reproduce 9-Class ImageNet results,0Z5rHjU2mfX
826,826,"However, because they do not report results for 9-Class ImageNet, which is the main dataset studied in the original paper, I found the evidence provided in this report insufficient for assessing the reproducibility of the original work",0Z5rHjU2mfX
827,827,"For the assessment, I suggest the authors to communicate with the original authors to resolve the difficulties with 9-Class ImageNet and/or try to reproduce action recognition results.",0Z5rHjU2mfX
828,828,*Problem statement: ,0Z5rHjU2mfX
829,829,"The paper clearly states the reproducing details, together with the detailed results and analysis.",0Z5rHjU2mfX
830,830,*Presentation: ,0Z5rHjU2mfX
831,831,The paper is well-organized and well-written.,0Z5rHjU2mfX
832,832,*Communication with original authors:,0Z5rHjU2mfX
833,833,The authors had some communication with the original authors.,0Z5rHjU2mfX
834,834,*Code: ,0Z5rHjU2mfX
835,835,The code is available on GitHub and can be reproduced,0Z5rHjU2mfX
836,836,*Recommendations for reproducibility:,0Z5rHjU2mfX
837,837,The authors provided useful comments for reproducing the original paper,0Z5rHjU2mfX
838,838,I have read the code and found those comments are consistent with the provided codes.,0Z5rHjU2mfX
839,839,*A few concerns ,0Z5rHjU2mfX
840,840,**Some minor typos:,0Z5rHjU2mfX
841,841,** It will be even better if the authors can provide a simple illustration on the two major algorithms,0Z5rHjU2mfX
842,842,The illustration would help others to follow this report,0Z5rHjU2mfX
843,843,** It will be better if the authors can provide a detailed readme or other instructions for the code,0Z5rHjU2mfX
844,844,"Currently, although the codes are well-organized, it still requires much time for users to debug some deatils",0Z5rHjU2mfX
845,845,"** Some minor typos: Section 5.1.2, present -> presented",0Z5rHjU2mfX
846,846,"This paper aims to reproduce the results of (Pang et.al,19) which present Max-Mahalanobis center (MMC) loss to defending the adversarial attack.",67Q9tnozPe
847,847,I appreciate the empirical efforts of the authors,67Q9tnozPe
848,848,"However, investigating the adversarial robustness does not only require one to reproduce the number of the original paper but also needs more thinking of using a stronger attack to give a 'true evaluation' of the proposed method.",67Q9tnozPe
849,849,"As clearly stated in https://arxiv.org/pdf/2002.08347.pdf,  Pang et.al,19 doesn't succeed in defending stronger adversarial attacks",67Q9tnozPe
850,850,Then I give a clear reject.,67Q9tnozPe
851,851,Summary: The summary is clear and highlights the major results of the reproduction.,67Q9tnozPe
852,852,Scope of Reproducibility: clearly stated and adhered to,67Q9tnozPe
853,853,Code: re-used author’s code,67Q9tnozPe
854,854,Communications with Original Authors: Done fairly,67Q9tnozPe
855,855,Hyper-parameter search: code reuses the author’s hyper-parameters ,67Q9tnozPe
856,856,Ablation study: looks at adversarial trading and optimizer selection,67Q9tnozPe
857,857,"Discussion on results: relatively little discussion about difficulties of reproduction, but does describe difficulties of reproduction",67Q9tnozPe
858,858,Recommendations: none given,67Q9tnozPe
859,859,"Results beyond paper: additional implementation of methods in Python, some initial results validating MMC’s ability to train models that reject out-of-distribution inputs",67Q9tnozPe
860,860,"However, the experiment is only on a single image, so does not provide much value",67Q9tnozPe
861,861,Overall organization and clarity: the paper has a lot of clarity and grammatical issues,67Q9tnozPe
862,862,I’ve documented a few below,67Q9tnozPe
863,863,I also recommend substantially updating the figure captions so that the figures are relatively self-contained and comprehensive.,67Q9tnozPe
864,864,2.2 I found this section and the notation presented quite hard to follow,67Q9tnozPe
865,865,I recommend looking at what math is actually needed to set up remarks 2-4 and then providing more explicit definitions of those terms,67Q9tnozPe
866,866,"For example, l.89 references $N_{k, \bar{k}}$ before the definition",67Q9tnozPe
867,867,"Consider dropping the $D_{k, \bar{k}}$ notation entirely and directly defining the term in l.89 (e.g., “is proportional to the number of points in class k when $\bar{k}$ has the highest prediction amongst other classes",67Q9tnozPe
868,868,"Call this $N_{k, \bar{k}}$”).",67Q9tnozPe
869,869,2.3 I think this would be clearer if the way to compute the centers (and some of the intuition) were discussed first,67Q9tnozPe
870,870,"For example, before l.105,  consider including some of the content of remark 5.",67Q9tnozPe
871,871,Minor comments:,67Q9tnozPe
872,872,L.5 define SCE before using an acronym,67Q9tnozPe
873,873,L.59 “we then present demerits of MMC loss” — > “we then present the merits of the MMC losses”,67Q9tnozPe
874,874,L.78 This section has several clarity issues,67Q9tnozPe
875,875,"In particular, consider providing more context on how l.75-77 leads to l.78 as a conclusion.",67Q9tnozPe
876,876,L.94 “function in during the training procedure” — non-grammatical,67Q9tnozPe
877,877,L.97 “tend to spread over the space in an sparsely” — non-grammatical,67Q9tnozPe
878,878,L.127 non-grammatical (NG),67Q9tnozPe
879,879,L.131 “also not losing out the high accuracies” — NG,67Q9tnozPe
880,880,L.133 “supervisor inappropriate supervisory signals” —NG,67Q9tnozPe
881,881,L.135 “this section roughly” — seems that this comes from the template?,67Q9tnozPe
882,882,"L.201 “we validate the merits of MMC, that is” — run-on sentence",67Q9tnozPe
883,883,"The authors attempted to reproduce the results from the paper ""Contextualizing Hate Speech Classifiers with Post-hoc Explanation"" by Kennedy et al",wIgGMxXAYS
884,884,2020.,wIgGMxXAYS
885,885,+ The authors attempted to reproduce several claims from the original paper,wIgGMxXAYS
886,886,+ they address the fact that their computational set up was different and in fact slower than that reported in the original paper,wIgGMxXAYS
887,887,+The authors provide an explanation about the models in Section 3 before describing their reproducibility experiments,wIgGMxXAYS
888,888,- It was not clear if or how the authors reproduced the train/val/test sets from the original paper,wIgGMxXAYS
889,889,"In one case, they state that ""Train and test parts were arbitrarily produced for Stormfront sentences"" and similarly for the other datasets",wIgGMxXAYS
890,890,"- In addition, while the original paper reports performance measures along with confidence intervals (e.g",wIgGMxXAYS
891,891,"57.76 ± 3.9 for precision), the current paper reports the performance measure",wIgGMxXAYS
892,892,So for the claims that are stated as not reproducible  it was not clear whether the performance measures were within the confidence intervals.,wIgGMxXAYS
893,893,"- minor issues: in several places the ""instruction text"" seems to mistakenly remain",wIgGMxXAYS
894,894,e.g,wIgGMxXAYS
895,895,lines 145 -147,wIgGMxXAYS
896,896,"""Provide information on computational requirements for each of your experiments",wIgGMxXAYS
897,897,"For example, the number of",wIgGMxXAYS
898,898,147 CPU/GPU hours and memory requirements,wIgGMxXAYS
899,899,"You’ll need to think about this ahead of time, and write your code in a",wIgGMxXAYS
900,900,148 way that captures this information so you can later add it to this section,wIgGMxXAYS
901,901,"""",wIgGMxXAYS
902,902,- some minor typos in the paper that should be corrected prior to acceptance,wIgGMxXAYS
903,903,e.g,wIgGMxXAYS
904,904,"""Sromfront"" line 166.",wIgGMxXAYS
905,905,The authors evaluate both the method and validity of the results in their reproduction of Kennedy et al (2020),wIgGMxXAYS
906,906,Results of  some of the experiments could be replicated,wIgGMxXAYS
907,907,But some others did not yield comparable results with the original paper.,wIgGMxXAYS
908,908,The authors run the method on a new dataset as well,wIgGMxXAYS
909,909,"However, it is not clear how a baseline method would perform on this new dataset.",wIgGMxXAYS
910,910,The code is not provided! “The code was easy to run and  allowed us to verify the correctness of our re-implementation.” But all of your experiments on the code provided by Kennedy et al (2020) (this one stated in the paper as well),wIgGMxXAYS
911,911,"If you are not using or providing your code, why do you say this? How can we verify this point? ",wIgGMxXAYS
912,912,"There are many copy paste from the original paper, e.g",wIgGMxXAYS
913,913,"“We chose two public corpora for our experiments which feature the logical parts of hate speech, versus only the use of  slurs and explicitly hostile language …”",wIgGMxXAYS
914,914, This should be paraphrased or just summarized much better,wIgGMxXAYS
915,915,Changing some words with their synonyms is neither paraphrasing nor summarization,wIgGMxXAYS
916,916,"In the example sentence, the word “we” causes confusion about what you and original authors do",wIgGMxXAYS
917,917,The other example in this line is the aforementioned “implementing the code”,wIgGMxXAYS
918,918,"Number formatting, e.g., 7896 -> 7,896",wIgGMxXAYS
919,919,Why do you not have the standard deviation for your results in Table 6? It is not clear what is happening in the paragraph you explain Tables 6 and 7,wIgGMxXAYS
920,920,“have able to reproduced” -> have able to reproduce,wIgGMxXAYS
921,921,- Reproducibility Summary: the paper includes the reproducibility summary on the first page,E4cRWiGoE3
922,922,"The summary is clear, well written, and major findings (i.e",E4cRWiGoE3
923,923,"failure in reproducing part of the paper, but successful replication on the rest) are incorporated in the summary.",E4cRWiGoE3
924,924,"- Scope of Reproducibility: it is well described, specifically the authors investigate an approach to generate debiased embeddings by removing the frequency component of word embeddings",E4cRWiGoE3
925,925,The authors list 2 reproduced/verified claims from the original paper.,E4cRWiGoE3
926,926,- Code: the authors tried to re-use the code provided by the original authors,E4cRWiGoE3
927,927,"Unfortunately, the code was not well documented and the authors struggled in re-running it",E4cRWiGoE3
928,928,This hampered the reproducibility effort,E4cRWiGoE3
929,929,"The code provided with this paper is a revised version of the original code, complemented with more detailed comments and documentation.",E4cRWiGoE3
930,930,- Communication with original authors: the authors did not have any communication with the original authors.,E4cRWiGoE3
931,931,"- Hyperparameter Search: the authors do not perform any hyperparameter search, since the reproduced algorithm is a post-training algorithm which does not require to train any parameter.",E4cRWiGoE3
932,932,"- Ablation Study: there is no ablation study, but it does not make sense to have it.",E4cRWiGoE3
933,933,- Discussion on results: the experimental results are well presented and compared with the original results,E4cRWiGoE3
934,934,The authors describe easy parts and challenges in reproducing the original paper,E4cRWiGoE3
935,935,Due to difficulties in interpreting the original code the authors could not reproduce part of the experiments.,E4cRWiGoE3
936,936,"- Recommendations for reproducibility: the authors do not mention any recommendations for the original authors, but they explicitly list missing details that were important for the reproducibility.",E4cRWiGoE3
937,937,- Results beyond the paper: the authors present some results beyond the original paper: they report a qualitative analysis with some biased words comparing the results before and after debiasing.,E4cRWiGoE3
938,938,- Overall organization and clarity: the paper is clear and well organized,E4cRWiGoE3
939,939,I list some typos in the following:,E4cRWiGoE3
940,940,    - Line 27: weren’t -> were not,E4cRWiGoE3
941,941,    - Line 33: [1] highlight -> use \citet,E4cRWiGoE3
942,942,    - Line 49: doesn’t -> does not,E4cRWiGoE3
943,943,    - Line 71: [4] identified -> use \citet,E4cRWiGoE3
944,944,    - Line 82: tasks for it : -> extra blank space,E4cRWiGoE3
945,945,    - Line 82: cateogorization -> categorization,E4cRWiGoE3
946,946,    - Line 83: Word Analogy : -> extra space,E4cRWiGoE3
947,947,    - Line 88: Concept Categorization : -> extra space,E4cRWiGoE3
948,948,    - Line 114: set of assumption -> set of assumptions,E4cRWiGoE3
949,949,    - Line 115: assumptions : -> extra space,E4cRWiGoE3
950,950,    - Caption of Figure 5: doesn’t -> does not,E4cRWiGoE3
951,951,    - Line 137: the difference is cosine similarity -> something is wrong in this sentence,E4cRWiGoE3
952,952,    - Line 144: doesn’t -> does not,E4cRWiGoE3
953,953,Overall evaluation:,E4cRWiGoE3
954,954,Pros:,E4cRWiGoE3
955,955,- The paper is clear and well written;,E4cRWiGoE3
956,956,- The authors could reproduce some of the results of the original paper;,E4cRWiGoE3
957,957,- The authors provide an elaborate list of details that are missing from the original paper.,E4cRWiGoE3
958,958,Cons:,E4cRWiGoE3
959,959,"- The authors could not reproduce part of the experiments (co-reference resolution task) due to the poor quality of the code provided, however they could try to re-implement part of the original approach.",E4cRWiGoE3
960,960,"The authors attempted to reproduce the claims in the paper ""Double-Hard Debias: Tailoring Word Embeddings for",E4cRWiGoE3
961,961,"Gender Bias Mitigation"" by Wang et al",E4cRWiGoE3
962,962,2020,E4cRWiGoE3
963,963,+ the reproducibility report is clearly organized.,E4cRWiGoE3
964,964,+ it was clear in the paper how the two main claims of the original article were reproduced.,E4cRWiGoE3
965,965,+ it was also clear how the reproducibility experiments were carried out and that the authors did not introduce any experimenter or measurement bias themselves in the process.,E4cRWiGoE3
966,966,+ it was evident which claim was easier to reproduce and why and what was difficult.,E4cRWiGoE3
967,967,- it was not clear whether the authors attempted to reach out to the original authors to validate some of their assumptions ,E4cRWiGoE3
968,968,"- likewise, they mentioned difficulty in understanding/executing code but it was not clear whether the authors were contacted.",E4cRWiGoE3
969,969,Authors worked on replicating the paper by Wang et al,E4cRWiGoE3
970,970,(2020) on tailoring embeddings for gender bias mitigation,E4cRWiGoE3
971,971,"Efforts were put to replicate the neighborhood metric test, however, the results were not reproducible due to lack of clarity in information/code by the original authors",E4cRWiGoE3
972,972,"Analysis on word embedding quality was reproducible within 5% of the original reported value, however, authors were not able to attempt replicating the coreference resolution experiments due to unreadable codes as they claimed",E4cRWiGoE3
973,973,No communication was attempted with the original authors to clarify the codebase for replication purposes,E4cRWiGoE3
974,974,"Overall, while the carried out experiments were solid (additional qualitative analysis on gender bias aspect was helpful) and the shared codebase for the replicated part would be useful to the researchers, I thought the work lacked substantial materials to be beneficial to the community based on its current standings.",E4cRWiGoE3
975,975,The authors provide a nice summary of the original article in the first section of the paper,SkWyPOGL1I
976,976,Chapter 5.1,SkWyPOGL1I
977,977,on Discrete Disparity Volume is a nice addition for further clarification,SkWyPOGL1I
978,978,"However, sometimes I believe they could have been more clear on a number of occasions, especially when it comes to the experimental results",SkWyPOGL1I
979,979,Here are some comments that should be addressed:,SkWyPOGL1I
980,980,- The authors of the reproducibility study coded everything from the scratch,SkWyPOGL1I
981,981,"For the purpose of the clarity they should have commented on the architecture in Figure 2 of the original paper: was everything clear and straightforward to implement? where there any ambiguities or points where a decision was to be made that could not be deduced from the Figure 2? For example, a bit more details on OCNet incorporation could be useful for the readers who would like to get a complete picture of the reproduction",SkWyPOGL1I
982,982,This is also a part where they got help from the authors of the original article.,SkWyPOGL1I
983,983,- In Section 3.2 on line 110 it is reported that the depth up to 100m were used for training on KITTI dataset,SkWyPOGL1I
984,984,"However, in the original article,  the authors report using depths up to 80m",SkWyPOGL1I
985,985,"Why the discrepancy and have they asked the authors of the original paper why did they decide on 80m instead? What would be the reproduced results if the reproducibility authors have used 80m as well? The results they report in the reproducibility manuscript are compared against the results from the original article,  yet there seems to be a difference in the training data",SkWyPOGL1I
986,986,**This is not a good practice.**,SkWyPOGL1I
987,987,- In all results reported in Chapter 4 there is always a piece of information not being explicitly stated,SkWyPOGL1I
988,988,"For example, in Section 4.1.1 (Table 2) and Section 4.1.2 (Table 3), one needs to read up to the beginning of Chapter 4 to conclude that these results are for KITTI dataset",SkWyPOGL1I
989,989,"In Section 4.2.2 (Table 5), one needs to compare results from Table 4 (Cityscape) and Table 3 (KITTI) to conclude that KITTI was used in Table 5 results",SkWyPOGL1I
990,990,"Furthermore, in Section 4.2.2 it could also be more clear what bases were used",SkWyPOGL1I
991,991,"- Furthermore, there are no results reported on the Made3D dataset, although this was explicitly stated by the reproducibility authors in Section 3.2",SkWyPOGL1I
992,992,on line 111.,SkWyPOGL1I
993,993,- There are no results reported for attention maps from the self-attention module.,SkWyPOGL1I
994,994,- There are no results reported for pixel-wise depth uncertainty,SkWyPOGL1I
995,995,"- Chapter 2, line 87: this is not a major claim or hypothesis of the original paper",SkWyPOGL1I
996,996,It is more of a side-result that can be easily visually verified (check 2.2 and 2.3 of the original paper).,SkWyPOGL1I
997,997,"- Tables 1 and 2: explicitly mentioning what the baseline is (Monodepth2 with ResNet18), would help the readers",SkWyPOGL1I
998,998,"- In Table 3: I believe Monodepth2 should be classified as ""M"" (self-supervised), while DORN should be ""D"" (supervised).",SkWyPOGL1I
999,999,- Table 6: The reported increase in memory consumption is not as dramatic as the authors claim,SkWyPOGL1I
1000,1000,Could it be that one is simpler than the other? Have they tried asking the authors of the original paper what they used for the results reported in the original paper?,SkWyPOGL1I
1001,1001,Some minor comments:,SkWyPOGL1I
1002,1002,- There is a few minor grammatical errors,SkWyPOGL1I
1003,1003,A bigger issue is that sometimes the wording and the sentence structure is a bit harder to follow and understand,SkWyPOGL1I
1004,1004,This could be improved.,SkWyPOGL1I
1005,1005,"- I am not sure why the reproducibility authors prefer to use the term ""atrous spatial pyramid pooling"" instead of ""dilated convolution"" (as used in reporting the results in the original paper), but they should (gently) mention that these two terms refer to the same concept.",SkWyPOGL1I
1006,1006,- The authors wrote code cleanly and it is not hard to read,SkWyPOGL1I
1007,1007,"On a few places a few more comments would make things easier to read, but it is not a big issue.",SkWyPOGL1I
1008,1008,"The authors did a good coding job, but the paper itself could have been better written",SkWyPOGL1I
1009,1009,There is still work to be done to make it more clear and self-explanatory when it comes to the implementation details and the reported results.,SkWyPOGL1I
1010,1010,The report aims to reproduce and evaluate the paper [1] ,SkWyPOGL1I
1011,1011,The paper's main claims are that it produces nearly SOTA monocular depth estimation results through a combination of techniques - self attention and discrete disparity volume,SkWyPOGL1I
1012,1012,"To this end, the report attempts to reconstruct these results, borrowing from other sources and code available (e.g",SkWyPOGL1I
1013,1013,Monodepth2 [2]),SkWyPOGL1I
1014,1014,Clarity: The report is well written and easy to read.,SkWyPOGL1I
1015,1015,Originality: The report and code seems to be the first attempt at reconstructing/implementing the paper,SkWyPOGL1I
1016,1016,"Significance: As monocular depth estimation is an important computer vision task in scenarios such as autonomous driving, the work is significant",SkWyPOGL1I
1017,1017,Pros and cons: ,SkWyPOGL1I
1018,1018,"Pros: With regards to pros, generally, the results presented align with the paper's claims that self-attention and DDVs help in improving monocular depth estimation",SkWyPOGL1I
1019,1019,"The results beat Monodepth2 convincingly, but cannot match DORN [3]",SkWyPOGL1I
1020,1020,Cons: The paper does not explain the underlying concepts properly,SkWyPOGL1I
1021,1021,"I would have hoped for some explanations on self-attention implementation and in particular, the discrete disparity volume ideas",SkWyPOGL1I
1022,1022,Examples:,SkWyPOGL1I
1023,1023,1) How is the DDV constructed and what computational/implementation challenges did it pose? ,SkWyPOGL1I
1024,1024,2) How was the system tuned? ,SkWyPOGL1I
1025,1025,3) Comments on the Atrouss Spatial Pyramidal Pooling?,SkWyPOGL1I
1026,1026,4) A system diagram with explanation and some implementation notes on components,SkWyPOGL1I
1027,1027,[1] http://openaccess.thecvf.com/content_CVPR_2020/papers/Johnston_Self-Supervised_Monocular_Trained_Depth_Estimation_Using_Self-Attention_and_Discrete_Disparity_CVPR_2020_paper.pdf,SkWyPOGL1I
1028,1028,[2] Monodepth2: https://arxiv.org/pdf/1806.01260.pdf,SkWyPOGL1I
1029,1029,[3] DORN: https://arxiv.org/pdf/1806.02446.pdf,SkWyPOGL1I
1030,1030,"The manuscript presents a replication study on the Lacoste, A",wMUNGLllANn
1031,1031,et,wMUNGLllANn
1032,1032,"al paper “Synbols: Probing Learning Algorithms with Synthetic Datasets” which presents a tool for generating images of unicode symbols with control over parameters like font, language, resolution, background texture, etc",wMUNGLllANn
1033,1033,"Paper also presents experiments showing use of Synbols in standard supervised learning setting (establishing baselines), in out-of-distribution testing, for active learning, unsupervised representation learning, and object counting",wMUNGLllANn
1034,1034,The authors of this report have been able to back up the results from the original paper,wMUNGLllANn
1035,1035,      ,wMUNGLllANn
1036,1036,- The report clearly define and describe the experimental setting of the original paper,wMUNGLllANn
1037,1037,The overall organisation and clarity of the document is excellent.,wMUNGLllANn
1038,1038,- The authors were able to successfully implement both the proposed algorithms from the description of the algorithms in the original paper/appendix (they had a fluid communication with the original authors for testing reproducibility),wMUNGLllANn
1039,1039,"Also, the implementation of all the reproduced experiments is provided in a github repository, which is remarkable.",wMUNGLllANn
1040,1040, - The report contains a summarised discussion on the state of reproducibility of the original paper,wMUNGLllANn
1041,1041,Results obtained are very close to the values reported by the original authors.,wMUNGLllANn
1042,1042,	  ,wMUNGLllANn
1043,1043,      ,wMUNGLllANn
1044,1044,Weak points:,wMUNGLllANn
1045,1045,- The authors were unable to (completely) run (and check) the original experimental setting for some DNN models due computational constraints (limiting their analysis to just one shot/seed),wMUNGLllANn
1046,1046,"Still, results obtained in those cases are still similar to those in the original paper.",wMUNGLllANn
1047,1047,"- Not running all the datasets (e.g., 1M) when analysing the amortised times makes the analysis a bit incomplete",wMUNGLllANn
1048,1048,"Perhaps Authors should have had a look for other compute resources (codeOcean, Kaggle, etc.).",wMUNGLllANn
1049,1049,Minor: ,wMUNGLllANn
1050,1050,- Typo in footnote 2.,wMUNGLllANn
1051,1051,"- Typo in Methodology: ""using only seed of the same dataset""",wMUNGLllANn
1052,1052,"The authors of this report reproduce part of the original paper (OP), and add a few experiments which test the quality of learned representations by trying different downstream classifiers",wMUNGLllANn
1053,1053,The authors found minor discrepancies between the hyperparameters reported in the OP and those contained in the code.,wMUNGLllANn
1054,1054,Format: the authors did not follow the provided latex template.,wMUNGLllANn
1055,1055,Scope: the authors reproduce some of the results from the OP,wMUNGLllANn
1056,1056,Code: at a glance the provided code appears complete,wMUNGLllANn
1057,1057,"The authors mostly reuse the code provided in the OP, with some additions",wMUNGLllANn
1058,1058,It seems that the added code contains implementations copied from other open-source repositories,wMUNGLllANn
1059,1059,"This is fine, but it is polite to explicitly reference those in the Readme.",wMUNGLllANn
1060,1060,Communication & hyperparameters: the authors have done their due diligence,wMUNGLllANn
1061,1061,Ablation/extensions: the authors add a few experiments which are consistent with the results of the OP,wMUNGLllANn
1062,1062,"It would have been very surprising for these experiments to deviate from the existing results, and the motivation for this choice is unclear",wMUNGLllANn
1063,1063,"It would have been more interesting to find, e.g",wMUNGLllANn
1064,1064,"settings where the dataset generation fails, is computationally hard, or makes no sense.",wMUNGLllANn
1065,1065,"Discussion: the authors make appropriate discussions and remarks on reproducibility, basically confirming that the material provided with the OP makes things easily reproducible.",wMUNGLllANn
1066,1066,"- it is unusual to report the validation loss, which is used to tune hyperparameters",wMUNGLllANn
1067,1067,The test loss is normally reported instead.,wMUNGLllANn
1068,1068,"- it would be good to clearly separate what was originally done in the OP from what was reproduced, and from what was added (not in the OP).",wMUNGLllANn
1069,1069,- the text is well structured and readable.,wMUNGLllANn
1070,1070,This work is an effort towards reproducing « Softmax Deep Double Deterministic Policy Gradients » by Pan et al,XhSN4Mm6fBM
1071,1071,(2020),XhSN4Mm6fBM
1072,1072,"This algorithm (SD3) claims superior performance compared to TD3 by using the softmax operator during bootstrapping, instead of the conservative « min » rule used in TD3",XhSN4Mm6fBM
1073,1073,"The main finding of this reproducibility report is that there seems to be indeed a small benefit of SD3 vs TD3, but it is not as marked as in the original paper, and it was even less clear on new environments.",XhSN4Mm6fBM
1074,1074,Overall the report is clearly written and easy to follow,XhSN4Mm6fBM
1075,1075,"My main criticism is that, as far as I can understand, the authors mostly re-used the existing open source implementation from Pan et al., and applied it to PyBullet environments (vs",XhSN4Mm6fBM
1076,1076,the original MuJoCo ones),XhSN4Mm6fBM
1077,1077,There is no hyper-parameter search nor ablation study,XhSN4Mm6fBM
1078,1078,"I do sympathize with the lack of computational resources, but maybe a different work should have been selected if the authors did not have enough to dig deeper",XhSN4Mm6fBM
1079,1079,"This makes the overall contribution somewhat limited, especially since the authors did not try to use the same MuJoCo environments, so it is not 100% clear if the differences (vs",XhSN4Mm6fBM
1080,1080,"the original paper) only comes from the environments themselves, or also from other (unknown) factors",XhSN4Mm6fBM
1081,1081,"The authors do mention they will provide a TensorFlow re-implementation of the algorithm, but since it wasn’t used for the report, is said to be less efficient than the publicly available PyTorch implementation, and there is no mention on whether or not it can reproduce the same results, I am not sure that it brings added value here.",XhSN4Mm6fBM
1082,1082,"In the end, it is not clear if the discrepancy vs",XhSN4Mm6fBM
1083,1083,"the original paper is because SD3 is not that good compared to TD3 on the PyBullet environments, or because hyper-parameters need to be adjusted for these environments, or because a mistake was made somewhere",XhSN4Mm6fBM
1084,1084,This makes it difficult to draw meaningful conclusions from this report.,XhSN4Mm6fBM
1085,1085,Minor detail: on l,XhSN4Mm6fBM
1086,1086,"88, should max_q be max_a?",XhSN4Mm6fBM
1087,1087,"Overall, this report represents a reasonably thorough reproduction of the Pan et al",XhSN4Mm6fBM
1088,1088,paper,XhSN4Mm6fBM
1089,1089,"However, there are some weaknesses (as outlined below), that should be improved.",XhSN4Mm6fBM
1090,1090,Strengths,XhSN4Mm6fBM
1091,1091,"- The suggestion on how to improve the performance of SD3 (lines 160-163) is interesting, and could lead to some non-trivial improvements.",XhSN4Mm6fBM
1092,1092,"- Evaluation the results in a set of comparable open-source environments is a good goal, and aids reproducibility overall.",XhSN4Mm6fBM
1093,1093,- Highlighting the differences between TD3 and SD3 is excellent for exposition and clarity.,XhSN4Mm6fBM
1094,1094,- This report is written in a way that makes it reasonably clear even without familiarity with the original paper.,XhSN4Mm6fBM
1095,1095,Weaknesses,XhSN4Mm6fBM
1096,1096,- One reason that the authors suggest for a discrepancy between the rewards in the Pan et al,XhSN4Mm6fBM
1097,1097,"paper and their reproduction is their use of PyBullet instead of MuJoCo (lines 127-128, lines 153-154)",XhSN4Mm6fBM
1098,1098,"However, this claim is not validated (or cited, other than the referenced GitHub issue).",XhSN4Mm6fBM
1099,1099,"- What do the edges in Figure 1 signify? Presumably that the algorithm uses ideas from an earlier one, but this should be clarified in the text.",XhSN4Mm6fBM
1100,1100,"- While the authors state that the results achieved are weaker than those in the original paper, there is no quantitative evidence presented, other than some graphs",XhSN4Mm6fBM
1101,1101,What is the magnitude of the effect? What is its statistical significance?,XhSN4Mm6fBM
1102,1102,"- While the RL background section (Section 2) is appreciated, there are some inaccuracies, including ""RL is often formalized as a Markov Decision Process"": it is the agent's interactions with the environment that are formalized by the MDP, but RL itself is the process of learning a good policy in that environment.",XhSN4Mm6fBM
1103,1103,- The report requires substantially more proofreading,XhSN4Mm6fBM
1104,1104,"Examples include: ""author's"" (line 21), ""consumption's"" (line 26), ""reprehensibility"" instead of ""reproducibility"" (line 165), lack of consistent capitalization throughout.",XhSN4Mm6fBM
1105,1105,Summary:,p1BXNUcTFsN
1106,1106,"This work reproduces the results  for ""Ensemble Distribution Distillation"" which uses a Dirichlet parametrization to distill deep ensembles to preserve uncertainty",p1BXNUcTFsN
1107,1107,"The reproducibility report shows an extensive evaluation of CIFAR-10, ablation studies on ensemble size and temperature annealing and also includes visualization for uncertainty.",p1BXNUcTFsN
1108,1108, ,p1BXNUcTFsN
1109,1109,Strengths:,p1BXNUcTFsN
1110,1110,* Detailed reproduction Ensemble Distribution Distillation for CIFAR-10,p1BXNUcTFsN
1111,1111,* Report included also all baseline models which offers also relative comparisons,p1BXNUcTFsN
1112,1112,* Nice simplex visualization for classification experiments.,p1BXNUcTFsN
1113,1113,Weaknesses:,p1BXNUcTFsN
1114,1114,* I only have comments about clarity of the paper below,p1BXNUcTFsN
1115,1115,Detailed comments and questions:,p1BXNUcTFsN
1116,1116,* Reproducibility summary: The scope of reproducibility is rather a description of the claims of the paper and is a bit vague,p1BXNUcTFsN
1117,1117,"From my understanding, this should describe the scope of this work.",p1BXNUcTFsN
1118,1118,"* Reproducibility summary: ""Most of the authors' experiments on the CIFAR-10 dataset"" were reproduced -> I think this should rather go to 'scope of reproducibility'",p1BXNUcTFsN
1119,1119,"Here, it would be also good to mention any pre-trained models that you used, e.g",p1BXNUcTFsN
1120,1120,VGG16.,p1BXNUcTFsN
1121,1121,* Section 2 (Scope of reproducibility): What is not included in this work? Which experiments were not run that were included in the original paper?,p1BXNUcTFsN
1122,1122,* Claims 3-5: What does the original paper claim here in comparison?,p1BXNUcTFsN
1123,1123,"* Table 3 & 4: Arrows up and down indicating ""higher is better"" or ""lower is better"" would be useful for all the metrics shown.",p1BXNUcTFsN
1124,1124,"Introduction is quite intuitive, giving a high-level context to the paper being reviewed",p1BXNUcTFsN
1125,1125,"The Scope of reproducibility was well highlighted,clear and concise.",p1BXNUcTFsN
1126,1126,All claims identified were supported by experiments.,p1BXNUcTFsN
1127,1127,"Although reviewer reproduced the paper using Tensorflow keras contrary to the original paper, The results obtained was still similar to the original paper, although not 100%.",p1BXNUcTFsN
1128,1128,"The overall reproduced paper is concise, explanatory and of good quality.",p1BXNUcTFsN
1129,1129,The authors of the reproducibility paper provide a nice summary of the original manuscript,2Z29gHCr5X8
1130,1130,"However, there are some major issues and shortcomings that prevent this reproducibility study to be accepted for publication",2Z29gHCr5X8
1131,1131,I list them here:,2Z29gHCr5X8
1132,1132,"- As authors of the reproducibility paper stated, only two out of four experiments were performed",2Z29gHCr5X8
1133,1133,"Considering that they did not need to code anything (the code was provided by the authors of the original paper), reproducing as many experiments as possible should have been a priority.",2Z29gHCr5X8
1134,1134,"- Even with the lack of time and computational requirements, instead of performing a full reproducibility of MNIST experiment (the simplest and the first experiment of the original paper), a full reproducibility of neuromorphic version of MNIST: N-MNIST woudl have been more appropriate",2Z29gHCr5X8
1135,1135,This was the second experiment in the original paper and it would have been considerably more interesting to see reproducibility results for it.,2Z29gHCr5X8
1136,1136,- The hyperparameter search was only performed for one of the four experiments,2Z29gHCr5X8
1137,1137,"In addition, it was the simplest experiment and the hyperparameter search was rather brief and not exhaustive",2Z29gHCr5X8
1138,1138,There is no mention of feedback from the authors of the original manuscript on the hyperparameter range they experimented with,2Z29gHCr5X8
1139,1139,"Since the code was already available, I believe this section should have been investigated more thoroughly.",2Z29gHCr5X8
1140,1140,- It seems that the hyperparameters used in the reproducibility study (as listed in lines 115 to 118) do not correspond to the hyperparameters used in the original paper (as listed in Section 2.1 of Supplementary Materials).,2Z29gHCr5X8
1141,1141,- The authors of the reproducibility study mention discovering hardcoded parameters in the source code that can not be explained,2Z29gHCr5X8
1142,1142,"I think a list of such parameters should have been made available somewhere (if not in the paper itself, then at least on the github webpage they provided)",2Z29gHCr5X8
1143,1143,"Furthermore, there is no attempt to explain or even guess what these parameters do",2Z29gHCr5X8
1144,1144,"I understand that just plain changing of these parameters and re-running the experiments would require additional time and resources, but that would have been in the best interest of this reproducibility study",2Z29gHCr5X8
1145,1145,"In addition, the authors of this study do not mention attempting to clarify these with the authors of the original manuscript.",2Z29gHCr5X8
1146,1146,"In the end I do believe that if authors of the reproducibility study were given more time and resources, they would have made a considerably better study",2Z29gHCr5X8
1147,1147,"Unfortunately, this is not the case.",2Z29gHCr5X8
1148,1148,"Overall, this was really nicely put together",2Z29gHCr5X8
1149,1149,"Reproducibility Summary: Provided, and contains a nice summary of the presented reproducibility results.",2Z29gHCr5X8
1150,1150,Scope of reproducibility:,2Z29gHCr5X8
1151,1151,"- Narrowed down to two datasets (MNIST and CIFAR-10), to deal with computational constraints",2Z29gHCr5X8
1152,1152,I think this is well justified given the computational complexity of the reproduced algorithms.,2Z29gHCr5X8
1153,1153,Code:,2Z29gHCr5X8
1154,1154,"- Original code used, with minor additions.",2Z29gHCr5X8
1155,1155,- Mentions checking major mathematical sections for mistakes/bugs.,2Z29gHCr5X8
1156,1156,  ,2Z29gHCr5X8
1157,1157,Communication with original authors:,2Z29gHCr5X8
1158,1158,- Clarification and recommendations for theory communicated to authors (who reciprocated).,2Z29gHCr5X8
1159,1159,Hyperparameter Search:,2Z29gHCr5X8
1160,1160,- Conducted a separate hyperparameter search on MNIST using bayesian optimization provided by WB ML dev tools,2Z29gHCr5X8
1161,1161,"They did the search over 3 parameters (epochs, learning rate, time window), see results beyond the paper for more details.",2Z29gHCr5X8
1162,1162,  ,2Z29gHCr5X8
1163,1163,Ablation Study:,2Z29gHCr5X8
1164,1164,- Not applicable as far as I can tell.,2Z29gHCr5X8
1165,1165,Discussion on results:,2Z29gHCr5X8
1166,1166,- Did a good job discussing the results.,2Z29gHCr5X8
1167,1167,- Provided average run-times and total compute used,2Z29gHCr5X8
1168,1168,- Justified the experiments reproduced well.,2Z29gHCr5X8
1169,1169,- Discussion is well written and clear.,2Z29gHCr5X8
1170,1170,"I think you should report which parameters are hardcoded in the original code, and what values they are set to",2Z29gHCr5X8
1171,1171,Possibly linking them to the actual notation of the algorithm rather than what is in the code.,2Z29gHCr5X8
1172,1172,"It might be nice to include a concrete set of recommendations, rather than listing what is wrong",2Z29gHCr5X8
1173,1173,"For example, ""Document all hard-coded parameters and give justification for their choice.""",2Z29gHCr5X8
1174,1174,  ,2Z29gHCr5X8
1175,1175,Results beyond the paper:,2Z29gHCr5X8
1176,1176,- A new hyperparameter search was conducted over the MNIST dataset,2Z29gHCr5X8
1177,1177,- Did a nice job uncovering some potential stability concerns for future work.,2Z29gHCr5X8
1178,1178,  ,2Z29gHCr5X8
1179,1179,Overall organization and clarity:,2Z29gHCr5X8
1180,1180,Well written and clear.,2Z29gHCr5X8
1181,1181,Questions/Concerns:,2Z29gHCr5X8
1182,1182,Did you only do a single run for the hyperparameter search? Confidence intervals for these novel results (Figure 1 and Figure 3) would be a nice addition,2Z29gHCr5X8
1183,1183,Maybe you did 12 runs? But it seems you did 12 different configurations,2Z29gHCr5X8
1184,1184,There might be high variability in the settings.,2Z29gHCr5X8
1185,1185,"In figure 2, which line corresponds to which settings? It might be nice to try and figure out how to visualize this information, as this would be useful for making inferences about hyperparameter selection.",2Z29gHCr5X8
1186,1186,"In this submission, the authors reproduce two of the four experiments originally performed by Zhang and Li (2020)",2Z29gHCr5X8
1187,1187," Confirming the original result, the authors find that Temporal Spike Sequence Learning Backpropagation improves spiking neural networks' performance to near SOA levels with reduced training time",2Z29gHCr5X8
1188,1188,Additional experiments were conducted to examine the influence of hyperparameters were also performed; the authors report that training times reported in the original paper may be pessimistic,2Z29gHCr5X8
1189,1189,I have no major concerns,2Z29gHCr5X8
1190,1190,There does appear to be a discrepancy between the original paper and replication in the CNN2/CFAIR-10 experiment,2Z29gHCr5X8
1191,1191,"However, the authors note that they were only able to run the experiment twice, and this may reflect network stochasticity (though the delta seems rather large)",2Z29gHCr5X8
1192,1192,"The other experiments replicate nicely, and the rest of the manuscript also contains a brief explanation of spiking neural networks and the authors' rationale for choosing to replicate these two experiments, which is nice.",2Z29gHCr5X8
1193,1193,The authors apparently reviewed the source code (Line 208ff) and found some undocumented parameters,2Z29gHCr5X8
1194,1194,"A slightly longer discussion of these (e.g., where they are, what they seem to do) might be helpful for future readers of both papers",2Z29gHCr5X8
1195,1195,"However, I leave this entirely to the authors' discretion",2Z29gHCr5X8
1196,1196,"The author reimplements the paper ""Fairness without demographics through Adversarially Reweighted Learning"".",YyXrarQhX2S
1197,1197,"On one hand, the good news is the author reproduces ARL's performance in the original paper",YyXrarQhX2S
1198,1198,The frustrating news is that the author finds the baseline DRO's performance is much higher compared to that originally reported.,YyXrarQhX2S
1199,1199,The author says the original paper does not provide the grid search results,YyXrarQhX2S
1200,1200,"Fortunately, I find that the authors of the original paper publish their code and the optimal hyperparameters in their [GitHub repository](https://github.com/google-research/google-research/tree/master/group_agnostic_fairness).",YyXrarQhX2S
1201,1201,The optimal hyper-parameters from this paper's Appendix B are very different from the original paper's grid search result (see GitHub).,YyXrarQhX2S
1202,1202,So I think the author should revisit the experiment and try to give some insight on what are the optimal hyperparameters.,YyXrarQhX2S
1203,1203,"This manuscript provides a pytorch implementation about the paper ""Fairness without demographics through adversarially reweighted learning""",YyXrarQhX2S
1204,1204,"The authors do not perfectly reproduce the results, but the trend seems to be consistent with the original paper",YyXrarQhX2S
1205,1205,Hyperparameter tuning is sufficient,YyXrarQhX2S
1206,1206,Additional contribution of this paper is to apply this method to image data.,YyXrarQhX2S
1207,1207,**Quality:** Overall quality of the reproduction is very good,10Fgr0kHs5
1208,1208,The writing reveals the good understanding of the original work by the authors of the reproducibility report.,10Fgr0kHs5
1209,1209,**Clarity:** I found the report to be written very clearly,10Fgr0kHs5
1210,1210,The ease and difficulty faced by the authors is clearly mentioned in the report,10Fgr0kHs5
1211,1211,"The exact reproductions, understandings and explanations are also conveyed very clearly, both verbally and pictorially.",10Fgr0kHs5
1212,1212,**Originality:** The submitted report is quite original on its own,10Fgr0kHs5
1213,1213,The authors put their own knowledge in the domain to judge and reproduce the original work,10Fgr0kHs5
1214,1214,The thought process of the authors are distinctly visible.,10Fgr0kHs5
1215,1215,"**Significance:** The report provides good insights on how the experiments in the original paper actually work, while also generating new hypothesis to be tested for future research, which is a positive outcome.",10Fgr0kHs5
1216,1216,**Pros**,10Fgr0kHs5
1217,1217,- detailed reporting of every aspect,10Fgr0kHs5
1218,1218,- clear listing of shortcomings in the original paper,10Fgr0kHs5
1219,1219,- thoughtfulness of the authors to present the reasons of unmatched results,10Fgr0kHs5
1220,1220,**Cons**,10Fgr0kHs5
1221,1221,"- detailed labelling within figures found missing, making it difficult to understand the diagrams in first glance",10Fgr0kHs5
1222,1222,"The replication study pretty much stuck to the assumptions, code and interpretations of the original paper",10Fgr0kHs5
1223,1223,"Unsurprisingly, they found issues with context -sensitive aspects of the study -- which do not have solid theoretical foundations",10Fgr0kHs5
1224,1224,They found issues with replicating the results on two datasets -- this part appears to be done correctly,10Fgr0kHs5
1225,1225,All this suggests that related theoretical frameworks need more work.,10Fgr0kHs5
1226,1226,### SUMMARY ,10Fgr0kHs5
1227,1227,"This work offers an in-depth exploration of the reproducibility of ""Deep Fair Clustering for Visual Learning"" which claims that:",10Fgr0kHs5
1228,1228,"- The proposed method ensures cluster validity and fairness on large-scale, high-dimensional visual learning while (a) seeking to find feature mappings amenable for structure discovery and (b) filtering out sensitive attributes.",10Fgr0kHs5
1229,1229,"- The process has been modeled as a minimax optimization problem where cluster analysis and assignment is independent of sensitive attributes (aka C(X) is statistically independent of G) with minimal utility loss, high accuracy score and fairness measure.",10Fgr0kHs5
1230,1230,"- The theoretical analysis has been backed by empirical demonstration on four visual datasets: MNIST-USPS, MTFL, Color Reverse MNIST, Office-31 using the following metrics: Accuracy, Normalised Mutual Information, Balance and Entropy.",10Fgr0kHs5
1231,1231,The submitted report addresses the above claims as follows:,10Fgr0kHs5
1232,1232,- Visualizes learned representations of encoder through t-SNE to demonstrate DFC's ten clearly separated clusters ensuring fairness.,10Fgr0kHs5
1233,1233,"- Considers DEC as DFC without minimax optimization (in particular without - separate subgroup clustering, fairness adversarial loss and structural preservation loss).",10Fgr0kHs5
1234,1234,"- Comparisons across DEC and DFC have been verified across the specified metrics (not for the following models stated in the original paper: DAC, AE, CIGAN, ScFC, SpFC, FCC, and FAlg) for the above mentioned datasets",10Fgr0kHs5
1235,1235,"This report further claims to extend the original code repository to include support for pre-training, different datasets, comparative methods with additional hyperparameter optimization using the Weights & Biases Sweeps feature",10Fgr0kHs5
1236,1236,### MERITS,10Fgr0kHs5
1237,1237,The report is well-written and intrinsic details of the original paper are expounded to reasonable depth,10Fgr0kHs5
1238,1238,Results have been reproduced satisfactorily.,10Fgr0kHs5
1239,1239,### RECOMMENDATIONS,10Fgr0kHs5
1240,1240,"- In Figure 1, it might be helpful to incorporate notations (For Example: feature encoder F(X) transforms data into Z) into the schematic representations as well.",10Fgr0kHs5
1241,1241,- Statistics in Section 3.1 can be discussed in a tabular form,10Fgr0kHs5
1242,1242,"As per the [Machine Learning Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf), I'd also recommend including links to downloadable versions of the dataset for Office-31, Color-reverse MNIST and MTFL.",10Fgr0kHs5
1243,1243,- Results of the original paper on the Balance metric of Office-31 dataset has not been reported accurately: Please recheck,10Fgr0kHs5
1244,1244,The reproducibility report may need to be updated accordingly,10Fgr0kHs5
1245,1245,"Also, re-verify your reported percentage intervals for various metrics",10Fgr0kHs5
1246,1246,"- Regarding the claim “Additional experiments aimed at validating the contribution of individual components of the DFC towards fairness”, please elucidate the experimentation as well as individual contributions of the following major components (DEC/Discriminator/Encoder)",10Fgr0kHs5
1247,1247,"These seem to have been (a) cryptically addressed in the report and (b) contributions have been fragmented across sections (in the reviewer's opinion) and hence, do not bolster your argument",10Fgr0kHs5
1248,1248,- I'd also recommend including the table of results into the README document on [your github repository](https://github.com/Joppewouts/Reproducing-Deep-Fair-Clustering),10Fgr0kHs5
1249,1249,"Optionally, you could include the list of dependencies in the report.",10Fgr0kHs5
1250,1250,### MINOR CORRECTIONS,10Fgr0kHs5
1251,1251,"- In Appendix A, Table 2 Caption: ""meand and log variance of the distribution""; **Correction:** ""_mean_ and log variance of the distribution""",10Fgr0kHs5
1252,1252,"- In Appendix A, Table 3 Caption: ""The architecture was chosen to resemble te inverse of the encoder""; **Correction:** ""The architecture was chosen to resemble _the_ inverse of the encoder"".",10Fgr0kHs5
1253,1253,"- In Section 3, Deep Embedded Clustering, Line 89: ""A soft assignement of the datapoints to cluster centroid""; **Correction:** ""A soft _assignment_ of the datapoints to cluster centroid"".",10Fgr0kHs5
1254,1254,"- In Section 3, Deep Fair *Clustering*, Line 100: ""an fairness adversarial loss""; **Correction:** ""_a_ fairness adversarial loss""",10Fgr0kHs5
1255,1255,(_Clustering_ instead of _clustering_ to ensure consistency of casing across headings/sub-headings),10Fgr0kHs5
1256,1256,"- In Section 3, Deep Fair Clustering, Line 110:  ""In other words, they expect, given a subgroup, that the clustering result is similar when trained only on the subgroup data and when trained on all subgroups and looking only at the samples of one subgroup, and encourage the distributions to be similar""; I believe this statement can be better phrased in terms of local subgroup vs global subgroup distribution and clustering",10Fgr0kHs5
1257,1257,"- In Section 3.1, “The The Multi-task Facial Landamark (MFTL)”; Line 129: “_The_ Multi-Task Facial _Landmark_”, ""constist of 12,995 face images""",10Fgr0kHs5
1258,1258,"**Correction:** ""_consist_ of 12,995 face images""",10Fgr0kHs5
1259,1259,"- In Section 4, Experimental Setup and Implementation, Line 165: ""Some experiments require sightly different model combinations""; **Correction:** ""Some experiments require _slightly_ different model combinations''",10Fgr0kHs5
1260,1260,"- In Section 4.3, Centroid Initialization, Line 188: ""student's t-dsitribution""; **Correction:** ""student’s _t-distribution_""",10Fgr0kHs5
1261,1261,"- In Section 4.4, DFC, Line 192: ""a discrminator module and""; **Correction:** ""a _discriminator_ module and""",10Fgr0kHs5
1262,1262,"- In Section 4.4.1, Clustering Assignment, Line 199: ""can be use as in the Deep Embedding clustering""; **Correction:** ""can be _used_ as in the Deep Embedding clustering""",10Fgr0kHs5
1263,1263,- Formatting of references can be enhanced: IEEE conference on computer vision and pattern recognition -> IEEE Conference on Computer Vision and Pattern Recognition (CVPR).,10Fgr0kHs5
1264,1264,"The manuscript presents a replication study on the paper ""Distribution-aware coordinate representation for human pose estimation"" ",rwTfoRdQcxW
1265,1265,Pros: ,rwTfoRdQcxW
1266,1266,"The authors are able to replicate the method, largely because of the available code from the original paper",rwTfoRdQcxW
1267,1267,The authors tested on an independent dataset,rwTfoRdQcxW
1268,1268,They also introduced additional comparison with other models,rwTfoRdQcxW
1269,1269,Both seem to be extension and qualitatively support the claims of the original paper,rwTfoRdQcxW
1270,1270,The authors were able to successfully implement both the proposed algorithms from the description of the algorithms in the original paper (they didn’t need to contact thus with the original authors for testing reproducibility),rwTfoRdQcxW
1271,1271,Cons: ,rwTfoRdQcxW
1272,1272,The missing of validation on the COCO dataset used by the original paper,rwTfoRdQcxW
1273,1273,This weakens the successful replication claim,rwTfoRdQcxW
1274,1274,The authors implementation is not in a github repository  ,rwTfoRdQcxW
1275,1275,* Reproducibility Summary,rwTfoRdQcxW
1276,1276,"  The report presents a well-written, concise reproduction study on the paper DARK",rwTfoRdQcxW
1277,1277,"The report contains a reproducibility summary that highlights the scope, methodology, results, and what was easy/difficult appropriately as required by the challenge.",rwTfoRdQcxW
1278,1278,* Scope of reproducibility,rwTfoRdQcxW
1279,1279,  The report investigates two central claims of the original paper.,rwTfoRdQcxW
1280,1280,* Code: whether reproduced from scratch or re-used author repository.,rwTfoRdQcxW
1281,1281,  Authors re-use the repository of the original papers for most of their experiments,rwTfoRdQcxW
1282,1282,They also implement their own code in a novel reproducible model development kit following the author's code,rwTfoRdQcxW
1283,1283,"I find this reproducibility kit (moai) fascinating, and a great example of a submission that introduces a reproducible paradigm to test original authors' codes.",rwTfoRdQcxW
1284,1284,* Communication with original authors,rwTfoRdQcxW
1285,1285,  The report mentions they did not communicate their results/findings with the original authors.,rwTfoRdQcxW
1286,1286,* Hyperparameter Search,rwTfoRdQcxW
1287,1287,  The authors seem to investigate a modest subset of hyperparameters for their work.,rwTfoRdQcxW
1288,1288,* Ablation Study,rwTfoRdQcxW
1289,1289,"  The authors perform an ablation study on the originally proposed algorithm by evaluating the claims on a new dataset, HUMAN4D",rwTfoRdQcxW
1290,1290,"This kind of robustness evaluation on a new dataset is very much welcome, as it adds valuable insights to the proposed algorithms.",rwTfoRdQcxW
1291,1291,* Discussion on results,rwTfoRdQcxW
1292,1292,  The report contains a limited discussion of the results on the HUMAN4D dataset using DARK,rwTfoRdQcxW
1293,1293,"For claim 1, the authors note DARK decoding performs better for all experiments except one",rwTfoRdQcxW
1294,1294,It would have been better to add a discussion of why this exception occurs,rwTfoRdQcxW
1295,1295,The report also extends the original results by adding a new experiment with comparing with CoM,rwTfoRdQcxW
1296,1296,"The report can be made stronger by exploring more ablations to shed light on the effectiveness of DARK, and/or adding more discussion to the effectiveness of DARK.",rwTfoRdQcxW
1297,1297,* Recommendations for reproducibility,rwTfoRdQcxW
1298,1298,  The authors highly commend the original paper on their state of reproducibility.,rwTfoRdQcxW
1299,1299,* Overall organization and clarity,rwTfoRdQcxW
1300,1300,  The paper is well organized and well written.,rwTfoRdQcxW
1301,1301,**Reproducibility Summary**:,PCpGvUrwfQB
1302,1302,The authors have provided a detailed summary meeting the requirements.,PCpGvUrwfQB
1303,1303,**Scope of reproducibility**:,PCpGvUrwfQB
1304,1304,"Yes, the reproducibility report has clearly and concisely stated the scope of reproducibility.",PCpGvUrwfQB
1305,1305,**Code**:,PCpGvUrwfQB
1306,1306,"Yes, the authors have re-used the original author's code repository and also tried with another differential PnP (i.e EPnP) module as described in the reproducibility report.",PCpGvUrwfQB
1307,1307,**Communication with original authors**,PCpGvUrwfQB
1308,1308,"Yes, the authors connected with one of the BPnP paper's original authors through their Github repo",PCpGvUrwfQB
1309,1309,  The authors did not reach out to HigherHRNet paper's original authors,PCpGvUrwfQB
1310,1310,**Hyperparameter Search**:,PCpGvUrwfQB
1311,1311,"Yes, the authors have attempted to reproduce the hyperparameter search, but the $\beta$ coefficient from the original author's (BPnP)'s code did not work for the authors of the reproducibility report.",PCpGvUrwfQB
1312,1312,**Ablation Study**:,PCpGvUrwfQB
1313,1313,"Yes, the authors used an alternative implementation of the BPnP module to review the results and reproducibility",PCpGvUrwfQB
1314,1314, The authors tried ignoring the higher-order derivatives of the BPnP.,PCpGvUrwfQB
1315,1315,**Discussion on results**:,PCpGvUrwfQB
1316,1316,"Yes, the reproducibility report contains a brief discussion on the state of reproducibility of the original papers, but does not highlight which parts are easy to reproduce or which parts were harder",PCpGvUrwfQB
1317,1317, They could have mentioned the difficulty with the $\beta$ parameter here.,PCpGvUrwfQB
1318,1318,**Recommendations for reproducibility**:,PCpGvUrwfQB
1319,1319,"No, the authors did not provide any recommendation to the original authors for improving reproducibility.",PCpGvUrwfQB
1320,1320,**Results beyond the paper**:,PCpGvUrwfQB
1321,1321,The authors have tried additional differentiable PnP implementation (EPnP) to verify the claim,PCpGvUrwfQB
1322,1322, That is a good point,PCpGvUrwfQB
1323,1323," Another good point is that the authors tried to reduce the complexity and run time of the model using a faster BPnP, then evaluated the results and provided the detail pros and cons of using the technique; bonus point to the authors for that",PCpGvUrwfQB
1324,1324, The authors include significantly more quantitative and qualitative results than the original paper.,PCpGvUrwfQB
1325,1325,**Overall organization and clarity**:,PCpGvUrwfQB
1326,1326,Nicely written and coherent.,PCpGvUrwfQB
1327,1327,**Pros**:,PCpGvUrwfQB
1328,1328,Significantly more quantitative and qualitative results.,PCpGvUrwfQB
1329,1329,**Con**:,PCpGvUrwfQB
1330,1330,The authors highlight the best results in the tables using a red color,PCpGvUrwfQB
1331,1331, A better choice would be green or yellow or just to leave it uncolored.,PCpGvUrwfQB
1332,1332,"Two papers are reproduced here: backpropagatable PnP & HigherHRNet, for the problem of 6DOF object pose estimation",PCpGvUrwfQB
1333,1333,The results are evaluated in the UAVA dataset,PCpGvUrwfQB
1334,1334,"The work contains the mentioned points, including communication with original authors, and discussion of the reults",PCpGvUrwfQB
1335,1335,"Overall, the results are meaningful",PCpGvUrwfQB
1336,1336,"They even present results beyond the original paper, such as section 4.2",PCpGvUrwfQB
1337,1337,The report aims to verify the effectiveness of using Backpropogatable PnP (BPNP) on a pose estimation task with drones,PCpGvUrwfQB
1338,1338,"Following the original paper, the setup uses heatmap regression, from which the object pose is extracted and refined through PnP, given 3D geometry",PCpGvUrwfQB
1339,1339,The incorporation of geometric constraints (e.g,PCpGvUrwfQB
1340,1340,the projection loss from the BPnP [1] paper) is claimed to improve estimation of keypoints,PCpGvUrwfQB
1341,1341,"In addition to BPnP, the report also examines the effect of scale aggregation from the HigherHRNet paper which proposes a scheme to bottom up scheme for aggregation in stacked hourglass type setups for heat map computation",PCpGvUrwfQB
1342,1342,The report goes about doing this through two types of drones of varying sizes - Mavic (larger) and Tello (smaller) using the dataset UAVA which contains ground truth annotations needed (e.g,PCpGvUrwfQB
1343,1343,6D pose).,PCpGvUrwfQB
1344,1344,BPNP scheme is compared with a reference differentiable implementation (EPnP) via PyTorch3D,PCpGvUrwfQB
1345,1345,"The report was well written, and the experiments thoughtfully carried out",PCpGvUrwfQB
1346,1346,"In general, the numbers show improvements in keypoint estimation after incorporating the the projection losses",PCpGvUrwfQB
1347,1347,The comparison with EPnP is also quite favourable,PCpGvUrwfQB
1348,1348,They also show that the 'faster' version of BPnP reduces computational time without loss of accuracy,PCpGvUrwfQB
1349,1349,"Pros: Major ideas in paper explained clearly, with cogent implementation results",PCpGvUrwfQB
1350,1350,I would be keen on using BPnP for practical tasks,PCpGvUrwfQB
1351,1351,Cons: Hyperparameter sweep tries not touched upon in detail,PCpGvUrwfQB
1352,1352,"In particular, how does one chose the weighting parameters? Were any stability issues encountered? I would have liked to see a more varied list of scales as in the original paper rather than just the two drones, and (please correct me if I am not reading it correctly) the numbers are not markedly better/worse across implementations (Table 4).",PCpGvUrwfQB
1353,1353,[1] BPnP: https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.pdf,PCpGvUrwfQB
1354,1354,# Reproducibility Summary : ,bhiGno-Cxq
1355,1355,Complete,bhiGno-Cxq
1356,1356,# Scope of reproducibility:,bhiGno-Cxq
1357,1357,Clearly defined,bhiGno-Cxq
1358,1358,# Code: ,bhiGno-Cxq
1359,1359,Re-implemented.,bhiGno-Cxq
1360,1360,# Communication with original authors,bhiGno-Cxq
1361,1361,Reports one communication exchange with original authors.,bhiGno-Cxq
1362,1362,# Hyperparameter Search,bhiGno-Cxq
1363,1363,The report mentions the need to do some random search on unmentioned hyperparameters (such as momentum coefficient),bhiGno-Cxq
1364,1364,"There is no clear mention of hyperparameter search, what was the search space, budget and algorithm used for the main hyperparameters however",bhiGno-Cxq
1365,1365,What I understand is that they re-used the original ones.,bhiGno-Cxq
1366,1366,# Ablation Study:,bhiGno-Cxq
1367,1367,There is no ablation study,bhiGno-Cxq
1368,1368,# Discussion on results,bhiGno-Cxq
1369,1369,The reproduction of the results is well detailed and the table help understand what was reproduced and what was not.,bhiGno-Cxq
1370,1370,# Recommendations for reproducibility,bhiGno-Cxq
1371,1371,There is no clear recommendations.,bhiGno-Cxq
1372,1372,# Results beyond the paper,bhiGno-Cxq
1373,1373,"They tried applying the model on a task that was not in the original paper, the semantic segmentation",bhiGno-Cxq
1374,1374,The dataset was however extremely small which makes me uncertain about how insightful the results are.,bhiGno-Cxq
1375,1375,# Overall organization and clarity,bhiGno-Cxq
1376,1376,The overall organisation of the paper and the presentation of the results is clear,bhiGno-Cxq
1377,1377,There is many mistakes (see minor comments below) that should be corrected,bhiGno-Cxq
1378,1378,Section ‘Methodology’ is hard to understand and brings no value with respect to original paper,bhiGno-Cxq
1379,1379,Pointing the reader to original paper to better understand the maths beyond BayesBiNN would make a better job in my opinion.,bhiGno-Cxq
1380,1380,# Comments,bhiGno-Cxq
1381,1381,Algorithm 1 is difficult to understand without more explanations.,bhiGno-Cxq
1382,1382,The ‘Methodology’ section was difficult to follow,bhiGno-Cxq
1383,1383,"The explanations of the equations are too succinct, lack motivations and explanations",bhiGno-Cxq
1384,1384,"Looking at the original paper, I find it easier to understand the equations based on their explanations, so I am wondering what is the value of the presentation of the equations in this report.",bhiGno-Cxq
1385,1385,"Line 145:  However, the results were against our intuition and the result of segmentation were a zoomed segmented image of the input with lots of noise.",bhiGno-Cxq
1386,1386,It’s not clear what is meant here,bhiGno-Cxq
1387,1387,What was the intuition? What is the result and what should it have been? Should it not be zoomed?,bhiGno-Cxq
1388,1388,# Minor comments,bhiGno-Cxq
1389,1389,Line 36,bhiGno-Cxq
1390,1390,STE and BOP acronyms should be introduced.,bhiGno-Cxq
1391,1391,Line 100,bhiGno-Cxq
1392,1392,by a randomly -> by randomly,bhiGno-Cxq
1393,1393,Line 118,bhiGno-Cxq
1394,1394,reporduce -> reproduce,bhiGno-Cxq
1395,1395,Line 120: but the validation split is made 0,bhiGno-Cxq
1396,1396,I don’t understand what it means to make a split 0.,bhiGno-Cxq
1397,1397,Line 136 Semantic Semantic -> Semantic Segmentation,bhiGno-Cxq
1398,1398,Line 137 it’s full-precision -> its full precision,bhiGno-Cxq
1399,1399,Line 138 it’s performance -> its performance,bhiGno-Cxq
1400,1400,Line 140 we have use -> we have used,bhiGno-Cxq
1401,1401,Line 167 The gave -> They gave,bhiGno-Cxq
1402,1402,Line 178 in it’s total -> in its total,bhiGno-Cxq
1403,1403,Line 184 that it’s -> that its,bhiGno-Cxq
1404,1404,This paper summarizes the reproducibility of BayesBNN and gives a clear scope of reproducibility,bhiGno-Cxq
1405,1405,It provides a re-implemented codebase to reproduce the results of the original paper,bhiGno-Cxq
1406,1406,"In addition, this paper also discusses the extended results for semantic segmentation",bhiGno-Cxq
1407,1407,The paper is well-written and easy to understand,bhiGno-Cxq
1408,1408,Couples questions:,bhiGno-Cxq
1409,1409,1,bhiGno-Cxq
1410,1410,"For the case of unmatched CIFAR-10 results, I am wondering why the Batch-Norm layers may cause a large-cap of test accuracy.",bhiGno-Cxq
1411,1411,2,bhiGno-Cxq
1412,1412,"For the semantic segmentation task, is it possible the limited size of training data may affect the model performance?",bhiGno-Cxq
1413,1413,3,bhiGno-Cxq
1414,1414,"Line 183, what do you mean by “cleaner deep learning”? ",bhiGno-Cxq
1415,1415,Thank you for your great paper!,Vqtf4kZg2j
1416,1416,The paper successfully proves that the original paper is reproducible and it could provide the post-hoc casual explanations for black-box classifiers through the casual reference,Vqtf4kZg2j
1417,1417,"Furthermore, the paper establishes its own evaluation system to evaluate the original paper from different aspects",Vqtf4kZg2j
1418,1418,"Moreover, the paper extends the application domain from images to texts, which is great for generalization",Vqtf4kZg2j
1419,1419,"However, I think it would be good if you can add more details about what is different between your implementations and the original paper's, like the solution to reduce algorithm complexity",Vqtf4kZg2j
1420,1420,"Last but not least, adding a few figures about the models' architecture would be great for readers!",Vqtf4kZg2j
1421,1421,Good job in reproducing the results,Vqtf4kZg2j
1422,1422,The original paper carries out methods for generating causal post-hoc explanations of black-box classifiers based on a low-dimensional representation of input data,Vqtf4kZg2j
1423,1423,This paper tries to reproduce those results in detail and provide a more efficient implementation,Vqtf4kZg2j
1424,1424,"While reproducing the results of the original paper, the authors of this paper take a further step ahead:",Vqtf4kZg2j
1425,1425,1,Vqtf4kZg2j
1426,1426,They provide a higher resolution transition for the first causal factor for MNIST 1/4/9 classifier.,Vqtf4kZg2j
1427,1427,2,Vqtf4kZg2j
1428,1428,"They also dive into hyperparameter search for the Principled procedure for selecting (K, L, λ) as explained by the original paper.",Vqtf4kZg2j
1429,1429,3,Vqtf4kZg2j
1430,1430,They have also tried out the proposed method on the SST text dataset and tabulated the duration for both text as well as for image datasets.,Vqtf4kZg2j
1431,1431,"Also, the resulted figure from this implementation is similar to the figures reported in the original paper",Vqtf4kZg2j
1432,1432,They have also mentioned that they have not found this method to be scalable in contrary to the original paper but they have not mentioned any ideas on how to scale up but they are relying on future papers to do so.,Vqtf4kZg2j
1433,1433,"On a final note, this is a solid work and will be very helpful to gain insights if the original paper is reproducible or not and to what extent can the algorithms mentioned in the paper can be used to solve the explainability problem of black-box classifiers.",Vqtf4kZg2j
1434,1434,Thank you for your great paper!,P30M7d9DyXw
1435,1435,"Summary: The authors tried to reproduce the original paper which claimed that complex-valued DNNs effectively increase the difficulty of inferring inputs for the adversary attacks compared to the baseline and the proposed privacy-protecting complex-valued DNN effectively preserves the accuracy when compared to the baseline, but did not get the satisfying results",P30M7d9DyXw
1436,1436,"Moreover, the authors proposed certain good points to question about the original paper.",P30M7d9DyXw
1437,1437,- Pros: ,P30M7d9DyXw
1438,1438,1,P30M7d9DyXw
1439,1439,The authors are really rigorous to provide certain good points to doubt the original paper (Section 5 Discussion in this paper),P30M7d9DyXw
1440,1440,It proves that the authors read the papers carefully and devotes themselves to designing the experiments.,P30M7d9DyXw
1441,1441,2,P30M7d9DyXw
1442,1442,"The authors describe the details of the experiment very carefully, and the idea is clear to me.",P30M7d9DyXw
1443,1443,- Cons: ,P30M7d9DyXw
1444,1444,1,P30M7d9DyXw
1445,1445,The authors haven't carried out the whole experiments (such as the authors did not successfully implement VGG-16 / Alexnet and the inference attacks),P30M7d9DyXw
1446,1446,2,P30M7d9DyXw
1447,1447,The result section is confusing to me as I can't figure out which one is the new results without ReLU in the generator,P30M7d9DyXw
1448,1448,"Overall, I really appreciate the discussion section, so I would clearly recommend the paper to be accepted!",P30M7d9DyXw
1449,1449,The authors did well by reproducing the original work even though there was no readily available data.,P30M7d9DyXw
1450,1450,Summary:,l0PyfnGP1L
1451,1451,"This work reproduces the main results of ""Fairness by Learning Orthogonal Disentangled Representations""",l0PyfnGP1L
1452,1452,This includes implementing the model and evaluating it with different image datasets on downstream tasks.,l0PyfnGP1L
1453,1453,Strengths:,l0PyfnGP1L
1454,1454,* Evaluation of approach evaluated on 5 datasets,l0PyfnGP1L
1455,1455,Weaknesses:,l0PyfnGP1L
1456,1456,* I found the scope of this paper limited to the main results of the paper,l0PyfnGP1L
1457,1457,"At least, the authors should mention what is not included in the report",l0PyfnGP1L
1458,1458,"Further, it would have been good, if the authors had added an ablation study or the sensitivity analysis.",l0PyfnGP1L
1459,1459,* It was not clear whether the hyperparameters used (3.3) were the same used in the paper or not.,l0PyfnGP1L
1460,1460,"**Quality:** It is a fair quality report, which lacks on crucial details",l0PyfnGP1L
1461,1461,Understanding of the authors on the original work was absent.,l0PyfnGP1L
1462,1462,**Clarity:** The information presented is clear but the amount is less.,l0PyfnGP1L
1463,1463,"**Originality:** It is a normal report, nothing to fancy to be rated original.",l0PyfnGP1L
1464,1464,**Significance:** The report currently is low on significance,l0PyfnGP1L
1465,1465,"It would have been better if the authors of the report provided some more details on how the original paper presented their works, and how the authors of the report see it and understand from their perspectives.",l0PyfnGP1L
1466,1466,**Pros**,l0PyfnGP1L
1467,1467,- Report covers all aspects of the reproducibility,l0PyfnGP1L
1468,1468,- report of successful communication with authors of original work,l0PyfnGP1L
1469,1469,**Cons**,l0PyfnGP1L
1470,1470,- far less detailed to be called a report,l0PyfnGP1L
1471,1471,- authors understanding and perspective of original work missing,l0PyfnGP1L
1472,1472,"- authors of the report just say that the results are dissimilar, however a much detailed discussion regarding the same giving reasons was felt missing",l0PyfnGP1L
1473,1473,"- overall, the report could have been improved with diagrams and figures added to enhance the information presented",l0PyfnGP1L
1474,1474,"This report reproduces the paper ""Parameterized explainer for graph neural network."" [1]",tt04glo-VrT
1475,1475,A reproducibility summary is provided but not well-structured according to the guideline provided,tt04glo-VrT
1476,1476,"First, the scope of reproducibility primarily discusses the experimental results consistency obtained without mentioning their implementation, difference compared with the original paper etc",tt04glo-VrT
1477,1477,"This section seems more suitable to be renamed as ""reproduced results""",tt04glo-VrT
1478,1478,"Second, the methodology requires a more detailed discussion so as to better discuss the implementation differences compared with the original paper",tt04glo-VrT
1479,1479,The authors mentioned that there was no need to contact with the original authors but the implementation differences/difficulties should have been communicated with original authors,tt04glo-VrT
1480,1480,"What's more, AUC is the only evaluation metric",tt04glo-VrT
1481,1481,"Although the authors discussed the suitability of AUC score for this task, additional evaluation metrics could be considered to go beyond the paper",tt04glo-VrT
1482,1482,"So could the due diligence in hyperparameter sweep, discussion of the results, datasets other than the ones used in the original paper",tt04glo-VrT
1483,1483,"Overall, this reproduction provides some findings for reproducing the original paper but not comprehensive and could have gone beyond the original paper",tt04glo-VrT
1484,1484,"[1] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang",tt04glo-VrT
1485,1485,Parameterized explainer for graph neural network,tt04glo-VrT
1486,1486,"Advances in Neural Information Processing Systems, 33, 2020.",tt04glo-VrT
1487,1487,"This paper is well written, and the results are compared with those of the original paper",tt04glo-VrT
1488,1488,The authors have reimplemented the code using a different library and have made minor changes to their code to reproduce the results,tt04glo-VrT
1489,1489,The results are not in the same threshold as the original paper and the authors have not studied the reason behind it and have just hypothesized what might be the reason which is not enough at all,tt04glo-VrT
1490,1490,"Besides, I do not see any novelty or additional contribution beyond the original paper.",tt04glo-VrT
1491,1491,"Overall, my evaluation of the paper titled “Reproducing: Parameterized Explainer for Graph Neural Network”” is as below:",tt04glo-VrT
1492,1492,"•	The authors provide a fair representation of the objectives of the paper, its results, challenges, and contributions",tt04glo-VrT
1493,1493,(Reproducibility Summary: 10/10) ,tt04glo-VrT
1494,1494,•	The authors provide the areas of the original paper where they have tried to reproduce the results and adhere to them along with the paper,tt04glo-VrT
1495,1495,(Scope of reproducibility: 10/10),tt04glo-VrT
1496,1496,•	The code from the original paper has been used and reimplemented using PyTorch,tt04glo-VrT
1497,1497,"Overall, there are enough documentations, and the code sounds clean",tt04glo-VrT
1498,1498,(Code: 10/10) ,tt04glo-VrT
1499,1499,"•	Scope of the communication with the authors of the original paper is not mentioned, and in fact, the authors did not see any need to contact them (Communication with original authors: 0/10)",tt04glo-VrT
1500,1500,"•	The code is implemented using a different library (PyTorch), while the original implementation is in TensorFlow",tt04glo-VrT
1501,1501,The results from the new implementation do not exactly match with the original paper and the authors have not really studied why this is the case,tt04glo-VrT
1502,1502,Studying different aspects of the two implementations and detailed comparison will be very helpful for the community,tt04glo-VrT
1503,1503,"But unfortunately, the current study does not make such an effort to study the underlying reasons behind the differences",tt04glo-VrT
1504,1504,(Hyperparameter Search: 3/10)             ,tt04glo-VrT
1505,1505,•	The authors have just reimplemented the code using a different library,tt04glo-VrT
1506,1506,This study does not introduce any novelty and does not go beyond what the original paper introduces,tt04glo-VrT
1507,1507,(Ablation Study 0/10),tt04glo-VrT
1508,1508,"•	There are detailed discussions on the study, what has been done, the outcome, and fair comparisons of the results from two libraries is provided (Discussion on results 10/10)   ",tt04glo-VrT
1509,1509,"•	The detailed illustration of the results is very helpful, but the authors do not provide any recommendation on future work for reproducibility",tt04glo-VrT
1510,1510,They mainly summarized what they have tried and the differences in the outcome with respect to the original paper,tt04glo-VrT
1511,1511, (Recommendations for reproducibility 5/10)        ,tt04glo-VrT
1512,1512,•	I did not see any new novel contribution from this paper,tt04glo-VrT
1513,1513,(Results beyond the paper   0/10),tt04glo-VrT
1514,1514,"•	The paper is organized, coherent, clear, and well structured",tt04glo-VrT
1515,1515,(Overall organization and clarity 10/10),tt04glo-VrT
1516,1516,        ,tt04glo-VrT
1517,1517,I recommend reject of this paper.,tt04glo-VrT
1518,1518,"This work aims at reproducing the iFlow method proposed in ""Identifying Through Flows for Recovering Latent Representations""",CoKdaIQzeRx
1519,1519,"The original paper made some claims against the baseline approach iVAE, and this work reran the experiments and validated some of its claims, but not all: in particular, this work showed that in the 2D case, iVAE does not have mode collapsing issues as claimed by the original paper and actually outperforms iFlow in terms of MCC",CoKdaIQzeRx
1520,1520,"Other than this, this work is able to reproduce the major claims by the original paper.",CoKdaIQzeRx
1521,1521,Pros:,CoKdaIQzeRx
1522,1522,1,CoKdaIQzeRx
1523,1523,This work conducted many experiments and was able to rerun most of the experiments in the original paper despite some experiments needing changes to the code,CoKdaIQzeRx
1524,1524,Cons:,CoKdaIQzeRx
1525,1525,1,CoKdaIQzeRx
1526,1526,"In section 6 the authors suspected that ""One possible explanation is the bug regarding the batch size, which could mean the paper used a batch size of eight rather than the reported 64."" Is there evidence that under batch size 8 VAE gets worse performance and occasionally gets mode collapsing?",CoKdaIQzeRx
1527,1527,2,CoKdaIQzeRx
1528,1528,"I like the extension to the MNIST case, but unfortunately, this work didn't propose a feasible implementation.",CoKdaIQzeRx
1529,1529,3,CoKdaIQzeRx
1530,1530,"In terms of presentation style, the captions of figure 1 are too small, and it's not very clear what are the two rows of figure 3",CoKdaIQzeRx
1531,1531,Is the top row iVAE and the bottom iFlow?,CoKdaIQzeRx
1532,1532,"Overall, this work has reproduced the major claims of the original paper, and also showed some potential bugs of the original paper in the 2D case",CoKdaIQzeRx
1533,1533,I am leaning towards its acceptance.,CoKdaIQzeRx
1534,1534,Reproducibility Summary,CoKdaIQzeRx
1535,1535,The authors did an excellent job of explaining the summary of the report,CoKdaIQzeRx
1536,1536,They also mention a detailed description of the reproduced results and the difficulty faced during the reproducibility of the original paper.,CoKdaIQzeRx
1537,1537,Scope of reproducibility,CoKdaIQzeRx
1538,1538,The authors have clearly stated the scope of reproducibility with clarity in the claims they learned from the original paper with further details explaining the clams.,CoKdaIQzeRx
1539,1539,Code,CoKdaIQzeRx
1540,1540,"From my understanding, the code was provided by the original authors with detailed information about the hyper-parameter search",CoKdaIQzeRx
1541,1541,But the authors do a commendable job in preparing the code-base for figures and thoroughly checking the code for any possible scope of improvement.,CoKdaIQzeRx
1542,1542,Communication with original authors,CoKdaIQzeRx
1543,1543,As mentioned by the authors they were not able to get clarification from the original authors,CoKdaIQzeRx
1544,1544,It would be great if they could share the doubts they had during the reproducibility which were not clarified by the original authors,CoKdaIQzeRx
1545,1545,Hyperparameter Search,CoKdaIQzeRx
1546,1546,The original authors have described the hyper-parameters needed for the successful replication of the numbers,CoKdaIQzeRx
1547,1547,But the author’s effort of verifying every seed and python version to get the closest results is really commendable and also making the code publicly available is a crucial step for advancing the current work.,CoKdaIQzeRx
1548,1548,Ablation Study,CoKdaIQzeRx
1549,1549,The authors did a great job of showing ablation for both iFlow and iVae methods.,CoKdaIQzeRx
1550,1550,Discussion on results,CoKdaIQzeRx
1551,1551,There is a discussion section mentioned in the paper that repeats how the experiments support the claims mentioned in the original paper,CoKdaIQzeRx
1552,1552,It would have been great to see if the authors can provide a thorough description of the method strengths and weakness and why the experiments can support the claim,CoKdaIQzeRx
1553,1553, ,CoKdaIQzeRx
1554,1554,Results beyond the paper,CoKdaIQzeRx
1555,1555, ,CoKdaIQzeRx
1556,1556,The authors have mentioned a section in which they mention the experiment with a real dataset to check if the claim still holds,CoKdaIQzeRx
1557,1557,It is also nice to see the difficultly faced while experimenting with a new dataset,CoKdaIQzeRx
1558,1558,It would have been great to see if both the paragraphs (sec,CoKdaIQzeRx
1559,1559,4.4.7 sec,CoKdaIQzeRx
1560,1560,6.2) would be structured more appropriately for easier readability.,CoKdaIQzeRx
1561,1561,Overall organization and clarity,CoKdaIQzeRx
1562,1562,I found the report to be well organized and extremely easy to read,CoKdaIQzeRx
1563,1563,"The plots, however, could be slightly updated to improve readability.",CoKdaIQzeRx
1564,1564,## Overview,Lwb6qIpEW9-
1565,1565,"This report aims to investigate the reproducibility of ""Towards visually explaining variational autoencoders""",Lwb6qIpEW9-
1566,1566,The reproducibility summary is insightful and details the step necessary as well as the difficulty they encountered while trying to reproduce the results of the paper,Lwb6qIpEW9-
1567,1567,"It shows that even if more and more authors are now making code available through different means (paper with code, GitHub), there are still some ways to go before reproducibility in AI is the norm",Lwb6qIpEW9-
1568,1568,## Quality:,Lwb6qIpEW9-
1569,1569,The quality of the report is mixed.,Lwb6qIpEW9-
1570,1570,### Pros,Lwb6qIpEW9-
1571,1571,- The methodology is well explained and the plan to reproduce the results is solid and well thought out,Lwb6qIpEW9-
1572,1572,The authors did an extensive effort to reproduce and also re-implement the results from the original paper,Lwb6qIpEW9-
1573,1573,"The code they made available online to reproduce the results from the report work well, they are sufficiently commented and I could reproduce the results from the report",Lwb6qIpEW9-
1574,1574,Contrary to the original paper where a large proportion of the code was missing,Lwb6qIpEW9-
1575,1575,"- The dataset, parameters, and setup for the experiments are well presented",Lwb6qIpEW9-
1576,1576,- The results are well presented and show that the implementation is functional,Lwb6qIpEW9-
1577,1577,### Cons,Lwb6qIpEW9-
1578,1578,"However, the paper is not well structured in my opinion:",Lwb6qIpEW9-
1579,1579,- the last paragraph of the introduction should absolutely state the results of the report upfront,Lwb6qIpEW9-
1580,1580,"It should be written in the introduction what was achieved by the paper, the reader should not have to read the whole paper to have the most relevant information",Lwb6qIpEW9-
1581,1581,The report presents the three major claims that are made by the original paper and should answer them in the same paragraph,Lwb6qIpEW9-
1582,1582,- the methodology section is missing a high-level explanation of the concepts they are aiming to implement,Lwb6qIpEW9-
1583,1583,"As it stands, a reader not familiar with the original paper could not follow the experiments that are being presented",Lwb6qIpEW9-
1584,1584,"For example, the disentanglement problem is not defined nor explained in the report",Lwb6qIpEW9-
1585,1585,"- section 4.1 ""Discussion of results"" should have been included in the section results",Lwb6qIpEW9-
1586,1586,This makes the results and the figure hard to follow,Lwb6qIpEW9-
1587,1587,## Clarity,Lwb6qIpEW9-
1588,1588,The paper is well written and the article is easy to read,Lwb6qIpEW9-
1589,1589,"However, it would need more proofreading",Lwb6qIpEW9-
1590,1590,- What are the equation referring to at line 87 and line 130 ??,Lwb6qIpEW9-
1591,1591,"- Why in table 1 (a) line ""conv 2"" Our reproduction ""0.644"", our best ""0.320"" the best results is lower than the reproduction? ",Lwb6qIpEW9-
1592,1592,"Also, the references are inconsistent, for example, citation lines 320-322 and citation lines 339-341 are citing the same conference and are not formatted in the same way",Lwb6qIpEW9-
1593,1593,## Final words,Lwb6qIpEW9-
1594,1594,"The authors of the report did an excellent work reproducing the results from the original paper and had an interesting take on why their results are not consistent with the original paper, however, the paper is poorly presented and is missing crucial high-level information on the main concept they are trying to present",Lwb6qIpEW9-
1595,1595,"Finally, the review is supposed to be double blind, however, the GitHub repository where the authors made the code available was not anonymized",Lwb6qIpEW9-
1596,1596,"The report clearly summarizes the problem statement of the original paper ""Towards Visually Explaining Variational Autoencoders""",Lwb6qIpEW9-
1597,1597,The submission has covered all experiments in the original paper,Lwb6qIpEW9-
1598,1598,"Particularly, when the original implementations are missing, the authors of the submission use good reasonings and additional material to decide their own setting",Lwb6qIpEW9-
1599,1599,"The authors also use good reasonings to discuss why their results are different from the original paper, especially on the difference for the UCSD Ped1 dataset",Lwb6qIpEW9-
1600,1600,The submission itself has covered essential implementation details,Lwb6qIpEW9-
1601,1601,"After reading this submission, I don't see any further details that are needed for the implementation",Lwb6qIpEW9-
1602,1602,Minor comments: ,Lwb6qIpEW9-
1603,1603,1) It would be helpful to provide a learning rate scheduler in supplementary.,Lwb6qIpEW9-
1604,1604,"2) Do the authors try to choose two latent dimensions not randomly, but through a search of which two dimensions are the most informative (either for the overall entropy, or the mutual information between the input and the latent encodings?)",Lwb6qIpEW9-
1605,1605,Is it possible/feasible to compute these metrics without exhaust the GPU? ,Lwb6qIpEW9-
1606,1606,"Reproducibility Summary: Included, covers the relevant topics",Lwb6qIpEW9-
1607,1607,Scope of Reproducibility: Re-used existing code for MNIT experiments; custom implementation for the other datasets,Lwb6qIpEW9-
1608,1608,Code: linked to github repo,Lwb6qIpEW9-
1609,1609,"Communication with Original Authors: the authors were contacted, but no response was received directly",Lwb6qIpEW9-
1610,1610,It seems like a reasonable and fair effort was made.,Lwb6qIpEW9-
1611,1611,Hyper-parameter Search: The reproduction largely relies on its own implementation,Lwb6qIpEW9-
1612,1612,Ablation Study:,Lwb6qIpEW9-
1613,1613,Discussion: Clear discussion of ways that the original paper is hard to reproduce,Lwb6qIpEW9-
1614,1614,Including hard to follow code and mismatches between the implementation and the description in the paper,Lwb6qIpEW9-
1615,1615,Recommendations for Reproducibility: Well described in section 4.2,Lwb6qIpEW9-
1616,1616,Results Beyond Paper: New baseline proposed and experimented with,Lwb6qIpEW9-
1617,1617,"Overall Organization/Clarity: well written paper, with a few minor errors in language",Lwb6qIpEW9-
1618,1618,Errata,Lwb6qIpEW9-
1619,1619,L.87 equation reference is broken,Lwb6qIpEW9-
1620,1620,"This report reproduces the paper ""Explaining Low dimensional Representation"" [1]",cqAHExg2f
1621,1621,A detailed reproducibility summary is provided summarizing the results obtained using the implementation based on the codebase of the original authors along with a new PyTorch implementation from the efforts of this reproduction report authors,cqAHExg2f
1622,1622,"Communication with original authors (how to choose the ε hyper-parameter), hyperparameter search, discussion on results, recommendations for reproducibility and results beyond the paper (replicated the algorithm in PyTorch, new datasets and the use of more complex transformations to explain differences between clusters) are also properly constructed",cqAHExg2f
1623,1623,"Overall, this reproduction is well-constructed",cqAHExg2f
1624,1624,"Possible stretches could be the discussion of the evaluation metrics being used with respect to their suitabilities, additional metrics for a more comprehensive report and providing recommendations to the original authors for improving reproducibility.",cqAHExg2f
1625,1625,   ,cqAHExg2f
1626,1626,"[1] Gregory Plumb, Jonathan Terhorst, Sriram Sankararaman, and Ameet Talwalkar",cqAHExg2f
1627,1627,"Explaining groups of points in low-dimensional representations, 2020.",cqAHExg2f
1628,1628,This paper checks the original paper's four claims (listed in lines 97--103) on the algorithm TGT (Transitive Global Translations),cqAHExg2f
1629,1629,"As pointed out (lines 105 - 107) by the authors, the original paper's claims are investigated via the original paper's original code",cqAHExg2f
1630,1630,The authors of this report use new code instead to verify some extended experiments,cqAHExg2f
1631,1631,We doubt whether this is a well-focused reproducibility investigation.,cqAHExg2f
1632,1632,We list our concerns below.,cqAHExg2f
1633,1633,1,cqAHExg2f
1634,1634,Language,cqAHExg2f
1635,1635,"There are quite a few grammar problems, and the paper appears carelessly composed.",cqAHExg2f
1636,1636,2,cqAHExg2f
1637,1637,"In the 'methodology' paragraph, by putting 'We also replicate their findings by re-implementing the authors' method in PyTorch...' does it mean that all or only a part of the computing in the original paper is re-implemented?",cqAHExg2f
1638,1638,3,cqAHExg2f
1639,1639,"Given that the communication with the original authors does not yield meaningful result and is only because of misunderstanding, why is that paragraph still listed?",cqAHExg2f
1640,1640,4,cqAHExg2f
1641,1641,Could the problem with the deprecated software package be solved by Docker or Anaconda? Note that any software package shall be deprecated at some time point in the future.,cqAHExg2f
1642,1642,5,cqAHExg2f
1643,1643,"For the clustering's uncertainty, the authors reported in the 'What was difficult' paragraph, is it because of the random seed or different implementation techniques? Is the final result sensitive to different initialisation of random clustering? It seems to us that the paper only reports the observation but has not paid any effort to uncover the cause.",cqAHExg2f
1644,1644,6,cqAHExg2f
1645,1645,What does 'GCE' stand for? (lines 80 & 83),cqAHExg2f
1646,1646,"The report is on reproducing the paper ""Explaining Low dimensional Representation"" by Plumb et al",cqAHExg2f
1647,1647,"Overall, the submission does rather thorough reproduction and even go beyond the original paper to add some extension (though missing parts such as hyperparameter search)",cqAHExg2f
1648,1648,"However, writing of the report can be largely improved as currently it looks more like a student's report rather than a work that can be published in the special issue of a journal.",cqAHExg2f
1649,1649,Reproducibility Summary: is provided and adequately reports the major finding of the submission,cqAHExg2f
1650,1650,Scope of reproducibility: is clearly stated and followed later,cqAHExg2f
1651,1651,Code: the authors of this submission both used the code provided by the original authors and then re-implemented it from scratch,cqAHExg2f
1652,1652,The github link to the code gives an error (potentially because the repository hasn't been made public yet),cqAHExg2f
1653,1653,In anyway there was no opportunity to check the code of the submission or its docs.,cqAHExg2f
1654,1654,"Communication with original authors: the reports mentions communication with original authors regarding a choice of epsilon hyperparameter, but the authors emphasised the difficulty of selecting clusters, but no mentioning whether this question was attempted to clarify with the original authors.",cqAHExg2f
1655,1655,Hyperparameter Search: in the experiments with the original code the authors only used the same hyperparameters as in the original paper,cqAHExg2f
1656,1656,No hyperparameter sweep has been performed.,cqAHExg2f
1657,1657,Ablation Study: No ablation study has been done,cqAHExg2f
1658,1658,Discussion on results: the discussion at the end is rather well done with discussions which claims from the original paper were confirmed and which were questioned during reproduction,cqAHExg2f
1659,1659,"However, presentation of the results themselves is the poorest part of the report from the presentation point of view",cqAHExg2f
1660,1660,"The results sometimes not thoroughly discussed but the text just refer to corresponding tables and figures, which in itself is not satisfactory, moreover, those tables and figures are also missing some details and can be unclear.",cqAHExg2f
1661,1661,"Recommendations for reproducibility: the only recommendation left is in terms of this selection of clusters, which seems to cause the main issue with reproducibility",cqAHExg2f
1662,1662,Results beyond the paper: the report provides results beyond the paper when the authors investigate a more expressive transformation than considered in the original paper,cqAHExg2f
1663,1663,Though the results show that this extension does not actually bring significant benefits it is interesting to see these results and negative results are also very useful for the community,cqAHExg2f
1664,1664,Overall organisation and clarity: the part that can be mostly improved,cqAHExg2f
1665,1665,"As mentioned before, the results section is the poorest here lacking the good presentation.",cqAHExg2f
1666,1666,Some particular points (mostly on organisation and clarity):,cqAHExg2f
1667,1667,"1.	Lines 5-6, “They show their method …” – the sentence is not grammatically consistent",cqAHExg2f
1668,1668,"2.	Line 43, x_s is not defined",cqAHExg2f
1669,1669,3.	Notation x for a low-dimensional representation in Introduction is not very suitable as it is later used to denote a point in the input space in Methodology,cqAHExg2f
1670,1670,"4.	Lines 197-198, “Since the clusters in the Tensorflow…” – the sentence is not grammatically consistent",cqAHExg2f
1671,1671,"5.	Section 5.3.2 and 5.3.3, “deltas” and “logit gammas” appear without introduction (translation and scaling were not called like this before)",cqAHExg2f
1672,1672,They are being explained only later in Section 5.3.4,cqAHExg2f
1673,1673,"6.	Section 5.3.5, G is not defined and j is reused here as it denoted the group in lower dimensional space before",cqAHExg2f
1674,1674,"7.	Line 252, “Based on the reproduction…” – unfinished sentence ",cqAHExg2f
1675,1675,"8.	Section 6.1, first paragraph – translation of jupyter notebook into python scripts doesn’t seem to be a problem",cqAHExg2f
1676,1676,"9.	Line 268, “This also means …” – something wrong with this sentence",cqAHExg2f
1677,1677,10.	Section 6.2 is redundant as this idea has been discussed at the beginning,cqAHExg2f
1678,1678,"11.	Figures 5-10 require more explanation, axes are not labelled, missing legend and overall explanation what is going on",cqAHExg2f
1679,1679,“0th dimension” does not look good in the text,cqAHExg2f
1680,1680,Minor:,cqAHExg2f
1681,1681,"1.	Line 171, there seems to be a typo and the second x_2 should be x_3",cqAHExg2f
1682,1682,2.	Figures 2 and 3 – make all subfigures of the same size,cqAHExg2f
1683,1683,This reproducibility review is of high quality: the authors were successfully reproducing the earlier results in another modeling ecosystem and the reporting is clear,kNqh-T-OJc
1684,1684,"If there would be anything to improve, it would be about finding some examples that pose potential problems for reproducibility; even with good reproducibility there can be potential pitfalls and reporting what has been done to identify and point out the weak points would be informative if such examples can be found.",kNqh-T-OJc
1685,1685,The authors have done a fine job in reproducing the original paper,kNqh-T-OJc
1686,1686,* The reproducibility summary is provided,kNqh-T-OJc
1687,1687,* It seemed fairly easy to reproduce the results from the paper,kNqh-T-OJc
1688,1688, ** The authors converted the original code to Pytorch and started their analysis from scratch,kNqh-T-OJc
1689,1689,Their results tallied with that of  the original paper,kNqh-T-OJc
1690,1690," ** Further, the authors ran the code on a new Crop dataset checking code/idea generalizability",kNqh-T-OJc
1691,1691,This is a great move.,kNqh-T-OJc
1692,1692,"* Even though no communication with the original authors was needed, some areas to tidy up in the code and notebooks are provided by the authors but these are minor.",kNqh-T-OJc
1693,1693,* Overall approach to reproduce the results and the paper are clear,kNqh-T-OJc
1694,1694,"Authors attempted to reproduce the main experiments in the original paper, namely (1) per-image attention maps on MNIST, (2) anomaly detection explanations on MNIST, (3) anomaly detection on UCSD Ped1 and MVTec-AD, and (4) latent space disentanglement on dSprites",b9wHFv2V_j
1695,1695,The authors of the original paper only provided code for generating visual explanations on MNIST.,b9wHFv2V_j
1696,1696,"Authors distinguish two visualization methods: (1) visualizing explanations for VAEs by calculating the attention maps per latent dimension and aggregating these, and (2) generating attention maps from the sum of the inferred mean vector in order to visualize anomalies",b9wHFv2V_j
1697,1697,"Authors then compare the two methods for anomaly detection, which I believe is incorrect",b9wHFv2V_j
1698,1698,(1) aims at highlighting the areas of the input image that explain the data (e.g,b9wHFv2V_j
1699,1699,"if the VAE was trained on 1's, which regions of the image look like a 1?)",b9wHFv2V_j
1700,1700,"On the other hand, (2) aims at highlighting the regions of the input image that are not well explained by the distribution of the training data (i.e",b9wHFv2V_j
1701,1701,"if the VAE was trained on 1's, which regions of the image do *not* look like a 1?)",b9wHFv2V_j
1702,1702,"Therefore, the first method should be used to reproduce Figure 1 in the original paper, whereas the second method should be used to reproduce Figure 4.",b9wHFv2V_j
1703,1703,"Authors used the provided code for the aforementioned experiments, where they had to remove an input parameter to make it work",b9wHFv2V_j
1704,1704,"There is an issue on this regard in the repository, where the first author of the original paper wrote the following: ""[...] To me, it definitely has to do with the new updates of PyTorch",b9wHFv2V_j
1705,1705,"But unfortunately I'm not exactly sure how they affect auto-gradients in our case.""",b9wHFv2V_j
1706,1706,"I would encourage authors to double-check that the code behaves as expected, or run the provided code using version 1.0 of PyTorch as suggested in the repository.",b9wHFv2V_j
1707,1707,"The remainder of the submission attempts to reproduce the rest of experiments, but all results (including baselines) are quite far from those reported in the original paper",b9wHFv2V_j
1708,1708,"It is unclear why this happens and, while authors suggest that it might be due to shorter training runs, I suspect that this might also be related to some mistakes in the implementation of the attention maps.",b9wHFv2V_j
1709,1709,"In general, the submission failed at reproducing the original results",b9wHFv2V_j
1710,1710,It is unclear whether this is due to a difference in the experimental setup or due to implementation errors,b9wHFv2V_j
1711,1711,"For this reason, I lean towards rejecting the paper and encourage authors to investigate why the results on MNIST could not be replicated before moving on to the more complex experiments in the paper.",b9wHFv2V_j
1712,1712,Minor comment: captions for Figures 1 and 2 are very short and it is difficult to understand what is being shown.,b9wHFv2V_j
1713,1713,Some typos:,b9wHFv2V_j
1714,1714,"- Reproducibility Summary: papers -> paper's, its' -> its",b9wHFv2V_j
1715,1715,- Introduction: models -> model's (or models'),b9wHFv2V_j
1716,1716,- Figure 2: Repliction -> Replication,b9wHFv2V_j
1717,1717,- UCSD Ped1 Results: preform -> perform,b9wHFv2V_j
1718,1718,This review is detailed and high quality,b9wHFv2V_j
1719,1719,It aims to reproduce VAE results on multiple data sets,b9wHFv2V_j
1720,1720,The reproducibility report is robust since the authors are able to replicate results in some of the data sets (MNIST),b9wHFv2V_j
1721,1721,"This, together with the detailed reporting, demonstrates that they had acquired the necessary understanding of the methodology in order to reproduce the original analyses",b9wHFv2V_j
1722,1722,"Reproducibility efforts fail in other data sets, and plausible explanations are provided",b9wHFv2V_j
1723,1723,Communication with the original authors has taken place appropriately.,b9wHFv2V_j
1724,1724,The work is significant as it robustly highlights potential problems in the original publication in terms of reproducibility and includes thorough reporting and discussion about the contributing factors.,b9wHFv2V_j
1725,1725,This paper provides a detailed description of the setup for reproducibility and well-discussed experiments and results,b9wHFv2V_j
1726,1726,"Given the limited resources and lack of support from authors of the original paper, it shows the difficulties for reproducing the main results for anomaly detection",b9wHFv2V_j
1727,1727,It is not very clear about the scope of reproducibility based on the description from the reproducibility summary on page 1,b9wHFv2V_j
1728,1728,It sounds more like a description of the main results in the original paper,b9wHFv2V_j
1729,1729,“To produce attention maps to localize anomalies for the MNIST dataset the repository of the authors could be used.” It is not clear here whether reproduced from the reused author repository or not.,b9wHFv2V_j
1730,1730,The writing of this paper needs to be polished,b9wHFv2V_j
1731,1731,Examples below show the sentences that may need to rephrase: ,b9wHFv2V_j
1732,1732,“The papers primary claim is that ...”,b9wHFv2V_j
1733,1733,"“ ..., its’ supplement and external source code.”",b9wHFv2V_j
1734,1734,"The authors have attempted to reproduce the results of the article ""Towards Visually Explaining Variational Autoencoders"" by Liu et",mXpKOalTtN8
1735,1735,al,mXpKOalTtN8
1736,1736,(CVPR 2020),mXpKOalTtN8
1737,1737,They have presented a summary report which has all the required elements,mXpKOalTtN8
1738,1738,The authors have made a significant effort to reproduce the results of the original paper,mXpKOalTtN8
1739,1739,They mentioned that they obtained some of the code from the authors and sourced the rest from other github-repositories,mXpKOalTtN8
1740,1740,They also report having contacted the authors to clarify information in the original paper,mXpKOalTtN8
1741,1741, ,mXpKOalTtN8
1742,1742,The report is reasonably well organized though the explanation of the original paper in the Introduction could use improvement,mXpKOalTtN8
1743,1743,They could include some more details about the methodology so as to enable understanding it without reading the original paper.,mXpKOalTtN8
1744,1744,Some of the other concerns I have with the report are as follows: ,mXpKOalTtN8
1745,1745,1,mXpKOalTtN8
1746,1746,They report that the original paper's authors' code was incomplete but do not clarify what additions/adjustments they had to make to the code,mXpKOalTtN8
1747,1747,They have not provided their modified codebase.,mXpKOalTtN8
1748,1748,2,mXpKOalTtN8
1749,1749,The authors' primary focus is to reproduce the anomaly detection results and report that they were unable to reproduce all the results of the original paper,mXpKOalTtN8
1750,1750,They also report that their results for the quantitative metrics do not match those of the original paper,mXpKOalTtN8
1751,1751,"However, the authors have not tried to explain why  The code for quantitative metrics used to evaluate the results was written by the authors themselves",mXpKOalTtN8
1752,1752,Is it possible that there were errors/differences in the evaluation code? There is no mention of any correspondence between them and the original paper's authors to discuss the differences in the results,mXpKOalTtN8
1753,1753,Their final conclusions do not include any analysis of the differences in the results,mXpKOalTtN8
1754,1754,3,mXpKOalTtN8
1755,1755,It is impressive that they have reported the training time for each of the data sets that they have tested on but they have not due diligence on the hyperparameter sweep,mXpKOalTtN8
1756,1756,The only hyperparameter they have performed a search on is the number of epochs,mXpKOalTtN8
1757,1757,They have used the default values  (as specified by the original paper's authors in their supplementary material) for the other hyperparameters,mXpKOalTtN8
1758,1758,4,mXpKOalTtN8
1759,1759,Though the reproduction of the results of latent disentanglement using the dSprites dataset is included in the scope of the reproducibility they have not performed these tests,mXpKOalTtN8
1760,1760,"In summary, though the authors have made a significant effort and shown the difficulties in reproducing the results of the original paper,  I believe that this report could use a lot of improvement",mXpKOalTtN8
1761,1761,"The original paper was difficult to reproduce because of the lack of certain parts of the code and poor documentation, however, the authors did a good job in reproducing and verifying various claims",mXpKOalTtN8
1762,1762,"As per the results, the quantitative values reported here are lower than expected",mXpKOalTtN8
1763,1763,The paper is very well written,mXpKOalTtN8
1764,1764,What is lacking is a thorough hyperparameter exploration,mXpKOalTtN8
1765,1765, ,mXpKOalTtN8
1766,1766,The authors provide a very nice summary and concrete that is being elaborated in the document,mXpKOalTtN8
1767,1767,They cover fully the reproducibility of the paper in terms of scope where they reused the original author's code,mXpKOalTtN8
1768,1768,It would be interesting to perform a hyperparameter search in order to verify the claims of the authors and maybe find new hyperparameters not experimented with the authors,mXpKOalTtN8
1769,1769,"In general, the report is well written and has a clear structure so I recommend it for being accepted",mXpKOalTtN8
1770,1770,*Scope of reproducibility:*,-rn9m0Gt6AQ
1771,1771,"The authors clearly state the scope of their experiments (reproducing the embedding experiment, restricted-self attention with BERT and the sequence-to-sequence experiments), and cleanly execute on them",-rn9m0Gt6AQ
1772,1772,*Code:*,-rn9m0Gt6AQ
1773,1773," In some instances, the authors re-used code from the original authors, and in some instances, they wrote their own",-rn9m0Gt6AQ
1774,1774,I did not see a reference to the code written for this paper,-rn9m0Gt6AQ
1775,1775,*Communication with original authors:*,-rn9m0Gt6AQ
1776,1776,The paper authors reach out to the original authors,-rn9m0Gt6AQ
1777,1777,"It looks like a good discussion took place, and the original authors were helpful in clarifying some of the ambiguities that arose through the paper (and also in providing code + datasets)",-rn9m0Gt6AQ
1778,1778,*Hyperparameter Search:*,-rn9m0Gt6AQ
1779,1779,Neither the replication study nor the original paper used a hyperparameter search,-rn9m0Gt6AQ
1780,1780,"However, the replication study included results on the variance between the 5 random seeds (original paper reported the mean)",-rn9m0Gt6AQ
1781,1781,*Ablation Study:*,-rn9m0Gt6AQ
1782,1782,I don't believe the replication study performed any ablations,-rn9m0Gt6AQ
1783,1783,*Discussion on results:*,-rn9m0Gt6AQ
1784,1784,The replication study presented an excellent description of the reproducibility of the original paper and made clear when the results reproduced and did not reproduce,-rn9m0Gt6AQ
1785,1785,"They clearly stated that some details were ambiguous, but that they were ultimately able to resolve those details",-rn9m0Gt6AQ
1786,1786,*Recommendations for reproducibility:*,-rn9m0Gt6AQ
1787,1787,The replication study authors and original paper authors seem to have clarified some ambiguities during their discussion,-rn9m0Gt6AQ
1788,1788,Those would be useful to add to the original paper,-rn9m0Gt6AQ
1789,1789,*Results beyond the paper:*,-rn9m0Gt6AQ
1790,1790,The replication study investigates the idea that under-parameterization of the models could lead to a decrease in test accuracy when the hyperparameter controlling attention mass on impermissible tokens increases (less able to pay attention to impermissible tokens),-rn9m0Gt6AQ
1791,1791,They investigated this by increasing the embedding dimension and found it did not improve test accuracy,-rn9m0Gt6AQ
1792,1792,*Overall organization and clarity*,-rn9m0Gt6AQ
1793,1793,* I appreciated the reproduction of the tables from the paper with the author|reproduced! It made it easy to follow along with,-rn9m0Gt6AQ
1794,1794,* Thank you for including the breakdown of the computational requirements for running each task (table 2),-rn9m0Gt6AQ
1795,1795,This is great,-rn9m0Gt6AQ
1796,1796,"* I think the explanation of the seq2seq tasks makes sense if you have read the original paper, but could be confusing if someone has not",-rn9m0Gt6AQ
1797,1797,Please try writing more on this!,-rn9m0Gt6AQ
1798,1798,	,-rn9m0Gt6AQ
1799,1799,The authors follow the Reproducibility Summary very well.,-rn9m0Gt6AQ
1800,1800,The authors re-used the original code repository,-rn9m0Gt6AQ
1801,1801,They also ported the code to PyTorch Lightning to make it easier to reproduce the research in the future.,-rn9m0Gt6AQ
1802,1802,They did not change any hyperparameters and used the same value in the original paper.,-rn9m0Gt6AQ
1803,1803,The authors had fair contact with the authors of original papers and they had discussed minor issues.,-rn9m0Gt6AQ
1804,1804,There are no changes in hyperparameters,-rn9m0Gt6AQ
1805,1805,The implementation results are the average of five-run times of training the model,-rn9m0Gt6AQ
1806,1806,"They used all datasets except the Reference Letters, due to privacy concerns.",-rn9m0Gt6AQ
1807,1807,The authors added two Blue scores in the seq-seq model for translation.,-rn9m0Gt6AQ
1808,1808,"They do not propose any beyond results or improving the original paper, but they have a good discussion that can describe the deep understanding of the paper.",-rn9m0Gt6AQ
1809,1809,There are minor grammar typos.,-rn9m0Gt6AQ
1810,1810,The paper is well written and easy to follow,-rn9m0Gt6AQ
1811,1811,Authors seem to understand the original paper very well and have done a good job in organizing the content,-rn9m0Gt6AQ
1812,1812,"Given one of the classification dataset was not publicly available, reproducing for one task was not feasible",-rn9m0Gt6AQ
1813,1813,Apart from which authors have experimentally verified the claims on other tasks/datasets.,-rn9m0Gt6AQ
1814,1814,Authors have been in constant touch with original paper authors and it is appreciable that original authors have helped in reproducing the experiments by providing data/code as requested by authors along with providing more details/clarifying queries.,-rn9m0Gt6AQ
1815,1815,It is good that the authors had re-implemented BERT model (as the source code was not available initially) and discussed the challenges in re-implementing with information provided in paper which is important for Reproducibility challenge,-rn9m0Gt6AQ
1816,1816,"However, they were not able to replicate the results",-rn9m0Gt6AQ
1817,1817,Once the source code for BERT experiments was provided by original authors they were able to reproduce the results and verify authors claims,-rn9m0Gt6AQ
1818,1818,It would have been interesting if the authors could have identified the reason for performance drop in their re-implementation setting,-rn9m0Gt6AQ
1819,1819,"In table 4, it is interesting to note that there is no change in accuracy for 'Embedding model' on occupation prediction task which is not expected as removing impermissible tokens negatively impacts performance",-rn9m0Gt6AQ
1820,1820,Any explanation for this behavior w.r.t impermissible tokens ?,-rn9m0Gt6AQ
1821,1821,"Table 5 includes accuracies from paper and reproduced results, however the original paper does not report accuracy for the task 'En-De MT' - where do these numbers come from ?",-rn9m0Gt6AQ
1822,1822,"Additionally, authors experiment with different embedding sizes hoping to counter the performance drop as λ increases - however it was inconclusive",-rn9m0Gt6AQ
1823,1823,Having said that authors provide another insight into why they believe could be the reason for performance drop,-rn9m0Gt6AQ
1824,1824,It would have been interesting to see the impact of the way to remove impermissible tokens on performance trend.,-rn9m0Gt6AQ
1825,1825,"Overall, the paper provides good discussion points and clearly outlines what was provided, what was challenging to infer based on the information in paper/code",-rn9m0Gt6AQ
1826,1826,They perform all the experiments mentioned in the original paper and verify the claims along with providing insights and implementation aspect,-rn9m0Gt6AQ
1827,1827,This work helps in understanding the internal details required to reproduce the original paper,-rn9m0Gt6AQ
1828,1828,"Hence, I would recommend to accept this paper.",-rn9m0Gt6AQ
1829,1829,***Reproducibility Summary***,eAy3DVh0xp
1830,1830,The authors provide a complete and useful reproducibility summary.,eAy3DVh0xp
1831,1831,***Scope of reproducibility***,eAy3DVh0xp
1832,1832,The authors clearly state the experiments they are trying to reproduce from the original paper and their setup.,eAy3DVh0xp
1833,1833,***Code***,eAy3DVh0xp
1834,1834,"The authors reimplemented some code and referred to some parts of the original code, which is not complete and is missing parts to reproduce certain experiments.",eAy3DVh0xp
1835,1835,***Communication with the original authors***,eAy3DVh0xp
1836,1836,The authors of the report clearly indicate that they have had limited communication with the original authors of the paper through GitHub issues,eAy3DVh0xp
1837,1837,"Since there were some issues in reproducing some of the results, more communication is encouraged to further investigate the issues.",eAy3DVh0xp
1838,1838,***Hyperparameter search***,eAy3DVh0xp
1839,1839,The authors coducted reasonable hyperparameter searches considering their computational resources and the amount of details provided in the original paper.,eAy3DVh0xp
1840,1840,***Ablation study***,eAy3DVh0xp
1841,1841,The authors have conducted additional ablation studies on the idea proposed in the original paper as an extension of that work.,eAy3DVh0xp
1842,1842,***Discussion on results***,eAy3DVh0xp
1843,1843,"The discussions of the results is through, highlighting what was reproduced, what was not and possible causes.",eAy3DVh0xp
1844,1844,***Results beyond the paper***,eAy3DVh0xp
1845,1845,The authors have included ablation studies as additional results beyond the original paper.,eAy3DVh0xp
1846,1846,***Overall organization and clarity***,eAy3DVh0xp
1847,1847,The report is well organized and it is quite clear.,eAy3DVh0xp
1848,1848,"Overall this is a good report, with reasonable experiments that highlight issues in reproducing some of the experiments in the original paper",eAy3DVh0xp
1849,1849,More communication with the authors is encouraged to try to understand better whether the original results are not correct or there are missing details in this reproduction.,eAy3DVh0xp
1850,1850,"The authors reproduce the paper ""Towards visually explaining variational autoencoders"" [Liu et al",eAy3DVh0xp
1851,1851,2020],eAy3DVh0xp
1852,1852,"The authors provide a brief summary of the paper and try to reproduce all the experiments mentioned in the paper, however for many of the critical experiments the results reported are different from the original paper",eAy3DVh0xp
1853,1853,"For instance, for the MVTec-AD dataset, the authors report numbers in table 2 and for some of the instances, they have double-digit differences in their AUROC measurements",eAy3DVh0xp
1854,1854,"In addition, some of the provided figures are not very informative",eAy3DVh0xp
1855,1855,For example in Fig.2 and Fig.3 qualitative results of the reproduced model as well as the original model are provided however since the frames are not exactly paired it is sometimes hard to do a fair comparison,eAy3DVh0xp
1856,1856,"Furthermore, for some frames that look similar, the attention amps look vastly different.",eAy3DVh0xp
1857,1857,"Furthermore, the language of the report can also be improved.",eAy3DVh0xp
1858,1858,"Also an extension of the original paper, authors investigate applying this method  to stacked RBMs but as they claim they could not obtain any meaningful and explainable results.",eAy3DVh0xp
1859,1859,"In this report, the authors try to reimplement, reproduce and extend results from the paper Liu et al",eAy3DVh0xp
1860,1860,(2020) - Towards visually explaining variational autoencoders,eAy3DVh0xp
1861,1861,The original paper takes a step towards visually explaining generative models by using visual attention maps conditioned on the latent space of a variational autoencoder (VAE),eAy3DVh0xp
1862,1862,The authors of the reproducibility report some parts of the original authors’ code and did slight hyper parameter tuning,eAy3DVh0xp
1863,1863,"Most of the results are claimed to be reproduced for parts of the original paper for which the code was available, while for some other parts, reproducing results was hard",eAy3DVh0xp
1864,1864,"The authors of this report also did not have an extensive communication with the original authors, except minor ones on Github by opening an issue",eAy3DVh0xp
1865,1865,"The report not only tries to reproduce the results from the original paper, but also tried an extension of the work by using Restricted Boltzmann Machines",eAy3DVh0xp
1866,1866,"The authors of the report provide a clear description of the datasets, and include a couple of ablation studies which are useful.",eAy3DVh0xp
1867,1867,Some of the parts which seemed missing or confusing were:,eAy3DVh0xp
1868,1868,"- Include appropriate citations - for example, for Restricted Boltzmann Machines",eAy3DVh0xp
1869,1869,- Line 72 - Figure 9 and 8 in the appendix are referred to,eAy3DVh0xp
1870,1870,But these don’t exist in the appendix,eAy3DVh0xp
1871,1871,- Notation in the Equation 1 can be clarified,eAy3DVh0xp
1872,1872,- Line 138 - Could be made clearer on which parts of the original code are being referred and including more details directly from the paper here would also help.,eAy3DVh0xp
1873,1873,- Formatting of Figure 1,eAy3DVh0xp
1874,1874,- Line 188 - …similar to that of the paper (see 2),eAy3DVh0xp
1875,1875,—> Seems the referred number “2” is Table 2? This can be properly labeled and linked,eAy3DVh0xp
1876,1876,"Overall, the authors seems to reproduce the results and make an educated hyper parameter selection",eAy3DVh0xp
1877,1877,Ablation studies and a couple of insufficiencies from the original paper are also highlighted,eAy3DVh0xp
1878,1878,"If the manuscript can be proof-read and typos and missing figures can be fixed, it can be taken into consideration in the final score",eAy3DVh0xp
1879,1879,"Even though the authors failed to reproduce results regarding performance, they find that the privacy claim is valid",zLzuHtQopDp
1880,1880,They provide a detailed discussion about what was easy and what was difficult when they prepare the reproducibility report,zLzuHtQopDp
1881,1881,The report is well-written and easy to follow,zLzuHtQopDp
1882,1882,The authors did well,zLzuHtQopDp
1883,1883,"Though the codes for the original work was not available, they reproduce the work by coding from scratch.",zLzuHtQopDp
1884,1884,This paper attempts to replicate the work in Interpretable Complex-Valued Neural Networks For Privacy Protection which in general was not possible due to missing details in the original paper.,zLzuHtQopDp
1885,1885,The author provided a good description of the work that they have done to replicate the original,zLzuHtQopDp
1886,1886,The authors identified the details that were missing in the paper,zLzuHtQopDp
1887,1887,"They also mention time constrains, which did not allow them to pursue for example some further hyperparameter tuning.",zLzuHtQopDp
1888,1888,The authors tried to contact the original authors without any success.,zLzuHtQopDp
1889,1889,"What would have been interesting is to provide a full summary of what would have made the reproduction easier, besides providing the pseudo-code.",zLzuHtQopDp
1890,1890,"All-in-all, it is a good paper.",zLzuHtQopDp
1891,1891,"This paper tries to reproduce the results provided in ""Identifying Through Flows for Recovering Latent Representations""",1E7p3r8-zxV
1892,1892,"the authors reproduce the experiments in the original paper obtaining results, which are reasonably close to the original paper's reported numbers, diverging by only about 2.5%",1E7p3r8-zxV
1893,1893,The reproducibility report is written in a clear way having details of the experiments conducted,1E7p3r8-zxV
1894,1894,"Furthermore, the authors investigate further experiments comparing the model with additional Flow-based models to give context to the devised model",1E7p3r8-zxV
1895,1895,"Furthermore, further discussion is done on the applicability as well as the practicality of the iFlow model",1E7p3r8-zxV
1896,1896,I have a few minor questions:,1E7p3r8-zxV
1897,1897,Why are the MCC results so different on different seeds? [Figures 1-3],1E7p3r8-zxV
1898,1898,"In figure 4, the data samples on which iFlow and Flow are compared look actually different and for the iFlow the data itself is more disentangled",1E7p3r8-zxV
1899,1899,Can you please provide some more comparable results for this?,1E7p3r8-zxV
1900,1900,This reproduction report does extensive works on different aspects to verify the original paper iFlow,1E7p3r8-zxV
1901,1901,The authors give a comprehensive study on three objects: 1) reproduce the results of MCC-scores presented in the original paper; 2) they elaborate on the fairness of the metrics used in the original paper; 3) they also evaluate the usability and practical advantages by working on different commonly used machine learning/presentation learning datasets,1E7p3r8-zxV
1902,1902,"The authors of this report find some interesting points which are not presented in the original paper, and they also give a clear presentation of the experiments, analysis, and conclusion",1E7p3r8-zxV
1903,1903,"Therefore, I feel this report is a good reproduction of the iFlow paper, which gives us a better and confidential understanding of the paper",1E7p3r8-zxV
1904,1904, ,1E7p3r8-zxV
1905,1905,This paper discusses reproducing the work included in Learning to Deceive with Attention-Based Explanations by Pruthi et al,VDlDzmfZaxg
1906,1906,Pruthi et al,VDlDzmfZaxg
1907,1907,claim that attention weights can easily be manipulated without significancy accuracy loss and that human subjects can be deceived by this attention weights,VDlDzmfZaxg
1908,1908,The paper focuses on reproducing the first part of the claim as the second part requires a user study,VDlDzmfZaxg
1909,1909,"The paper focuses on three models: embedding, biLSTM, and BERT",VDlDzmfZaxg
1910,1910,"There are two evaluation tasks: classification (occupation, pronoun-based, sentiment analysis), and sequence-to-sequence",VDlDzmfZaxg
1911,1911,"The paper shows that the results do reproduce the results fairly well, and the reproducibility aspect was acceptable (with quick responses from the authors).",VDlDzmfZaxg
1912,1912,"Overall, the writing style in this paper needs to improve",VDlDzmfZaxg
1913,1913,"There are many grammatical mistakes, and the paper does not flow very well",VDlDzmfZaxg
1914,1914,The first half of the paper did a good job of explaining the problem and the models used,VDlDzmfZaxg
1915,1915,"However, the second half of the paper, including the results and reproducibility portions, needs more work",VDlDzmfZaxg
1916,1916,"I would have like to see the differences in training time / evaluation time, better explanation of Table 4 (especially what the column names are), and more information on what correspondence occurred with the original authors (did they give a more thorough code? define the anonymization functions better?) ",VDlDzmfZaxg
1917,1917,"The report describes the efforts in replicating the results of ""Learning to Deceive with Attention-Based Explanations"", where it is shown how 1) attention weights can be manipulated without loss in performance and 2) humans can be deceived by the obtained attention weights",VDlDzmfZaxg
1918,1918,The report describes the efforts in replicating the first claim,VDlDzmfZaxg
1919,1919,"The authors of the report used the code of the original paper to replicate the results, adding the transformer part and missing anonymization functions",VDlDzmfZaxg
1920,1920,"Overall, the results have been extensively replicated and the experiments well described",VDlDzmfZaxg
1921,1921,There are however some parts not well written (e.g,VDlDzmfZaxg
1922,1922,"typos, half-sentences, missing links),  with missing explanations (e.g",VDlDzmfZaxg
1923,1923,"multi-class sentiment analysis, transformer, anonymization) and with critiques to the claims of the paper (e.g",VDlDzmfZaxg
1924,1924,"importance of the user study, capabilities of deceiving of the attention weights) not supported by experiments",VDlDzmfZaxg
1925,1925,"Overall, I think the reproduction effort has been well conducted, but the report needs to be improved (see weaknesses below) to be accepteded",VDlDzmfZaxg
1926,1926,Below I detail what I think are the strengths and the weaknesses of the report,VDlDzmfZaxg
1927,1927,Strengths:,VDlDzmfZaxg
1928,1928,+ The authors managed to replicate all the quantitative results of the main paper (on public datasets) with fair efforts for the missing components they added.,VDlDzmfZaxg
1929,1929,"+ The experiments are very detailed, from the data splits to the hyperparameters used.",VDlDzmfZaxg
1930,1930,+ Discussions on the difference between the reproduced and the original results are also thorough and well justified.,VDlDzmfZaxg
1931,1931,Weaknesses:,VDlDzmfZaxg
1932,1932,"- Maybe it is due to the lack of time, but the report misses careful proofreading",VDlDzmfZaxg
1933,1933,There are Tables not well linked (e.g,VDlDzmfZaxg
1934,1934,"lines 136,172) wrongly cited equations (Eq",VDlDzmfZaxg
1935,1935,"0, line 74), typos (e.g",VDlDzmfZaxg
1936,1936,"""Reproducability"", line 207), missing end of sentence points (line 182), upper case start of sentences (e.g",VDlDzmfZaxg
1937,1937,"line 161), half-sentences (line 202)",VDlDzmfZaxg
1938,1938,Proofreading the report is necessary to ensure its quality,VDlDzmfZaxg
1939,1939,- In Table 3 multiple numbers have an empty standard deviation for the A.M,VDlDzmfZaxg
1940,1940,column,VDlDzmfZaxg
1941,1941,"Why is this the case? The table looks a bit weird with all the empty space after some +-, thus it would be good to add those values (preferably) or eliminate the +-.",VDlDzmfZaxg
1942,1942,- Some parts and definitions are not clear,VDlDzmfZaxg
1943,1943,"For instance, I was not able to find definitions for A.M",VDlDzmfZaxg
1944,1944,"(Table 3-4) and I (equation ""0"")",VDlDzmfZaxg
1945,1945,"Similarly, the report mentions an extension of the paper by performing multi-class sentiment analysis (line 89) that however is not detailed in the following section and is not reported in the results (since the comparison is with the values reported in the original paper)",VDlDzmfZaxg
1946,1946,These details should be included to avoid misconceptions.,VDlDzmfZaxg
1947,1947,"- The attention weights \alpha are computed from the dot product between QK^T  ""softmaxed""",VDlDzmfZaxg
1948,1948,"However, line 54 reports the non-softmaxed version of the dot-product to compute the attention, which is inaccurate",VDlDzmfZaxg
1949,1949,I would suggest the authors,VDlDzmfZaxg
1950,1950,to re-define the attention A by explicitly showing the contribution of \alpha and how it is computed.,VDlDzmfZaxg
1951,1951,- The difficulty of the reproduction due to 1) missing transformer code and 2) missing anonymization (lines 209-210) are not extensively described in the report,VDlDzmfZaxg
1952,1952,"Since the report should highlight the encountered difficulties (if any) in reproducing the original results, I would extend section 2 to include a discussion of any components the authors needed to add to reproduce the results + every effort (e.g",VDlDzmfZaxg
1953,1953,"missing libraries, dataset set up) that was required to reproduce them.",VDlDzmfZaxg
1954,1954,"- The scope of the report is to reproduce the quantitative results of the original work, without reproducing the user studies",VDlDzmfZaxg
1955,1955,"While I agree that the three subjects of the original paper do not constitute a large set allowing drawing general conclusions, I do not think it is fair to say that 1) the human study is unnecessary and unfounded (lines 213-215) and 2) raising doubts on the capabilities to deceive of the model (line 218)",VDlDzmfZaxg
1956,1956,"These are personal thoughts with no scientific/experimental grounds, since not results are shown to support that humans might not be deceived",VDlDzmfZaxg
1957,1957,I strongly suggest removing any claim not supported by 1) experiments 2) experience in the reproduction.,VDlDzmfZaxg
1958,1958,"- The sentence in lines 214-215: ""it added bulk to an unstructured paper, which in our minds, would have benefited from less individual projects""  is non-sensical and not founded anyway since 1) the unstructured paper is the original one? (peer-reviewed and accepted to ACL 2020) 2) what are the individual projects?",VDlDzmfZaxg
1959,1959,"The authors did a good work, they joggle round missing data and came up with same result as the original authors.",VDlDzmfZaxg
1960,1960,Reproducibility Summary:,lE0wqKGROKa
1961,1961,- The summary did a nice job outlining the report.,lE0wqKGROKa
1962,1962,Scope of reproducibility:,lE0wqKGROKa
1963,1963,"- Makes clear what the scope of the report is, and provides nice justification for why certain parts were left out (i.e",lE0wqKGROKa
1964,1964,missing data sets).,lE0wqKGROKa
1965,1965,Code:,lE0wqKGROKa
1966,1966,- Uses the original code,lE0wqKGROKa
1967,1967,- Does an extensive look and dissection of the code base to find some issues worth noting,lE0wqKGROKa
1968,1968,For example:,lE0wqKGROKa
1969,1969,"  - orthogonalization applied to both the P-path and Q-path), which wasn't obvious from the original text",lE0wqKGROKa
1970,1970,They do a small experiment to test the model's sensitivity to this change.,lE0wqKGROKa
1971,1971,  - The final prediction is calculated differently than reported,lE0wqKGROKa
1972,1972,  - Unreported fine-tuning of the embeddings for the two paths independently,lE0wqKGROKa
1973,1973,  - An odd choice in how the dev set is chosen from the training set.,lE0wqKGROKa
1974,1974,  ,lE0wqKGROKa
1975,1975,Communication with original authors:,lE0wqKGROKa
1976,1976,- Mentioned sending email to clarify parts of the code with no response.,lE0wqKGROKa
1977,1977,  ,lE0wqKGROKa
1978,1978,Hyperparameter Search:,lE0wqKGROKa
1979,1979,"- None done in the reproduction, and none done here.",lE0wqKGROKa
1980,1980,  ,lE0wqKGROKa
1981,1981,Ablation Study:,lE0wqKGROKa
1982,1982,N/A,lE0wqKGROKa
1983,1983,Discussion on results:,lE0wqKGROKa
1984,1984,- Did a nice job raising concerns about the original paper's findings.,lE0wqKGROKa
1985,1985,"- Raised many concerns, gave evidence for why these concerns would be problematic (even testing what the changes would be), and gave concrete steps for improvement.",lE0wqKGROKa
1986,1986,Results beyond the paper:,lE0wqKGROKa
1987,1987,- Expand the set of evaluation metrics,lE0wqKGROKa
1988,1988,- Test claim's generality on different architectures,lE0wqKGROKa
1989,1989,"Specifically, the authors applied the metrics to a Bi-directional LSTM model.",lE0wqKGROKa
1990,1990,Overall organization and clarity,lE0wqKGROKa
1991,1991,"Overall, the report is well written and relatively clear",lE0wqKGROKa
1992,1992,Some things I would suggest/some questions:,lE0wqKGROKa
1993,1993,- How many runs did you do for each experiment? Is this similar to what the original authors did?,lE0wqKGROKa
1994,1994,"- table 6, how were the confidence intervals calculated?",lE0wqKGROKa
1995,1995,"- The sub-section ""other attention mechanisms"" in section 7 is a bit out of place",lE0wqKGROKa
1996,1996,"Maybe you can remove the this entirely, and add it to the final discussion? Because you don't run more experiments here, it just doesn't seem to fit",lE0wqKGROKa
1997,1997,"If you do run experiments here, this needs to be made much clearer.",lE0wqKGROKa
1998,1998,"- The original paper does not seem to do a parameter sweep of any kind, so this would have been a nice inclusion in the report",lE0wqKGROKa
1999,1999,Event if it wasn't for all the data sets.,lE0wqKGROKa
2000,2000,- The original paper chooses the best model (out of a set of unknown size) from the validation accuracy and reports the test accuracy,lE0wqKGROKa
2001,2001,I'm not apart of the community this paper is aimed at (i.e,lE0wqKGROKa
2002,2002,"NLP/Explainability), so I'm not sure how common this is as a practice",lE0wqKGROKa
2003,2003,"Usually, I would prefer to see multiple runs and either a median or average reported with confidence intervals",lE0wqKGROKa
2004,2004,Of course this can be ignored if this isn't standard practice in this community.,lE0wqKGROKa
2005,2005,Again,lE0wqKGROKa
2006,2006,I think this report is well put together and the biggest weakness is the lack of hyperparameter search and ambiguity around how the models were reported (i.e,lE0wqKGROKa
2007,2007,"best of, mean, median) w/o confidence intervals.",lE0wqKGROKa
2008,2008,* Reproducibility Summary,lE0wqKGROKa
2009,2009,  The report contains a well-defined and articulate reproducibility summary as prescribed by the challenge.,lE0wqKGROKa
2010,2010,* Scope of reproducibility,lE0wqKGROKa
2011,2011,  ,lE0wqKGROKa
2012,2012,"  The report contains well-defined scope involving two central claims of the original paper - attention weights not being faithful in plausible explanations in LSTM, and methods to reduce the conicity in order to increase the plausibility of the explanations.",lE0wqKGROKa
2013,2013,* Code: whether reproduced from scratch or re-used author repository.,lE0wqKGROKa
2014,2014,"  Authors provide their own codebase link, which consists of the code re-used from the original repository",lE0wqKGROKa
2015,2015,The codebase is well structured with proper README in the appropriate places.,lE0wqKGROKa
2016,2016,* Communication with original authors,lE0wqKGROKa
2017,2017,"  The report mentions that they have contacted the original authors, but they did not hear back from them",lE0wqKGROKa
2018,2018,This is unfortunate but sadly happens quite frequently,lE0wqKGROKa
2019,2019,I applaud the author's effort to reach out to the original authors despite the no reply.,lE0wqKGROKa
2020,2020,* Hyperparameter Search,lE0wqKGROKa
2021,2021,  It does not appear that the authors performed an additional hyperparam search than what was reported in the original paper.,lE0wqKGROKa
2022,2022,* Ablation Study,lE0wqKGROKa
2023,2023,  The authors compute several extra experiments as part of the ablation of the original work,lE0wqKGROKa
2024,2024,They add a new evaluation method to clarify the conclusions of the original paper further using LIME,lE0wqKGROKa
2025,2025,"This is a splendid idea, and the correlation results with Pearson correlation (funny it's a correlation of a correlation!) and JS divergence shows the need for such study",lE0wqKGROKa
2026,2026,"The results are mixed, as the main selling point of the paper (Orthogonality and Diversity) does not correlate well with LIME",lE0wqKGROKa
2027,2027,I would be curious to hear from the authors if they read this review.,lE0wqKGROKa
2028,2028,"  The authors also tested for generalization using Bidirectional LSTM to test the author's claims further (and add a note on why other mechanisms, such as Transformers, are not straightforward to evaluate in the same setting)",lE0wqKGROKa
2029,2029,The authors find the proposed methods do not unconditionally improve the explanations,lE0wqKGROKa
2030,2030,"These kinds of cross-architecture robustness experiments add tons of value to the original paper, and I commend the authors for doing the same.",lE0wqKGROKa
2031,2031,* Discussion on results,lE0wqKGROKa
2032,2032,  The report provides a clear and concise discussion of their findings,lE0wqKGROKa
2033,2033,"The authors provide faithful results, both of which experiments worked and which did not",lE0wqKGROKa
2034,2034,"The authors summarized their findings on the original paper and conclude Orthogonal LSTM does clearly leads to lower conicity than Vanilla LSTM, however, the results are mixed",lE0wqKGROKa
2035,2035,"Table 4 is a great summary, clearly defining how each of the claims is supported or not by their reproducibility study.",lE0wqKGROKa
2036,2036,* Recommendations for reproducibility,lE0wqKGROKa
2037,2037, ,lE0wqKGROKa
2038,2038,"  The report goes above and beyond to conduct a thorough code review of the original paper, which is a stellar contribution to both reproducible research and to the understanding of the code provided by the original paper",lE0wqKGROKa
2039,2039,The authors further provide ample discussion and conclude the benefits of orthogonality and diversity training are more relevant for simpler tasks.,lE0wqKGROKa
2040,2040,* Overall organization and clarity,lE0wqKGROKa
2041,2041,  ,lE0wqKGROKa
2042,2042,  The report does not have any significant typos,lE0wqKGROKa
2043,2043,It is well organized into appropriate sections.,lE0wqKGROKa
2044,2044,1,U2E22LewEX1
2045,2045,The authors have clearly identified the following claims in the paper for reproducibility:,U2E22LewEX1
2046,2046,   1,U2E22LewEX1
2047,2047,Superior privacy preserving properties of the complex-values networks compared to the standard networks.,U2E22LewEX1
2048,2048,   2,U2E22LewEX1
2049,2049,Similar classification performance as real-valued networks on both datasets.,U2E22LewEX1
2050,2050,2,U2E22LewEX1
2051,2051,The authors of this report have implemented the code by themselves as the original implementation wasn't open-sourced,U2E22LewEX1
2052,2052,"Despite the lack of information, the authors have also done hyper parameter sweep",U2E22LewEX1
2053,2053,The authors have clearly provided the implementation details in their report.,U2E22LewEX1
2054,2054,3,U2E22LewEX1
2055,2055,Due to lack of time they couldn't communicate with the original authors and have mentioned it in the report.,U2E22LewEX1
2056,2056,4,U2E22LewEX1
2057,2057,"In their discussion and results section, the authors have clearly identified the parts of the paper that were easy to implement and those that weren't",U2E22LewEX1
2058,2058,"Through extensive experiments, the authors have shown that while the privacy preserving claims of the complex networks hold, their classification performance is impacted compared to the baseline model",U2E22LewEX1
2059,2059,This is in contrast to the claim reported in the paper,U2E22LewEX1
2060,2060,However the authors have also considered if the difference in the accuracy might boil down to implementation of underlying architecture,U2E22LewEX1
2061,2061,"Overall the authors have suggested that to further support the the main claim of privacy preservation, the paper can be augmented by adding pseudo-code and figures.",U2E22LewEX1
2062,2062,The reproducibility of this paper involved coding the experiments since no code was available from the original authors,U2E22LewEX1
2063,2063,Considerable effort has been taken to recreate the experiments as closely as possible but no effort was taken to contact the original authors for hyper-parameters to test the reported metrics,U2E22LewEX1
2064,2064,"The reproducibility report, however, outlines the main claims made by the original authors and test these claims in their experiments",U2E22LewEX1
2065,2065,They have contrary results to the original claim that complex valued networks perform similar or better than real-valued networks,U2E22LewEX1
2066,2066,"However, this is difficult to compare without training the network on the original parameters with the right optimiser",U2E22LewEX1
2067,2067,"Moreover, the models have only been trained once which is not adequate to obtain optimal model performance and hence not adequate effort to report believable results.",U2E22LewEX1
2068,2068,The reproducibility report is able to verify that complex valued networks are less susceptible to inversion attacks but with weaker results,U2E22LewEX1
2069,2069,This could again be attributed to the difference in model parameters and optimisation technique used and hence an expectation of identical results is not reasonable.,U2E22LewEX1
2070,2070,This report does reproducibility study of Xiang et al,U2E22LewEX1
2071,2071,ICLR 2020 Interpretable Complex-Valued Neural Networks for Privacy Protection.,U2E22LewEX1
2072,2072,Reproducibility Summary: present with adequate and sufficient summary of the performed reproducibility study.,U2E22LewEX1
2073,2073,Scope of reproducibility: is clearly stated and the report follows it,U2E22LewEX1
2074,2074,Code: the authors of the report implement the code from scratch as the original authors didn't make their code publicly available,U2E22LewEX1
2075,2075,The link to the repository is provided,U2E22LewEX1
2076,2076,All the results from the report seem to be able to be reproduced by following the easy to follow jupyter notebook,U2E22LewEX1
2077,2077,"Although the code looks clean and readable from a quick glance, the documentation for the code is very limited or absent.",U2E22LewEX1
2078,2078,Communication with original authors: the authors of the report claim that they didn't communicate with the original authors due to lack of time,U2E22LewEX1
2079,2079,"This claim without any further clarifications looks a bit odd as communication with the original authors may resolve some issues which the authors of the report mentioned as required lots of time, such as some missing implementation details in the original paper.",U2E22LewEX1
2080,2080,Hyperparameter Search: no hyperparameter search has been performed with discussion that this was due to limit of time,U2E22LewEX1
2081,2081,"Considering the amount of experiments performed and that the implementation was done from scratch, the claim looks reasonable.",U2E22LewEX1
2082,2082,Ablation Study: no ablation study is performed beyond the original paper.,U2E22LewEX1
2083,2083,Discussion on results: The report explicitly discusses which parts were easy and difficult to reproduce and which claims from the original paper were confirmed in their experiments,U2E22LewEX1
2084,2084,Recommendations for reproducibility: though there are no explicit recommendations the authors emphasise which implementation details were missing in the original paper that caused the most problems in the reproducibility study,U2E22LewEX1
2085,2085,"Results beyond the paper: No results beyond the paper, moreover some of the more computationally heavy results from the original paper were not reproduced",U2E22LewEX1
2086,2086,Overall organization and clarity: The report is very well written and easy to follow,U2E22LewEX1
2087,2087,This is a well-written and faithful replication of the original work,r3R_osip5G
2088,2088,"The authors describe the methods well and do their best to reproduce and extend studies of the original methods to multilingual settings, providing their own code along the way",r3R_osip5G
2089,2089,"Though some of the original datasets were not accessible to the authors, they swapped these out for extended studies which seem to show consistent results.",r3R_osip5G
2090,2090,"Overall, I think the paper was well-written and met the expectations of a great reproduction",r3R_osip5G
2091,2091,"Perhaps, some additional discussion on the intuition behind adding multilingual experiments and the potential connection to conicity might be worthwhile to motivate the additional experiments a bit more",r3R_osip5G
2092,2092,"In general, additional discussion of findings might have been helpful to the reader, but not necessary",r3R_osip5G
2093,2093,The description of the original method served as a nice summary of the original work as well.,r3R_osip5G
2094,2094,Typos:,r3R_osip5G
2095,2095,"In the conclusion: ""Majority of..."" -> ""The majority"" ",r3R_osip5G
2096,2096,"Appendix 1, citation for adam is broken.",r3R_osip5G
2097,2097," I can confirm that the authors included a clear and concise reproducibility summary, noting that they reused some of the original code and data and made a few modifications to original code base to extend the experiments",r3R_osip5G
2098,2098,The authors highlight that there were some challenges working with the data where licensing was involved and in other cases where the datasets were large,r3R_osip5G
2099,2099," The authors do not try new hyper-parameters, but re-use the hyper-parameters from the original paper",r3R_osip5G
2100,2100,"Ablation or recommendations to the original authors are not provided, however the authors do provide some results for independent experimentation on the CLS dataset",r3R_osip5G
2101,2101,The paper is otherwise well written and easy to read,r3R_osip5G
2102,2102,* Reproducibility Summary,r3R_osip5G
2103,2103,  The report contains a well-defined and articulate reproducibility summary as prescribed by the challenge.,r3R_osip5G
2104,2104,* Scope of reproducibility,r3R_osip5G
2105,2105,  The report contains well-defined scope involving six central claims of the original paper,r3R_osip5G
2106,2106,The paper also investigates additional claims on Transformers and multilingual data.,r3R_osip5G
2107,2107,* Code: whether reproduced from scratch or re-used author repository.,r3R_osip5G
2108,2108,  Authors provide their own codebase link,r3R_osip5G
2109,2109,"However, I was unable to open it - seems the link https://github.com/KacperKubara/Transparency does not exist",r3R_osip5G
2110,2110,Neither did I find any code in the appendix,r3R_osip5G
2111,2111,I'll be happy to increase my score if this is fixed.,r3R_osip5G
2112,2112,* Communication with original authors,r3R_osip5G
2113,2113,  The report mentions they did not communicate their results/findings to the original authors.,r3R_osip5G
2114,2114,* Hyperparameter Search,r3R_osip5G
2115,2115,"  The authors did not perform any additional hyperparam search, they seem to have only run the default ones provided by the author",r3R_osip5G
2116,2116,"This is a missed opportunity in my view, as the authors could have explored various hyperparam choices to see if the results hold more robustly.",r3R_osip5G
2117,2117,* Ablation Study,r3R_osip5G
2118,2118,  The authors perform an ablation study by introducing Transformers and multi-lingual data,r3R_osip5G
2119,2119,This kind of ablation study is perfect and highly appreciated for a reproducibility report.,r3R_osip5G
2120,2120,  It's interesting to find the conicity of Transformers to be much smaller,r3R_osip5G
2121,2121,"However, that doesn't totally imply Vanilla Transformers to have a more plausible explanation of attention weights",r3R_osip5G
2122,2122,"Also, the choice of evaluating the same on Transformers is tricky, as multi-head attention systems it is generally difficult to pinpoint the attention contribution of a single word",r3R_osip5G
2123,2123," The authors could have compared their results with that of what BERT looks at https://arxiv.org/abs/1906.04341, but in any case, this additional set of experiments are quite welcome.",r3R_osip5G
2124,2124,* Discussion on results,r3R_osip5G
2125,2125,  The report provides information on which parts of reproduction are easy and which are difficult,r3R_osip5G
2126,2126,"Not surprisingly, certain datasets are difficult to procure and some are harder to train",r3R_osip5G
2127,2127,The report does a good job mentioning these.,r3R_osip5G
2128,2128,* Recommendations for reproducibility,r3R_osip5G
2129,2129,  The authors highly commend the original paper on their state of reproducibility.,r3R_osip5G
2130,2130,* Overall organization and clarity,r3R_osip5G
2131,2131,"  Overall, minor typos but not that significant",r3R_osip5G
2132,2132,"In Section 4.1 the authors might have forgotten to comment out the line ""Logically group related results into sections""",r3R_osip5G
2133,2133,"The report aims to reproduce the results from the following paper ""CNN-generated images are surprisingly easy to spot..",HhcM-JXyhl3
2134,2134,"for now""",HhcM-JXyhl3
2135,2135,"The report is very complete, well written and well executed and although I have not read the original paper I was able to understand the ideas behind the original paper just by reading the report.",HhcM-JXyhl3
2136,2136,This report reimplements the algorithm proposed in the original paper and confirms the validity of the experimental results in the paper,HhcM-JXyhl3
2137,2137,The pre-trained model used in this reproducibility assessment paper can be accessed in an anonymous git repo and the report gave details about how the reproducibility test is organized,HhcM-JXyhl3
2138,2138,"Additionally, the report also points out the performances in the original paper depend on the choice of the training data set and the choice of the data generator",HhcM-JXyhl3
2139,2139," This is the limit of the work in the original paper, which helps practitioners to better use the proposed algorithm",HhcM-JXyhl3
2140,2140,The paper seeks to reproduce the results of the paper titled ‘CNN-generated images are surprisingly easy to spot..,HhcM-JXyhl3
2141,2141,for now’,HhcM-JXyhl3
2142,2142,They sought to validate two  main claims in the original paper – that data augmentation and data diversity helps with generalization in the context of real/fake image classification,HhcM-JXyhl3
2143,2143,They don’t validate the third claim that data augmentation aids robustness,HhcM-JXyhl3
2144,2144,"In addition to the above, the paper also investigates the situations then the training data generator and classifiers are changed",HhcM-JXyhl3
2145,2145,"Overall, the reproducibility study is reasonable",HhcM-JXyhl3
2146,2146,"And, despite the shortcomings mentioned below, it should be useful to the audience of this challenge as well as the wider AI/ ML community interested in the original work",HhcM-JXyhl3
2147,2147,"However, due to the concerns below, I recommend rejecting this paper with a rating of marginally below threshold (which I’m willing to revise based on discussions with the authors).",HhcM-JXyhl3
2148,2148,"In the following, an evaluation of this paper on the metrics suggested by the RC 2020 challenge is presented:",HhcM-JXyhl3
2149,2149,Reproducibility Summary: ,HhcM-JXyhl3
2150,2150,The authors have provided a brief and clear summarization of the problem statement and the proposed approach and have reported their major findings.,HhcM-JXyhl3
2151,2151,Scope of Reproducibility:,HhcM-JXyhl3
2152,2152,The authors clearly enumerate the scope of the reproducibility study to validate two claims from the paper with a partial recreation of the original experiments as well as several additional ones,HhcM-JXyhl3
2153,2153,The study is designed and conducted accordingly.,HhcM-JXyhl3
2154,2154,Code: ,HhcM-JXyhl3
2155,2155,The code for the original paper is publicly available,HhcM-JXyhl3
2156,2156,"However, the authors have re-implemented the code for the experiments performed and use the original implementation sparingly",HhcM-JXyhl3
2157,2157,The code base is submitted with readable code and docs,HhcM-JXyhl3
2158,2158,Communication with original authors,HhcM-JXyhl3
2159,2159,"The authors mention that most experiments could be reproduced using minimal communication with the original authors, given the details in the main paper and the well-documented code repository",HhcM-JXyhl3
2160,2160,There were a few experimental settings that lacked clarity (or were misunderstood by the report authors) which were resolved by communicating with the original authors,HhcM-JXyhl3
2161,2161,Hyperparameter search ,HhcM-JXyhl3
2162,2162,"For the reproducibility experiments, the authors use the hyperparameter details provided in the main paper and do not perform any parameter tuning",HhcM-JXyhl3
2163,2163,"However, the authors perform data augmentation (Blur + JPEG) differently from that of the main paper",HhcM-JXyhl3
2164,2164,The variation in the implementation is also hypothesized as the reason for the discrepancy between the obtained and original results,HhcM-JXyhl3
2165,2165,"Thus, a hyperparameter tuning for the same is also expected",HhcM-JXyhl3
2166,2166,A discussion on the hyperparameter search for the additional experiments performed is missing,HhcM-JXyhl3
2167,2167,Discussion of results ,HhcM-JXyhl3
2168,2168,I have the few concerns: ,HhcM-JXyhl3
2169,2169,Data diversity and generalization: These results are presented in Table 1,HhcM-JXyhl3
2170,2170,The authors conclude (line 154) that these results are similar to the original reported results,HhcM-JXyhl3
2171,2171,This is clearly not the case when differences as high as 24% in absolute terrms (Deepfake 20-class: 66.3% --> 93.7%; SAN 2-class: 52.9 --> 72.3) are seen,HhcM-JXyhl3
2172,2172,"Even when the differences are low, they seem statistically significant (StarGAN 2-class: 87.3 --> 83.5)",HhcM-JXyhl3
2173,2173,"However, the trend that diversity leads to better generalization seems to hold.",HhcM-JXyhl3
2174,2174,"It is speculated that the difference in results is due to the different blurring function used and the way blurring, and JPEG compression is applied to the data",HhcM-JXyhl3
2175,2175,This could be the reason but doesn’t seem to be verified,HhcM-JXyhl3
2176,2176,"The authors also demonstrate via Figure 1 that the correlation of improved performance with more diversity is stronger when accompanied by data augmentation (blur, jpeg compression) than without.",HhcM-JXyhl3
2177,2177,The observation “dependent on the data set” in lines 173-174 is concerning,HhcM-JXyhl3
2178,2178,"Since this reference to the data set is test data, all it says is that the results are not expected to hold across different test scenarios.",HhcM-JXyhl3
2179,2179,Data augmentation and generalization: This analysis is reproduced in Table 2,HhcM-JXyhl3
2180,2180,"We observe statistically significant differences even in this experiment, even for the ‘No Aug’ scenario where no blur or JPEG compression is applied",HhcM-JXyhl3
2181,2181,"For example, (SAN No Aug: 93.6 --> 87.2)",HhcM-JXyhl3
2182,2182,"Secondly, the trend of improved performance with data augmentation is really mixed with 4 settings out of 10 -- StarGAN, SITD, SAN and DeepFake bucking the trend",HhcM-JXyhl3
2183,2183,Whether this can be entirely attributed to variations in implementation is not investigated ,HhcM-JXyhl3
2184,2184,"However, the overall trend that diversity (when accompanied by augmentations) shows an improvement in performance seems clear and stands validated by the report",HhcM-JXyhl3
2185,2185,The authors also make a key observation that data diversity alone (without augmentation) is insufficient to achieve generalization which was not clear from the experiments in the original paper.,HhcM-JXyhl3
2186,2186,Recommendations for reproducibility ,HhcM-JXyhl3
2187,2187,The authors were able to reproduce two major claims of the paper and obtain better performance than originally reported for some experiments,HhcM-JXyhl3
2188,2188,"They suggest that performing the data augmentation – “blur + JPEG,” simultaneously rather than sequentially helps improve overall performance",HhcM-JXyhl3
2189,2189, ,HhcM-JXyhl3
2190,2190,Results beyond the original paper:,HhcM-JXyhl3
2191,2191,"Two additional investigations are carried out that go beyond the original paper: (a) changing the generator used for training (pre-trained StyleGAN2), and, (b) training a different classifier (VGG and DCT-ResNet).",HhcM-JXyhl3
2192,2192,"While some trends are clear (ProGAN better than StyleGAN2 for training), others not quite and it seems that the authors are not careful in making deductions.",HhcM-JXyhl3
2193,2193,"-	Line 206: it should be 87.6 --> (86.3, 87.1, 89.2, 97.1}",HhcM-JXyhl3
2194,2194,-	(l,HhcM-JXyhl3
2195,2195,"209-212): For StyleGAN2, other proposed augmentations don’t worsen results for StyleGAN2: not for GauGAN, CRN, IMLE, SITG, SAN",HhcM-JXyhl3
2196,2196,"For ProGAN: blurring doesn't hurt for StyleGAN, StyleGAN2, BigGAN, StarGAN, SITD, DeepFake",HhcM-JXyhl3
2197,2197,"Other's don't improve for StarGAN, SAN, DeepFake.",HhcM-JXyhl3
2198,2198,The question of whether the performance improvement and generalization trends hold for other classifiers is not properly discussed,HhcM-JXyhl3
2199,2199,It also lacks a detailed discussion on the implementation and hyperparameter tuning for the additional experiments performed.,HhcM-JXyhl3
2200,2200,Overall clarity and organization ,HhcM-JXyhl3
2201,2201,The report is well structured in general and has a reasonable clarity,HhcM-JXyhl3
2202,2202,The readability could have been improved by organizing the experiments in the report in the same order as that of the main paper (where discussion on data augmentation precedes diversity),HhcM-JXyhl3
2203,2203,The authors should address the following issues in their draft:,HhcM-JXyhl3
2204,2204,-	Typos in specifying the learning rates (lines 124-126): 1e-3 or 10^-3 instead of 1^-3 etc.,HhcM-JXyhl3
2205,2205,-	(line 10) It’s not clear what is meant by - “if the results extend beyond the original contribution”.,HhcM-JXyhl3
2206,2206,-	The authors don’t clearly mention whether the data augmentations were applied only to the synthetic images or the real images as well.,HhcM-JXyhl3
2207,2207,"This report is reproducing most of the experiments from the paper ""On Warm Starting Neural Network Training""",N43DVxrjCw
2208,2208,"While the general trends are visible in the reproduced results, there are many details that are not the same",N43DVxrjCw
2209,2209,"As the reproduced paper doesn't provide confidence intervals in their figures (which the original paper does) and they show test instead of validation results, it is not very easy to compare results",N43DVxrjCw
2210,2210,Equivalent experiments in most of the examples achieve different max accuracy,N43DVxrjCw
2211,2211,"Even more concerning is that gap that is shown in the very first experiment in order to motivate the work, is almost not existing in Table 1 of reproduction",N43DVxrjCw
2212,2212,Other experiments also show unexpectedly good performance of warm start models in the reproduced results,N43DVxrjCw
2213,2213,"Overall, experiments results within the report show inconsistent behavior and instability.",N43DVxrjCw
2214,2214,The report is well written and organised and it contains a summary section at the beginning,N43DVxrjCw
2215,2215,Report authors clarified questions with original authors,N43DVxrjCw
2216,2216,The link to the implementation doesn't work,N43DVxrjCw
2217,2217,I've searched the repository and that project is deleted,N43DVxrjCw
2218,2218,"I like the report, but I am giving grade 5 because the results are not matching the original paper results, and since they are less extensive and elaborative and with no confidence intervals, I have more confidence in the results of the original paper",N43DVxrjCw
2219,2219,"Additionally, the link to the code doesn't work, so it's not possible to see the setup of the reproduced experiments.",N43DVxrjCw
2220,2220,Below are comments related to different experiments in the order in which they appear in the report:,N43DVxrjCw
2221,2221,"- Experiment in Figure 1: Report authors use only 200/400 epochs instead of 350/700 epochs used in the original paper, but they are able to show the same effect",N43DVxrjCw
2222,2222,"It is good that even with fewer time resources, we can show the same effect.",N43DVxrjCw
2223,2223,"- Table 1: While the original paper gives us validation accuracies, the report authors give us training and test accuracies, so it's not comparable",N43DVxrjCw
2224,2224,"For LR, there is no gap according to Table 1, but similar is true for original Table 1",N43DVxrjCw
2225,2225,"However, while in the original Table 1 gap is obvious between random init and warm start, in the reproduced study, that's not true",N43DVxrjCw
2226,2226,"Most of the gaps are within one standard deviation and gaps are inconsistent, meaning that in some cases warm start is better",N43DVxrjCw
2227,2227,"Also, test results here are much worse than original validation results",N43DVxrjCw
2228,2228,Did they train for the same number of epochs? The difference is too big to be explained by validation vs test set difference.,N43DVxrjCw
2229,2229,- Figure 2: there is no confidentiality interval that exists in the original paper,N43DVxrjCw
2230,2230,"As authors of the report saw that SVHN behaves differently, they added additional experiments here, which show slightly different behavior and require a convergence threshold of 99.9%",N43DVxrjCw
2231,2231,I would suggest changing the scale of Figures 2 b) and c) to show only information above 80% or 85 % of accuracy so that it is easier to see the gap.,N43DVxrjCw
2232,2232,- Results in Figure 4 are different than in the original study,N43DVxrjCw
2233,2233,"In this report, warm start models are performing consistently better than in the original paper, so we could see there was almost no gap visible in Table 1, and in Figure 4 performance of warm start is better than the random start, even though that's not true in the original paper",N43DVxrjCw
2234,2234,"Again, the report deals with test errors while original papers worked with validation errors, but that shouldn't affect the conclusion",N43DVxrjCw
2235,2235,It is also interesting to note that the report is able to achieve better results on the test dataset with a few warm start experiments than the original paper achieves on the validation dataset with any model,N43DVxrjCw
2236,2236,That is suspicious to me.,N43DVxrjCw
2237,2237,"- While the original paper has confidence intervals for Figure 4, that is not given in reproduction (Figure 3) and the patterns show small but visible differences, which might be explained with confidence intervals",N43DVxrjCw
2238,2238,"Again, the test accuracy that is achieved by the report is up to 5% higher than in the original paper.",N43DVxrjCw
2239,2239,- What is the x-axis of Figure 5? Is it the number of epochs?,N43DVxrjCw
2240,2240,- Figure 6 (matches original Figure 7): Behavior is inconsistent for different lambda values,N43DVxrjCw
2241,2241,"No pattern can be spotted in this figure, except that for lambda=0 train time is strongly the highest",N43DVxrjCw
2242,2242,It is very strange that for lambda=0 training time is twice bigger than expected for around 25 thousand examples.,N43DVxrjCw
2243,2243,"- In general, results reported in those figures are less smooth, which is probably because the information is displayed in more coarse grain, but it may also be due to the instability of the models or issues with reproducibility",N43DVxrjCw
2244,2244,"However, it is not clear from the report which one it is, and it seems it's rather from the former.",N43DVxrjCw
2245,2245,- Authors add experiments on warm start with augmented data in which warm start slightly outperforms random initialization,N43DVxrjCw
2246,2246,"However, in some of the repeated experiments, we can see similar behavior in the report, so it looks like warm start performs better throughout this report and not because of the data augmentation.",N43DVxrjCw
2247,2247,"- Figure 11: Why does shrink-perturb go down after on the graph in the bottom left corner when x > 0.6? Most of the results in this figure are inconsistent with the original paper as they show that fresh has the best performance, but results are inconsistent",N43DVxrjCw
2248,2248,"Also, on the contrary from the rest of the report, here we can see that warm has the worst performance in all the cases.",N43DVxrjCw
2249,2249,"In the proposed paper, one of the main pros is that you've plotted and calculated each relation that was measured in the original paper and obtained mainly near or the exact values toward the original paper",N43DVxrjCw
2250,2250,But some minor issues showed off during reviewing your paper.,N43DVxrjCw
2251,2251,"First of all, you've forgotten to blind your names and affiliations, not a major problem but it might affect reviewers' ideas sometimes",N43DVxrjCw
2252,2252,"After that, you've also forgotten to numerize lines, so I have to mention them using paragraphs' titles",N43DVxrjCw
2253,2253,"According to the ""What was easy"" paragraph, you'd better re-write ""since many of the parameters are reported in the original paper"" into ""since many of the parameters ***were*** reported in the original paper."" There also was so good that you've explained all the conditions you've put your dataset under test, like the optimizers, loss functions, etc",N43DVxrjCw
2254,2254,and I guess you've tested as far as I know enough conditions to explain your model and results.,N43DVxrjCw
2255,2255,"As it seems, the original paper didn't do the ***data augmentation***, but it's so good to multiply the amount of data to exploit enough accuracy.",N43DVxrjCw
2256,2256,"In the ""effect of hyperparameter"" paragraph, you've made a typo and wrote ***vales*** instead of ***values***, in the ""We iterate over all pairs for these vales"" sentence.",N43DVxrjCw
2257,2257,"In the ""effect of data augmentation"" paragraph, you've mentioned that ""However, because the learning rate is low, the models are not fully converged even after 350 epochs"", although you've previously mentioned that your model converged to 99% of accuracy, so it wasn't crystally clear for me",N43DVxrjCw
2258,2258,"In the sentence ""the original paper’s authors look at the difference"", it would better to re-write ***look*** to ***looked***, and ***difference*** to ***differences***",N43DVxrjCw
2259,2259,"In the last sentence of this paragraph ""Due to the limits of this report, we also leave the careful comparison between data augmentations and Shrink & Perturb as future research in this area"", it would be better to explain ***limits*** more.",N43DVxrjCw
2260,2260,"The link you've attached as ""https://github.com/CS-433/cs-433-project-2-fesenjoon."" didn't work properly, I couldn't find your project, according to the link you've attached in the ***Experimental setup*** paragraph.",N43DVxrjCw
2261,2261,"But in total, you've represented your method very well.",N43DVxrjCw
2262,2262,1,N43DVxrjCw
2263,2263,In this report the authors have tackled the following reproducibility claims from the original paper:,N43DVxrjCw
2264,2264,  - Warm-starting a Neural Network training has poorer generalization compared to training with new data + old data from scratch,N43DVxrjCw
2265,2265,  - Application of Shrink+Perturb technique proposed by authors to rectify and improve the warm-starting generalization.,N43DVxrjCw
2266,2266,2,N43DVxrjCw
2267,2267,The authors have implemented the code by themselves and tried various hyperparameters as mentioned by the original authors,N43DVxrjCw
2268,2268,"They communicated with the original authors to clarify some of the implementation details mentioned in the paper like when to stop for training convergence, hyperparameters to tune, etc",N43DVxrjCw
2269,2269,"However, the public link for the code implemented by the report's authors is not accessible.",N43DVxrjCw
2270,2270,3,N43DVxrjCw
2271,2271,"In addition to implementing the code, the authors have also tried other techniques that might help with the warm-starting problem such as data-augmentation, early-stopping, regularization etc.",N43DVxrjCw
2272,2272,4,N43DVxrjCw
2273,2273,"In the results and discussions part of the report, the authors have described the implementation details of the project",N43DVxrjCw
2274,2274,"The authors have reported that warm-starting definitely has a generalization gap, shrink and perturb method is effective in reducing the gap.",N43DVxrjCw
2275,2275,"This paper aims at evaluating the claims made by ""Towards Transparent and Explainable Attention Models"" by trying to reproduce the experiments in the original paper",gEUo6MXTjpH
2276,2276,"Although the authors were not able to run all the experiments in that paper due to some dataset links missing, they have covered a decent amount of datasets and conducted extensive experiments, including an additional LIME experiment to validate their results from a different perspective",gEUo6MXTjpH
2277,2277,"This paper is eventually able to validate claims 1 & 2, claim 3 to some extent, but not claim 4: in fact, on some datasets and some metrics (such as JS divergence), this paper observes opposite results.",gEUo6MXTjpH
2278,2278,Pros:,gEUo6MXTjpH
2279,2279,1,gEUo6MXTjpH
2280,2280,"This work notices some issues of the presentation of the original paper, namely the fact that the original paper ignored one of the classes when plotting Figure 2 for some reason, which if included seems to cast some shadows over the desirable outcome",gEUo6MXTjpH
2281,2281, I think this insight is important for future researchers to get a complete understanding of the claims made by the original work.,gEUo6MXTjpH
2282,2282,2,gEUo6MXTjpH
2283,2283,"This work runs each experiment multiple times to report the central tendency, which is useful to have.",gEUo6MXTjpH
2284,2284,3,gEUo6MXTjpH
2285,2285,"After finding inconsistent results compared to the original work, this paper conducted a further LIME experiment which provides further evidence of the validity of their experiments.",gEUo6MXTjpH
2286,2286,Cons:,gEUo6MXTjpH
2287,2287,1,gEUo6MXTjpH
2288,2288,"Given the different observations and the fact that this work used the code from the original work, I think it'd be nice to contact the authors of the original paper.",gEUo6MXTjpH
2289,2289,2,gEUo6MXTjpH
2290,2290,"Not being able to evaluate all datasets seems to be a limitation, especially for reporting the mean statistics",gEUo6MXTjpH
2291,2291,3,gEUo6MXTjpH
2292,2292,"In table 3, sometimes both the Pearson correlation and the JS divergence improve",gEUo6MXTjpH
2293,2293,This seems to warrant further investigation: why do they both improve? Is it because the attentions are so spiky that they incur a large penalty when measuring JS?,gEUo6MXTjpH
2294,2294,Questions:,gEUo6MXTjpH
2295,2295,1,gEUo6MXTjpH
2296,2296,"It's a bit surprising that in table 2 the vanilla LSTM needs so many words as its rationale, especially for classification tasks with so few classes",gEUo6MXTjpH
2297,2297,Did you check the extracted rationales and see if those are words with strong sentiments? Could it be a bug?,gEUo6MXTjpH
2298,2298,Typos & Presentation Issues:,gEUo6MXTjpH
2299,2299,1,gEUo6MXTjpH
2300,2300,3.1.3 p -> log p,gEUo6MXTjpH
2301,2301,2,gEUo6MXTjpH
2302,2302,figure 1: caption too small,gEUo6MXTjpH
2303,2303,3,gEUo6MXTjpH
2304,2304,sec 4.4: not unsurprising -> not surprising,gEUo6MXTjpH
2305,2305,"Overall, this paper has conducted extensive experiments and repeated most of the experiments in the original paper",gEUo6MXTjpH
2306,2306,"This paper noticed some results that seem to be neglected by the original work, and showed some contradicting results from the original work",gEUo6MXTjpH
2307,2307,"I think this paper is valuable to researchers who are interested in the original work, and recommend its acceptance",gEUo6MXTjpH
2308,2308,"The submission is a nonanonymous report, which violates the policy",gEUo6MXTjpH
2309,2309,"As for the detailed reports, it is good and it gives a good reproduction of the original paper",gEUo6MXTjpH
2310,2310,The authors clearly show what questions they want to answer through this report,gEUo6MXTjpH
2311,2311,"According to these lines, the authors conduct experiments and analyses to provide results",gEUo6MXTjpH
2312,2312,"With different empirical studies, the authors verify that 1) the prediction performance is almost the same as the original attention mechanism; 2) diversity-driven LSTM indeed gives more transparency attention",gEUo6MXTjpH
2313,2313,"However, the other two claims from the original paper are not so convincing or hard to be investigated",gEUo6MXTjpH
2314,2314,"One slight suggestion is the presentation, the last section, Sec 5, the authors could remove the 5.2-4 since they have been shown in the first section",gEUo6MXTjpH
2315,2315,"The score is only raised by the nonanonymous policy, I am sorry for this",gEUo6MXTjpH
2316,2316,This paper provides a reproducible test of the paper Mohankumar et al,gEUo6MXTjpH
2317,2317,(2020) which claims that current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions in Natural Language Processing tasks and develop new LSTM units to achieve the goal.,gEUo6MXTjpH
2318,2318,"In the reproduction, the authors first give sufficient background information about the paper",gEUo6MXTjpH
2319,2319,"Then provide enough experimental details about the dataset used, the training model, and the hyperparameters for optimization",gEUo6MXTjpH
2320,2320,The authors also discuss each experimental result separately and compare the numbers/observations with the original paper.,gEUo6MXTjpH
2321,2321,I feel this piece of investigation is above the acceptance bar and I recommend acceptance.,gEUo6MXTjpH
2322,2322,This report confirms most of the claims in the original paper and points out some minor issues of the proposed iFlow method that are not fully supported by the experimental observations,8jmIpypMzE
2323,2323,"Furthermore, the report also mentions that the MCC scores of the iVAE",8jmIpypMzE
2324,2324,reported by the authors are significantly worse than those in the iVAE paper,8jmIpypMzE
2325,2325,The limit of the report is the lack of experimental evaluations traversing different hyperparameter values due to the computation bottleneck,8jmIpypMzE
2326,2326, The report states the coverage limit clearly,8jmIpypMzE
2327,2327,The overall paper presents good ablation with a clear introduction to the problem statement in the reproduction summary.,8jmIpypMzE
2328,2328,It was good to see a detailed hyper-parameter search over the baseline model,8jmIpypMzE
2329,2329,"But, it would have been great if the authors would have extended the work over real datasets in support of the points mentioned in the discussion section",8jmIpypMzE
2330,2330,"Also, if the authors can share more details regarding the time complexity for running the experiments which would help future researchers to tackle this problem first.",8jmIpypMzE
2331,2331,The discussion section mentioned by the authors listing down the strengths and weakness of the method which is a great reference point for future work,8jmIpypMzE
2332,2332,"From the paper, it seems that the authors may not have any direct communications with the original authors",8jmIpypMzE
2333,2333,They mainly obtained information from the original paper and the original codebase,8jmIpypMzE
2334,2334,The authors have clearly stated the scope of reproducibility with clarity in the claims they learned from the original paper with further sections explaining the clams.,8jmIpypMzE
2335,2335,"Small suggestion: In page 9: section-""Baseline improvement experiments"" if the table can be written in a more representable manner.",8jmIpypMzE
2336,2336,"The report described the authors' efforts in reproducing the work ""Learning to Deceive with Attention-Based Explanations""",DivtnNApCJY
2337,2337,The report is well written in general,DivtnNApCJY
2338,2338,"The authors have reproduced most of the experiments from the original paper, except for the ones requiring a private dataset, which is understandable",DivtnNApCJY
2339,2339,"In place of the missing experiment, the authors added a new result with a multi-class classification problem, which supplements the original results",DivtnNApCJY
2340,2340,"In addition, the authors reported and analyzed the variance of the proposed methods, which were not present in the original paper",DivtnNApCJY
2341,2341,The discussion of the report is also insightful.,DivtnNApCJY
2342,2342,"Though I think the report did a good job in overal, one aspect where it can be further improved is that the hyper-parameters used in the experiments were not discussed: did the authors used the same ones from the original work? is the proposed method sensitive to the selection of different hyper-parameters? Some investigations in this  direction would be valuable.",DivtnNApCJY
2343,2343,The authors performed well,DivtnNApCJY
2344,2344,They had good communication with the original authors and was able to substitute for missing dataset.,DivtnNApCJY
2345,2345,"As a first remark, the report seems to repeat content",8JHrucviUf
2346,2346,"This is probably not the author's fault, I noticed the same problem with other reports",8JHrucviUf
2347,2347,That is probably a problem with the instruction template,8JHrucviUf
2348,2348,This is something for the organizers to take into consideration.,8JHrucviUf
2349,2349,General remarks,8JHrucviUf
2350,2350,The report follows the prescribed format,8JHrucviUf
2351,2351,The authors reproduced the experiments in pytorch,8JHrucviUf
2352,2352,The original implementation was in tensorflow,8JHrucviUf
2353,2353,The author found errors in the implementation and a lot of gaps in the parameter tuning,8JHrucviUf
2354,2354,"In some cases, they get results opposite from the claims of the paper",8JHrucviUf
2355,2355,The biggest problem of the paper they evaluated was the fact that the text and code were not in agreement,8JHrucviUf
2356,2356,"In fact, the code was convoluted and it was difficult to actually get the knowledge out of it",8JHrucviUf
2357,2357,The report is well organized and there is a clear correspondence between the code and the issues the report addresses,8JHrucviUf
2358,2358,Problems of the report,8JHrucviUf
2359,2359,"The report has a lot of references to nowhere, this is probably a problem with their latex referencing system",8JHrucviUf
2360,2360,"There are references to sections, tabs, and an appendix",8JHrucviUf
2361,2361,"While this is a technicality that can easily be resolved for the tables and sections, I couldn't locate the appendix",8JHrucviUf
2362,2362,As mentioned in the comments the authors point that the pytorch function for the node convolution in pytorch is different from the one used in tensorflow,8JHrucviUf
2363,2363,The original paper authors attribute the differences to that,8JHrucviUf
2364,2364,"In my opinion, this difference can not be responsible for the discrepancies",8JHrucviUf
2365,2365,The report is well written and has a concise explanation,8JHrucviUf
2366,2366,"Additionally, the authors provide a great summary at the beginning",8JHrucviUf
2367,2367,I would like to know if the numbers from the TensorFlow implementation (authors' paper) match the results reported in the main paper,8JHrucviUf
2368,2368,It would interesting to perform a hyperparameter search and provide code and docs so it would be reviewed before accepting the paper,8JHrucviUf
2369,2369,After the authors of the report make some claims and report an improvement for their own implementation,8JHrucviUf
2370,2370,I suggest accepting the report,8JHrucviUf
2371,2371,As noted in the reproducibility summary the goal of Adversarially Reweighted Learning (ARL) is to improve the fairness of a classifier for disadvantaged groups,P6-9f50PuMY
2372,2372,"In contrast to other approaches the group label is not available, but has to identified from the examples.",P6-9f50PuMY
2373,2373,The authors use the original code to reproduce the experiments as well as reimplement the software from scratch based on the information in the original paper,P6-9f50PuMY
2374,2374,"Communication with the authors of the original paper was attempted, but failed",P6-9f50PuMY
2375,2375,Reproduction of the experiments (using the original code) showed large differences to the results in the original paper,P6-9f50PuMY
2376,2376,Replication with the reimplemented software yielded comparable results instead,P6-9f50PuMY
2377,2377,"Using the optimized hyperparameters from the replication improved the results of the reproduction, too",P6-9f50PuMY
2378,2378,These results are discussed in the reproducibility report in detail and the missing information for full reproducibility was pointed out there.,P6-9f50PuMY
2379,2379,The report is well written and clearly shows the easy and difficult parts of reproducing and replicating the original paper,P6-9f50PuMY
2380,2380,"Additionally, the authors analyzed if the improved accuracy obtained by ARL was significant",P6-9f50PuMY
2381,2381,"But here (in table 4) the AUC values for ARL are for different groups, AUC(avg) for Adult and COMPAS data sets, AUC(minority) for the LSAC data set, and those for the baseline cannot be found in tables 2 or 3",P6-9f50PuMY
2382,2382,I recommend to correct table 4 and reevaluate the significance,P6-9f50PuMY
2383,2383,"It may be that the improvement for AUC(avg) is not significant, but the goal of ARL is a significant improvement for AUC(minority) or AUC(min).",P6-9f50PuMY
2384,2384,"This work replicates the paper ""Fairness without Demographics through Adversarially Reweighted Learning."" The replicating authors try the original code and implement new code to verify the results of the work and find that the results generally do not hold under a significance test",P6-9f50PuMY
2385,2385,This is important to thoroughly check and replicate because such fairness mechanisms may be relied upon in real-world settings.,P6-9f50PuMY
2386,2386,Pros:,P6-9f50PuMY
2387,2387,+ I think the testing of both a re-implementation and original code is hugely important and well done by the authors.,P6-9f50PuMY
2388,2388,"+ I also appreciate the use of statistical robustness checking to make sure that results hold (or in this case, might not hold)",P6-9f50PuMY
2389,2389,This is the strong point of the paper to me as it expands upon the original paper and checks the quality of the results.,P6-9f50PuMY
2390,2390,"+ It appears that the original codebase has been updated to include hyperparameters, so this work may have done its job! That being said, a note of this should likely be made in this paper to reflect the update.",P6-9f50PuMY
2391,2391,Cons:,P6-9f50PuMY
2392,2392,"+ While I agree that the original work should have included hyperparameters and made the code able to reproducible the exact results, I'm not sure that I follow why the authors couldn't do the exact same thing as with their pytorch implementation and run the grid search per the original paper specifications",P6-9f50PuMY
2393,2393,"It seems that with the replicated PyTorch code, they were able to get quite close to the original results with their grid search so it seems feasible to have done the same thing with the original tensorflow code",P6-9f50PuMY
2394,2394,"As a result, I'm not sure it's fair to call the original paper not reproducible in such strict terms",P6-9f50PuMY
2395,2395,Typos/Style:,P6-9f50PuMY
2396,2396,"Overall, this work could use another pass to clean up typos/style, some of which are below.",P6-9f50PuMY
2397,2397,"In the paper “fairness without Demographics through Adversarially Reweighted Learning” [1], Lahoti et al",P6-9f50PuMY
2398,2398,ask themain research question: “How can we train a ML model to improve fairness when we do not know the protected groupmemberships?”,P6-9f50PuMY
2399,2399,"--> Fairness should be capitalized in the paper title, there shouldn't be a period after the question mark in quotes",P6-9f50PuMY
2400,2400, original TensorFlow implementation by Lahoti et al.1,P6-9f50PuMY
2401,2401,--> don't need a period after the footnote and in general this list should be separated by semi-colons not periods,P6-9f50PuMY
2402,2402, approach should match the results presented in Lahoti et al.,P6-9f50PuMY
2403,2403,--> two periods,P6-9f50PuMY
2404,2404,of all differences we refer to Table??in the Appendix,P6-9f50PuMY
2405,2405,--> Table isn't linked correctly ,P6-9f50PuMY
2406,2406,as tested on their openly available implementation onGitHub2,P6-9f50PuMY
2407,2407, --> footnote usually goes after the punctuation ,P6-9f50PuMY
2408,2408,"""Many scientific research proves"" -> ""Much scientific research proves"" or ""Many scientific research papers prove""",P6-9f50PuMY
2409,2409,"""Note that since the P-value cannot exceed 1, it is customary to report it without 0 before the decimal point (e.g.P=.031)."" --> If this is customary, the statement can likely be omitted.",P6-9f50PuMY
2410,2410,The optimal hyperparameter of the original paper is now available in their Github repository,P6-9f50PuMY
2411,2411,Can you reproduce their original results for both the baseline and ARL,P6-9f50PuMY
2412,2412,Can you provide reasons why the original paper has a bad baseline result? Is it because they were not choosing proper hyper-parameters?,P6-9f50PuMY
2413,2413,This paper provides a holistic reproducibility report of the original paper on the algorithm named 'deep fair clustering' (DFC),DXVAJGohUKs
2414,2414,The following issues are examined.,DXVAJGohUKs
2415,2415,1,DXVAJGohUKs
2416,2416,"On the data sets 'Color Reverse MNIST' and 'MNIST-USPS' (they are the two data sets among four used in the original paper), DFC is tested against all the four metrics used in the original paper: 'Accuracy', 'NMI', 'Balance', and 'Entropy'",DXVAJGohUKs
2417,2417,The mathematical details of these metrics are provided,DXVAJGohUKs
2418,2418,"In particular, a part of DFC and the data pre-processing are re-implemented since the corresponding code for the original paper is not accessible.",DXVAJGohUKs
2419,2419,2,DXVAJGohUKs
2420,2420,Extension by using no pre-trained cluster centres,DXVAJGohUKs
2421,2421,"It is shown that by discarding pre-trained cluster centres, DFC has a noticeable drop in performance.",DXVAJGohUKs
2422,2422,3,DXVAJGohUKs
2423,2423,Extension by using different divergence functions for regularisation,DXVAJGohUKs
2424,2424,"In particular, JS and CS divergences are used to replace the KL divergence used in the original paper",DXVAJGohUKs
2425,2425,The comparison of performance is well documented.,DXVAJGohUKs
2426,2426,4,DXVAJGohUKs
2427,2427,Extension by using non-binary/corrupted sensitive attributes,DXVAJGohUKs
2428,2428,"It is shown that with corruption, DFC has a noticeable drop in performance in all the four metrics.",DXVAJGohUKs
2429,2429,"The paper is well organised, and the clear structure makes the report easy to follow.",DXVAJGohUKs
2430,2430,Possible problems:,DXVAJGohUKs
2431,2431,1,DXVAJGohUKs
2432,2432,"The words' fair' and 'effective' have both general English meaning and context-specific definitions, and both are tightly connected to machine learning and artificial intelligence",DXVAJGohUKs
2433,2433,Using such words without giving a brief introduction leaves the readers confused,DXVAJGohUKs
2434,2434,"The confusion is not reduced, e.g., even after authors write 'feature representations are considered fair if they are statistically independent of sensitive attributes.'",DXVAJGohUKs
2435,2435,2,DXVAJGohUKs
2436,2436,The point of 'sensitive attributes' in section 2 is confusing,DXVAJGohUKs
2437,2437,I am particularly lost at how this is related to turning the background of images to white or black.,DXVAJGohUKs
2438,2438,3,DXVAJGohUKs
2439,2439,"Usually, the change of regularisers would significantly modify the performance of an algorithm",DXVAJGohUKs
2440,2440,"Examples include enhancing sparsity by l1 or l0 penalty, enhancing prediction accuracy by l2 penalty, and so on",DXVAJGohUKs
2441,2441,"I am not sure if it is fair (see, this word 'fair' has a different meaning than it has in the paper) to test the claims' robustness by changing regularisation.",DXVAJGohUKs
2442,2442,4,DXVAJGohUKs
2443,2443,How is the 'background' of an image or the 'background colour' defined?,DXVAJGohUKs
2444,2444,5,DXVAJGohUKs
2445,2445,"A typo in line 149, p(x=q(x)) should be p(x)=q(x).",DXVAJGohUKs
2446,2446,The author(s) provide precise details in their report regarding the proposed algorithm and data set (MNIST) details,DXVAJGohUKs
2447,2447,The objective function and model description are well defined,DXVAJGohUKs
2448,2448,They provide every detail of the code and algorithms that have been used by other papers and provide the citation,DXVAJGohUKs
2449,2449,The evaluation criteria; accuracy and NMI were used to evaluate cluster validity,DXVAJGohUKs
2450,2450,"They didn't discuss details of their result in Tables 3, 4, and 5, but they were understandable.",DXVAJGohUKs
2451,2451,Reproducibility Summary : The report has this summary that contains major findings.,eNj0zqNUkBU
2452,2452,"Scope of reproducibility: The report states the scope clearly, and follows it.",eNj0zqNUkBU
2453,2453,Code: The authors reproduced the code in PyTorch.,eNj0zqNUkBU
2454,2454,"Communication with original authors: Fair communications are mentioned, but I am not sure whether the original authors have evaluated the results in this report.",eNj0zqNUkBU
2455,2455,"Hyperparameter Search: It is mentioned in the report that hyperparameters were varied, but not all results are included in the report.",eNj0zqNUkBU
2456,2456,Ablation Study: The ablation study is not comprehensive.,eNj0zqNUkBU
2457,2457,"Discussion on results: The report discusses the state of reproducibility of the original paper, and mentions the easy parts and difficult parts",eNj0zqNUkBU
2458,2458,The numerical results of this report are consistent with those in the original paper,eNj0zqNUkBU
2459,2459,Only the results on ResNet-18 deviate.,eNj0zqNUkBU
2460,2460,Recommendations for reproducibility: It seems the report does not discuss a lot on how the original authors can improve reproducibility.,eNj0zqNUkBU
2461,2461,Results beyond the paper: The experiments in the report are almost the same as those in the original paper.,eNj0zqNUkBU
2462,2462,Overall organization and clarity: The overall quality is good.,eNj0zqNUkBU
2463,2463,The report clearly presents its scope and reimplements SAdam on pytorch where they compare their implementation with the version obtained from the authors which is in tensorflow,eNj0zqNUkBU
2464,2464,The report has multiple positive aspects:,eNj0zqNUkBU
2465,2465,- communication with the authors,eNj0zqNUkBU
2466,2466,- reimplementation of the code,eNj0zqNUkBU
2467,2467,- clear reproduction of the original paper,eNj0zqNUkBU
2468,2468,- clear description of the problem and the presentation of the report ,eNj0zqNUkBU
2469,2469,- additional experiments on the resnet model,eNj0zqNUkBU
2470,2470,Major shortcomings of the paper include:,eNj0zqNUkBU
2471,2471,- lack of a range of hyperparameter search for the robustness of results,eNj0zqNUkBU
2472,2472,- testing the algorithm in contexts other than simple vision problems,eNj0zqNUkBU
2473,2473,"The area of algorithmic improvements is a tricky subject where it is hard to confirm with strong confidence that the results are robust across wide variety of hyperparameters, and confirming the validity of the results in a way that translates to different structures of problems",eNj0zqNUkBU
2474,2474,I think one of the strongest points a reproducibility can make is to show that this is the case,eNj0zqNUkBU
2475,2475,"That said, the present report does a great job at reproducing the core components of the investigated paper; I encourage the authors to extend their hyperparameter search for robustness and comparisons as well as applying the algorithm to wider (different) sets of optimization problems.",eNj0zqNUkBU
2476,2476,"This manuscript provides an pytorch implementation and reproducibility report of ""SAdam: A Variant of Adam for Strongly Convex Functions""",eNj0zqNUkBU
2477,2477,Most of the claims are consistent with the original paper,eNj0zqNUkBU
2478,2478,"Experimental results include L2 regularized softmax regression, 4 layer CNN, ResNet 18 on MNIST, CIFAR10 and CIFAR100 datasets with exhaustive hyperparameter tuning",eNj0zqNUkBU
2479,2479,It would be interesting to see the results on ImageNet.,eNj0zqNUkBU
2480,2480,The report is interesting,DmNeiy8i5lu
2481,2481,I want to know how this approach is different than the approach mentioned in Keras-vis (https://github.com/raghakot/keras-vis),DmNeiy8i5lu
2482,2482,A comparative analysis with the Keras-vis approach will be beneficial,DmNeiy8i5lu
2483,2483,It is also required to show some failed cases as well.,DmNeiy8i5lu
2484,2484,The authors do a commendable job in reporting the state of reproducibility of the original paper and provide a concise summary of their findings,DmNeiy8i5lu
2485,2485,"However, I feel that the current report can be significantly improved with additional experiments to further understand the results obtained by the authors",DmNeiy8i5lu
2486,2486,I have presented a detailed evaluation of the reproducibility report with the associated metric below.,DmNeiy8i5lu
2487,2487,Reproducibility report: The authors present a comprehensive summary that outlines their results and contribution accurately,DmNeiy8i5lu
2488,2488,"Scope of reproducibility: The authors present a clear enumeration of the reproducibility scope, that is derived directly from the original paper",DmNeiy8i5lu
2489,2489,"Code: The authors reused the code from the original authors, citing difficulty in writing the code from scratch from the paper itself",DmNeiy8i5lu
2490,2490,"However, they fail to provide explicit details of the issues they faced",DmNeiy8i5lu
2491,2491,It would be helpful if they could outline the problems they faced or propose possible additions to the original paper that can be added to the supplementary information in order to facilitate replication by the community,DmNeiy8i5lu
2492,2492,The github repo submitted by the authors is well-organized and serves as a good codebase to reproduce the results of this paper.,DmNeiy8i5lu
2493,2493,Communication with original authors: The authors present a clarification except citing their proactiveness to contact the original authors and clarify the discrepancy in their obtained results,DmNeiy8i5lu
2494,2494,They also present a discussion pertaining to the colors in the visualization results,DmNeiy8i5lu
2495,2495,"However, it is unclear if the authors experimented with different batch sizes or having the same image with different images in the batch to observe if the visualization results changed, as alluded to by the original author.",DmNeiy8i5lu
2496,2496,Hyperparameter Search: I feel that the authors lacked in performing a hyperparameter search and evaluating the robustness of the results,DmNeiy8i5lu
2497,2497,"Given that the results are susceptible to the statistics of input batch, I would have liked to see experiments to study the impact of batch size or different seed (allowing for different images in the batch) on the visualization results for each image",DmNeiy8i5lu
2498,2498,This issue stands as a significant weakness of the report and therefore the robustness of the results of the original paper remain undetermined,DmNeiy8i5lu
2499,2499,Ablation Study: The authors' description of the original methodology is not convincing enough and thus it is difficult to comment if the authors understood the original methodology well,DmNeiy8i5lu
2500,2500,The absence of ablation experiments further raise questions about this issue,DmNeiy8i5lu
2501,2501,It would be helpful if the authors could explore some ablation experiments to comment about the different aspects of PFV functioning,DmNeiy8i5lu
2502,2502,"For instance, what is the role of the element-wise multiplication with the activation map? Does it help in making the PFVs more focal?",DmNeiy8i5lu
2503,2503,Discussion on results: The authors were able to explore the scope of reproducibility mentioned earlier in their report,DmNeiy8i5lu
2504,2504,"However, the dicussion is not very clear",DmNeiy8i5lu
2505,2505,"Specifically, what parts of the paper did they feel lacked clarity thus preventing them from writing the code from scratch? Also, how did the code base from the original authors help clarify those ambiguities? The current report lacks details beyond mentioning ""required a bit of advanced knowledge in the CNN architectures""",DmNeiy8i5lu
2506,2506,I believe adding this detail is crucial to understand the drawbacks of the original paper with respect to its reproducibility,DmNeiy8i5lu
2507,2507,Recommendations for reproducibility: The authors provide no additional recommendation to the original paper authors based on their experience,DmNeiy8i5lu
2508,2508,A good reproducibility report often entails specific recommendations that could help make the original paper easier to understand and reproduce,DmNeiy8i5lu
2509,2509,Results beyond the paper: The authors perform additional experiments using different model architectures,DmNeiy8i5lu
2510,2510,I feel this is very helpful and extremely commendable,DmNeiy8i5lu
2511,2511,"However, they do not provide further insight into the results",DmNeiy8i5lu
2512,2512,"For instance, why does PFV work fine for Resnet but not InceptionNet even though both of them consist of skip connections (which is cited as the drawback of PFV)",DmNeiy8i5lu
2513,2513,"Similarly, I am not clear why PFV fails to visualize objects correctly",DmNeiy8i5lu
2514,2514,The results seem to portray that PFV was able to locate these objects in Section 4.4,DmNeiy8i5lu
2515,2515,These ambiguities in the paper need to be resolved and currently serve as severe drawbacks,DmNeiy8i5lu
2516,2516,"Furthermore, did the authors explore visualization results for images where more than one entity in present? Is PFV, being a class agnostic unsupervised method, able to identify all entities in the image or does it focus on one of them? Does this result change with changing batch statistics?",DmNeiy8i5lu
2517,2517,Overall organization and clarity: I found some grammatical errors and typos in the paper,DmNeiy8i5lu
2518,2518,I believe the overall presentation of the paper could be improved as well,DmNeiy8i5lu
2519,2519,"Overall, I think the authors have made a good preliminary attempt at replicating the paper and I hope this experience has provided them valuable insights into the method and the field in general",DmNeiy8i5lu
2520,2520,"However, the report in its current state has severe drawbacks as mentioned above",DmNeiy8i5lu
2521,2521,"If the authors can address these concerns, I am happy to change my evaluation",DmNeiy8i5lu
2522,2522,The paper reproduces Principal Feature Visualisation for CNNs by reusing the provided code of the original authors and running it on a chosen set of images,DmNeiy8i5lu
2523,2523,"The authors build their study on four claims extracted from the original paper (contrast, lightweight, ease of interpretability, unsupervised) and use a dataset comprising images from Open image dataset v6, example images of the code repository and ""some images online""",DmNeiy8i5lu
2524,2524,The authors state the the four claims can be verified.,DmNeiy8i5lu
2525,2525,"While the paper shows interesting results of running the reused code on the chosen set of images, the paper neither critically assesses the implementation of the approach nor follows a structured approach to reproduce experimental outcomes and findings",DmNeiy8i5lu
2526,2526,The original paper evaluates the approach for the debugging classification errors (for dog breeds) and transfer learning (using the Pascal VOC2012 dataset for fine-tuning),DmNeiy8i5lu
2527,2527,It would be highly interesting to design and conduct experiments for other target classes / target datasets or use other CNN architectures,DmNeiy8i5lu
2528,2528,"While the authors actually vary the latter (and use established pre-trained models, such as AlexNet or MobileNet), they only show the results on five images and do not further assess the applicability for debugging or transfer learning.",DmNeiy8i5lu
2529,2529,# Reviewer Guidelines Rubric,Gsj8MmNkA6L
2530,2530,---,Gsj8MmNkA6L
2531,2531,* *Reproduciblity Summary:* Followed template,Gsj8MmNkA6L
2532,2532,* *Scope of reproducibility*: Is stated and adhered to.,Gsj8MmNkA6L
2533,2533,* *Code*:,Gsj8MmNkA6L
2534,2534,  * Used original authors code,Gsj8MmNkA6L
2535,2535,  * Additional experiment code is included as supplemental material,Gsj8MmNkA6L
2536,2536,"  * Code looks reasonable, `README.md`  and   `requirements.txt`  included.",Gsj8MmNkA6L
2537,2537,"* *Communication with Original Authors*: mentioned briefly, sounds cordial, only appears on summary.",Gsj8MmNkA6L
2538,2538,*  *Hyperparameter Search*,Gsj8MmNkA6L
2539,2539,  * Reused many parameters from appendix of original paper.,Gsj8MmNkA6L
2540,2540,  * Did search over $\lambda$ ,Gsj8MmNkA6L
2541,2541,  * Also experimented with centroid size and penalty constant $c$ as hyperparameters ,Gsj8MmNkA6L
2542,2542,  * Only tested one hyper parameter at a time instead of simultaneously,Gsj8MmNkA6L
2543,2543,* *Ablation Study*:,Gsj8MmNkA6L
2544,2544,  * I believe some parts of the hyperparameter search accomplish this (eg  $\lambda = 0$) but otherwise not discussed,Gsj8MmNkA6L
2545,2545,* *Discussion on Results*:,Gsj8MmNkA6L
2546,2546,  * Reproduced results closely on several standard data sets,Gsj8MmNkA6L
2547,2547,  * Reproduced the baseline model (Deep Ensemble) as well DUQ,Gsj8MmNkA6L
2548,2548,  * No side-by-side comparisons with original paper,Gsj8MmNkA6L
2549,2549,"  * Other than on summary page, does not really describe which parts were easy or difficult to reproduce.",Gsj8MmNkA6L
2550,2550,* *Recommendations for reproducibility*:,Gsj8MmNkA6L
2551,2551,  * Not provided,Gsj8MmNkA6L
2552,2552,* *Results beyond the paper*:,Gsj8MmNkA6L
2553,2553,   * Introduced extension E-DUQ and some preliminary exploration,Gsj8MmNkA6L
2554,2554,* *Overall organization and clarity*,Gsj8MmNkA6L
2555,2555,  * Looks fine in general,Gsj8MmNkA6L
2556,2556,"  * LaTex is annoying sometimes, but would be good to get plots/tables on same page they are discussed somehow.",Gsj8MmNkA6L
2557,2557,---,Gsj8MmNkA6L
2558,2558,### Summary,Gsj8MmNkA6L
2559,2559,"This paper does accomplish the goal of reproducing the original paper to a large degree, but the discussion of reproducibility is short - it would be stronger if there were more in-depth explanation of what was easy, what was hard, and what the original authors could improve",Gsj8MmNkA6L
2560,2560,It is also difficult to judge how successful the reproducibility exercise is because the authors do not provide a side-by-side comparison,Gsj8MmNkA6L
2561,2561,The text could quote the original results directly rather than alluding to them ambiguously.,Gsj8MmNkA6L
2562,2562,"This paper does reproduce the baseline model used for comparison, which strengthens its results and is appreciated.",Gsj8MmNkA6L
2563,2563,"The extension E-DUQ could be useful, but isn't explored enough in this paper and is only tested on one example",Gsj8MmNkA6L
2564,2564,It might be better to expand it as a stand-alone paper instead.,Gsj8MmNkA6L
2565,2565,__Summary__,Gsj8MmNkA6L
2566,2566,The original paper (OP) proposes a deep learning  architecture based on a single deterministic neural network that produces uncertainty estimates,Gsj8MmNkA6L
2567,2567,The OP proves the performance of the method in a toy setting (2 moons) and 2 usual OOD classification problems (MNIST and CIFAR10),Gsj8MmNkA6L
2568,2568,The OP also studies slight modifications of the proposed loss function and the consequences on performance of their method.,Gsj8MmNkA6L
2569,2569,The reproducibility report (RR) reproduces most of the experiments of the OP,Gsj8MmNkA6L
2570,2570,"Additionally, the RR performs an additional experiment to better illustrate one of the stated limitations of the paper (namely its inability to separate espistemic from aleatoric uncertainty).",Gsj8MmNkA6L
2571,2571,"Finally, the RR proposes an extension to the method from OP by learning a parameter previously fixed.",Gsj8MmNkA6L
2572,2572,__Positive points__,Gsj8MmNkA6L
2573,2573,- The RR reproduced most of the experiments from the OP and managed to achieve similar results,Gsj8MmNkA6L
2574,2574,"As a consequence, the RR's authors confirmed that they were able to generally reproduce the results from the OP and validate its claims.",Gsj8MmNkA6L
2575,2575,"- The RR's authors made the effort to contact the OP's authors to show them their report, which is very positive.",Gsj8MmNkA6L
2576,2576,- The RR added a figure (Fig,Gsj8MmNkA6L
2577,2577,1) of the architecture which was not present in the OP,Gsj8MmNkA6L
2578,2578,This is helpful in understanding the neural network studied and the mechanism proposed.,Gsj8MmNkA6L
2579,2579,"- The RR discussed clearly what kind of hardware was used and the time required to perform the computations, which is of great use to the reader trying to reproduce the results of the OP.",Gsj8MmNkA6L
2580,2580,"- The RR performed an additional experiment on both the MNIST and CIFAR10 datasets, showing that DUQ was not robust to a noise perturbation of the input",Gsj8MmNkA6L
2581,2581,This experiment clearly and explicitly confirms a limitation mentioned by the OP.,Gsj8MmNkA6L
2582,2582,- The RR proposes an extension to the original idea to try to alleviate the limitation mentioned above,Gsj8MmNkA6L
2583,2583,The RR then performs experiments illustrating the performance of this extension and comparing it with DUQ,Gsj8MmNkA6L
2584,2584,"By doing so, the authors of the RR have shown their understanding of the paper",Gsj8MmNkA6L
2585,2585,Proposing a new method is a great initiative.,Gsj8MmNkA6L
2586,2586,__Negative points__,Gsj8MmNkA6L
2587,2587,"- The RR claims that all reproduced results are matched within 1% accuracy, but these quantities are never computed during the paper",Gsj8MmNkA6L
2588,2588,"Worse, this claim appears to be false (Tb",Gsj8MmNkA6L
2589,2589,5 of the RR vs Tb,Gsj8MmNkA6L
2590,2590,1 of the OP),Gsj8MmNkA6L
2591,2591,This makes trusting the conclusions of the report difficult.,Gsj8MmNkA6L
2592,2592,- A clear conclusion on what was being reproduced and the state of reproducibility is often missing,Gsj8MmNkA6L
2593,2593,"Sec 4.1: what are you trying to reproduce? Are you satisfied with the result? Sec 4.2: the RR mentions that the trend is similar: can you ground this statement with a quantified metric? Moreover, at l.122, the RR mentions that ""[their] analysis supports the claims made by [OP] authors""",Gsj8MmNkA6L
2594,2594,It would be useful to precise which claims exactly are being validated by the RR analysis,Gsj8MmNkA6L
2595,2595,Same at l.173,Gsj8MmNkA6L
2596,2596,"In addition, some statements are made regarding the reproducibility but are often vague (Sec 4.2, l.122: ""slightly more accuracy per rejection"", Sec 4.3: similar rejection classification plot, similar performance, etc).",Gsj8MmNkA6L
2597,2597,- The RR could have been better organized,Gsj8MmNkA6L
2598,2598,"The order of the figures does not match the progression of the discussion (for instance, Fig.8 is reference directly after Fig.1)",Gsj8MmNkA6L
2599,2599,The same can be said for tables,Gsj8MmNkA6L
2600,2600,"Additionally, the RR could have benefited from better formatting (Sec 3.1 especially)",Gsj8MmNkA6L
2601,2601,This overall makes the RR difficult to read,Gsj8MmNkA6L
2602,2602,- It is sometimes unclear what is the contribution of the RR and what is the contribution of the OP,Gsj8MmNkA6L
2603,2603,"For instance, is Tb",Gsj8MmNkA6L
2604,2604,3 (Sec 4.2.3) trying to reproduce the OP results or is it an additional results? The same can be said for the experiment on c from Sec 4.2.3,Gsj8MmNkA6L
2605,2605,In Sec,Gsj8MmNkA6L
2606,2606,"4.2, you compute the computational time taken by the deep ensemble and DUQ",Gsj8MmNkA6L
2607,2607,"Is it your own experiment? Is it a reproduction from the OP? If then, what is your conclusion on the reproducibility?",Gsj8MmNkA6L
2608,2608,- Limited details on the training procedure for the additional experiments is given,Gsj8MmNkA6L
2609,2609,A description or at least a pointer to the RR code would have been appreciated,Gsj8MmNkA6L
2610,2610,"Moreover, background on some choices (for instance, why the value of sigmas in Fig.6 and Fig.7?) would have helped understand the design choices.",Gsj8MmNkA6L
2611,2611,===,Gsj8MmNkA6L
2612,2612,"Overall, I find that this was an encouraging report that addressed the general reproducibility of the paper",Gsj8MmNkA6L
2613,2613,I appreciated the initiative to propose new experiments,Gsj8MmNkA6L
2614,2614,Reorganizing the report would greatly improve its readability,Gsj8MmNkA6L
2615,2615,"While the new experiments were interesting, I found that the report would benefit from a deeper discussion on the current state of reproducibility",Gsj8MmNkA6L
2616,2616,"Adding details on the methodology of the experiment, what the RR authors set to reproduce in this specific experiment, and a clear conclusion on the reproducibility of the experiment would also improve this report to help it make a more grounded statement on the status of reproducibility of the paper in general.",Gsj8MmNkA6L
2617,2617,===,Gsj8MmNkA6L
2618,2618,__Additional comments__,Gsj8MmNkA6L
2619,2619,- Fig,Gsj8MmNkA6L
2620,2620,"1 is very helpful, but takes too much space in the RR",Gsj8MmNkA6L
2621,2621,Consider making it smaller.,Gsj8MmNkA6L
2622,2622,- Is it not clear what is i in the equations of Sec 3.1,Gsj8MmNkA6L
2623,2623,The summation signs do not indicate what variable is being summed.,Gsj8MmNkA6L
2624,2624,- In Tb,Gsj8MmNkA6L
2625,2625,1: is the training time a new result? A reproduced one? This is not said clearly.,Gsj8MmNkA6L
2626,2626,- The authors study changing the constant number in the two-sided gradient penalty (Sec 4.2.3),Gsj8MmNkA6L
2627,2627,"At the first reading, it is not clear what exactly is changed since the letter c is already used in the precise equation of the gradient, but not with the same meaning as in Sec 4.2.3",Gsj8MmNkA6L
2628,2628,Additional precision on the exact quantity that is being changed and additional care to the notation could improve the understanding of what the RR tried to do.,Gsj8MmNkA6L
2629,2629,"- The term ""sensitivity"" in Sec 3.1 is not defined.",Gsj8MmNkA6L
2630,2630,"The authors test two claims made by the original paper: the ability of the proposed approach to detect OOD, and the role of different hyperparameters as an ablation study",Gsj8MmNkA6L
2631,2631,"In addition to that, the authors explore sensitivity of the proposed algorithm to noise, and propose an extension for explicit detection of aleatoric and epistemic uncertainty.",Gsj8MmNkA6L
2632,2632,"* Re noise sentivity of the algorithm (Figure 6), it is quite surprising that Deep Ensemble or E-DUQ perform so much worse that DUQ, even when we reject 95% of data",Gsj8MmNkA6L
2633,2633,Can the authors give more details and better explain why that is the case?,Gsj8MmNkA6L
2634,2634,"* In Table 1-5, how many splits (or randomly generated train datasets) were used?",Gsj8MmNkA6L
2635,2635,* Please provide confidence intervals in tables (e.g,Gsj8MmNkA6L
2636,2636,5-95% quantiles) for performance metrics,Gsj8MmNkA6L
2637,2637,"Otherwise, it is hard to assess what is significant",Gsj8MmNkA6L
2638,2638,"For AUROC, you need to report bootstrapped values.",Gsj8MmNkA6L
2639,2639,* More details about lengthscale prediction would be needed (section 4.2.2),Gsj8MmNkA6L
2640,2640,Minor:,Gsj8MmNkA6L
2641,2641,"* The authors make several grammatical mistakes, e.g., in the usage of articles ""the""",Gsj8MmNkA6L
2642,2642,"The abstract has no subject in their sentences, periods are often separated from the final word",Gsj8MmNkA6L
2643,2643,"Overall, although the text is understandable, the writting would need some polishment, and it would be nice if the English could be checked by a spell checker.",Gsj8MmNkA6L
2644,2644,* Eq,Gsj8MmNkA6L
2645,2645,"54-55, what is m_{c,t}? Not defined in the text.",Gsj8MmNkA6L
2646,2646,"* The explanation about the gradient penalties is not self-contained, that is, it is not understandable without having to go back to the original paper, so either remove it or make it self-contained.",Gsj8MmNkA6L
2647,2647,"* Because this is a reproducibility challenge, the authors should provide more details on experimental setup to generate Tables 1-5 (what hyperparameters, setups, splittings, number of runs, etc, etc).",Gsj8MmNkA6L
2648,2648,The authors of this report provide a complete open-source reimplementation of the original method as well as the figures provided in the original paper,vOIGINzuJR6
2649,2649,They find some discrepancies and variance in the results,vOIGINzuJR6
2650,2650,The authors do not go beyond the original content of the paper in terms of understanding the method; they do superficially report some unsuccessful attempts to improve it.,vOIGINzuJR6
2651,2651,Format: the authors correctly follow the format required for the challenge.,vOIGINzuJR6
2652,2652,Scope: the authors (attempt to) reproduce all the results from the paper,vOIGINzuJR6
2653,2653,Code: at a glance the provided code appears complete,vOIGINzuJR6
2654,2654,It was written from scratch since no code is provided in the Original Paper,vOIGINzuJR6
2655,2655,Communication & hyperparameters: the authors have done their due diligence,vOIGINzuJR6
2656,2656,"Ablation: the authors perform the same ablation study as in the OP, finding some differences, but do not report further ablative modifications",vOIGINzuJR6
2657,2657,They also perform a similar sensitivity analysis.,vOIGINzuJR6
2658,2658,Discussion: the authors make appropriate discussions and remarks on reproducibility,vOIGINzuJR6
2659,2659,Comments:,vOIGINzuJR6
2660,2660,"- there seems to be a typo in eq (4), \lambda was replaced by some text",vOIGINzuJR6
2661,2661,- broken reference on line 171,vOIGINzuJR6
2662,2662,"- Figure 1: the labels of the plots are pretty small, in academic papers labels should have the same font size than the main text ",vOIGINzuJR6
2663,2663,"- “We first defined performance as train target accuracy, [then test]”, this is a nice insight, but not the correct way to do this",vOIGINzuJR6
2664,2664,"If using the training data, then you are only measuring memorization, if using the test data, then you’ve “cheated” and your test data is no longer test data, and no longer gives you an unbiased estimate of the performance of your method",vOIGINzuJR6
2665,2665,"The correct way to do this would be to split your training data in two, i.e",vOIGINzuJR6
2666,2666,"create a validation set, and use this validation set solely to determine early stopping",vOIGINzuJR6
2667,2667,- missing Figure reference on lines 224 and 236,vOIGINzuJR6
2668,2668,- the authors critique the OP for not having the same number of random seeds per hyperparameter setting,vOIGINzuJR6
2669,2669,Is this referenced somewhere in the OP? Or is this from an email exchange? These numbers should be made explicit.,vOIGINzuJR6
2670,2670,- Figure 3 & sensitivity analysis: the authors note that “there is very little similarity to be found in any of the accuracy landscapes”,vOIGINzuJR6
2671,2671,"Experiments have natural variance, especially since different random seeds are used (and small code differences probably still remain)",vOIGINzuJR6
2672,2672,Considering the closeness (there isn’t much difference between 0.71 and 0.73) of the bounds of the colorbars it would be quite unexpected for the same peaks and valleys to show up,vOIGINzuJR6
2673,2673,"This shows that this method is robust to these two hyperparameter choices, or alternatively, that these two hyperparameters do not influence the results.",vOIGINzuJR6
2674,2674,"- stylistic comment: the authors use the “we” voice in a lot of contexts, this makes it hard to distinguish between what is something that they contribute (e.g",vOIGINzuJR6
2675,2675,"novel measures, novel hyperparameters or other choices) and something done by the original authors",vOIGINzuJR6
2676,2676,"Alternatively, the passive voice can be used, for example:",vOIGINzuJR6
2677,2677,  - “We combine these two loss terms into a single term” -> “These two loss terms are combined into a single term”,vOIGINzuJR6
2678,2678,"  - “after completing training, we train two classifiers” -> “after completing training, two classifiers are trained”",vOIGINzuJR6
2679,2679,"- on language & grammar: the text is very readable, but contains some typos and grammatical mistakes and would benefit from additional proofreading.",vOIGINzuJR6
2680,2680,"This report presents an attempt at reproducing results from « Fairness by learning orthogonal disentangled representations » (Sarhan et al., 2020)",vOIGINzuJR6
2681,2681,"In that paper, a representation learning algorithm is proposed to learn « fair » representations (i.e., representations from which sensitive attributes cannot be easily predicted) by learning two separate representations (one for the class of interest and one for sensitive attributes) that should be both disentangled and orthogonal to each other",vOIGINzuJR6
2682,2682,"This reproducibility work is only partially able to reproduce the original paper’s results, in part at least due to the lack of open source implementation and some missing experimental details.",vOIGINzuJR6
2683,2683,"The report’s authors clearly state their objective, explain what they have done (and what was easy/difficult), and compare their results to the original paper",vOIGINzuJR6
2684,2684,"This is a significant effort since they had to start from scratch, and they attempted to reproduce more than just the main results (they also include an ablation study and a sensitivity analysis)",vOIGINzuJR6
2685,2685,I also appreciate that there is a well documented open source repository to easily re-run these experiments.,vOIGINzuJR6
2686,2686,My main concern is that it remains unclear why some of the results are so far off from the original paper,vOIGINzuJR6
2687,2687,"In particular, the YaleB and CIFAR-100 results are worrying, since the model is clearly unable to learn the task at hand",vOIGINzuJR6
2688,2688,"I would have expected the authors to dig deeper on that, for instance by doing more ablations / hyper-parameter search to at least identify which parts of the loss / model are responsible for this issue",vOIGINzuJR6
2689,2689,"Without such an analysis, there remains some doubt regarding the correctness of the implementation.",vOIGINzuJR6
2690,2690,Another potential concern is that it is not clear from the descriptions of the datasets in 3.2 that the definitions of the sensitive attributes match what was done by Sarhan et al,vOIGINzuJR6
2691,2691,on the YaleB and CIFAR datasets,vOIGINzuJR6
2692,2692,Could you please clarify that point in the report?,vOIGINzuJR6
2693,2693,Small remarks:,vOIGINzuJR6
2694,2694,- On .81 should « based on z_T » be « based on z_S »?,vOIGINzuJR6
2695,2695,- I find eq,vOIGINzuJR6
2696,2696,3 a bit confusing because q_phi_S is applied with z_T as input instead of z_S: how is that possible?,vOIGINzuJR6
2697,2697,- In eq,vOIGINzuJR6
2698,2698,4 there is probably a missing \ in the Latex code,vOIGINzuJR6
2699,2699,- l,vOIGINzuJR6
2700,2700,171 has a missing reference,vOIGINzuJR6
2701,2701,- What are the horizontal dashed lines in the plots?,vOIGINzuJR6
2702,2702,- References to figures in the Appendix seem broken (ex: l,vOIGINzuJR6
2703,2703,"224, but there are more)",vOIGINzuJR6
2704,2704,- l,vOIGINzuJR6
2705,2705,360: « We should check whether these are in fact the last hyperparameters we used » => was this sentence supposed to be included?,vOIGINzuJR6
2706,2706,The paper provides a thorough explanation of the reproducibility efforts,OownuYG0SOC
2707,2707,"It could possibly be improved by providing more experiments, especially on the attempts to apply the approach to new examples",OownuYG0SOC
2708,2708,"(Hyper)parameter choices are mostly adopted from the paper and could have been experimented with, too.",OownuYG0SOC
2709,2709,The summary is somewhat vague in some places,OownuYG0SOC
2710,2710,"For instance, the authors state: ""The accuracy, however, when running Experiment 3 (38%) is much lower than in the paper",OownuYG0SOC
2711,2711,"This is because we divided the amount of Monte-Carlo samples by 5"" This needs some motivation as to why the number of samples was changed and whether this is a problem or not.",OownuYG0SOC
2712,2712,"Great to see, authors have been in touch with the original paper authors.",OownuYG0SOC
2713,2713,"This work aims to reproduce and examine the claims made in ""Generative Causal Explanations of Black Box Classifiers""",OownuYG0SOC
2714,2714,The authors do so by reproducing the original results presented in the paper and additionally evaluating against imagenet to test the behavior on a more challenging dataset,OownuYG0SOC
2715,2715,"The authors remark that they are unable to fully reproduce the results, though this is likely because they did not evaluate using the exact same procedure due to a reduction in the number of Monte Carlo samples",OownuYG0SOC
2716,2716,They also evaluate using the image net dataset and show worse behavior,OownuYG0SOC
2717,2717,"However, it isn't clear to me if this degraded performance can also be attributed to the same issues that prevented reproducibility on the original experiments",OownuYG0SOC
2718,2718,"Overall, I think the authors made a nice attempt at reproduction (MC samples aside), and do well to consider an additional dataset",OownuYG0SOC
2719,2719,"I found the rationale behind the extension to be solid, and very important when thinking through considerations that go into applying a model in more realistic settings",OownuYG0SOC
2720,2720,I have two core issues with this reproduction:,OownuYG0SOC
2721,2721,1,OownuYG0SOC
2722,2722,The paper is generally difficult to follow,OownuYG0SOC
2723,2723,The paper reads closer to an outline than a finished report,OownuYG0SOC
2724,2724,"I would encourage the authors to spend some additional time on organization, making sure that the key takeaways are made plain and that the report reads fluidly throughout",OownuYG0SOC
2725,2725,2,OownuYG0SOC
2726,2726,"This reproduction tests one additional dataset is commendable, but I would have liked to see some examination that gets closer to the behavior of the method",OownuYG0SOC
2727,2727,"For example, robustness to hyperparameters, model complexity, and other behaviors",OownuYG0SOC
2728,2728,3,OownuYG0SOC
2729,2729,"It is not entirely clear to me why the authors chose to use less MC samples, since it hampered the ability to fully evaluate the claims of the paper",OownuYG0SOC
2730,2730,"In this paper, the authors reported results from the reproducibility study of [1].",BHqmQ6tnb3N
2731,2731,Pros: ,BHqmQ6tnb3N
2732,2732,"1), successfully reproduced the results reported in the original paper",BHqmQ6tnb3N
2733,2733,"2), went beyond the empirical study in the original paper by testing the method with networks of more complicated architectures and additional datasets of increased learning difficulty to test the scope of the usability of the method",BHqmQ6tnb3N
2734,2734,"3), performed study of searching for hyperparameters: K, L, \lambda",BHqmQ6tnb3N
2735,2735,"4), good communications with authors of original paper",BHqmQ6tnb3N
2736,2736,Cons:,BHqmQ6tnb3N
2737,2737,"1), the writing needs substantial improvement",BHqmQ6tnb3N
2738,2738,"2), study of using more complicated generative model is needed for more complicated classifiers or learning tasks",BHqmQ6tnb3N
2739,2739,The proposed method in the original paper may still perform well using more complicated generative models for cases where there is increased complicity in the learning task and classifier,BHqmQ6tnb3N
2740,2740, ,BHqmQ6tnb3N
2741,2741,"The report details reproduction of the paper ""Generative causal explanations of black-box classifiers"" by O’Shaughnessy et al",BHqmQ6tnb3N
2742,2742,The authors tried to do all the right things and even go beyond only reproducibility but also to extend the original experiments but some parts are rushed and some conclusions drawn are questionable.,BHqmQ6tnb3N
2743,2743,Reproducibility Summary: is provided but it is quite short and cover the main points of the reports only at the very high level,BHqmQ6tnb3N
2744,2744,It could have been extended to include more specific details.,BHqmQ6tnb3N
2745,2745,Scope of reproducibility: is clearly stated and is followed in the later text.,BHqmQ6tnb3N
2746,2746,Code: the authors used the code provided by the original authors of the paper,BHqmQ6tnb3N
2747,2747,They mentioned several issues with the code which they had to overcome (sometimes with the help of the original authors),BHqmQ6tnb3N
2748,2748,The release code by the authors of the report looks quite readable and well organised.,BHqmQ6tnb3N
2749,2749,Communication with original authors: The report mentions communication with the original authors but mostly in terms of working with their code,BHqmQ6tnb3N
2750,2750,"However, the authors emphasise that the main issue with reproducibility was selection of hyperparameters and no discussion of that with the original authors is mentioned.",BHqmQ6tnb3N
2751,2751,"Hyperparameter Search: the authors emphasise that hyperparameter search method provided in the original paper is not very clear, nor it is well motivated, nor different values are checked in the original paper",BHqmQ6tnb3N
2752,2752,"Moreover, the authors claim that the method from their experiments is quite sensitive to this choice of hyperparameters",BHqmQ6tnb3N
2753,2753,"However, although claimed to conducted experiments on hyperparameter search, the authors only report results for tuning only 1 of the 3 main hyperparameters",BHqmQ6tnb3N
2754,2754,Even with these experiments the provided results do not seem to confirm the authors' claim on high sensitivity,BHqmQ6tnb3N
2755,2755,I may be missing something but the results of figures 4 and 5 look consistent enough for me,BHqmQ6tnb3N
2756,2756,Ablation Study: no ablation study has been conducted,BHqmQ6tnb3N
2757,2757,Discussion on results: the report does have this discussion and clearly states which parts of their reproducibility study was easy and difficult,BHqmQ6tnb3N
2758,2758,"However, some drawn conclusions by the authors are questionable",BHqmQ6tnb3N
2759,2759,"For example, the authors claim that from their experiments the model is highly sensitive to selection of hyperparameters",BHqmQ6tnb3N
2760,2760,"Considering that this is true (questionable on its own (see above)), then the authors claim that the model is not generalisable to a more difficult dataset, however, they do not report at all on hyperparameter selection for this dataset",BHqmQ6tnb3N
2761,2761,From this it might be the case that this is still the same issue of sensitivity to hyperparameters rather than the additional issue with generalisability,BHqmQ6tnb3N
2762,2762,Recommendations for reproducibility: are provided to some extent,BHqmQ6tnb3N
2763,2763,The main recommendation is considering the hyperparameter selection procedure,BHqmQ6tnb3N
2764,2764,Results beyond the paper: the authors go in several directions beyond the paper,BHqmQ6tnb3N
2765,2765,They explore different base classifiers that the proposed method from the original paper should explain and they test the model on a more difficult dataset,BHqmQ6tnb3N
2766,2766,Overall organisation and clarity: the report is mostly well written and easy to follow,BHqmQ6tnb3N
2767,2767,There are a number of minor typos,BHqmQ6tnb3N
2768,2768,Some specific points:,BHqmQ6tnb3N
2769,2769,"1.	Line 29, “parameter selection of the paper” – bad wording",BHqmQ6tnb3N
2770,2770,"2.	Line 47, DAG is not introduced",BHqmQ6tnb3N
2771,2771,"3.	Lines 118-119, “we introduce the following two variants” which follows only by one option",BHqmQ6tnb3N
2772,2772,"4.	Lines 135-136, “These values …” should better be placed for the sentence “As described in Section 2.1.3” as the method from Figure 1 only determines values for K, L, and lambda and not the number of samples N_alpha and N_beta",BHqmQ6tnb3N
2773,2773,5.	Figure 2 requires more explanation about set up of the experiment and what different colours mean,BHqmQ6tnb3N
2774,2774,6.	Section 4.3 – everything should be adjusted to 3 input channels at once and there is no need to compare results without doing so,BHqmQ6tnb3N
2775,2775,Minor:,BHqmQ6tnb3N
2776,2776,"1.	Line 10, “of the of the” – the second of the is redundant",BHqmQ6tnb3N
2777,2777,"2.	Line 83, “traditioanally” -> “traditionally”",BHqmQ6tnb3N
2778,2778,3.	Both “grayscale” and “greyscale” is used in the text,BHqmQ6tnb3N
2779,2779,It should be consistent,BHqmQ6tnb3N
2780,2780,"4.	Line 117, “i.e.” is redundant",BHqmQ6tnb3N
2781,2781,"5.	Line 123, “",BHqmQ6tnb3N
2782,2782,"Which” -> “, which”",BHqmQ6tnb3N
2783,2783,"6.	Line 186, “In this table becomes” -> “From this table it becomes”",BHqmQ6tnb3N
2784,2784,7.	Line 196 – missing full stop at the end of the sentence,BHqmQ6tnb3N
2785,2785,"8.	Line 202, “Posterior to redesigning” -> “After redesigning”",BHqmQ6tnb3N
2786,2786,"9.	Line 206, “Especially for the horses” – unfinished sentence",BHqmQ6tnb3N
2787,2787,"10.	Line 217, “A number” -> “a number”",BHqmQ6tnb3N
2788,2788,"11.	Line 240, “pragmatic” – it seems the authors meant something else",BHqmQ6tnb3N
2789,2789,"This paper aims to reproduce and examine the claims made in the ""Generative causal explanations of black box classifiers"" paper",BHqmQ6tnb3N
2790,2790,The authors approach this problem by examining three aspects of behavior of the proposed model:,BHqmQ6tnb3N
2791,2791,1,BHqmQ6tnb3N
2792,2792,Reproducibility of the results presented in the original paper,BHqmQ6tnb3N
2793,2793,2,BHqmQ6tnb3N
2794,2794,Sensitivity to alternative black box models,BHqmQ6tnb3N
2795,2795,3,BHqmQ6tnb3N
2796,2796,Sensitivity to the choice of hyperparameters,BHqmQ6tnb3N
2797,2797,The authors find positive evidence of (1),BHqmQ6tnb3N
2798,2798,"However, they find a decay in performance with more complex models and a lack of robustness with respect to the choice of hyperparameters",BHqmQ6tnb3N
2799,2799,"Overall, I felt this was a solid reproduction",BHqmQ6tnb3N
2800,2800,"On the positive side: the authors do well to push the proposed methodology outside of the aspects presented within the paper, and I found the findings to be informative",BHqmQ6tnb3N
2801,2801,"On the less positive side: (a) Ideally, I would have liked to seen at least a set of hypotheses summarizing the explanation of the degradation of performance",BHqmQ6tnb3N
2802,2802,"(b) Some of the writing could use a little more fleshing out, the descriptions in the beginning of the text are spars and make it difficult to have full context without having read the prior paper within a very short window",BHqmQ6tnb3N
2803,2803,  ,BHqmQ6tnb3N
2804,2804,"The authors of this report have tried to reproduce the results of the paper ""Generative causal explanations of black-box classifiers"" by O’Shaughnessy et.al",sacRaG5zt_m
2805,2805,They have written up a Reproducibility Summary which has all the required elements,sacRaG5zt_m
2806,2806,"The scope of reproducibility is well-defined, and the report is well-structured and well-written",sacRaG5zt_m
2807,2807,The authors have reportedly re-written major portions of the code themselves and have not contacted the original paper's authors,sacRaG5zt_m
2808,2808,They have shared the link to the new codebase in the report which seems to have sufficient documentation,sacRaG5zt_m
2809,2809,They have explained the idea in the original paper well in their Introduction,sacRaG5zt_m
2810,2810,"It is impressive that they have not just tried to reproduce the results in the original paper but have also tested the framework on the entire MNIST dataset and the CIFAR-10 dataset, and included a discussion on the results on these datasets",sacRaG5zt_m
2811,2811,They have even tried to find causal relationships in a low-performance classifier on the MNIST dataset to extend the usefulness of the explanatory framework,sacRaG5zt_m
2812,2812,"They rightly question the usefulness of the explanations as some of the latent factors cannot be easily interpreted, especially on complex data sets",sacRaG5zt_m
2813,2813,They also try to explain the scenarios when the framework doesn't work as expected in disentangling the causal and non-causal latent factors,sacRaG5zt_m
2814,2814,A few suggestions to improve the report:,sacRaG5zt_m
2815,2815,1,sacRaG5zt_m
2816,2816,The authors should include the range of hyperparameters that they have tried before landing on the ones that they have used.,sacRaG5zt_m
2817,2817,2,sacRaG5zt_m
2818,2818,The authors should include the training times and the hardware they have used,sacRaG5zt_m
2819,2819,3,sacRaG5zt_m
2820,2820,"The authors rightly mention that it was difficult to quantify the results and the judgment of the results was based more on intuition, subject to personal biases and interpretations",sacRaG5zt_m
2821,2821,Do they have suggestions on what kind of metrics can be employed here? Consulting with the original paper's authors may have led to some good discussions about this aspect and possibly some useful metrics,sacRaG5zt_m
2822,2822,"In all, this is an impressive reproducibility report",sacRaG5zt_m
2823,2823,The authors have done due diligence in attempting to reproduce the results and have gone beyond by experimenting on more data sets and models,sacRaG5zt_m
2824,2824,"The authors reproduced the results in the paper ""Generative causal explanations of black-box classifiers""",sacRaG5zt_m
2825,2825,"Though source codes are open-sourced by the original authors, they claimed difficulties of using the original codes and re-implemented most of the codes",sacRaG5zt_m
2826,2826,"Beyond that, they experimented with more datasets and analyzed the experiment results",sacRaG5zt_m
2827,2827,They also proposed to evaluate the proposed framework to check its ability to examine poor models.,sacRaG5zt_m
2828,2828,The report is clearly written with detailed analysis,sacRaG5zt_m
2829,2829,The authors did substantial work to re-implement most of the framework,sacRaG5zt_m
2830,2830,"They discussed the challenges in using the hyper-parameter search methods proposed in the origin paper, i.e",sacRaG5zt_m
2831,2831,it's too expensive for large models,sacRaG5zt_m
2832,2832,"Overall, this report clearly described the effort they made to reproduce the results",sacRaG5zt_m
2833,2833,The ablation study is well motivated with reasonable results,sacRaG5zt_m
2834,2834,"Summary: This is a replication of the NeurIPS paper ""Interventional Few-Shot Learning"" by Yue et al",iwAC32Fov_3
2835,2835,(2020),iwAC32Fov_3
2836,2836,"This replicability-report focuses a lot on replicating four specific experiments from this paper: Resnet10 Baseline, Resnet10 IFSL using both the SIB and MTL algorithm for each (4 configurations)",iwAC32Fov_3
2837,2837,The report seems to corroborate the results for the SIB algorithm and obtain close but slightly different results for the MTL algorithm using different computational resources but the same code provided by Yue at al,iwAC32Fov_3
2838,2838,(2020).,iwAC32Fov_3
2839,2839,"With respect to the pros, this replicability study is clear and easy to read",iwAC32Fov_3
2840,2840,It is also very succinct,iwAC32Fov_3
2841,2841,"The report also contain all the main minimal expected elements for the Replicability Challenge, including the type of contact the author of the report had with the authors of the original paper",iwAC32Fov_3
2842,2842,This report has information of what to expect when running the code provided by Yue et al,iwAC32Fov_3
2843,2843,(2020),iwAC32Fov_3
2844,2844,"On the other hand, a better detail of why the replication focused on only the four experimental cases could have improved the report",iwAC32Fov_3
2845,2845,The original NeurIPS paper had a large evaluation protocol with various assumptions tested,iwAC32Fov_3
2846,2846,"Thus, this report provides a valuable but limited information about the original paper",iwAC32Fov_3
2847,2847,"Also, since the same code provided by the authors was used, it could have improve the replicability study if more information about the hyperparameter sweep, and may be other IFSL model assumptions, were added",iwAC32Fov_3
2848,2848,"This could have been done, for instance, by including a discussion of the parameters used to replicate the experiments",iwAC32Fov_3
2849,2849,"Finally, the report reads more like a summary of the study performed",iwAC32Fov_3
2850,2850,"In fact, there is really not distinct separation between the summary and the body of the report.",iwAC32Fov_3
2851,2851,This short submission describes the author's attempts to reproduce Yue et al,iwAC32Fov_3
2852,2852,"(2020)'s ""Interventional Few-Shot Learning"" results",iwAC32Fov_3
2853,2853,It focuses on one dataset (miniImageNet) and two methods (meta-transfer learning and simulated information bottleneck) that the original paper proposes can be augmented with the interventional strategy (IFSL),iwAC32Fov_3
2854,2854,"Using author-provided code, this submission reports that including IFSL does improve the model's performance, though not to the same extent reported in the original paper",iwAC32Fov_3
2855,2855,A baseline discrepancy on the meta-transfer learning experiment was also observed.,iwAC32Fov_3
2856,2856,"Unfortunately, the submission includes very few details--it is essentially just the reproducibility summary--which limits the potential value of this work",iwAC32Fov_3
2857,2857,"The author obtained the best results using Yue et al.'s  hyperparameters; for all runs ""the results were much worse"" when these parameters were optimized by the author",iwAC32Fov_3
2858,2858,"However, the significance of this claim is tough to assess because no methodological details or quantitative results were reported in this submission",iwAC32Fov_3
2859,2859,"The only results are reported in Table 1 and 2 and these are unclear: over how many runs was the average taken? What is indicated after the ± (SD, SE, etc)? ",iwAC32Fov_3
2860,2860,"The manuscript does not include any exploration or explanation of the results reported here, nor is the original method tested outside of the original framework",iwAC32Fov_3
2861,2861,"References to the broader literature (and the original paper!) are missing, and the writing could be more polished.",iwAC32Fov_3
2862,2862,"I regret that I cannot be more positive, but I do not believe the work is ready for publication",iwAC32Fov_3
2863,2863,Thank you for this excellent work!  ,iwAC32Fov_3
2864,2864,The authors clearly explain the Scope of Reproducibility.,iwAC32Fov_3
2865,2865,The authors successfully conducted experiments to reproduce the paper,iwAC32Fov_3
2866,2866,I would love to know if they use the same batch-size as the original paper? ,iwAC32Fov_3
2867,2867,I like that the authors conducted experiments on two different settings (MTL and SIB),iwAC32Fov_3
2868,2868,"In both experiments, I can see that the proposed approach improves baselines.",iwAC32Fov_3
2869,2869,Issues regarding the report: ,iwAC32Fov_3
2870,2870,It would be better if authors can tell why there is a large gap between their numbers and original paper numbers for the MTL experiment.,iwAC32Fov_3
2871,2871,It would be better if authors report experiment results with different hyperparameters,iwAC32Fov_3
2872,2872,Authors should try contacting the original paper author via their emails,iwAC32Fov_3
2873,2873,"Overall, this is a nice work to reproduce the original paper and I recommend it for publication",iwAC32Fov_3
2874,2874,Limited scope,XLn0zjYiRQj
2875,2875,--------------------,XLn0zjYiRQj
2876,2876,"The authors clearly describe the scope and adhere to it, however, the scope is extremely limited:",XLn0zjYiRQj
2877,2877,"- one dataset only (CIFAR-10), when the original article used 2 other small-scale ones (MNIST and CIFAR-100) and a larger-scale one (mini-ILSVRC12)",XLn0zjYiRQj
2878,2878,"- one loss combination (NCE+RCE), when the original article tested 3 other ones extensively as well as 5 baselines, and has additional results for 2 other variants",XLn0zjYiRQj
2879,2879,"- one noise setting (symmetric with a noise rate of 0.4) when the original article reports most results on 9 (1 clean, 4 symmetric, 4 asymmetric)",XLn0zjYiRQj
2880,2880,"The authors also did not justify why they chose that specific combination, out of the 48 bolded ones in original tables 2 and 3 (among 972 entries when including baselines and ""clean"" experiments).",XLn0zjYiRQj
2881,2881,"The only source of variation envisioned in the experimental setup was running the code on different hardware and environments: local CPU, Google Colab, and AWS GPU, all using the same software.",XLn0zjYiRQj
2882,2882,Limited effort,XLn0zjYiRQj
2883,2883,-------------------,XLn0zjYiRQj
2884,2884,"Of the different hardware settings, only one result was reported, and there was no detailed analysis of failures, or mention of mitigation strategies",XLn0zjYiRQj
2885,2885,For instance:,XLn0zjYiRQj
2886,2886,"- ""the code failed to run over certain epochs in CPU machine"": in what way? Could it have worked on a smaller dataset (e.g., MNIST), or maybe with a smaller batch size?",XLn0zjYiRQj
2887,2887,"- the runtime exceeded the 12 hours limit on Colab: maybe the authors could have implemented a checkpointing strategy, and restarted training a few time.",XLn0zjYiRQj
2888,2888,"In addition, the report contains:",XLn0zjYiRQj
2889,2889,"- no summarization or explanation of the original paper, methods, or experimental setting",XLn0zjYiRQj
2890,2890,"- no ablation studies: only one model was retrained as described, the baselines were not retrained",XLn0zjYiRQj
2891,2891,- no hyperparameter search: only the values found in the code were used,XLn0zjYiRQj
2892,2892,"- one single run with a given random seed, even though the original article reported mean±std over 3 runs for most results",XLn0zjYiRQj
2893,2893,Confusing reporting of results,XLn0zjYiRQj
2894,2894,------------------------------------------,XLn0zjYiRQj
2895,2895,"The symmetric noise setting with a parameter of $\eta = 0.4$ is characterized as ""noisy labels of 4%"" when the original article indicate it would be 40% of corruption ($1 - \eta$ or 60% of labels would stay the same).",XLn0zjYiRQj
2896,2896,"The original result is reported as ""86.02%"", omitting the standard deviation of ""± 0.09"".",XLn0zjYiRQj
2897,2897,"The reproduced result of 85.97% is reported as ""within 1%"" of the original one, which is technically correct as $0.05 < 1$, but confusing",XLn0zjYiRQj
2898,2898,Why mention a 1% threshold? Was it determined in advance?,XLn0zjYiRQj
2899,2899,Conclusion,XLn0zjYiRQj
2900,2900,----------------,XLn0zjYiRQj
2901,2901,"In the end, this report does not shed any additional light on the original paper or code base, except that the code runs and did not produce an unexpected result.",XLn0zjYiRQj
2902,2902,"The [task description](https://paperswithcode.com/rc2020/task) for the challenge mentions: _""Just re-running code is not a reproducibility study""_, and I don't think this reports clears the bar, I recommend to reject it.",XLn0zjYiRQj
2903,2903,Issues regarding the report: ,XLn0zjYiRQj
2904,2904,Authors test the code only on the CIFAR-10 with 0.4 Symmetric Noise Rate,XLn0zjYiRQj
2905,2905,I believe it would be better to show at least one more experiment (maybe with 0.8),XLn0zjYiRQj
2906,2906,Authors need to give a better explanation of how hyperparameters are selected,XLn0zjYiRQj
2907,2907," Further hyperparameter search can be useful, authors can change model optimizer and learning.",XLn0zjYiRQj
2908,2908,"Another important point, there is no information regarding the stopping criteria?",XLn0zjYiRQj
2909,2909,It will be better to plots how training loss changes during training,XLn0zjYiRQj
2910,2910," It can be also better how to test accuracy change during training,",XLn0zjYiRQj
2911,2911,If possible authors can train and test one of the competing models as well,XLn0zjYiRQj
2912,2912,We can see how improvements happen,XLn0zjYiRQj
2913,2913, Maybe improvement related to the optimizer or something else?,XLn0zjYiRQj
2914,2914,"The author(s) provide very details in their report regarding the proposed algorithm and data set (ZINC) details, the report is 17 pages which I believe is too long",ntxHNb7y429
2915,2915,The model description is well defined,ntxHNb7y429
2916,2916,"They provide every detail of the code and algorithms that have been used by other papers and provide the citation, but I believe it could be abstracted",ntxHNb7y429
2917,2917,The evaluation criteria; logP score were used to evaluate the optimization.,ntxHNb7y429
2918,2918,"In the very early sentences of your review paper, In line 4, ""an adaptive, neural network-based, penalty that is supposed to"" it's better to put a comma, right after the ***penalty***, ""an adaptive, neural network-based penalty, that is supposed to."" The link you prepared in line 69 as your GitHub repository, did not work properly.",ntxHNb7y429
2919,2919,"In my opinion, it was better to test your algorithm with other optimizers rather than Adam, and other Loss functions except for binary cross-entropy",ntxHNb7y429
2920,2920,It helps you obtain more accurate results by searching in different conditions.,ntxHNb7y429
2921,2921,"In line 94, you made a typo and wrote ***pratical*** instead of ***practical***.",ntxHNb7y429
2922,2922,What is the y-axis in Fig1 plots?! What does it refer to?,ntxHNb7y429
2923,2923,"You'd better add Legend to each of your images, even if you're partializing them to a) b) c).",ntxHNb7y429
2924,2924,"Overall, my evaluation of the paper titled “A reproducibility study of “Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space” is as below:",ntxHNb7y429
2925,2925,"•	The authors provide a comprehensive summary of the objectives of the paper, its results, challenges, and contributions",ntxHNb7y429
2926,2926,(Reproducibility Summary: 10/10) ,ntxHNb7y429
2927,2927,•	The authors provide the areas of the original paper where they have tried to reproduce the results and adhere to them along with the paper,ntxHNb7y429
2928,2928,(Scope of reproducibility: 10/10),ntxHNb7y429
2929,2929,•	The code from the original paper has been used and modified for the hyperparameter search performed on the paper,ntxHNb7y429
2930,2930,"Overall, there are enough documentation, and the code sounds clean",ntxHNb7y429
2931,2931,(Code: 10/10) ,ntxHNb7y429
2932,2932,•	Scope of the communication with the authors of the original paper is mentioned (Communication with original authors: 10/10),ntxHNb7y429
2933,2933,•	Effect of different hyperparameters that change the results has been studied in detail which complements the results of the original paper,ntxHNb7y429
2934,2934,(Hyperparameter Search: 10/10)             ,ntxHNb7y429
2935,2935,"•	Although the authors have performed comprehensive hyperparameters search and have quantified the effect on the result, the level of novelty in the paper is limited",ntxHNb7y429
2936,2936,Maybe the authors could perform some study on the neural network-based penalty and see if they can add additional improvement to the original paper,ntxHNb7y429
2937,2937,(Ablation Study                                           6/10),ntxHNb7y429
2938,2938,•	There are detailed discussions with graphs on the effect of each hyperparameter on the outcome,ntxHNb7y429
2939,2939,The authors have been very meticulous,ntxHNb7y429
2940,2940,Have examined the method in detail and have suggested valuable improvement to the original paper (Discussion on results 10/10)   ,ntxHNb7y429
2941,2941,"•	The detailed illustration of the results is very helpful, but the authors do not provide any recommendation on future work for reproducibility",ntxHNb7y429
2942,2942,They mainly summarized what they have tried and the differences in the outcome with respect to the original paper,ntxHNb7y429
2943,2943, (Recommendations for reproducibility 5/10)        ,ntxHNb7y429
2944,2944,•	I did not recognize significant novel contributions from this paper,ntxHNb7y429
2945,2945,(Results beyond the paper                          5/10),ntxHNb7y429
2946,2946,"•	The paper is organized, coherent, clear, and well structured",ntxHNb7y429
2947,2947,(Overall organization and clarity 10/10),ntxHNb7y429
2948,2948,        ,ntxHNb7y429
2949,2949,I would like to congratulate the authors on their commendable effort in preparing this reproducibility report,vvLWTXkJ2Zv
2950,2950,I believe the authors' effort will significantly contribute to the improvement of the reproducibility of this work,vvLWTXkJ2Zv
2951,2951,I have presented a more detailed evaluation below.,vvLWTXkJ2Zv
2952,2952,__Pros:__,vvLWTXkJ2Zv
2953,2953,1,vvLWTXkJ2Zv
2954,2954,"The authors did a great job in presenting the reproducibility summary, especially the easy and hard parts of their reproducibility effort.",vvLWTXkJ2Zv
2955,2955,2,vvLWTXkJ2Zv
2956,2956,The introduction and scope of reproducibility are clearly presented and nicely setup the rest of the report,vvLWTXkJ2Zv
2957,2957,I would also like to appreciate the authors' effort to present the mathematical foundations of the method,vvLWTXkJ2Zv
2958,2958,"However, I feel the grammar and writing style could be improved for better readability",vvLWTXkJ2Zv
2959,2959,3,vvLWTXkJ2Zv
2960,2960,The authors built up on the original code base which only had the codes for the Prediction task with memory,vvLWTXkJ2Zv
2961,2961,I believe this is a significant achievement and hope that this experience was highly educational and rewarding for the authors,vvLWTXkJ2Zv
2962,2962,"However, the original code repo has now been updated",vvLWTXkJ2Zv
2963,2963,"So, it would be helpful if the authors could comment on discrepancies (if any) in their implementation and the one provided by the original authors in their final version",vvLWTXkJ2Zv
2964,2964,4,vvLWTXkJ2Zv
2965,2965,The authors mentioned performing a hyperparameter search and reported finding different set of hyperparameters than originally reported in the paper,vvLWTXkJ2Zv
2966,2966,Having a plot of the performance variance for different hyperparameters (in the supplementary) could add value to this effort.,vvLWTXkJ2Zv
2967,2967,5,vvLWTXkJ2Zv
2968,2968,I would like to appreciate the several ablation studies performed by the authors to evaluate various components of the proposed methodology,vvLWTXkJ2Zv
2969,2969,The inferences from these studies though could be presented better,vvLWTXkJ2Zv
2970,2970,6,vvLWTXkJ2Zv
2971,2971,The authors performed useful additional experiments beyond the ones performed in the original paper,vvLWTXkJ2Zv
2972,2972,These experiments definitely add to my understanding of the method,vvLWTXkJ2Zv
2973,2973,"Although I could follow the results of these experiments, I was slightly confused about the inferences presented by the authors",vvLWTXkJ2Zv
2974,2974,"However, since this is a reproducibility report, I gave more weightage to the rationale behind the experiments rather than their interpretations.",vvLWTXkJ2Zv
2975,2975,7,vvLWTXkJ2Zv
2976,2976,The authors allude to some reproducibility recommendations but it's not very explicit,vvLWTXkJ2Zv
2977,2977,I think these recommendations are extremely useful and it would help to have them clearly mentioned,vvLWTXkJ2Zv
2978,2978,"Despite the several merits of the report, I have few concerns primarily pertaining to the presentation and writing/organization style of the report",vvLWTXkJ2Zv
2979,2979,I have detailed them below.,vvLWTXkJ2Zv
2980,2980,__Cons:__,vvLWTXkJ2Zv
2981,2981,1,vvLWTXkJ2Zv
2982,2982,It would help to have a figure/flowchart that outlines the several experiments that the authors tried to replicate and/or investigate in detail with additional experiments,vvLWTXkJ2Zv
2983,2983,I would strongly suggest adding this figure in the introduction or scope section,vvLWTXkJ2Zv
2984,2984,This figure would establish the context and methodology of the original paper as well as give the reader an overview of the rest of the report,vvLWTXkJ2Zv
2985,2985,2,vvLWTXkJ2Zv
2986,2986,"With the original code repo being updated recently, the authors may have to briefly comment about the discrepancies (if any) in their implementation",vvLWTXkJ2Zv
2987,2987,"Additionally, they need to update the report to incorporate the information that the original repo now contains the codes to both tasks",vvLWTXkJ2Zv
2988,2988,I don't think this needs to be very detailed but updating this information will add to the completeness of the report,vvLWTXkJ2Zv
2989,2989,3,vvLWTXkJ2Zv
2990,2990,"Although the authors report that they communicated with the original authors, it is slightly unclear as to how this communication served to improve the authors' understanding and/or implementation of the methodology",vvLWTXkJ2Zv
2991,2991,"Also, I am curious to know if the authors communicated the observed discrepancy about the ShanghaiTech dataset and the engineering fix they had to deploy",vvLWTXkJ2Zv
2992,2992,4,vvLWTXkJ2Zv
2993,2993,It would help to have a plot of the performance variation with changing hyperparameters,vvLWTXkJ2Zv
2994,2994,This result would indicate how sensitive the results are to hyperparameter values,vvLWTXkJ2Zv
2995,2995,"Also, did the authors try different seeds? I understand this could be difficult to do on complicated dataset but it is an essential component to establish that one method statistically outperforms another method",vvLWTXkJ2Zv
2996,2996,"As a recommendation, I would propose the authors to run their ""light weight"" experiments on multiple seeds and report the standard deviation in metrics",vvLWTXkJ2Zv
2997,2997,5,vvLWTXkJ2Zv
2998,2998,I think the overall presentation style needs to be improved,vvLWTXkJ2Zv
2999,2999,The current report has very interesting experiments and excellent insights,vvLWTXkJ2Zv
3000,3000,These insights need to highlighted properly,vvLWTXkJ2Zv
3001,3001,"For instance, it will be helpful to have the results reported by the original authors alongside the results obtained in this report to clearly compare and contrast the discrepancies or agreements",vvLWTXkJ2Zv
3002,3002,"Furthermore, it would help improve the readability if the authors could add a flowchart or table that summarized their findings in terms of which modules turned out to be redundant for each of the tasks",vvLWTXkJ2Zv
3003,3003,Having such a figure or table will significantly improve the impact of their findings,vvLWTXkJ2Zv
3004,3004,6,vvLWTXkJ2Zv
3005,3005,The discussions and implications presented by the authors don't seem very convincing,vvLWTXkJ2Zv
3006,3006,"For instance, the authors mention _""Figure 9 shows both models having trouble reconstructing the input frames, but the Prediction model reconstructs the image with considerably more artifacts making it easier to classify correctly.""_ It is hard to conclude this without definite reconstruction error (MSE, perhaps?) values",vvLWTXkJ2Zv
3007,3007,7,vvLWTXkJ2Zv
3008,3008,"Furthermore, did the authors try an epsilon-greedy type of approach to mitigate the problem of mode collapse in memory module? This would entail using the memory module as proposed with probability of (1-epsilon) and using a random combination of all memory items with epsilon probability",vvLWTXkJ2Zv
3009,3009,"The value of epsilon could become another hyperparameter, but setting it to some constant value (eg",vvLWTXkJ2Zv
3010,3010,0.1) could perhaps serve as an acceptable solution,vvLWTXkJ2Zv
3011,3011,"However, this would be a minor comment and mostly out of my own curiosity",vvLWTXkJ2Zv
3012,3012,I have not used this concern to judge the merit of the reproducibility report,vvLWTXkJ2Zv
3013,3013,8,vvLWTXkJ2Zv
3014,3014,"Finally, I would encourage the authors to add clear reproducibility recommendations",vvLWTXkJ2Zv
3015,3015,I believe the authors are at an appropriate stage to provide specific recommendations for this work and a good reproducibility report should have them.,vvLWTXkJ2Zv
3016,3016,"Once again, I would like to congratulate the authors on their work and I hope they can address some of the concerns mentioned above to improve the impact of this report",vvLWTXkJ2Zv
3017,3017,"I believe that if the authors could add the aforementioned summary figures and/or tables and improve the overall readability of the final version of the report, this work could be extremely useful to the field in general",vvLWTXkJ2Zv
3018,3018,The authors reproduced the work of Learning Memory-guided Normality for Anomaly Detection,vvLWTXkJ2Zv
3019,3019,"In this work, the authors have:",vvLWTXkJ2Zv
3020,3020,- Reused part of the original code and scripted some parts that were not available,vvLWTXkJ2Zv
3021,3021,- Have communicated with the authors regarding the changes,vvLWTXkJ2Zv
3022,3022,- Did a thorough ablation study,vvLWTXkJ2Zv
3023,3023,"And finally, went beyond the scope of the reproduction and suggested further improvements.",vvLWTXkJ2Zv
3024,3024,In the reviewer's opinion it is a thoroughly conducted study.,vvLWTXkJ2Zv
3025,3025,Small typos: Space missing line 45,vvLWTXkJ2Zv
3026,3026,"This paper tries to reproduce the ICLR2019 paper ""Learning to learn without forgetting by maximizing transfer and minimizing interference,"" by Matthew Riemer et al.",d0svLMnvzWK
3027,3027,The authors present an straightforward 1to1 reproduction of the original paper experimental setting,d0svLMnvzWK
3028,3028,They ran the code according to the instructions given in the official implementation and got very similar results,d0svLMnvzWK
3029,3029,This is OK for the the ML Reproducibility Challenge,d0svLMnvzWK
3030,3030,"However, the authors failed to provide meaningful results/insights for a majority of the analysis performed",d0svLMnvzWK
3031,3031,"In this regard, many components of the proposed results, ablation and running times studies in the manuscript are incomplete or limited, without offering clear descriptions and/or insight into the outcomes.",d0svLMnvzWK
3032,3032,Itemised comments:,d0svLMnvzWK
3033,3033,"- Introduction is too short (seems a bullet list) and non-informative about the problem addressed, its importance, motivation, etc",d0svLMnvzWK
3034,3034,"Also, La-MAML should be better introduced and described.",d0svLMnvzWK
3035,3035,- Repetitiveness: Methodology and Computational requirements sections are mostly the same,d0svLMnvzWK
3036,3036,Also Section 5.2.,d0svLMnvzWK
3037,3037,- Pseudocode in Section 3.3 shouldn't be a low-resolution figure.,d0svLMnvzWK
3038,3038,- Tables 3 and 5 shouldn't be low-resolution figures,d0svLMnvzWK
3039,3039,Lack of consistency wrt other tables.,d0svLMnvzWK
3040,3040,- Sections 4.1.3 and 4.1.4 placed after section 5.,d0svLMnvzWK
3041,3041,- Ablation analyses are not explained,d0svLMnvzWK
3042,3042,The reader cannot get any useful insight from section 5,d0svLMnvzWK
3043,3043,- Results and additional results are vaguely described,d0svLMnvzWK
3044,3044,This makes it very hard for the reader to follow their interpretation.,d0svLMnvzWK
3045,3045, ,d0svLMnvzWK
3046,3046,"This report repeats a large number of experiments from the original article, using the original code, confirming the claims in scope",d0svLMnvzWK
3047,3047,"The authors spent time and ingenuity adapting and packaging the code to run on 4 different environments, to satisfy runtime and memory requirements of different experiments",d0svLMnvzWK
3048,3048,"However, repeating the original experiments does not shed much light on the original paper, and this work does feature re-implementation, new experiments, ablation studies, or hyper-parameter re-optimization",d0svLMnvzWK
3049,3049,The small discrepancies that appear in some figures are not discussed.,d0svLMnvzWK
3050,3050,"As the [task description](https://paperswithcode.com/rc2020/task) of the challenge says, ""Just re-running code is not a reproducibility study"", and this submission does not provide additional insight on the original code or paper.",d0svLMnvzWK
3051,3051,Reproducibility summary,d0svLMnvzWK
3052,3052,--------------------------------------,d0svLMnvzWK
3053,3053,"The summary is filled up correctly, and summarizes well the report, although there are inconsistencies:",d0svLMnvzWK
3054,3054,"- the ""Scope of Reproducibility"" paragraph mentions only 1 claim, when Section 2 mentions 3, though they broadly overlap",d0svLMnvzWK
3055,3055,"- the ""Methodology"" paragraph only mentions one runtime environment (Google Colab with T4 GPUs), when Section 3 (Methodology), especially 3.6 (Computational requirements), expresses the need for using 4 different systems and says most experiments were using a different one (Kaggle Notebook with P100 GPU).",d0svLMnvzWK
3056,3056,Scope of reproducibility,d0svLMnvzWK
3057,3057,---------------------------------,d0svLMnvzWK
3058,3058,"The claims are clearly stated, and reproduced results connected to them",d0svLMnvzWK
3059,3059,"There is not much justification of how the results support the claims, except for the explanations in the original article.",d0svLMnvzWK
3060,3060,Code and experiments,d0svLMnvzWK
3061,3061,----------------------------------,d0svLMnvzWK
3062,3062,The authors re-used code and hyper-parameters from the original repository.,d0svLMnvzWK
3063,3063,No additional hyper-parameter search was reported.,d0svLMnvzWK
3064,3064,No additional ablation studies were reported.,d0svLMnvzWK
3065,3065,"The report provides timing information for all experiments, and discusses speed differences and trade-offs of different systems",d0svLMnvzWK
3066,3066,"It could have been more clear which reported durations were obtained in which setting (Table 6 tags some results with ""\*\*"", and Section 3.6 suggests it may be ""3",d0svLMnvzWK
3067,3067,"Institute GPU"", but it's not mentioned in the caption).",d0svLMnvzWK
3068,3068,Discussion on results,d0svLMnvzWK
3069,3069,------------------------------,d0svLMnvzWK
3070,3070,"The submission reports accuracies ""within 4%"" of the original values, but does not elaborate on which experiments were further off from the originals, or how that ""4%"" (presumably 4 percentage points) was computed.",d0svLMnvzWK
3071,3071,In fact:,d0svLMnvzWK
3072,3072,"- is not so meaningful when reported values for BTI can be ""-2.73 ± 0.45"", and",d0svLMnvzWK
3073,3073,"- In Table 3/4, the RA metric for ""Sync"" on ""Permutations"" went from ""70.54 ± 1.54"" to ""59.3 ± 2.3""",d0svLMnvzWK
3074,3074,"Most of the results do seem to line up, and consistent with the provided confidence intervals, and there are reasonable explanations (one bad run can become likely when running many experiments, the original authors might have had a lucky random seed, this might go away when re-tuning hyper-parameters...), but this was not acknowledged or discussed in any way.",d0svLMnvzWK
3075,3075,"Similarly, Figure 3 shows a similar effect of the learning rate value as Figure 2 (original), but the whole graph is 5 to 10 % lower in absolute accuracy, without it being acknowledged or explained.",d0svLMnvzWK
3076,3076,The paper is poorly written with an open-ended conclusion and the results are not exhaustive,7NDziwkaDm6
3077,3077,"Also, there is a lot of ambiguity around reproducibility since the author found a bug in their code",7NDziwkaDm6
3078,3078,They state to provide the full results after resolution.,7NDziwkaDm6
3079,3079,"The work of RetinaFace is reproduced here, to address the long standing problem of face detection, and evaluated on the dataset of WIDER",7NDziwkaDm6
3080,3080,"The reproduced method achieves on average 11%-18% worse than the original reported results, which seems to due to a bug in the reproducing effort?? This important aspect is not clear",7NDziwkaDm6
3081,3081,  The bright side is this work comes with detailed implementation description.,7NDziwkaDm6
3082,3082,The submission implemented RetinaFace using Knet framework,7NDziwkaDm6
3083,3083,It contains most of the features from the original paper,7NDziwkaDm6
3084,3084,"In particular, the author conducted some ablation studies focusing on landmark detection, context module, and cascaded structure",7NDziwkaDm6
3085,3085,The results are discussed in detail in the report,7NDziwkaDm6
3086,3086,"The strength of the submission is its solid, from-scratch Julia implementation",7NDziwkaDm6
3087,3087,Julia has gained lots of popularity in the data science community,7NDziwkaDm6
3088,3088,But there are relatively fewer off-the-shelf models for state-of-the-art computer vision models,7NDziwkaDm6
3089,3089,The submission is a good example that can be referenced by people who want to study implementation object detectors in Julia.,7NDziwkaDm6
3090,3090,The performance is lower than the numbers reported in the original paper,7NDziwkaDm6
3091,3091,The author attributed this to a bug in the Knet framework,7NDziwkaDm6
3092,3092,The author observed some progress in training after the bug is fixed but could not finish the training in time,7NDziwkaDm6
3093,3093,"Due to this reason, I feel reluctant to give higher scores, but at the same time encourage to have this submission accepted due to its language & framework choices and in-depth discussion about the experiments.",7NDziwkaDm6
3094,3094,"The proposed paper reproduces the result of ""Fourier Domain Adaptation for Semantic Segmentation (CVPR 2020)"", by primarily utilizing the code released by the original authors",MBIIiRE0EXv
3095,3095,The paper has clearly outlined the details of the algorithm and also compared the results with those presented by the original authors,MBIIiRE0EXv
3096,3096,"However, the following points might improve the quality of the work",MBIIiRE0EXv
3097,3097,1) There seems to be a difference between the result obtained in the paper with that presented by the original author,MBIIiRE0EXv
3098,3098,A detailed discussion of the possible reason for this difference is expected,MBIIiRE0EXv
3099,3099,"2) Although, we understand that it might be difficult to secure access to computing resources",MBIIiRE0EXv
3100,3100,"Nevertheless, it is expected to perform a thorough hyperparameter search",MBIIiRE0EXv
3101,3101,The paper is well structured and easy to follow,MBIIiRE0EXv
3102,3102,The authors provide a good summary with well defined scope of reproducibility,MBIIiRE0EXv
3103,3103,"However, the authors could not verify claims on Synthia dataset but instead try to debrief the experimental setup and overall method",MBIIiRE0EXv
3104,3104,"Overall, the authors are able to evaluate approach on GTA5 dataset along with providing computation details for training time",MBIIiRE0EXv
3105,3105, The authors perform experiments with multiple backbone and are able to reproduce within decent error rate,MBIIiRE0EXv
3106,3106,The limitation is authors do not show any qualitative evaluation for comparison with paper which is crucial for semantic segmentation analysis,MBIIiRE0EXv
3107,3107,Effect of the size of the domain β is not analyzed qualitatively,MBIIiRE0EXv
3108,3108,Authors provide detailed experimental setup along with summary of code flow which helps in navigating the source code,MBIIiRE0EXv
3109,3109, The authors had communicated with original authors to resolve queries,MBIIiRE0EXv
3110,3110,They do not provide any additional/extra experiments,MBIIiRE0EXv
3111,3111,It is not clear in sec 4.3.1 authors mention 'we could not find the author’s approach to generate pseudo labels in the paper' but in sec 4.3.2 they claim to improve the original pseudo label approach ? Section 4.3.3 may not be relevant here.,MBIIiRE0EXv
3112,3112,The authors have provided a summary of their experiments regarding the reproducibility of the paper - FDA: Fourier Domain Adaptation for Semantic Segmentation which was published in the CVPR 2020,MBIIiRE0EXv
3113,3113,The central claim of the original paper is that a simple Fourier transform can be used to achieve state of the art performance when tested on the semantic segmentation task,MBIIiRE0EXv
3114,3114,The authors of this reproducibility report were able to reproduce most of the results from the original paper (except on one dataset) and they optimized the original paper’s code to enable an easier loading of the model weights.,MBIIiRE0EXv
3115,3115,Pros:,MBIIiRE0EXv
3116,3116,Including a codeflow in the paper is very nice,MBIIiRE0EXv
3117,3117,It tries to show how the overall structure of the codebase is to make it easier for anyone trying to implement and reuse code.,MBIIiRE0EXv
3118,3118,Typos et al for improvements:,MBIIiRE0EXv
3119,3119,The authors can definitely benefit from cleaning their document and making sure tables and references are hyperlinked appropriately,MBIIiRE0EXv
3120,3120,Some of these typos and pointers are mentioned below:,MBIIiRE0EXv
3121,3121,- Line 43: (include a space at new line start) … methods,MBIIiRE0EXv
3122,3122,In the …,MBIIiRE0EXv
3123,3123,"- Hyperlinks in some equations are missing - for example, in Eq1 (line 64) and Eq 3 (line 68-69) ",MBIIiRE0EXv
3124,3124,Authors might include those as it helps in easier readability and navigation.,MBIIiRE0EXv
3125,3125,- Some of the citations are missing / wrong,MBIIiRE0EXv
3126,3126,"For example, citation numbered [13] in Line 65 does not lead to anywhere",MBIIiRE0EXv
3127,3127,There are only 10 citations under references,MBIIiRE0EXv
3128,3128,"- Hyperlinks for tables - for example, for Table 3, 4, 5, 6",MBIIiRE0EXv
3129,3129,- Table 1 and 2 are not linked anywhere (though they appear in the vicinity of the related discussion),MBIIiRE0EXv
3130,3130,- Typo: varified -> verified (line 88),MBIIiRE0EXv
3131,3131,- table 6 -> Table 6 (line 120),MBIIiRE0EXv
3132,3132,- However in the last round -> However in the last round (line 100),MBIIiRE0EXv
3133,3133,- MBT mIoU,MBIIiRE0EXv
3134,3134,-> MBT mIoU,MBIIiRE0EXv
3135,3135,(Line 101),MBIIiRE0EXv
3136,3136,- training of the mordels -> training of the models (line 110),MBIIiRE0EXv
3137,3137,- via mail -> via email (line 157) —> Unless this was really done via snail mail,MBIIiRE0EXv
3138,3138,"The claims listed under the ""Scope of reproducibility"" section can be improved to reflect what the authors of the reproducibility report did and what to expect in the rest of the document.",MBIIiRE0EXv
3139,3139,"The ""Method descriptions"" section can be improved",MBIIiRE0EXv
3140,3140,The equations and used notation is not very clear,MBIIiRE0EXv
3141,3141,"If this section can provide a good overview of the methods which is self-sufficient to understand, it will be helpful in better readability.",MBIIiRE0EXv
3142,3142,The authors can add some additional ablation studies to dig into and understand better the ideas from the original paper.,MBIIiRE0EXv
3143,3143,"Overall, by addressing these issues and cleaning the document, the authors can improve their writing significantly and can be taken into consideration while making the decision.",MBIIiRE0EXv
3144,3144,# Reproducibility Summary,Or5sv1Pj6od
3145,3145,Yes it is provided.,Or5sv1Pj6od
3146,3146,# Scope of Reproducibility,Or5sv1Pj6od
3147,3147,Train ELECTRA with a similar dataset (OpenWebText) and evaluate on GLUE,Or5sv1Pj6od
3148,3148,"Done a single GPU, this should reflect the claim of the ELECTRA paper of its effectiveness and relatively low computational cost to baselines such as BERT.",Or5sv1Pj6od
3149,3149,# Communication,Or5sv1Pj6od
3150,3150,The reproduction authors communicated with one of the original authors (partially through github) to answer most questions.,Or5sv1Pj6od
3151,3151,# Hyperparameter Search,Or5sv1Pj6od
3152,3152,The authors perform thorough hyperparameter search and it is reported clearly in the paper.,Or5sv1Pj6od
3153,3153,# Ablation Study,Or5sv1Pj6od
3154,3154,The authors perform some useful ablation with respect to discriminator and generator sizes,Or5sv1Pj6od
3155,3155,I think there is much to explore here such as other techniques that are used in GANs (e.g,Or5sv1Pj6od
3156,3156,different training schedules for the discriminator and generator).,Or5sv1Pj6od
3157,3157,# Discussion,Or5sv1Pj6od
3158,3158,It's clear from the discussion that ELECTRA is easy to reproduce,Or5sv1Pj6od
3159,3159,This seems attributed to the nature of the work — the ability to train on a single GPU — and the clarity and documentation associated with the original paper.,Or5sv1Pj6od
3160,3160,# Recommendations,Or5sv1Pj6od
3161,3161,"No clear recommendation is made to the original authors, although the ablation experiments about generator size could be relevant.",Or5sv1Pj6od
3162,3162,# Results beyond the paper,Or5sv1Pj6od
3163,3163,The experiments and results are primarily in the spirit of the original work.,Or5sv1Pj6od
3164,3164,# Overall Organization and Clarity,Or5sv1Pj6od
3165,3165,The paper was clear and easy to follow,Or5sv1Pj6od
3166,3166,"That being said, if needing to save space then some figures and tables could move to the appendix with only the main results in the main text.",Or5sv1Pj6od
3167,3167,The reproducibility report shows ELECTRA can be reproduced quite well,Or5sv1Pj6od
3168,3168,The report is detailed and coherent,Or5sv1Pj6od
3169,3169,An additional insight is provided which is “sensitive to capacity allocation between generator and discriminator” by the authors,Or5sv1Pj6od
3170,3170,"Finally, figure 3 is great.",Or5sv1Pj6od
3171,3171,The code had to be shared with the reviewers.,Or5sv1Pj6od
3172,3172,There are some adjustments in this re-implementation,Or5sv1Pj6od
3173,3173,This is both a risk to the reproducibility as the difference increases and a strength to demonstrate the core idea of ELECTRA works.,Or5sv1Pj6od
3174,3174,“can also influenced the” -> can also influence the,Or5sv1Pj6od
3175,3175,This was a clearly written paper reproducing the original ELECTRA paper,Or5sv1Pj6od
3176,3176,The replication study authors do a great job of discussing what was difficult and easy to replication about the original paper (including the code that was released with the paper),Or5sv1Pj6od
3177,3177,I enjoyed reading it and thought they did an excellent job!,Or5sv1Pj6od
3178,3178,*Scope of reproducibility:*,Or5sv1Pj6od
3179,3179,This paper reproduces the GLUE scores from ELECTRA in PyTorch and on a single GPU,Or5sv1Pj6od
3180,3180,*Code:*,Or5sv1Pj6od
3181,3181,The paper authors reimplemented ELECTRA in PyTorch (originally in TensorFlow),Or5sv1Pj6od
3182,3182,"The authors state they will release the code, but I did not see it",Or5sv1Pj6od
3183,3183,*Communication with original authors:*,Or5sv1Pj6od
3184,3184,"It appears the replication study authors communicated with the paper authors to clear up minor details, but that the majority of questions were answered by the Github repo or the code",Or5sv1Pj6od
3185,3185,*Hyperparameter Search:*,Or5sv1Pj6od
3186,3186,Neither the replication study nor the original paper used a hyperparameter search.,Or5sv1Pj6od
3187,3187,*Ablation Study:*,Or5sv1Pj6od
3188,3188,I don't believe the replication study performed any ablations,Or5sv1Pj6od
3189,3189,I was surprised to see such similar results for ELECTRA even though the replication study did not use the WikiBooks dataset (and only used OpenWebText).,Or5sv1Pj6od
3190,3190,*Discussion on results:*,Or5sv1Pj6od
3191,3191,The replication study presented an excellent description of the reproducibility of the original paper and made clear when the results reproduced and did not reproduce,Or5sv1Pj6od
3192,3192,"They clearly stated that some details were ambiguous, but that they were mostly able to resolve those details",Or5sv1Pj6od
3193,3193,The scores in the original implementation and in the new implementation mostly match,Or5sv1Pj6od
3194,3194,*Recommendations for reproducibility:*,Or5sv1Pj6od
3195,3195,The republication study pointed out areas where the original paper did a great job of giving enough details for reproducing the work and areas where the original paper could revisit (such as the discrepancy in the WNLI tasks,Or5sv1Pj6od
3196,3196,*Results beyond the paper:*,Or5sv1Pj6od
3197,3197,The replication study aggregated information from several sources to give additional context to the paper (such as the efficiency results),Or5sv1Pj6od
3198,3198,This was great to see,Or5sv1Pj6od
3199,3199,*Overall organization and clarity*,Or5sv1Pj6od
3200,3200,* The replication study authors were very clear about where their implementation differed from the original work and included a table making it very easy to read (table 5),Or5sv1Pj6od
3201,3201,Thank you!,Or5sv1Pj6od
3202,3202,This reproducibility report is relatively easy to follow,0z31bGAM8Hr
3203,3203,Their re-implementation supports the claims in the original paper,0z31bGAM8Hr
3204,3204,"At the time of writing this review (2021, Feb 8), the authors of the original paper released the code",0z31bGAM8Hr
3205,3205,This could make reproducibility easier,0z31bGAM8Hr
3206,3206,"In this paper, the authors discussed their attempt to reproduce the results reported in [1]",0z31bGAM8Hr
3207,3207,Pros:,0z31bGAM8Hr
3208,3208,"1), implemented the method proposed in the original paper from scratch using a different framework, i.e., Tensorflow",0z31bGAM8Hr
3209,3209,Pytorch was used in the original paper.,0z31bGAM8Hr
3210,3210,"2), successfully demonstrate the value of adding the Gaussian filtering layer during curriculum learning comparing to the baseline model",0z31bGAM8Hr
3211,3211,One thing that the authors should make it clear is that whether the baseline model was also trained with curriculum,0z31bGAM8Hr
3212,3212,Cons:,0z31bGAM8Hr
3213,3213,"1), only performed reproducibility study of a small part of the results reported in the original paper.",0z31bGAM8Hr
3214,3214,"2), the reported performance even though demonstrates the benefit of CBS, but does not reach the level reported in the original paper",0z31bGAM8Hr
3215,3215,"3), very limited study of hyperparameter search",0z31bGAM8Hr
3216,3216,"4), no ablation study",0z31bGAM8Hr
3217,3217,"Summary: This is a replication of the ICLR paper ""Your Classifier is Secretly an Energy Based Model and You Should Treat it Like On"" by Grathwohl  et al",ShrPBsjByVa
3218,3218,(2020),ShrPBsjByVa
3219,3219,This replicability-report focuses on studying the results both on accuracy and model calibration of the original paper,ShrPBsjByVa
3220,3220,The report seems to corroborate the results of the Joint Energy-Based Model (JEM) and provides interesting results on the inception scores obtained.,ShrPBsjByVa
3221,3221,"With respect to the pros, this replicability study is clear, well organized, easy to read, and seem to study the paper in great detail considering the limited time",ShrPBsjByVa
3222,3222,"The report also contains all the main expected elements for the Replicability Challenge, including the type of contact the author of the report had with the authors of the original paper, scope, code, computational platform, difference of the hyperparameter in the original paper vs",ShrPBsjByVa
3223,3223,"the replicability study, the implication (discussion) of the results, limitations, and additional insights",ShrPBsjByVa
3224,3224,This report has a clearly delineated summary.,ShrPBsjByVa
3225,3225,"On the other hand, there are a few elements of the report that could have been clarified a little more",ShrPBsjByVa
3226,3226,"For instance, the authors claimed they performed additional experiments with uniform noise and computed its scores - in the words of the authors this evaluation went beyond what was reported in the original ICLR paper",ShrPBsjByVa
3227,3227,"However, the original ICLR paper did consider uniform noise -- see section F.2",ShrPBsjByVa
3228,3228,Clarification on this can improve the report's clarity,ShrPBsjByVa
3229,3229,I am puzzled by the authors' choice to reproduce the results of the paper without looking at their code while only looking at their code when needed,ShrPBsjByVa
3230,3230,Why not just use the provided code and build upon it?,ShrPBsjByVa
3231,3231,"The authors state that the limited computational resources were a bottleneck in their reproduction, which is fair",ShrPBsjByVa
3232,3232,"However, given such a limitation one would expect experiments to be chosen more carefully with a directed purpose, but this does not seem to be the case",ShrPBsjByVa
3233,3233,"For example, Table 2 seems pointless to me",ShrPBsjByVa
3234,3234,The focus of the paper is the joint energy-based models (JEM) and the 95.8% purely discriminator accuracy was presented only to provide a point of reference,ShrPBsjByVa
3235,3235,Why spend time on this?,ShrPBsjByVa
3236,3236,"Also, Table 3 is puzzling",ShrPBsjByVa
3237,3237,"The authors achieve a better inception score, far better than everything else (including purely generative models) listed in Table 1 of the original paper",ShrPBsjByVa
3238,3238,"This would be a surprising result that requires an explanation or a discussion, but the authors do not acknowledge it at all",ShrPBsjByVa
3239,3239,"Well, the inception score with CIFAR10 (10 classes) is by definition <=10, so the reported 19.39 must be an incorrect number",ShrPBsjByVa
3240,3240,In fact Tables 4 and 5 have the same issue.,ShrPBsjByVa
3241,3241,I also do not find the other experiments to provide much new information,ShrPBsjByVa
3242,3242,"For these reasons, I recommend the reproducibility report be rejected.",ShrPBsjByVa
3243,3243,Errata:,ShrPBsjByVa
3244,3244,1,ShrPBsjByVa
3245,3245,"line 52, Due 'to' issues with training time",ShrPBsjByVa
3246,3246,2,ShrPBsjByVa
3247,3247,The reference [3] https://www.cs.toronto.edu/ kriz/cifar.html,ShrPBsjByVa
3248,3248,is not accessible.,ShrPBsjByVa
3249,3249,3,ShrPBsjByVa
3250,3250,"line 101, the right parenthesis of max_y p(y|x_i)",ShrPBsjByVa
3251,3251,4,ShrPBsjByVa
3252,3252,The reference for wide resnet should be included.,ShrPBsjByVa
3253,3253,1,ShrPBsjByVa
3254,3254,The authors clearly state the scope and limitation of the reproduction experiments.,ShrPBsjByVa
3255,3255,2,ShrPBsjByVa
3256,3256,"The results are “reproduced"" using a scaled down architecture (Wide ResNet 28-2 instead of Wide ResNet 28-10)",ShrPBsjByVa
3257,3257,The authors claim that the disparity in reported training time in the original paper and the authors’ own estimate is the reason for scaling down the architecture,ShrPBsjByVa
3258,3258,3,ShrPBsjByVa
3259,3259,Only one aspect of the results presented in the original paper is reproduced,ShrPBsjByVa
3260,3260,"However, the authors present fresh generative samples from noise and show how the generative samples evolve over SGLD steps",ShrPBsjByVa
3261,3261,4,ShrPBsjByVa
3262,3262,The authors have used exactly the same hyperparameters used in the original paper,ShrPBsjByVa
3263,3263,The authors mention that the hyperparameters may not have been optimal for the downgraded architecture used for reproduction of results,ShrPBsjByVa
3264,3264,5,ShrPBsjByVa
3265,3265,"The authors have written the code from scratch (repository made public), but refer to the original code only for a few pre-processing steps that are not clear in the original paper",ShrPBsjByVa
3266,3266,6,ShrPBsjByVa
3267,3267,The authors have communicated with the original authors for clarifications on training instabilities and training time.,ShrPBsjByVa
3268,3268,7,ShrPBsjByVa
3269,3269,Typos:,ShrPBsjByVa
3270,3270,	* Line 101: Missing closing parentheses,ShrPBsjByVa
3271,3271,"	* Line 108: a buffer, instead of an buffer",ShrPBsjByVa
3272,3272,	,ShrPBsjByVa
3273,3273,"The report aims to reproduce the following paper: ""Multi-scale Interactive Network for Salient Object Detection""",whqqXkXlwYQ
3274,3274,The report is based on re-running the existing code-base,whqqXkXlwYQ
3275,3275,The report adds additional 3 datasets.,whqqXkXlwYQ
3276,3276,"On the positive side, the report is clearly written and easy to follow and contains enough details to understand the original paper.",whqqXkXlwYQ
3277,3277,"On the negative side, the experimental evaluation does not add new hyperparameters to test limiting the scope of the report given that it just re-runs the original paper implementation",whqqXkXlwYQ
3278,3278,"Moreover, I'm not convinced about the F-measure argument, that it is not suitable to handle properly ""black images"", since bot precision and recall should handle properly images without silent objects",whqqXkXlwYQ
3279,3279,Maybe the issue is in the implementation of the F-measure?,whqqXkXlwYQ
3280,3280,Overview:,whqqXkXlwYQ
3281,3281,The authors reproduce the paper using officially released github repo for MINet,whqqXkXlwYQ
3282,3282,They tested the approach extensively by trying it out on 3 additional datasets with necessary dataset-specific metric fixes.,whqqXkXlwYQ
3283,3283,Positives:,whqqXkXlwYQ
3284,3284,1,whqqXkXlwYQ
3285,3285,The authors test the method on three additional datasets.,whqqXkXlwYQ
3286,3286,Negatives:,whqqXkXlwYQ
3287,3287,1,whqqXkXlwYQ
3288,3288,The authors should try to optimize hyperparameters on the reported datasets since the code is already available,whqqXkXlwYQ
3289,3289,I do understand that there might be time/resource constraints,whqqXkXlwYQ
3290,3290,(Minor),whqqXkXlwYQ
3291,3291,2,whqqXkXlwYQ
3292,3292,The numbers on the additional datasets in itself do not reveal much unless compared with other approaches,whqqXkXlwYQ
3293,3293,It would be great to test EGNet or SCRN on the additional datasets (whichever is less effort) and include in the report,whqqXkXlwYQ
3294,3294,"If the results are publicly available in a different paper, it would be best to just add it to the report for completeness.",whqqXkXlwYQ
3295,3295,"The scope of the report is stated in the paper, however, it does not break down the scope into detailed items",whqqXkXlwYQ
3296,3296,It would be better to have a more detailed scope definition.,whqqXkXlwYQ
3297,3297,The code is from the original authors,whqqXkXlwYQ
3298,3298,The authors of this paper made some efforts in trying the proposed approach in different datasets.,whqqXkXlwYQ
3299,3299,Not much communication with the original authors other than reuse the GitHub code from the original authors.,whqqXkXlwYQ
3300,3300,"There is a hyperparameter described in the paper, but there is not much hyperparameters search in the report.",whqqXkXlwYQ
3301,3301,"It is interesting to see the comparison between different computer specifications to run the experiments, though they are not motivated well",whqqXkXlwYQ
3302,3302,I think it would be better if the authors could find out which salient region detection approach works the best for the retrieval task,whqqXkXlwYQ
3303,3303,"Specifically, can you tweak the hyperparameters of the proposed approach? Would that change the retrieval performance? ",whqqXkXlwYQ
3304,3304,I recognize that the paper has done a bit more experiments to check the various versions of the proposed approach on different datasets,whqqXkXlwYQ
3305,3305,There is a detailed discussion.,whqqXkXlwYQ
3306,3306,The original paper is reproducible,whqqXkXlwYQ
3307,3307,"However, according to this paper, it is better to provide details about the time spent on the training",whqqXkXlwYQ
3308,3308,"According to Table 2, it seems that the reports have attempted some backbone models",whqqXkXlwYQ
3309,3309,I am wondering why we skip efficient networks like MobileNet v2 here if we are looking for a bit more efficient way in training and testing,whqqXkXlwYQ
3310,3310,It is a bit confusing which computer in table 3 is used in reporting the training time and testing time in table 2,whqqXkXlwYQ
3311,3311,"Also, it would be better if the two machine has the same CPUs or the same GPUs, so that we can have some control groups",whqqXkXlwYQ
3312,3312,"Right now, there are too many differences in the two computers",whqqXkXlwYQ
3313,3313,"Overall the authors have done a very good job on reproducing the ECCV 2020 paper ""Autoregressive Unsupervised Image Segmentation""",gtudcKh8dBW
3314,3314,"As stated in the report, it was relatively straightforward implementing the architecture described in the original paper with only two hurdles; one relating to the loss function, which since it was taken from a previous paper, the authors were able to use information from that paper to reproduce the ECCV paper; second relating some issues around the attention layer, for which they contacted the authors, hence resolving it.",gtudcKh8dBW
3315,3315,"All the steps undertaken are explained concisely and appropriately, including what was easy and what wasn't",gtudcKh8dBW
3316,3316,Authors managed to approximate results in one of the two datasets used in the original paper,gtudcKh8dBW
3317,3317,Bottomline is that all the results presented do not match those of the original paper - more or less,gtudcKh8dBW
3318,3318,"The reason is that the authors were not able to use the same resources as the original paper, leading to different batch size and also architecture (4 vs 5 layers)",gtudcKh8dBW
3319,3319,Such changes can have an unpredictable effect to the final results therefore I am not sure one can be certain that the results obtained match - or not - the ones presented in the original paper.,gtudcKh8dBW
3320,3320,"Having said that, there is only so much that can be done if the resources are not available; nonetheless the results are affected.",gtudcKh8dBW
3321,3321,Overall it is a very good effort and the authors have done a great job in reproducing the paper,gtudcKh8dBW
3322,3322,"It seems that the original paper has had enough information, allowing a relatively straightforward reproduction with two caveats; one being having to contact the authors; and second resorting to a previously published paper to get some information on the loss function.",gtudcKh8dBW
3323,3323,**Strengths:**,gtudcKh8dBW
3324,3324,1,gtudcKh8dBW
3325,3325,Further hyperparameter investigation on the Potsdam dataset.,gtudcKh8dBW
3326,3326,The reproduction does some further hyperparameter scans on the batch size and output stride,gtudcKh8dBW
3327,3327,They show how some of the accuracy lost from other necessary changes can be regained with some tweaks,gtudcKh8dBW
3328,3328,This is a good contribution to increasing data on how well the underlying segmentation method works.,gtudcKh8dBW
3329,3329,2,gtudcKh8dBW
3330,3330,Good discussion of implications of the experiments on reproducibility success.,gtudcKh8dBW
3331,3331,"While there are some gaps in the completeness of the reproduction, as in the following, these are for the most part clearly described by the submission",gtudcKh8dBW
3332,3332,Section 2.1 lists specifically which experimental results were replicated,gtudcKh8dBW
3333,3333,The fact that unsupported claims are possibly unsupported due to differences in the reproduction are clearly described in other parts of the submission.,gtudcKh8dBW
3334,3334,3,gtudcKh8dBW
3335,3335,The submission clarifies details in the original significantly.,gtudcKh8dBW
3336,3336,Some of these are specifically listed in the Section 5.3 (describing the communications with the authors),gtudcKh8dBW
3337,3337,Architecture specifics such as the number of residual blocks and strides are also given in a clearer and more complete way in this submission than the original paper.,gtudcKh8dBW
3338,3338,"The original paper has some references to supplementary material that no longer seem to be available in the final published form, as far as I could find",gtudcKh8dBW
3339,3339,"Based on the references, some of these details may have been in that document.",gtudcKh8dBW
3340,3340,"This submission collects (from the code, discussion with authors, and possibly other sources) and describes some of them in a clear way that looks like it'd be a really good resource for any reproduction or follow-up work on the original paper.",gtudcKh8dBW
3341,3341,**Weaknesses:**,gtudcKh8dBW
3342,3342,4,gtudcKh8dBW
3343,3343,Reproduction trains fewer epochs for reasons not described.,gtudcKh8dBW
3344,3344,The command-line arguments in: \,gtudcKh8dBW
3345,3345,https://github.com/Max-Manning/autoregunsupseg/blob/master/run_experiments.sh \,gtudcKh8dBW
3346,3346,seem to specify 20 epochs for all trainings.,gtudcKh8dBW
3347,3347,"Section 3.4 of the repro says: ""All experiments were run for 10 epochs.""",gtudcKh8dBW
3348,3348,"This discrepancy and its effects isn't discussed further, unless I've missed it",gtudcKh8dBW
3349,3349,Some clarification in the rebuttal may help.,gtudcKh8dBW
3350,3350,5,gtudcKh8dBW
3351,3351,Experiments on COCO-stuff are on a smaller model.,gtudcKh8dBW
3352,3352,The reproduction reduces the number of residual blocks in the auto-encoder,gtudcKh8dBW
3353,3353,This is done due to limitations on available compute resources.,gtudcKh8dBW
3354,3354,Hard to draw any conclusion at all from the smaller model on COCO-Stuff,gtudcKh8dBW
3355,3355,"The observed drop in accuracy could be from either the smaller model, or a failure to reproduce",gtudcKh8dBW
3356,3356,"Results on Potsdam with the smaller model do not disambiguate between the two, as the tradeoff for model size vs accuracy can differ between datasets.",gtudcKh8dBW
3357,3357,"The authors of the repro acknowledge this and explain well how it affect their conclusions, so this is a not a strong negative for the score.",gtudcKh8dBW
3358,3358,It might help to note the comparison to baseline accuracy numbers,gtudcKh8dBW
3359,3359,E.g,gtudcKh8dBW
3360,3360,the comparison to other unsupervised ssemantic segmentation methods in Table 3 of the original paper.,gtudcKh8dBW
3361,3361,Positives:,gtudcKh8dBW
3362,3362,1,gtudcKh8dBW
3363,3363,The authors implement the method using pre-existing well tested code released for a previous paper.,gtudcKh8dBW
3364,3364,2,gtudcKh8dBW
3365,3365,The authors are able to reproduce the results on Postdam dataset to within 1%.,gtudcKh8dBW
3366,3366,Negatives:,gtudcKh8dBW
3367,3367,1,gtudcKh8dBW
3368,3368,The authors take a rather narrow outlook in reproducing the paper,gtudcKh8dBW
3369,3369,"Due to computational limits, they reduce the batch size and use a smaller model which makes verifying the claims rather difficult",gtudcKh8dBW
3370,3370,"The authors report that they ""do not find evidence supporting this claim..""",gtudcKh8dBW
3371,3371,"It should be mentioned that due to computational constraints, they were unable to run the full model in this summary line itself.",gtudcKh8dBW
3372,3372,2,gtudcKh8dBW
3373,3373,The authors apply their judgement and refute the claims made by the original paper,gtudcKh8dBW
3374,3374,They mention that their models couldn't differentiate edges well,gtudcKh8dBW
3375,3375,"But, this could very well be due to a training issue or just to architectural simplifications",gtudcKh8dBW
3376,3376,It would have been better if the authors also communicated with the original authors to determine the reason for the wide gap in performance on COCO-Stuff.,gtudcKh8dBW
3377,3377,3,gtudcKh8dBW
3378,3378,The authors could have summarised the original paper better in Introduction section.,gtudcKh8dBW
3379,3379,"Instead of reproducing exact results from the original paper, the submission implemented their own version and verified that the assumptions and results of the original paper do hold",_TGQQdIBhEv
3380,3380,"In general, this is a good complementary study for the original paper.",_TGQQdIBhEv
3381,3381,"The subject of study in the original paper is a new multi-resolution classification model called RANet, aiming for speeding up image classification under the assumption that some easy images can be classified at the lower resolution",_TGQQdIBhEv
3382,3382,"The submission has an implementation of the RANet model in Tensorflow (with Keras), and the code is open sourced",_TGQQdIBhEv
3383,3383,"This enabled Tensorflow users to try out RANet in the future, since the original authors implemented RANet in PyTorch.",_TGQQdIBhEv
3384,3384,2 main assumptions from the original paper are verified,_TGQQdIBhEv
3385,3385,"The first is the existence of such distribution from easy- to hard-to-classify images, which is verified by showing that different subnets have different classification errors",_TGQQdIBhEv
3386,3386,"The second is that RANet have an effective reduction in computational cost for image classification, which is verified using a classification task and a new earth quake detection dataset.",_TGQQdIBhEv
3387,3387,"Being a good complementary study already, the submission could offer improvements in the following ways: 1) the submission should discuss how easy it is to reproduce the exact results in the original paper using the original authors' PyTorch implementation",_TGQQdIBhEv
3388,3388,It will form a better story for the current submission,_TGQQdIBhEv
3389,3389,2) The authors should remove affiliation information on the computational resources used,_TGQQdIBhEv
3390,3390,This may run into anonymity troubles for more serious venues.,_TGQQdIBhEv
3391,3391,**Strengths:**,_TGQQdIBhEv
3392,3392,1,_TGQQdIBhEv
3393,3393,Illustration of performance during training.,_TGQQdIBhEv
3394,3394,Table 1 is a nice result for reproduction purposes,_TGQQdIBhEv
3395,3395,The training dynamics of networks with conditional/adaptive components are known to be unstable sometimes,_TGQQdIBhEv
3396,3396,"This reproduction shows, independently, how different subnetworks train within a CIFAR-10 experiment.",_TGQQdIBhEv
3397,3397,2,_TGQQdIBhEv
3398,3398,"Considers an additional ""spalling"" dataset not in the main paper.",_TGQQdIBhEv
3399,3399,"While not central to reproducing the claims in the original paper, it is good to investigate whether a method works on new data",_TGQQdIBhEv
3400,3400,"One generally hopes that AI & CV can generalize beyond the standard datasets such as ImageNet and COCO, and the concrete spalling dataset in the reproduction has some major difference in the domain considered.",_TGQQdIBhEv
3401,3401,3,_TGQQdIBhEv
3402,3402,Full re-implementation of the RANet architecture.,_TGQQdIBhEv
3403,3403,"The experiments in the reproduction were done in Keras, while the original RANet implementation is in PyTorch",_TGQQdIBhEv
3404,3404,"Since there are still small differences in behavior between the two, it is good to know that the RANet architecture is not so implementation-dependent that it doesn't generalize beyond this.",_TGQQdIBhEv
3405,3405,**Weaknesses:**,_TGQQdIBhEv
3406,3406,4,_TGQQdIBhEv
3407,3407,Only reproduces some of the smaller-scale experiments.,_TGQQdIBhEv
3408,3408,The only dataset used in both the original paper and this reproduction is CIFAR-10,_TGQQdIBhEv
3409,3409,It is reasonable to cite computational requirements as a barrier to reproducing the experiments,_TGQQdIBhEv
3410,3410,"However, it is a particularly major limitation when reproducing RANet, as the purpose of RANet is to adaptively reduce the resolution at which inference is done",_TGQQdIBhEv
3411,3411,"CIFAR-10 images are already very low-resolution (32x32), having been selected from TinyImages.",_TGQQdIBhEv
3412,3412,"If I understand correctly, the architecture in RANet is also adaptive w.r.t",_TGQQdIBhEv
3413,3413,"to the number of layers processed (so some input-dependent adaptive behavior can be observed with this too), but this is also in previous work such as Veit & Belongie",_TGQQdIBhEv
3414,3414,"The central new claim in RANet is in the handling of scale/resolution, that is better illustrated by ImageNet experiments (such as Fig 6c of Yang et al).",_TGQQdIBhEv
3415,3415,5,_TGQQdIBhEv
3416,3416,Some lack of clarity in dataset construction.,_TGQQdIBhEv
3417,3417,"For the spalling dataset: were the images, or the patches, split between train and test? It is unclear to me based on my reading (around line 82)",_TGQQdIBhEv
3418,3418,"In Fig 3, it looks like some of the patches can overlap? Is it possible in the dataset contruction that there could be two overlapping patches, from the same image, for which one patch is in the training set and the other patch in the testing set? It would be less than ideal for train and test to be correlated in this way, due to having examples that share pixels.",_TGQQdIBhEv
3419,3419,**Misc Comments:**,_TGQQdIBhEv
3420,3420,"  * 46: Typo ""Tensoflow""",_TGQQdIBhEv
3421,3421,"  * 46: ""train own"" -> ""train our own""",_TGQQdIBhEv
3422,3422,"  * 47: Is ""be"" meant to be ""by""",_TGQQdIBhEv
3423,3423,  * 47: Should capitalize ImageNet,_TGQQdIBhEv
3424,3424,The report did a reasonable job in reproducing the research work on predicting future trajectories from past data by leveraging a goal-prediction generative model,5M4oJ5b6Dc_
3425,3425,The experiment demonstrated that the original work is quite reproducible and supports the claims made by the authors of the paper,5M4oJ5b6Dc_
3426,3426,It is also greatly appreciated that additional experiments are performed (reference shift and recurrent architecture) to further investigate the proposed idea,5M4oJ5b6Dc_
3427,3427,"However,  I do have a few concerns regarding the report:",5M4oJ5b6Dc_
3428,3428,1,5M4oJ5b6Dc_
3429,3429,The majority of the experiments presented seem to be running with the existing code from the original authors,5M4oJ5b6Dc_
3430,3430,The report did mention that some changes are required for certain experiments but didn't provide more details,5M4oJ5b6Dc_
3431,3431,It would be helpful if the authors could provide more details regarding which experiments in the report required non-trivial changes to the codebase to better judge the effort required for the work.,5M4oJ5b6Dc_
3432,3432,2,5M4oJ5b6Dc_
3433,3433,Writing could be improved,5M4oJ5b6Dc_
3434,3434,"For example, the definition of ADE and FDE should be made clear in the report (They are defined in Eq (1) and (2), which aren't referenced in the text.).",5M4oJ5b6Dc_
3435,3435,"While there are some strengths, this report needs substantial improvement in a few areas.",5M4oJ5b6Dc_
3436,3436,Strengths,5M4oJ5b6Dc_
3437,3437,"- The GitHub repository containing the code for reproducing the experiments appears to be thorough (although it breaks anonymity, and should have been anonymized before submission)",5M4oJ5b6Dc_
3438,3438,- There was an extra experiment on using an LSTM architecture instead of an MLP.,5M4oJ5b6Dc_
3439,3439,- The reproduction of the results from the original paper appears to be thorough.,5M4oJ5b6Dc_
3440,3440,Weaknesses,5M4oJ5b6Dc_
3441,3441,"- If using the code from Mangalam et al., how do the authors account for the differences (albeit small) in the results?",5M4oJ5b6Dc_
3442,3442,- It is not clear how the GitHub repository for the paper reproduction differs from the original repository provided by Mangalam et al,5M4oJ5b6Dc_
3443,3443,without going through the commits,5M4oJ5b6Dc_
3444,3444,"It appears that there are more comments and some additional experiments, but it would helpful if this were summarized in the report and the additional experiments were separated from the original repository in some way",5M4oJ5b6Dc_
3445,3445,"This is made even more confusing by the fact that the README on the main page of the repository is unchanged from the original one (other than the ""About"" messsage on the page).",5M4oJ5b6Dc_
3446,3446,- The extra experiment should be clarified,5M4oJ5b6Dc_
3447,3447,Why did the authors chose to run it? What could it demonstrate?,5M4oJ5b6Dc_
3448,3448,- Figure 1 and Table 1 are directly from the Mangalam et al,5M4oJ5b6Dc_
3449,3449,"paper, but this is not acknowledged.",5M4oJ5b6Dc_
3450,3450,- Notation (such as in Table 1) is not explained,5M4oJ5b6Dc_
3451,3451,"Generally, this report is very difficult to read without cross-referencing the original paper.",5M4oJ5b6Dc_
3452,3452,"- The same holds for key abbreviations used in the report but not explained, including FDE and ADE.",5M4oJ5b6Dc_
3453,3453,"This paper attempts to reproduce the work published in ECCV 2020, i.e., On Disentangling Spoof Trace for Generic Face Anti-Spoofing",4PKKAvEAE-s
3454,3454,"The authors take help from the original paper, the original authors and the official implementation, which took them around a month",4PKKAvEAE-s
3455,3455,They also verified different segments of the original implementation and propose several improvements over a few limitations of the original paper.,4PKKAvEAE-s
3456,3456,Strengths: ,4PKKAvEAE-s
3457,3457,The writing is good and easy to follow,4PKKAvEAE-s
3458,3458,The summary of the original paper is clarify and detailed,4PKKAvEAE-s
3459,3459,The authors provide figures to demonstrate whether they have succeeded to reproduce the original results.,4PKKAvEAE-s
3460,3460,Weakness：,4PKKAvEAE-s
3461,3461,"1)	As argued in the report, the reproduction took help of the authors and the official implementation",4PKKAvEAE-s
3462,3462,"Therefore, it is significant for the authors to put emphases on what are challenging to reproduce the results of the original paper",4PKKAvEAE-s
3463,3463,The FFT adopted in the final decision is confusing,4PKKAvEAE-s
3464,3464,I cannot make it out how it was injected in the original method,4PKKAvEAE-s
3465,3465,"2)	Typos, like the last sentence in the results part in Page 1",4PKKAvEAE-s
3466,3466,It is suggested to go over the paper carefully to improve the quality of the paper.,4PKKAvEAE-s
3467,3467,The study tried to reproduce the paper by following the official implementation,4PKKAvEAE-s
3468,3468,This report can be improved in following aspects:,4PKKAvEAE-s
3469,3469,1,4PKKAvEAE-s
3470,3470,"Language could be improved, e.g., line 149 contains typo, some sentences can be shortened.",4PKKAvEAE-s
3471,3471,2,4PKKAvEAE-s
3472,3472,"When referring to equations in the original paper, the authors do not explain the context",4PKKAvEAE-s
3473,3473,This makes the study hard to read.,4PKKAvEAE-s
3474,3474,3,4PKKAvEAE-s
3475,3475,The descriptions for figures were not clear to me.,4PKKAvEAE-s
3476,3476,# Summary and overall assessment,4PKKAvEAE-s
3477,3477,"The authors analyzed the reproducibility of ""On Disentangling Spoof Trace for Generic Face Anti-Spoofing"" which proposes a GAN to identify spoofing in images for robust face recognition",4PKKAvEAE-s
3478,3478,The strengths of this report is that they prototyped the paper ideas from scratch and proposed modifications to improve the methodology,4PKKAvEAE-s
3479,3479,"However, the overall report is very vague in writing (see comments below) and I was missing details to understand what has been done, e.g., scope of the reproducibility, description of the approach",4PKKAvEAE-s
3480,3480,Further I do not believe that the contributions to improve the approach is necessarily the scope of a reproducibility report,4PKKAvEAE-s
3481,3481,This can also change the overall results and make them not comparable to the original results.,4PKKAvEAE-s
3482,3482,# Detailed comments on each section,4PKKAvEAE-s
3483,3483,Reproducibility Summary/Scope of reproducibility:,4PKKAvEAE-s
3484,3484,* This is rather a summary of the work itself,4PKKAvEAE-s
3485,3485,I would have expected this paragraph would summarize the content of this report,4PKKAvEAE-s
3486,3486,"* ""Based on that, this paper suggests"": the paper should be citied.",4PKKAvEAE-s
3487,3487,Reproducibility Summary/Results: ,4PKKAvEAE-s
3488,3488,"* ""We succeeded to match the ACER of the original paper to within 0.53%""/ For OULU NPU Protocol-1 the numbers are 1.9% vs",4PKKAvEAE-s
3489,3489,"1.195%, which would result in a difference of 0.705% > 0.53%?",4PKKAvEAE-s
3490,3490,"* Last sentence ""Later, we proposed a few techniques to"" is incomplete.",4PKKAvEAE-s
3491,3491,Introduction:,4PKKAvEAE-s
3492,3492,"* ""Of all bio-metric authentication technologies, face recognition is the most intuitive and effective."" Can you cite the source or elaborate?",4PKKAvEAE-s
3493,3493,"* ""In the past, hand-crafted features like HOG and LBP were used to tackle the problem of face anti-spoofing."" Please cite relevant works.",4PKKAvEAE-s
3494,3494,"* ""In recent years, CNNs have been adopted as the preferred solution for this problem."" Please cite relevant works.",4PKKAvEAE-s
3495,3495,* The introduction could have given more details about solutions to spoofing,4PKKAvEAE-s
3496,3496,Is it a classic classification problem solved with hand-crafted features or CNN features?,4PKKAvEAE-s
3497,3497,Scope of reproducibility:,4PKKAvEAE-s
3498,3498,* Equation 1: Can you explain this equation?,4PKKAvEAE-s
3499,3499,Methodology:,4PKKAvEAE-s
3500,3500,"* Can you explain the methodology in details? What is used as input, what problem is optimized, what is the loss function?",4PKKAvEAE-s
3501,3501,Experimental setup:,4PKKAvEAE-s
3502,3502,* What model (architecture) was used here?,4PKKAvEAE-s
3503,3503,Implementation:,4PKKAvEAE-s
3504,3504,"* ""However, there is a functionality which is not available in PyTorch,"" Can you be concrete and say which functionality?",4PKKAvEAE-s
3505,3505,Resources:,4PKKAvEAE-s
3506,3506,"* ""During the implementation phase, we found inconsistencies between the paper and the official implementation."" Which ones?",4PKKAvEAE-s
3507,3507,Other:,4PKKAvEAE-s
3508,3508,"* ""The following section formatting is optional, you can also define sections as you deem fit."" I believe this can be removed.",4PKKAvEAE-s
3509,3509,This submission reproduces the self-supervised learning results of FixMatch on CIFAR-10 and studies how the interaction between supervised and unsupervised learning objectives might lead to confirmation errors.,3VXeifKSaTE
3510,3510,"**Reproducing results.** The submission successfully reproduced SSL results on CIFAR-10, with error rates within the ranges provided in the original paper (c.f",3VXeifKSaTE
3511,3511,Table 1),3VXeifKSaTE
3512,3512,"Authors reimplemented the method using Pytorch (the official code used TensorFlow), following the method description in the paper and checking the official code when something was unclear in the manuscript",3VXeifKSaTE
3513,3513,I believe that reproducing FixMatch using a different deep learning framework is an important contribution to the community.,3VXeifKSaTE
3514,3514,"**Beyond reproducing results.** Authors devote an important part of the submission to their hypothesis that FixMatch might suffer from confirmation errors, including an exhaustive literature review",3VXeifKSaTE
3515,3515,"This goes beyond reproducing results and changing some hyperparameters, and can be seen as an improvement to the original method",3VXeifKSaTE
3516,3516,"Unfortunately, it is unclear whether the reported gains (e.g",3VXeifKSaTE
3517,3517,Table 3) are statistically significant due to the lack of cross-validation or additional random seeds,3VXeifKSaTE
3518,3518,"The method has potential for a future workshop or conference submission if more experiments to back up the hypotheses are reported, thus I encourage authors to pursue these ideas.",3VXeifKSaTE
3519,3519,"Given that the submission not only reproduces some of the results of FixMatch, but also explores some of its potential limitations and proposes improvements to the method, I recommend its acceptance",3VXeifKSaTE
3520,3520,"Minor comment: the first half of the paper provides error rates, but then accuracy becomes the metric of reference",3VXeifKSaTE
3521,3521,"While both are essentially measuring the same thing, I would encourage consistency by using the same metric throughout the entire manuscript.",3VXeifKSaTE
3522,3522, I can confirm that the authors include a clear summary of their work,3VXeifKSaTE
3523,3523,"They perform all their experiments using the CIFAR-10 dataset and highlight that the original paper was missing some implementation related detailed, which was later clarified",3VXeifKSaTE
3524,3524,"The authors use the same hyper-parameters, as the original paper and perform some ablation studies for the confidence threshold",3VXeifKSaTE
3525,3525,The authors reproduce the experiments using pytorch and do not provide any recommendations to the original authors,3VXeifKSaTE
3526,3526,"The paper is otherwise well written, though slightly confusing at times",3VXeifKSaTE
3527,3527,The report aims to reproduce the results of the paper 'FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence' The report gives a summary of how the reproduction is conducted and briefly introduce the original paper,3VXeifKSaTE
3528,3528,The paper also gives the details including the hyper-parameters and computational infrastructures,3VXeifKSaTE
3529,3529,The authors provides the reproducing codes with detailed documentation.,3VXeifKSaTE
3530,3530,"The proposed work reproduces the result from ""Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention (CVPR 2020)""",r87dMGuauCl
3531,3531,The proposed work was able to reproduce the result (except for different dataset) from the original paper,r87dMGuauCl
3532,3532,"However, the consideration of the following points might improve the quality of the proposed work",r87dMGuauCl
3533,3533,1) A thorough hyperparameter search and a corresponding discussion is expected.,r87dMGuauCl
3534,3534,2) The original paper also demonstrates that the processing time and memory requirement is considerably decreased,r87dMGuauCl
3535,3535,It would be better to comment/validate this statement.,r87dMGuauCl
3536,3536,This report aims to reproduce a paper on classification of time sequences of satellite imagery using transformers,r87dMGuauCl
3537,3537,"The report describes the attempt to reproduce the original paper from scratch, then using the provided code",r87dMGuauCl
3538,3538,"In addition, the report adds on more investigation on another dataset.",r87dMGuauCl
3539,3539,"In general, the report is very cleanly written",r87dMGuauCl
3540,3540,It was easy to follow along,r87dMGuauCl
3541,3541,It is valuable that the authors include their misconceptions and how they fixed it,r87dMGuauCl
3542,3542,The report clearly states the scope of reproducability and follows it,r87dMGuauCl
3543,3543,"In their scope, the authors attempted to reproduce the paper from scratch",r87dMGuauCl
3544,3544,"Then, they discovered that the original paper falls short on the explanation of the exact architecture being used",r87dMGuauCl
3545,3545,The authors consult the the sources included with the original paper and contact the original paper authors,r87dMGuauCl
3546,3546,"Overall, this shows a healthy and admirable approach to scientific investigation and collaboration.",r87dMGuauCl
3547,3547,The report examines the architectural choices made in the original paper,r87dMGuauCl
3548,3548,"The report experiments with the proposed approach and compares with a more standard architecture (Vaswani et al., 2017)",r87dMGuauCl
3549,3549,This part of the report reveals insightful results that the standard architecture yields similar or even better results.,r87dMGuauCl
3550,3550,The report extends the original paper with an extra experiment on a new dataset.,r87dMGuauCl
3551,3551,Suggested improvements or extensions:,r87dMGuauCl
3552,3552,"- As far as I understood, the report does not conduct the hyper-parameter search",r87dMGuauCl
3553,3553,Nor it tries to determine the stability of the hyper-parameters,r87dMGuauCl
3554,3554,This investigation would greatly improve the report.,r87dMGuauCl
3555,3555,"- With an exception for the case study mentioned above, the report doesn't provide any ablation studies",r87dMGuauCl
3556,3556,- The figures are almost impossible to read in black and white,r87dMGuauCl
3557,3557,I encourage to modify the figures to make them more accessible to people with black and white printers and the color blind.,r87dMGuauCl
3558,3558,"Overall, this is a well-structured, well-written report that analyses the original paper",r87dMGuauCl
3559,3559,"Furthermore, the report extends the original paper with the results on an extra dataset",r87dMGuauCl
3560,3560,I recommend accept.,r87dMGuauCl
3561,3561,"The authors of this report aimed at reproducing the method presented in the paper ""Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention"" published at CVPR 2020 by Garnot et al.",r87dMGuauCl
3562,3562,"The authors not only did they attempt to reproduce the code and evaluate it on the dataset used in the original paper, but they also went on to use another dataset to expand upon the evaluation process, including some changes to the way the test set was selected in the original dataset.",r87dMGuauCl
3563,3563,Given that the authors of the original paper have made their code available allowed for a direct comparison between the reproduced code and the original code,r87dMGuauCl
3564,3564,"In fact, the authors of this report have had some issues on some aspects of the use of the original transformer implementation, but that was resolved in a communication with the authors of the original paper and of the Transformer one",r87dMGuauCl
3565,3565,"Minor discrepancies and issues with the implementation were also resolved via checking the provided github code of the original paper, hence confirming the reproducibility of the original paper.",r87dMGuauCl
3566,3566,"Finally, although the report is well written overall, I would have expected the conclusion to be a bit more elaborate on what was easy and what did not work as expected; but that is a very minor issue.",r87dMGuauCl
3567,3567,***Reproducibility Summary:***,nZvCsz5CjqN
3568,3568,       The authors have provided a detailed summary meeting the requirements of a reproducibility report.,nZvCsz5CjqN
3569,3569,***Scope of reproducibility:***,nZvCsz5CjqN
3570,3570,"        Yes, the reproducibility report has clearly and concisely stated the scope of reproducibility.",nZvCsz5CjqN
3571,3571,***Code:***,nZvCsz5CjqN
3572,3572,"        Yes, the authors have re-used the original author's code repository and their trained model with small modifications.",nZvCsz5CjqN
3573,3573,***Communication with original authors***,nZvCsz5CjqN
3574,3574,"      Yes, the authors connected with original authors through the original authors' Github repo.",nZvCsz5CjqN
3575,3575,***Hyperparameter Search:***,nZvCsz5CjqN
3576,3576,"      Yes, the authors have attempted to reproduced the hyperparameter search",nZvCsz5CjqN
3577,3577," The authors have also expanded the hyperparameter search to involve image size, the ""hard examples factor $m$"", different kernel sizes, lambda parameters, and other analysis not present in the original paper.",nZvCsz5CjqN
3578,3578,***Ablation Study:***,nZvCsz5CjqN
3579,3579,      I did not notice any ablation in the study.,nZvCsz5CjqN
3580,3580,***Discussion on results:***,nZvCsz5CjqN
3581,3581,"      Yes, the reproducibility report contains a brief discussion on the state of reproducibility of the original papers, but also provides  notes on where to modify the original work to fix the issue in the original pyTorch model",nZvCsz5CjqN
3582,3582," Despite differences in the experimental results, the authors carried out a paired t-test to further confirm the reproducibility of result.",nZvCsz5CjqN
3583,3583,***Recommendations for reproducibility:***,nZvCsz5CjqN
3584,3584,"      No, the authors did not provide any recommendation to the original authors for improving reproducibility.",nZvCsz5CjqN
3585,3585,***Results beyond the paper:***,nZvCsz5CjqN
3586,3586,      The authors have tried additional results that are not mentioned in the original paper,nZvCsz5CjqN
3587,3587, The authors include significantly more quantitative and qualitative results than the original paper.,nZvCsz5CjqN
3588,3588,***Overall organization and clarity:***,nZvCsz5CjqN
3589,3589,         Nicely written and coherent.,nZvCsz5CjqN
3590,3590,***Pros:***,nZvCsz5CjqN
3591,3591,         Significantly more quantitative and qualitative results.,nZvCsz5CjqN
3592,3592,         Extensive further exploration of the hyperparameter search and added new dimensions to the hyperparameter search (e.g,nZvCsz5CjqN
3593,3593,image size).,nZvCsz5CjqN
3594,3594,"Authors of this report provide summary of report, scope of reproducibility and communicated with original author of the “Interactive Two-Stream Decoder 3 for Accurate and Fast Saliency Detection”.",nZvCsz5CjqN
3595,3595,"This paper consists of mainly three parts: evaluation using the pretrained model, evaluation using retrained model and hyperparameter ablations",nZvCsz5CjqN
3596,3596,"Basically, there are available codes from the original authors, thus it is important to perform plenty of “hyperparameter search” and “ablation”.",nZvCsz5CjqN
3597,3597,"The good point of the paper is that the author tried to verify the original model provided by the original authors and reproduce and retrain the exact same model, then perform ablations",nZvCsz5CjqN
3598,3598,It is helpful for the following works.,nZvCsz5CjqN
3599,3599,"In order to quantitatively justify the reproducibility of the results from the retrained model, they carried out statistical analysis using paired t-test",nZvCsz5CjqN
3600,3600,I think that it is a good value of this report.,nZvCsz5CjqN
3601,3601,"Also, this report provides more various ablation studies not included in the original paper, including “image size and inference speed”, “factor m”, “dilation and erosion”, “lambda”, etc",nZvCsz5CjqN
3602,3602,Those plenties of experiments may be helpful for following researchers.,nZvCsz5CjqN
3603,3603,The scope definition is good and clear.,nZvCsz5CjqN
3604,3604,The design choice in choosing training and testing datasets would need a bit better motivation,nZvCsz5CjqN
3605,3605,This paper did make efforts in exploring different hyperparameter searches.,nZvCsz5CjqN
3606,3606,There are communications between authors and the original authors,nZvCsz5CjqN
3607,3607,All are done in Github issues,nZvCsz5CjqN
3608,3608,I am happy to see that the authors have made a few changes to the original code.,nZvCsz5CjqN
3609,3609,This paper picked up some experiments that are not done in the original papers,nZvCsz5CjqN
3610,3610,e.g,nZvCsz5CjqN
3611,3611,effects of image sizes and inference speed; hard examples factor m; dilation and erosion; lambda; optimized vs default parameters; constrained lambda.,nZvCsz5CjqN
3612,3612,It would be better if the authors could look further into the compatibility of the pre-trained models with other frameworks e.g,nZvCsz5CjqN
3613,3613,"TensorFlow, or ONNX.",nZvCsz5CjqN
3614,3614,I find the following items could be improved in the paper.,nZvCsz5CjqN
3615,3615,1,nZvCsz5CjqN
3616,3616,"Table 2-6 lacks the detailed description of ""our results""",nZvCsz5CjqN
3617,3617,How exactly the parameters are set to get the results?,nZvCsz5CjqN
3618,3618,2,nZvCsz5CjqN
3619,3619,I think figure 1 could be significantly improved if there are more descriptions to summarize the trends in the figures,nZvCsz5CjqN
3620,3620,3,nZvCsz5CjqN
3621,3621,"Figure 3 is a bit confusing, how the figures related to the proposed approach?",nZvCsz5CjqN
3622,3622,4,nZvCsz5CjqN
3623,3623,"Figure 6 provides very little information, and it should be replaced by a table or even description in the text.",nZvCsz5CjqN
