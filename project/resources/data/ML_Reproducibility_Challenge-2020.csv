,Description,forum
0,"**Reproducibility summary:** The report does not have the mandatory reproducibility summary.

**Scope of reproducibility:** In Section 3, the authors state that the aim of the section is ""to examine whether self-attention layers in practice do actually learn to operate like convolutional layers when trained on standard image classification tasks.""

**Code:** It's unclear whether the authors used the original authors' code (which is publicly available) or not. The authors say they ""closely follow the official implementation for reproducing the three embedding schemes and attention mechanisms"" (line 98) but do not discuss how their implementation is similar to or different from the official implementation. The authors' codebase is clean and organized, but lacks documentation.

**Communication with the original authors**: The authors did not discuss whether they communicated with the original authors. 

**Hyperparameter search:** The authors did not conduct a hyperparameter search.

**Ablation study**: The authors did not conduct any ablation studies.

**Discussion on results:** The authors reproduced the original paper's CIFAR-10 experiments and reported similar findings. The authors' discussion of their results is on the shorter side (2 paragraphs in Section 3.3). 

**Recommendations for reproducibility:** The authors did not provide recommendations for reproducibility.

**Results beyond the paper**: This report contains several results beyond the original paper. In Section 4, the authors discuss two recent works that attempt to replace convolutions with self-attention. While I found the introduced works interesting, the section lacks discussion of how these works relates to the original paper. In Section 5, the authors propose a new attention operation named Hierarchical Attention (HA) and demonstrate its effectiveness against normal Self-Attention (SA). As the authors emphasize, this operation improves accuracy while using substantially less number of parameters.

**Overall organization and clarity:** Overall, the report was organized and clearly written. However, the authors did not include the mandatory reproducibility summary, and did not discuss their experience reproducing the paper (e.g. methodology, computational requirements, what was easy/difficult, communication with the original authors). While the authors presented results beyond the scope of the original work, the report needs more work on the reproducibility front. I also suggest keeping the citation style consistent and using appropriate labels and references to reference tables and figures.",4hm5ufX69jo
1,"***Reproducibility Summary***
Unfortunately the authors have not included the mandatory first page reproducibility summary, and following the reviewer guidelines the submission is liable to desk rejection.

***Scope of reproducibility***
The paper does not state clearly the scope of the reproducibility. The authors reimplement the main experiment in the paper on Cifar, for which they obtain similar results. They also reproduce a similar experiment where they visualize the attention maps, however their attention maps differ significantly from those shown in the paper and there is no in-depth analysis of them.

***Code***
The authors have implemented their own code to reproduce the paper. Unfortunately, their code is attached but is not de-anonymized.

***Communication with original authors***
It is not clearly stated in the paper whether the authors of the reproducibility report contacted the original authors of the paper (or at least I couldn't find this information easily). This information could have been readily available if the authors had included a reproducibility summary.

***Hyperparameter search and ablation study***
The authors conducted limited hyperparameter searches and ablation studies, but the experiments in the original paper are limited in this regard too and therefore it does not seem highly relevant in the evaluation of this report.

***Discussion on results***
The authors do not discuss the results in depth. Most of the conclusions are limited to re-stating the results obtained without properly discussing the differences between their results and the ones found in the original paper. This can be seen in the discussion for the filter results, for example.

***Recommendations for reproducibility***
The authors do not directly provide recommendations to improve the reproducibility of the original paper.

**Results beyond the paper***
The authors put the focus of this report on a new type of attention they propose. Unfortunately, this is at the expense of the quality of the rest of the report, and as part of the reproducibility challenge the paper is does not focus enough on reproducing the original paper.

***Overall clarity***
The article could be better organized, providing first a reproducibility summary.

***Overall rating***
I argue for the rejection of this article based on the omission of the mandatory reproducibility summary, the lack of clarity and the focus on the results beyond the paper but not on reproducing the results of the original paper.",4hm5ufX69jo
2,"The authors do not provide reproducibility summary and the manuscript does not match the expected template. There is no discussion about the scope of reproducibility nor any discussion over what was easy/hard. Missing evaluations from the appendix section of the paper.

Overall, authors are able to replicate the experiments reported in the original paper. They also show additional experiments with other choice of attention-based networks which is interesting. Current analysis lacks visualizations for 'centers of attention' which is important. Why is the #params in table 1 and 2 same ? Is the accuracy(paper) in table 2 correct comparison ?

It is commendable that along with reproducing results, the authors propose hierarchical attention operation and evaluate it's performance. They show that it resolves the computational memory requirement as intended. However, keeping the scope of this venue in consideration the paper does not match the expected format and lacks detailed analysis.",4hm5ufX69jo
3,"*Problem statement: 
The paper clearly states the reproducing details, together with the detailed results and difficulties.

*Presentation:
The paper is well-organized and well-written. 

*Communication with original authors
The authors had some communication with the original authors.

*Code:
The code is well-organized and can be reproduced. I have tested the code on my side and all components work well. 

*Recommendations for reproducibility
The authors provided useful comments for reproducing the original paper. I have read the code and found those comments are consistent with the provided codes. 

*A few concerns
**The tense is not consistent in the whole paper. In some sections, the author used past tense while the present tense is used in some other sections. 
** It will be even better if the authors can provide a simple illustration on the two major algorithms, like Fig.3 and Fig.4 in the original paper. The figures and simple explanations would help readers to follow your report.",PRXM8-O9PKd
4,"The authors developed code for the original paper from scratch and communicated with the original authors for detail. They provide a in-depth, easily readable, well organized report and analysis. They include extensive HPO. They give valuable recommendations for future work. 

Unfortunately, code will only be submitted only after the review process.",PRXM8-O9PKd
5,"__Summary__

The original paper (OP) proposes a new activation function, snake. Snake is claimed to be usable with datasets both with and without cyclicity, on the contrary to previous activations. Experiments include low-dimension regression to illustrate the limitations of conventional activations, and applications of snake and baselines to timeseries to illustrate their ability (or not) to deal with cyclic data. 
Snake is also evaluated on common benchmarks (CIFAR10) to show that it performs well compared to baselines even when the dataset is not cyclic.
This reproducibility report (RR) reproduces all the experiments of the OP and confirms the OP's findings.
The RR also extends the study to two new settings (image generation and sentiment analysis), both non-cyclic, in which the performance of snake is studied.

__Positive points__
- The OP's code was not publicly available and the authors recoded all of the experiments themselves, which needs to be commended.
- Similarly, the authors optimized hyperparameters to make the implementations work despite the fact that they were missing in the original paper.
- The authors performed additional experiments with elaborate models. Both the feedforward and recurrent architectures that were used were not tested in the original paper, which demonstrates the applicability of the original idea to new settings and  adds value to the original idea from the OP.
- I appreciated the exhaustive replication of the several experiments that this paper contained.
- The RR authors contacted of the OP authors to get additional information on the points of the paper that lacked clarity, and used this information to replicate the OP results.
- The format of the reproducibility report was respected, and the report is overall clear. The authors clearly state that the results are reproducible. The report is overall well written and concise.

__Negative points__
- The general experiments are reproduced, but I find that a finer analysis is absent from the RR. The results are reproduced through a figure, but this figure is often not referred to. A small conclusion about the reproducibility of the specific experiment is systematically absent. No quantified metric on the closeness to the original performance is given, or commented, even though the paper gave quantified results for the financial experiment (OP's Tab.2). For instance, when the performance is illustrated through a graph, I would have expected a comment on the closeness between the accuracy of the OP's model and the RR's model.
- I find that the report lacked precision. For instance: (1) I am not sure which optimizer was used for most of the experiments, or what initialization; Sec 4.1 is an example of this. (2) p.6, l.146: ""orders of magnitude"": how many? Can you give an estimate of the time in each case? (3) p.7, l.159: can you be more precise than ""reasonably realistic samples""? Are there metrics that exist that could quantify by a number the quality of the images produced? (4) p.8, l.163: took much longer: can you be more precise? etc.
- I find that overall, the report does not take a step back w.r.t the experiments it makes. As a consequence, the RR did not reflect on some of the differences, notably Fig.3 (the snake network starts to diverge, on the contrary to OP's Fig. 4) and Fig.5 (the snake network does not fit the training points): how can these differences be explained? 
- The RR mentions having to find / guess some hyperparameters (Sec 3.3), but does not comment on the value that was ultimately found and used in the report, or the range of hyperparameters that was tested. It is true that the code is available on Github, but an explicit reference to it would help lift the doubt on certain implementation details. For now, the reader does not gain the knowledge missing from the OP by reading the RR.
- It was often difficult to separate the RR's conclusions from the OP's (for instance, it is confusing on a first read if the last paragraph of p.5 contains conclusions from the RR's authors or the OP's authors, or even if the experiment itself was in the OP (since the experiment does not appear in the main paper, but in the appendix)).

===

Overall, I believe the report could be improved by introducing each experiment better, providing the implementation details, and giving a conclusion on the reproducibility of each experiment. Adding numerical results indicative of reproducibility (was the reproduced end accuracy of the model within X% of the OP's results?) for each experiment would help ground the report, and help it discuss in more details the state of reproducibility of the OP.

===

__Additional comments__
- Some figures are not referenced (ex: Fig.2, Fig.3, Fig.4,). This is problematic because this reinforces the idea that the experiment was run in the RR without an analysis of its results and its significance. How can you tell that the results were indeed replicated?
- The RR mentions that reimplementing the initialization was not obvious from the paper and that one of the OP's authors helped. An additional comment about what was the problem / what was the solution found (or at least a pointer to the RR's code) would be useful.
- Regarding the discussion of Fig.8: it seems to me that the difference between the RNN and the MLP could simply be due to a regularization effect: the RNN also seems to learn the right lower frequency. 
- The authors mention in the reproducibility summary that they reimplemented everything from scratch, though looking at the code, it appears that some implementations (of the LaProp optimizer or of the base code for the GAN) were based on existing codebases. While obviously the authors are not expected to recode *everything* from scratch, it would be more transparent to indicate that some parts of the code were built on top of existing code in the RR. 
- A mention of the software used (Pytorch for the neural networks) would have been useful, as all libraries do not have necessarily the same default hyperparameters. Similarly, the authors mention in Sec. 3.5 that many experiments could be run locally: more details on the local hardware (which CPU for instance, a minima an indication on the quality of the ""local"" computer that was used) would be welcome.
- The RR mentions that making the parameter  a learnable was not properly explained in the OP. Here too, a pointer to the solution found would be appropriate.
- The introduction is extremely close to the one in the OP. I would suggest reformulating the introduction more, or possibly shorten it, to avoid repeating the OP.
- The figures were overall well made and close in design to the original paper, which helps when comparing results. Possible improvements could be to respect the color attribution for different models (Fig.2) and making sure that the scale of two plots coincide to make comparisons easier (Fig.10 top left and bottom left, Fig.13 for instance).",ysFCiXtCOj
6,"The paper is well written and clearly structured.
It is a valuable confirmation of the original paper.
I only have a few formatting criticisms:

sin, cos, and snake should not be italic, but rather \sin or {\rm sin
I think, „4e -4“ should be relaced by 0.0004 or 4\cdot 10^{-4}
In Section 4.4 „Effect of \alpha$, use \boldmath{\alpha}

",ysFCiXtCOj
7,"The presented report is well organized, systematic, clear and concise with most of the major experimental results reproduced and reported. To begin with, authors have clearly mentioned the key claims to be investigated in the scope of reproducibility with the focus on the properties of  the activation functions and their behaviour in the experiments reported in the original paper. It has also been proposed to conduct additional experiments beyond what is already covered in the source paper. These includes DCGAN for handwritten digit generation and LSTM model for sentiment analysis, each modified to include the proposed snake activation function. It is however to be noted that when result plots for different cases are not reported in the same graph, it helps to report them on the same scale, wherever possible (e.g. Figure 12 and 13), for easier comparison.

Due to the unavailability of the source code from the author's of original paper, the current authors, in consultation with the original authors, have done a good job of replicating the results, which is matching qualitatively to a great extent with the ones reported in the original paper. While the architectural details were available in the original text, the hyper-parameters and any other information missing were assumed and arrived at with trial and error.

Most of the results from the main content of the original paper has been reported and are found to be qualitatively matching. Figure 1 has demonstrated the inability of learning periodicity outside of the training region and thus substantiates the claim 1 of the scope of reproducibility. These reporting also matches with the corresponding figure in the original work for the widely used activation functions - ReLU and Tanh.

To substantiate claim 2 which states that the proposed snake activation function captures periodicity outside of the training samples and also maintains favourable optimization properties such as loss reduction comparable to or better than ReLU activation, corresponding results from original work has also been reproduced. Although the frequency of the sin wave used in the experiment appears to be different, it is helpful in demonstrating that even though periodicity is observed, significant deviations can still occur at far away places from the training samples. Nevertheless, differences in implementations can be a source of difference.

In addition to the demonstrations on synthetic datasets discussed earlier, the authors have further reproduced results on real-world/application oriented datasets that includes performance reporting of ResNet18 on CIFAR-10 dataset, atmospheric temperature prediction in Minamitorishima island (Japan), patient body temperature prediction and Wilshere 5000 index prediction. All these reported results corroborates the points in the original paper across varying and diverse datasets.

Finally, the authors have reported results for a few experiments from the Appendix section such as the one showing the effect of various value of $a$. It is also admirable that they have reported results and discussed implications of use of snake activation function in other scenarios such as training DCGANs and LSTMs. While the snake activation was shown to have reasonable and comparable training speed and test performances as evidenced by Figure 2 and 4, the extra experiments on non-toy datasets helped in highlighting the relatively slow training speed.

Thus, overall, the authors have tried to cover extensive set of experiments demonstrating pros and cons of the proposed snake activation function and have reproduced the results qualitatively to a great extent. The difficulty in reproducing some of the work such as comparison with RNN on regressing a Simple Periodic function where the details were missing and the data points had to be inferred from the graphs have been mentioned. Similarly, it is also suggested to clarify the implementation details for showing variance correction using ResNet101 on CIFAR-10 for better reproducibility.",ysFCiXtCOj
8,"In this report, the authors reproduce the experiment parts of the paper: ""Can Gradient Clipping Mitigate Label Noise?"" by Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Sanjiv Kumar.

The main contributions of this report are: 
1. The authors reproduce experiments of the original paper to prove that Partially Huberised Loss does have label noise robustness.
2. The authors report corrected hyperparameters that were reported incorrectly in the original paper.
3. Although it's not so much different from what the original paper insists, the authors provide results that are different from the original paper.
4. The authors clarify that the ResNet-50 architecture used in the original paper differs from the architecture of He et al. 2015 (this choice was not made clear in the original paper) and explains why this modification is necessary.

Minor issue:
In experiments using Long & Servedio dataset, there is no explanation for why is the accuracy becomes lower when the corruption rate ρ is lower.


Verdict:
The report contains some solid experiments and additional information and corrections not in the original paper. I can see researchers trying to build upon the original paper benefitting from reading this reproducibility report. For this reason, I recommend the report be accepted.

",TM_SgwWJA23
9,"The report aims to reproduce a paper on so-called Partially Huberised losses. These losses are used to mitigate the label noise. The report replicates all the experiments from the original paper and makes insightful discoveries.

The report is well-written and easy to follow. The original paper is summarized and all the experiments are described in details. The report also mentions the communication with the authors of the original paper which revealed some typos or mistakes in the original paper.

The original paper was reproduced from scratch with a different framework (PyTorch, instead of Tensorflow). This is good way to test the reproducability.

Suggestions for improvements:
- The very first section ""Scope of Reproducability"" should discuss, the scope. What the report reproduces and what it does not.
- The hyper-parameter search is the same as in the original paper. The report does not go an extra mile to search wider space. Nevertheless, I understand that such experiments may be time consuming and require a lot of computational resources.
- It is always desirable to include extra datasets, extra experiments, more architectures to more thoroughly test the claims of the original paper.
- Section 4.2.2 mentions that the reproduced experiments used the mixed precision for training. The report should clarify if this is different from the original paper and if it may influence the results.

Overall, this is a good report. I recommend accept.",TM_SgwWJA23
10,"The report aims to reproduce the results of the paper 'Can gradient clipping mitigate label noise?' The report gives a summary of how the reproduction is conducted and briefly introduce the original paper. The paper also gives the details including the hyper-parameters and computational infrastructures. 

The authors provides the reproducing codes with detailed documentation.",TM_SgwWJA23
11,"The given report examines the reproducibilty of a deep denoising model SADNet for natural images based on spatial-adaptive residual blocks. The model was proposed in the paper ""Spatial-Adaptive Network for Single Image Denoising"" by Chang et al. The model was reimplemented by the authors of the report using the residual spatial-adaptive block and the context block from Chang et al.
  
**Reproducibility Summary:**
The report contains a reproducibility summary as required by the template.

**Scope of Reproducibility:**
The authors of the report clearly state the scope of their reproducibility experiments.

**Code:**
The code was provided. The authors reimplemented the training loop and the model and reused some code from the original repository (the residual spatial-adaptive block and the context block). The authors use the dataloader provided with the dataset. Overall, the code is well written but some comments and doc-strings would highly improve the readability. Unfortunately, I was not able to run the code due to CUDA incompatibilities.

**Communication with Original Authors:**
The authors of the report state that no communication with Chang et al. was necessary to reproduce the results.

**Hyperparameter Search:**
The authors use the hyperparameter settings stated in the paper. No further hyperparameter sweeps were performed. Given the long training time of three days this choice seems reasonable. However, the authors changed the initialization scheme to the Kaiming initializer (instead of Xavier uniform initializer). 	    

**Ablation Study:**
No ablation study was carried out.

**Discussion on Results:**
The authors were able to reproduce most of the results from the original paper. The PSNR/SSIM values closely match the values reported by Chang et al. However, visually the results shown in Figure 5 are qualitatively worse than the original results. Also runtimes are compared with the runtime stated in the original work (using different hardware) which is not appropriate. Running the original code on the same hardware would have been a more meaningful way of comparing runtimes.

**Recommendations for Reproducibility:**
No recommendations made.

**Results Beyond the Paper:**
No results beyond the original work reported.

**Overall Organization and Clarity:**
The report is well-written and well-organized. The authors should add references for the first sentence of the introduction. Some small comments on the writing can be found at the end of the review.

**Summary of Review:**
The report at hand successfully reproduced the original paper. The authors are very clear about their experimental setup and the problems that they faced during reproduction. However, there are some minor problems in the discussion. Nonetheless, I recommend to accept the paper to the ML Reproducibility Challenge 2020.

**Minor Remarks:**

- line 90: recieves -> receive
- line 114-115: 1e-4 vs 10^8
- caption Figure 2 and Figure 3: validation dataset should not be capitalized
- Equation (1): the \delta in front of m_i is missing
- Equation (2): set-notation is not appropriate here: \delta p^s and \delta m^s should form a tuple instead of a set. However, this notation is also used in the original work.
- line 144: \sigma = {30, 50, 70}: here \in should be used instead of ""=""
",yiAI9QN9nYt
12,"
> Overall approach to reproduce the results and the adjoining paper are quite clear.

> The reproducibility summary is provided.

> It seemed easy to reproduce the results from the paper although the authors had to resort to default hyperparameters as these were not readily available. Their results still tallied with that of the original paper showing that the model architecture is robust to change in hyperparameter settings. 

> It is not clear if the authors rewrote the original PyTorch code.

> Even though no communication with the original authors was made, some areas to tidy up in the code are discussed by the authors.

",yiAI9QN9nYt
13,"The paper seeks to reproduce the results of the paper titled “VCNet: A Robust Approach to Blind Image Inpainting”. Due to computational constraints, some hyperparameters like the batch size (and consequently others as well) were modified. For the same reason, only a subset of the original training data was used for training the models. Due to the above reasons, the quantitative results are somewhat different from the original paper. Despite such (statistically significant) differences, it seems reasonable to deduce that the core claims of the original paper are found to hold. 

Overall, the reproducibility study is reasonable, and the changes necessitated by the lack of constraints do not substantially take away from the study conducted and reported herein. The study qualitatively substantiates the main claims of the original work and presents a novel ablation study that investigates the robustness aspect of the original model. This study will be useful to the audience of this challenge as well as the wider AI/ ML community interested in the original work. 

I have some concerns regarding the stated scope and the alignment of the study design with the stated scope but perhaps it can be addressed in a revision. Similarly, the discussion on the results and the clarity should be improved. Overall, my recommendation is a borderline acceptance of the paper if the authors address the shortcomings listed below. 
In the following, an evaluation of this paper on the metrics suggested by the RC 2020 challenge is presented:

Reproducibility Summary: 

A 1-page summary is provided and adheres to the style guidelines. The major findings have been reported in the summary.

Scope of Reproducibility: 

The authors have provided a brief and clear summarization of the problem statement and the proposed approach. However, more care should have been taken in stating the questions sought to be answered and aligning the study design to answering them.

(a) Q.1 – mask prediction – is evaluated. (b) Q.2 – quantitative results – instead should be inpainting quality as measured by PSNR and SSIM metrics. (c) Q.3 – whether it is possible to generate realistic inpainting results – is too vague. (d) Q.4 – whether PCN resolves the degrading inpainting performance – is not tested as an independent claim. (e) Q.5 – ablation studies mentioned in the paper – design choices (MPN, PCN ($\rho$), semantic consistency term, etc. is not tested. Instead, different use cases – generalization to unseen noise, raindrop removal, and, face swapping are investigated.

Code: 

The code for the original paper is not publicly available. The authors have implemented the code from scratch in PyTorch. As mentioned in the report, the original paper contains detailed description of the architecture making the process easier but still some parts needed more clarifications.

Communication with original authors: 

The authors mention that even though the original work is quite descriptive they had to reach out to the authors to clarify certain architectural details. The original authors gave access to the private repository and the concerning points were clarified.

Hyperparameter search: 

The authors of the report had limited access to computational resources. All the experiments were conducted on a single RTX 2080Ti. Due to the limited GPU memory, the maximum batch size that could be used for training was 4, which is different from the original proposed work. To compensate for the smaller batch size, the authors have finetuned the learning rate and the number of iterations accordingly. This setting should be useful to audience with similar constraints on the availability of resources.

The report also highlights key details that were crucial to reproducing the results but were missing in the original paper, for example, weight initialization and the architecture details of the discriminator. All the combinations of hyperparameter settings used in the experiments are provided in Table 1 of the report.

Ablation study: 

The authors claim to conduct an ablation study, but I am not sure if it can be called such. Ablation refers to studying the impact of modeling design choices on performance by carefully ablating (removing) parts of the model. Authors instead study the downstream used of the model – (a) unseen noise sources (including raindrops) during test, and, (b) the face swapping task. The first task is only demonstrated qualitatively (like in the original paper) while the authors are unable to replicate the results on the second task. However, neither is the second task properly defined in the original paper nor in this study. 

In addition to not conducting any independent ablation studies, the authors did not even replicate the ablation studies conducted in the original work (MPN, for example).

Discussion on results: 

The authors clearly state aspects that were easy/ difficult to reproduce and the steps they took to address the difficulties. 

This work focused on reproducing the following results (different batch size and hyperparameters; smaller training data):

Mask Prediction: The quantitative results (BCE metric) on the FFHQ dataset are off (worse) by about 10% relative error while the results on the Places2 dataset are off (worse) by about 73%. Qualitative results (examples presented) show that the predicted masks are close to the ground truth masks. This is not reflected in the quantitative evaluation and is not adequately explained.

Inpainting quality: The results on the FFHQ and Places2 datasets on the PSNR and SSIM metrics have a relative error of (18%, 0.4%) and (4.8%, 1.2%). Since the SSIM metric is almost identical to the original (despite the differences in the training set up), the differences do not seem to be perceptually relevant, and it can be reasonably claimed that the performance from the original work was reproduced. 

It is unclear if the above differences are due to insufficient hyperparameter tuning or due to the limited data used for training.

Recommendations for reproducibility:  

none

Overall clarity and organization: 

The report's overall clarity and structure is adequate. There are a few typos and awkward grammatical constructs that can be improved. The report should be carefully checked for such errors (e.g., typos in line 81 and 86; the mask is denoted using M not N).

Readability can be improved further by incorporating the more important equations in the methodology section.  In line 127 the authors claim that the original paper does not mention about alpha blending which is not true (refer page 5 of the original paper). The report should be carefully checked for syntactic error (typos in line 81 and 86; the mask is denoted using M not N).
",RKAKTnLzb8q
14,"The given report examines the reproducibility of VCNet model for blind image inpainting of natural images. The model was proposed in the paper ""VCNet: A Robust Approach to Blind Image Inpainting"" by Wang et al. and it consists of two parts: a mask prediction network that tries to classify the input image into corrupted and non-corrupted regions and an inpainting network that performes the reconstruction. Furthermore a discriminator network is used for training with an adversarial loss.

**Reproducibility Summary:**
The report contains a reproducibility summary as required by the template.

**Scope of Reproducibility:**
The authors of the report clearly state the scope of their reproducibility experiments.

**Code:**
The model was implemented from scratch and the code was submitted. It is very well written but some doc-strings would highly improve the readability. Although the training weights were provided, I was not able to re-run the model without downloading the very large datasets. A small demo would have been useful to verify the implementation.

**Communication with Original Authors:**
The authors state that they were in contact with the original authors since the beginning of the challenge and were given access to the private repository to double-check their reproduced code.

**Hyperparameter Search:**
Most of the hyperparameters were set to the values reported in the original work are used. However, the authors decreased the batch size and adapted the learning rate due to compuatational limitation.

**Ablation Study:**
The ablation studies from the original work were reproduced. The authors succeeded to reproduce the raindrop removal experiments and the experiments using different noise sources, but fail to reproduce the face-swapping experiments. However, the authors spent a lot of effort to reproduce the face-swapping experiments.

**Discussion on Results:**
The results are discussed in an appropriate way. The report contains visual results as well as quantitative results using the metrics from the original work (binary cross-entropy-loss, PSNR, and SSIM). The authors are sincere that they were not able to reproduce the face-swapping experiments from the paper.

**Recommendations for Reproducibility:**
The authors point out that some implementation details are missing in the original paper (input format, discriminator architecture and weight initialization).

**Results Beyond the Paper:**
No results beyond the paper are reported.

**Overall Organization and Clarity:**
The description of the experiments is well-written and the report is overall well-organized. However, there are some issues in the section that describes the methodology, i.e., Section 3:
1. The authors use GT to denote the ground truth image which can be very miss-leading. I would recommend to use G or O (the latter one was used in the original work). 
2. What is confidence-driven mask smoothing? The term is also not mentioned in the original work. A brief explanation would be useful.
3. In Equation 5: the variables V_{GT}^l and V_O^l are not defined. I suppose they are used to denote some feature maps in the l-th layer? However, this is just a guess.
4.The last two loss-terms L_{mrf} and L_{adv} are not further specified. The original work mentions that the WGAN-GP objective is used as adversarial term and the ID-MRF loss is used as texture consistency term. It would recommend to add these explanations to the report.

**Summary of Review:**
The authors did a great job to reproduce the results from the original work and I recommend to accept the paper to the ML Reproducibility Challenge 2020 after the issues in the above section have been adressed. I will improve my score as soon as these issues are fixed.

**Minor Remarks:**

- Equation (5): period should be placed after the equation (not in front of it)
- l 115: ""with gaining of 0.02"": I am not familiar with the term ""gaining"". Is it used to refer to the standard deviation/variance?
Figure 3 and 5: wrong wording: ""(5) The masks applied confidence-driven smoothing""
",RKAKTnLzb8q
15,"This study attempted to verify the claims of the Reformer paper:
1. LSH based attention achieves higher speed for long sequences: reproduced
2. Reformer has low memory footprint than baseline: reproduced
3. Reversible layers and shared key queries achieve similar performance with baseline: not reproduced
4. LSH based attention achieves similar performance as full attention: reproduced

The authors reported both training and validation losses for 1, 3 and 4. Claim 2 was supported by Figure 7.

The study also pointed out that the reformer may increase training/inference time due to increased rounds of hashing to match the baseline performance.
",3s8Y7dHYkN-
16,"General Remarks

The fact that the authors failed to anonymize the report by mentioning their names and also by referencing a public Github repo.  That had no effect on reviewing their work, so I don't consider it a problem. I think they did overall a good work reproducing the result and providing a practical ablation study, given the scale of the experiments. The authors did follow the template and included the summary page correctly.

Positives

The paper breaks down the scope of the report into verifying 7 claims of the paper. I also liked the glossary that they introduced in 3.1. Overall the authors managed to reproduce many claims. Some claims were not reproduced, because of several engineering technicalities that the original paper introduced.

This was an insightful observation

""In our experimentation we observe that Reformer enables training of deep models with long sequences on a single GPU,
supporting the democratization of machine learning. Practitioners interested in using Reformer should note however
that we also observed a non-negligible increase in time complexity due to the trade-off of memory for compute in the
Reversible Residual Layers and the trade-off of time for increased accuracy as the number of hashes used increases.
This behaviour is also observed elsewhere, e.g. Katharopoulos et al. [2020] and Tay et al. [2020]. ""

Need to be fixed

There are too many footnotes on websites. They should be moved to the references section. It will make reading easier. In fact, it might be more appropriate to create an appendix since the websites contain a lot of information that is important for understanding the report.
The mini Ablation study on how to reproduce table 1 is very useful and points a weakness of the paper.

The 4.2 section is confusing and needs to be rewritten. The authors make a reference to Github. This sentence ""The first half of the sequence is masked from the loss function, so the goal for the model is to learn that midway through the sequence it has to repeat 0w"" doesn't make sense to me.

Table 1 the column names need to be top-aligned


",3s8Y7dHYkN-
17,"
# Reproducibility Summary 
Main summary not clear. Is it the objective of the original paper, or the objective of the reproduction? The rest of the summary is clear.

# Scope of reproducibility
They reproduce results from original paper, and test with additional dimensionality reduction algorithms as well as on additional datasets.

# Code
Was extended with additional DR algorithms and re-implemented in PyTorch. It is not clearly mentioned however whether the algorithms were re-implemented solely based on information found in the paper or if the code has been used as a reference.

# Communication with original authors
No major communication, only to assert the dependencies needed to run the original cod.

# Hyperparameter Search
The hyperparameters are described but there is no mention of hyperparameter optimization.

# Ablation Study
The results are tested on additional DR algorithms. I would consider this as an ablation study since it verifies the effect of one of the components of the whole procedure.

# Discussion on results
The results are well described and compared with original work. The results section is more difficult to read than the rest of the report. The figures should be better described, it is very difficile to make sense of Figure 5 in particular


# Recommendations for reproducibility
None

# Results beyond the paper :
The method is tested with different DR and on 3 additional datasets. The comparison on the additional datasets is interesting because these have no low-dimensionality manifolds and thus should be trickier for the DR methods.

# Overall organization and clarity
The paper is very clear and especially well written with the exception of the results section. The figures lack explanation. See minor comments for grammatical errors.



# Comments

Section 3: Methodology
It should be made clear whether the re-implementation has been attempted with only looking at information available in the paper or if they used the available code-base as a reference. This makes an important difference for the reproduction as looking at the code-base could lead to re-implementation of important tricks that were not mentioned in the paper. 

It would be good to cite the work behind the linear and non-linear models listed in 3.1.1 and 3.1.2.

I don’t understand the role of the digression. I feel it states in different words what has already been said implicitly in the prior sections. 

Footnotes 14 and 15: I was clueless about what I should do to make the comparisons. Ideally these results should be integrated in the paper and presented clearly.

# Minor comments
Figure 1’s caption: I don’t understand the sentence: 'the difference between point 1 and 3 is explained each data point can be seen as an individual cluster. '

Section 3.5: we have ran -> we have run

Section 3.5: All experiments are ran -> All experiments were run

Footnote 13: be only using -> by only using

Section 5.1: set up was the obtaining > set up was obtaining

Section 5.2: Since we are aim -> Since we were aiming",hq3TxQK5cox
18,"### SUMMARY
The paper claims that: 
- The common data exploration workflow (of learning low dimensional representations, identifying features which help examine differences across clusters to determine what they represent as they correlate to an unobserved concept of interest) is treated as an interpretable machine learning problem where:
    - Global Counterfactual Explanations (GCE's) ensure pair-wise explanations for all points within a cluster 
    - Transitive Global Translations (TGT's) generalize the above compressed sensing solution to find the complete set of explanations are both symmetrical and transitive among all groups simultaneously and empirically demonstrate the same with the following datasets: synthetic, UCI (Iris, Boston Housing and Heart Diseases data) as well as single-cell RNA data with adequate correctness and coverage
- TGT's identify explanations that accurately explain models while being relatively sparse and reportedly match underlying patterns in the data.  

The submitted report addresses the above claims as follows:
- _Re-execution_ of existing code along with re-written code variants (upgraded to TF 2.x, Pytorch and without external dependencies such as on scvis) on all the above mentioned datasets establishes correctness, coverage, and sparsity, thus verifying both claims. 
- Additionally, experimentation with other linear and non-linear dimensionality reduction algorithms (truncated SVD, sparse PCA, Gaussian variational autoencoder, kernel PCA, manifold dimensionality reduction algorithms like isomap and local linear embedding) on the following additional datasets -(seeds, Wine and Glass. dataset excluding single-cell RNA) - explored along with dynamic scaling of data in latent spaces for the purpose of achieving a certain amount of variance in order to test applicability to differing data structures, uncovers the following limitations:
    - Constrained variable freedom interferes in manifold mapping where matrix based explanations/explanations beyond translations (such as with rotation/scaling) may be necessary. 
    - Structure of the data produces different type of clusters and hence, structure, shape, method of cluster annotation and variance in the latent space affects algorithmic performance. 
     - Highly non-linear dimensionality reduction algorithms perform worse in terms of explainability (probably due to sparsity).

### MERITS
The additional experimentation is rather impressive and the report reflects an intuitive understanding of concepts such as coverage, correctness, and counterfactual explanations.

### MINOR CORRECTIONS
- In Methodology, ""Exprimentation was done on a Macbook""; **Correction:** ""_Experimentation_ was done on a Macbook""
- In Section 1 - Introduction, for the argument regarding algorithmic decisions that involve grouping of data, the reviewer recommends improving the context of the statement by clarifying settings (for instance,  the original paper discusses naturally arising grouping under the context of encoders or decoders) or by supporting this claim through citations.
- In Section 2 - Scope of Reproducibility, ""do do not use the model""; **Correction:** ""_do_ not use the model""
- In Section 3.1.1 - Linear Methods, ""SPCA reduce the dimentionality in a linear fashion""; **Correction:** ""SPCA reduce the _dimensionality_ in a linear fashion"" 
- In Section 3.1.2 - Non-Linear Methods , ""While Isomap could be seen as a extension of KPCA, LLE can be understood as a combination of PCAs ran on local neighborhoods.""; **Correction:** ""While Isomap could be seen as _an_ extension of KPCA, LLE can be understood as a combination of PCAs _run_ on local neighborhoods""
- In Section 3.3 - Datasets, ""sigmoidial Kernel-PCA procedure""; **Correction:** ""_sigmoidal_ Kernel-PCA procedure""
- In Section 3.5 - Experimental setup and computational requirements, ""Exprimentation was done on a Macbook""; **Correction:** ""_Experimentation_ was done on a Macbook""
- In Section 5 - Discussion, ""latent space will yield different different mappings""; **Correction:** ""latent space will yield _different_ mappings or inconsistently different mappings""
- In Section 5.1 - What was easy, ""set up was the obtaining the right version""; **Correction:** ""set up was _obtaining_ the right version""
- Optionally, formatting of references can be enhanced: Explaining groups of points in low-dimensional representations -> Explaining Groups of Points in Low-Dimensional Representations.
- In general, the reviewer is of the opinion that the report could be structured in a more organized fashion:
    - In Section 5.1 - What was easy, it's recommended to move the discussions pertaining to ""the hardest part"" to Section 5.2.
    - Reduce the redundancy within the paper. For example, footnote 3 and footnote 1 essentially the same. 
    - In Figure 1, it might be helpful to reuse the author's notations of ````````````` $R_{initial}$, $R_{target}$ representations and  $X_{initial}$, $X_{target}$ preimages etc. 
    - For consistency, move all links to footnote. (For example: link in Section 3.5)
    - Reduce the back and forth between the report and paper. (For example: restate equation 9 in Section 3.4)

### RECOMMENDATIONS
- Please anonymize your submission. Also, note that your summary **must fit** in the first page. 
- The original authors of the paper consider a **similarity metric** across explanations which has not been examined in this reproducibility report. Kindly address the same.
- Optionally, you could consider discussing the best set of hyperparameters from the experiments that resulted in reported results. 
- Optionally, you could also demonstrate causal structures of the data and the inability of the [Plumb et al model](https://openreview.net/forum?id=MFj70_2-eY1) to capture the same (specifically with differently structured data, higher dimensional data or varying cluster variance, shape etc) to further strengthen your argument regarding shortcomings - similar to Table 1 of original paper. 
- As per the [Machine Learning Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf), you could include statistics of the datasets in a tabular form and a tabulation of important results in the README file on [your github repository](https://github.com/giguru/fact-ai).
- As far as the creative insight on compressions is concerned, this space has been fairly explored before, [even in the context of compressed sensing](https://ermongroup.github.io/blog/uae/). I hence recommend moving the discussion to applications or perhaps reviewing this discussion in context of a specific dataset or application like, in the case of [complementary biological representations](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02021-3). 
- For explanations beyond translations (x′=Mx+δ), the reviewer *appreciates the effort taken in this new direction* but critically, the argument of ""incorporating rotation as an explanation that comes at the cost of explainability"" sounds delicate. 
",hq3TxQK5cox
19,"The authors have chosen to reproduce results related to 'Rigging the Lottery: Making All Tickets Winners'. The code was based on that of the authors of the original paper. Results are consistent with the original ones for similar testbenches. However, tuning hyperprameters turned out to be challenging.

The reproducibility report is very well written, the problem is first formulated, the methodology is clearly presented, and the results are well described. The authors also indicate communications with those of the original paper, which is good to see.",riCIeP6LzEE
20,"Overall, this report is of very high quality and impact. The authors reproduce the original paper from scratch. The authors perform hyperparam sensitivity study. The authors also perform extensive ablation experiments and uncover an interesting finding on the dependency of initialization. This project checks all components needed for a good reproducibility report, and I believe this is worthy of a journal submission. I would like to thank the authors for their hard work!

* Reproducibility Summary

  The report contains a well-written, concrete reproducibility summary. The summary outlines the scope of the paper to reproduce the RigL algorithm from scratch by re-implementing the methodology on PyTorch. The summary also concisely highlights the major findings, where the report gets within 0.1% of the reported values in the original paper on CIFAR10.

* Scope of reproducibility

  The report investigates several central claims from the original paper, Riga. The report contains an investigation of the sensitivity of hyperparameters, model ablation, and choice of initialization too.

* Code: whether reproduced from scratch or re-used author repository.

  Authors re-implement the code of RigL from scratch in Pytorch, which was original written in Tensorflow. This makes the report extremely strong, as it helps to robustly validate the core claims of the original paper. The authors provide their code in the supplementary material. It is also very much appreciated that the authors plan to release the training plots, which would be a strong contribution towards the understanding of RigL.

* Communication with original authors

  Authors of the report communicated with the original authors successfully, who helped the authors clarify implementation and evaluation details.

* Hyperparameter Search

  Authors tune the hyperparameters with Optuna and carefully examine the impact of each hyperparam chosen in the original paper.

* Ablation Study

  The report goes a step further to perform an ablation study to investigate the impact of ERK initialization for a given target parameter count and training budget. The paper also performs experiments on redistribution, which is shown to help RigL with random initialization but not ERK. This is a very interesting finding!

* Discussion on results

  The report contains ample discussion on the results. Primarily, they find the central claim of the original paper holds true. RigL is also found to be fairly robust to the choice of hyperparameters. The report finds further evidence that the choice of initialization has a much greater impact on the final performance, and proposes interesting future directions for research.

* Recommendations for reproducibility

  The authors highly commend the original paper on their state of reproducibility and thank the original paper authors for their communication.

* Overall organization and clarity

  The paper is well organized and well written.",riCIeP6LzEE
21,This paper can be accepted without any modifications. ,UkIQrHoru_J
22,"# Reproducibility Summary

The summary is provided, although given that the results indicate ALBERT's findings were not reproduced, I think that the ""what was difficult"" section could have been written with more details. In other words, if the findings of the paper are refuted there should be substantial effort demonstrating good faith reimplementation and/or reasons why the original findings are wrong. 

It's also unclear whether the original findings were factually incorrect — they reported an incorrect number — or were overreaching in their claims.

# Scope of Reproducibility

The scope is two-fold:
- Reproduce the ALBERT model from a pre-trained checkpoint.
- Compare ALBERT performance to other baselines using more in-depth analysis.

# Communication

No communication was mentioned. EDIT: The reproduction authors do explain why they chose not to communicate.

# Hyperparameter Search

I think this section could use substantial improvement. Fine-tuning tends to require very careful hyperparameter choice in order to prevent issues such as ""catastrophic forgetting"". Also, it seems the authors use off-the-shelf fine-tuned models and did not perform their own fine-tuning. This is fine, but much more due diligence should be done to explain how the checkpoints they use were created.

# Ablation Study

The reproduction authors did not do any re-training with different hyperparameters or other ablation. Although they did run a new set of evaluation.

# Discussion

I think this paper would benefit from richer discussion. Specifically, it would be interesting to explain in detail why the evaluation methods used were chosen.

# Recommendations

The reproduction authors did not provide a recommendation to the original authors.

# Results beyond the paper

The reproduction authors were ambitious and set out to report many results and findings not in the original work. The line of inquiry is interesting — perhaps leaderboards like GLUE do not reflect all important model properties, such as fairness and robustness.

# Overall Organization and Clarity

Several parts of the paper were not clear. Even if this is a reproducibility paper, the authors should still summarize in detail the paper they are reproducing and any other techniques (such as TextAttack). This would help, but other parts of the writing were a bit ambiguous. For instance:

> However, ALBERT (10min 30s) took noticeably longer to produce the embeddings compared to BERT (8min 49s).

Does this mean that UMAP took 10m30s, or that ALBERT took 10m30s to output the embeddings used for visualization?

In addition, it would be helpful for the reader if the plots and figure were embedded more naturally in the text. It can be jarring when a crucial table takes a full page, and perhaps it could be broken into smaller sections and distributed throughout the text adjacent to its mentions.
",UkIQrHoru_J
23,"This paper reported on a comparative study of BERT and ALBERT for self-supervised learning in the context of language representations, concluding that the authors were unable to reproduce empirical evidence for the claims presented by the selected paper. However, in their view ""communication with the original authors did not seem appropriate, as this study does not seek to fully re-implement the original paper."" I find that this is an unfair approach to the selected paper, as in order to act politically and respectfully among our scholarly community, contacting the first/corresponding author would have been the right first action here. As concluded in https://www.aclweb.org/anthology/W16-6110.pdf , https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5998676/ , and https://www.aclweb.org/anthology/R19-1089.pdf, for, successful reproducibility in natural language processing requires more than having access to data and code, and in general, contacting the authors leads to obtaining the same main findings.",UkIQrHoru_J
24,"Authors replicated the work of Saxena et al. [2020] on multi-hop question answering over knowledge graphs using KB embeddings. Glad to see the core work was reproducible. The added experiments on the use of other recent KB embeddings and use of transformer architectures for question embeddings were helpful to highlight the impact of other methods on the overall framework. The absence of sufficient documentation and unavailability of hyperparameter values made the task difficult. The relation matching experiment was not possible to be replicated due to similar issues. Overall, the work appeared to be solid and would be beneficial to the community.   ",VFAwCMdWY7
25,"The report reran the open-sourced codes from the original paper and managed to reproduce most of the results presented in the original paper. The authors also experimented with different question encoder and achieved an improvement over the original results.

The authors also cleaned up the codes, added comments, and provided command line functions. 

The paper also reported hardware requirement and experiment run time.

It's glad to see that the authors try additional models beyond the original implementation. However, the authors mostly reused the default hyper-parameters, as well as the open-sourced codes from the original paper. This challenge recommends authors to either re-implement the original codes, or conduct hyper-parameter sweep. This paper failed to follow the instructions carefully.  

To improve the report:
Could you please provide more details about the ""relation matching"" module?  It's claimed that this is the key issue that causes 22.5% drop in performance in MetaQA KG-Full 3 hop questions, but it's not discussed in the report at all. I think briefly introducing this module will make this report stand alone and help reader understand why it causes a huge drop in performance.

",VFAwCMdWY7
26,"In this report, the authors investigate the ACL 2020 paper by Saxena et al. on knowledge base embeddings. 

Reproducibility Summary : Major findings are included in the summary.

Scope of reproducibility - Scope was clearly delineated and adhered to.

Code: Reused author repository, but made some changes to improve the code (enhance modularity) and make it easy to swap in pertained models for the question and knowledge graph embedding modules.

Communication with original authors: There was communication with the first author of the paper (virtual meetings). Efforts were made to try to address the parts of the paper that could not be reproduced, though the relation matching part of the code wasn't used which plays a part in the gap.

Hyperparameter Search: No hyper parameters sweep. Used default values. However they did tune hyper-params in their extended experiments. Things were especially difficult as little information about hyperparam values was provided in the paper and by the authors.

Ablation Study: Did do ablations over substituting the knowledge graph and question embedding modules.

Discussion on results: Descriptions on what was easy and difficult were provided. The scope of the reproduction was to reproduce results in the original paper which they were able to get close on one dataset (MetaQA) but remain far apart on the second (WebQSP) dataset, 11.7 percent absolute despite correspondence with the authors (although the reason is lack of relation matching)

The authors did extra work to test different knowledge graph embedding models and different transformer models for question embeddings. They found improvements by using TuckER and Sentence Bert respectively.

Overall organization and clarity: Paper is clear and organized and easy to read. It feels complete to me as well, showing what can be reproduced and what cannot - although more discussions on where the divergence for QebQSP could've been provided and the experiments without the relation matching make it unclear how close reproduction was performed for WebQSP.",VFAwCMdWY7
27,"The submission did a good job in communicating with the authors, and because of that it has identified a missing component (a JSON meta-data generator) and a discrepancy between code and paper on the neural network design. This enabled the submission to reproduce the running pipeline of the model mostly based on the original authors' public code.

However, the submission did not report any numerical results that can be used to judge the reproducibility of the original paper. It only quantitatively states that the produced results seem good with high confidence scores. The purpose of the reproducibility is to verify such numerical results, therefore the submission is a clear rejection.

That said, I believe if given more time, the submission could become a good reproduction of the original paper with clear numerical results, and perhaps even some additional ablation study.",4tyWL6P08yY
28,"This reproducibility report is about TabStructNet which was published in ECCV 2020. 

Authors of this report provide summary of report, scope of reproducibility and communicated with original author of the TabStructNet.


Basically, there are available codes from the original authors, so this report tried to discover missing points in the original paper. Also, they experimented with missing modules such as the post-processing module, JSON generation module, and FPN. Therefore, it is important to perform plenty of “hyperparameter search” and “ablation” to analyze them.

However, ablation studies and hyperparameter search are very limited in this report. Also, discussion on results is not enough. They just provide several examples and short conclusions.


Line4, there is a typo: EECV 2020 → ECCV 2020
",4tyWL6P08yY
29,"The paper reproduces TabStructNet, an approach to automatically translate tables to XML format. The authors of the submission make use the provided code repository of the original authors and document the required changes and extensions to get the code running as well as available divergences of the implementation with respect to the paper.

However, the paper does not reproduce a representative set of results of the original paper, as only exemplary inputs and outputs of the model are presented. While it provides useful information to the reader, as the available code repository is analysed in terms of functional reusability, the paper misses to analyse the quality of the implementation on original or representative datasets with original metrics. There is also no ablation study present in the submission. ",4tyWL6P08yY
30,"The authors have chosen to reproduce results related to Re-Hamiltonian Generative Networks. The code was built from scratch in Pytorch, based on the description of the original paper. Results are consistent with the original ones for similar testbenches, but show sub-optimal behavior on new data. It is also found that results are highly dependent on hyper-parameter tuning.

The reproducibility report is very well written, the problem is first formulated, the methodology is clearly presented, and the results are well described. The authors also indicate communications with those of the original paper, which is good to see.",Zszk4rXgesL
31,"The report clearly summarizes the problem statement of the original paper ""Hamiltonian Generative Networks"" (HGN) as scope of reproducibility: learn a Hamiltonian dynamics from a image sequence. As no source code was available, the authors reimplemented the program for the experiments from scratch based on the description in the original paper and questions about implementation details answered by the original authors.

The code is available on GitHub as a Python program based on Pytorch. It is properly documented and cleanly written. All dynamical systems from the original paper have been implemented and the experiments replicated. Results are comparable with the original paper in autoencoder mode, where training happens with the full input sequence. But using only the first five frames for training the HGN leads to significantly worse results for two systems (mass-spring and pendulum), but not for the other two (two-body and three-body). This difference is not discussed, only the average over all four systems. I suspect that this is caused by the choice of the Lagrage multiplier, as the authors did not obtain sufficient information about the automatic optimization of this hyperparameter.

Additionally, the authors present successful results using other integrators and extra dynamical systems. They also show that the calculation of the derivatives for the Hamiltonian equations of motion by backpropagation can be replaced by a network which learns them directly.

The discussion clearly indicates the state of reproducibility and summarizes the easy and difficult parts of that task. There were no explicit recommendations for better reproducibility, but the authors describe, what information missing in the original paper could have been helpful.

The reproducibility report is well written. I recommend to add a discussion about the differences of reproducibility between the dynamical systems and to make an explicit recommendation to the original authors for improving reproducibility.",Zszk4rXgesL
32,"Arguably, learning how physical systems work is one of the key unsolved problems in machine learning. The present report investigates an attempt to solve such a problem with significant results but one that doesn't reveal a lot about it's inner workings. The author(s) do a great job in reproducing, replicating, and presenting the challenges. The report also communicates the core problem very well and their contact with the authors of the original paper is transparent. 

What would improve the paper:
- testing hyperparameter sensitivity would be useful. using exactly as the ones indicated is good for replication but not enough to understand the scope of the sensitivity of the model. 
- testing extra environments is a great addition, and I think it would also benefit a lot from understanding failing cases a bit deeper. 
It seems the authors are aware of possible improvements and some of them can be standalone work by themselves.

====

trivial typos:
line 45 an —> and
line 214 or —> our",Zszk4rXgesL
33,"The reproducibility report of the Double Hard Debias:Tailoring Word Embeddings for Gender
Bias Mitigation (Wang et al) clearly identifies the three claims made by the author and sets the motivation to reproduce these claims.  The methodology used is also clearly outlined and there was communication with the original authors to ensure that experiments were performed without deviation from the original. The results obtained on reproduction match two of the three claims made by the original authors. The reproducibility reports also attempts to find reasons why one experiment did not have identical results reported on the original papers but these hypotheses were not validated.",FoazbSYjXoc
34,"Authors have presented a well-organized reproducibility report. Scope of reproducibility was categorized into clear and independent claims, which were representative of and covers the extent and novelty of the original source paper being reproduced. 

To verify the original results, claims were further mapped to the corresponding qualitative and quantitative findings of the source work, thereby making the subsequent sections easy to follow. Not all the results in the source paper has been attempted to be replicated, however, the reasons to exclude them have also been explicitly mentioned (either computational constraint or the results substantiating the claims which are also corroborated by other presented results).

Authors have used the source code made public by the authors of the original paper. While the codes were modified to fit in a pipeline to run them, as mentioned by the authors, they also expanded the set of WEAT scores to cover all the metrics reported in the source paper. Communication with original authors were also established, particularly regarding the deviations of results from the source paper. Although the differences could not be resolved, discussions on reasons for the same is also briefly included along with the observation that the results qualitatively holds, albeit to a lesser extent. For example, substantiating claim 2, the gender classification scores reported in Table 2 of the reproducibility report for top K gendered words were observed to be less when compared with Glove embeddings but were still greater than the reported values in the original work. Similarly, to substantiate claim 3, original and reproduced performances on word analogy and concept categorization benchmark datasets are presented in Table 3 of the reproducibility report and small differences are observed on concept categorization dataset.

Although it is mentioned as well as noted that the results specific to the claims in the scope of reproducibility will be reported, it is observed that Figure 2 of the original work pertaining to claim 1 in the reproducibility report regarding identification of word-frequency direction(s) among word embeddings' principal components is not included in the report. 

Of further note is the omission of reproducibility of scores and results pertaining to Hard (Debiased) Glove embeddings in tabular results i.e. Table 1,2 and 3. This is in contrast with the outlined scope of reproducibility where it was explicitly mentioned that the current work's focus is on comparing the Double-Hard Debiased embeddings against the Hard Debiased embeddings. Thus, this doesn't necessarily reproduces the results qualitatively, particularly when there are significant differences. For example, claim 2 in the reproducibility report is regarding reduced gender-bias in moving from Hard Debias to Double-Hard Debias and the same is also claimed in the original paper. In the original paper, this is demonstrated through clustering accuracy/gender classification scores. In the reproducibility report, while Double-Hard Debias results have been reported and are found to be worse when compared to source paper, reporting the Hard Debias results would have substantiated or refuted the gender bias reduction in qualitative sense (although still differing) when moving towards Double-Hard Debiasing.

Furthermore, the results, associated discussions particularly when calling out observed discrepancies/differences needs to be highlighted properly. For example in Table 3 in the reproducibility report, while differences in AP and Battig categorization tests have been highlighted and reported, differences for other tests, some of which are even more pronounced, are still marked as matching.

Finally, discussion on results and difficulty in reproducibility can be made more thorough, specifically, set of choices that could be made and lead to difference in results. For example, for results pertaining to Table 3 where classification scores on top K gendered words are reported, the original work mentions identifying such words based on cosine similarity with gender direction in the original GloVe embedding space. Similarly, it is mentioned that random seeds could also be one of the factors causing differences in the results. Discussion and documentation on the gender direction used, range of random seeds used (given that experiments in the scope were not too computationally expensive) are further needed to cover the extent of reproducibility.",FoazbSYjXoc
35,"In this report, the authors investigate the ACL 2020 paper by Saxena et al. on knowledge base embeddings. 

Reproducibility Summary : Major findings are included in the summary.

Scope of reproducibility - Scope was clearly delineated and adhered to.

Code: Reused author repository, but made some changes to improve the code (enhance modularity) and make it easy to swap in pertained models for the question-answer module.

Communication with original authors: There was communication with the first author of the paper (virtual meetings). Efforts were made to try to address the parts of the paper that could not be reproduced, though the relation matching part of the code wasn't used which plays a part in the gap.

Hyperparameter Search: No hyper parameters sweep. Used default values. However they did tune hyper-params in their extended experiments. Things were especially difficult as little information about hyperparam values was provided in the paper and by the authors.

Ablation Study: No ablations

Discussion on results: Descriptions on what was easy and difficult were provided. The scope of the reproduction was to reproduce results in the original paper which they were able to get close on one dataset (MetaQA) but remain far apart on the second (WebQSP) dataset, 11.7 percent absolute despite correspondence with the authors (although the reason is lack of relation matching due 

The authors did extra work to test different knowledge graph embedding models and different transformer models for question embeddings. They found improvements by using TuckER and Sentence Bert resprecively.

Overall organization and clarity: Paper is clear and organized and easy to read. It feels complete to me as well, showing what can be reproduced and what cannot - although more discussions on where the divergence for QebQSP could've been provided and the experiments without the relation matching make it unclear how close reproduction was performed for WebQSP.",FoazbSYjXoc
36,"
- Reproducibility Summary: the paper includes the reproducibility summary on the first page. The summary is clear, well written, and major findings (i.e. successful replication within a good margin) are incorporated in the summary.

- Scope of reproducibility: it is well described: specifically the authors investigate the roles of different BERT layers in the context of Reading Comprehension based Question Answering (RCQA). The authors list all the reproduced/verified claims from the original paper.

- Code: the authors re-implement the original approach since the official code is not available. The code folder contains a well-documented readme file and requirements are specified.

- Communication with original authors: the authors had email interactions with the original authors. The questions and answers are included in the supplementary material.

- Hyperparameter Search: the original authors did not provide any code, thus the authors could not re-use it for hyperparameter search. The authors report the selected hyperparameters, but they do not mention whether they did a grid search on the hyperparameter space.

- Ablation Study: there is no ablation study, but it does not make sense to have it.

- Discussion on results: the authors obtain F1 scores that are comparable to those in the original paper, but their model slightly underperforms with respect to the original one. The authors do not mention why the results are different and what are the plausible causes. Apart from this, the experimental results are well presented and compared with the original results. The authors describe easy parts and challenges in reproducing the original paper.

- Recommendations for reproducibility: the authors do not explicitly mention any recommendations for the original authors, but they do describe missing details that were important for the reproducibility.

- Results beyond the paper: the authors present some results beyond the original paper: they implemented Integrated Gradients (IG) in a different way and extended the original analysis of Jensen-Shannon Divergence heatmaps with different cut-offs.

- Overall organization and clarity: the paper is clear and well organized. I list some typos in the following:
    - Line 45: t-SNE -> please expand the acronym at least when first mentioned;
    - Line 81: don’t -> do not
    - Line 149: we represent show

Overall evaluation:
Pros:
- The paper is clear and well written;
- The authors could reproduce (within a decent margin) the results of the original paper;
- The authors performed some extra analyses beyond the original paper.

Cons:
- Some experimental results could have been explained with more details.
",LI1n_od-aEq
37,"The reproduced work of ""towards interpreting BERT for reading comprehension QA"" is well organized and clearly explained. The authors rebuilt the experiment from scratch due to the availability of the original paper code. The implementation and scope align with the original paper. A large part of the results matched, and the difference was explained which was mostly due to computation limit and sample difference. Overall, I'd recommend accepting.",LI1n_od-aEq
38,"Thank you for this carefully written paper that focused on  interpreting BERT for reading comprehension questions and answers (Q&A). It was successful in reproducing the main findings of its chosen paper, without an originally released codebase to build on. Obtaining this outcome, required, as expected regular interaction with the first author of the chosen paper. My major comments or suggested improvements relate to documenting the  reading comprehension Q&A; the authors excelled in reporting on the solution (processing methods, evaluation methods, processing outcomes), and even had findings beyond the chosen paper. However, the problem of  reading comprehension Q&A, its relevant literature, and implications to reading comprehension QA practice were not addressed. More over, I would have expected to see a statement relating human subject ethics in the paper, although its data originated from a previously openly released corpus. This contradiction between the (currently weak) domain substance and (currently outstanding) computing contributions made me rate the paper as Marginally above acceptance threshold.",LI1n_od-aEq
39,"The paper is a valuable check of the original paper.
A few corrections in formatting and language are needed.

span should not be italic, but rather {\rm span

A blank is missing in „fit(more“

„The number of experiments performed where realtively [sic] less“ -> „The number of experiments was relatively small“

„table[2]“ -> „Table 2“

„approximators.[table[5]]“ -> „approximators (see Table 5).“

I think, „5e-5“ should be replaced by 5\cdot 10^{-5}
",IU5y7hIIZqS
40,"Reproducibility Summary : The report includes this summary as the first page, which contains major findings. 
Scope of reproducibility: The report concisely and clearly states the scope, and follows it. 
Code: As mentioned in the report that the original code was closed sourced, the authors reproduced the code for several sets of experiments. 
Communication with original authors: The authors of this report contacted the original authors for multiple queries, and the original authors replied to these queries. However, I am not sure whether the original authors have evaluated the results in this report. 
Hyperparameter Search: It seems that the authors of the report mostly used from the hyperparameters in the original paper.
Ablation Study: The ablation study is not comprehensive. 
Discussion on results: The report discusses the state of reproducibility of the original paper, and mentions the easy parts and difficult parts. More specifically, for the case with linear function approximation, several results in the original paper could not be reproduced.
Recommendations for reproducibility: It seems the report does not discuss on how the original paper can improve its reproducibility. 
Results beyond the paper: The report tries to show the results in Table [5] using a different evaluation technique, but it seems that the new results are not complete.
Overall organization and clarity: The organization is ok, but there are several grammatical issues, e.g., several periods are missing.",IU5y7hIIZqS
41,"**Reproducibility summary:** The summary is overall clear and well summarizes the authors' reproducibility effort. 

**Scope of reproducibility:** The authors state four concrete claims they aim to verify. However, Claim 2 (""The deliberately texture biased model g also reduces cross bias"") was confusingly stated, as *g* is a ""biased"" model from a bias characterizing model class, that is designed to suffer from more bias. I believe the authors meant to say ""using *g* to train *f* helps reduce cross bias.""

**Code:** The authors used the original authors' code with minor modifications. The authors made their version of the code available with appropriate documentation.

**Communication with the original authors**: The authors communicated with the original authors and received clarifications on the HEX, RuBi, and Learned-Mixin implementations, and WNIDs for constructing the 9-Class ImageNet dataset. However, I wish the authors would have done additional communication with the original authors on the following fronts: (1) Even after receiving clarifications on HEX and Learned-Mixin, the authors write that they were unable to compare them to ReBias. It would have been nice if the authors did further communications with the original authors to get HEX and Learned-Mixin (as well as StylisedImageNet, another work compared in the paper although the authors do not discuss it) working. (2) The authors discuss that they were unable to train ReBias on 9-Class ImageNet. As this is the main dataset of the paper, I wish the authors had reached out to the original authors to resolve the difficulties.

**Hyperparameter search:** The authors didn't conduct a hyperparameter search. However, they were forced to reduce the batch size from 128 to 16 due to their limited computational resources, and tried other learning rates and step sizes to counteract the smaller batch size.

**Ablation study**: The authors didn't conduct any ablation studies.

**Discussion on results:** On Biased MNIST, I'm confused by the way the authors presented their results because their results are not within 1% of the original results in the paper. I suggest the authors to double check this claim and include a side-by-side comparison with the original results. On 9-Class ImageNet, the authors state that they succesfully trained the Vanilla and Biased model, while they failed to train the ReBias model due to exploding gradients. Still I wish the authors had included their results in the report so that the readers can get a sense of how different the results are. 

**Recommendations for reproducibility:** The authors don't provide explicit recommendations to the original authors. However, their descriptions of the difficulties they ran into (e.g. insufficient explanation of the implementation of prior works, insufficient explanation of the construction of 9-Class ImageNet, gradient explosion problem) may help the original authors improve the reproducibility of the work.

**Results beyond the paper**: For Biased MNIST, the authors conduct edadditional experiments with $\rho=0.98, 0.95, 0.9, 0.85$ and make an interesting observation that the Vanilla model beats ReBias for lower $\rho$, providing an enlarged picture. Furthermore, they posed two very interesting questions (bottom of page 6) although the authors did not attempt to answer them.

**Overall organization and clarity:** Overall, the report was organized and clearly written. However, there are several typos and grammatical errors that I hope the authors address in their revision.

**Minor comments:** (1) Figure 1 was helpful in understanding $\rho$ in the Biased MNIST experimental setup. (2) Tables 1-2 are unnecessary as Tables 3–4 subsume them. (3) Figure 2 was not very helpful in understanding the change in Vanilla/ReBias performance because the two lines are almost always on top of each other. (4) The authors made a typo in the learning objective for g in Algorithms 1, 3, and 4. 

**Summary**: Overall, the authors made good effort to reproduce the original work and provided additional insights which I appreciate. The authors successfully reproduced Biased MNIST results and failed to reproduce 9-Class ImageNet results. However, because they do not report results for 9-Class ImageNet, which is the main dataset studied in the original paper, I found the evidence provided in this report insufficient for assessing the reproducibility of the original work. For the assessment, I suggest the authors to communicate with the original authors to resolve the difficulties with 9-Class ImageNet and/or try to reproduce action recognition results.",0Z5rHjU2mfX
42,"*Problem statement: 
The paper clearly states the reproducing details, together with the detailed results and analysis.

*Presentation: 
The paper is well-organized and well-written.

*Communication with original authors:
The authors had some communication with the original authors.

*Code: 
The code is available on GitHub and can be reproduced. 

*Recommendations for reproducibility:
The authors provided useful comments for reproducing the original paper. I have read the code and found those comments are consistent with the provided codes.

*A few concerns 
**Some minor typos:
** It will be even better if the authors can provide a simple illustration on the two major algorithms. The illustration would help others to follow this report. 
** It will be better if the authors can provide a detailed readme or other instructions for the code. Currently, although the codes are well-organized, it still requires much time for users to debug some deatils. 
** Some minor typos: Section 5.1.2, present -> presented",0Z5rHjU2mfX
43,"This paper aims to reproduce the results of (Pang et.al,19) which present Max-Mahalanobis center (MMC) loss to defending the adversarial attack.

I appreciate the empirical efforts of the authors. However, investigating the adversarial robustness does not only require one to reproduce the number of the original paper but also needs more thinking of using a stronger attack to give a 'true evaluation' of the proposed method.

As clearly stated in https://arxiv.org/pdf/2002.08347.pdf,  Pang et.al,19 doesn't succeed in defending stronger adversarial attacks. Then I give a clear reject.",67Q9tnozPe
44,"Summary: The summary is clear and highlights the major results of the reproduction.
Scope of Reproducibility: clearly stated and adhered to
Code: re-used author’s code
Communications with Original Authors: Done fairly
Hyper-parameter search: code reuses the author’s hyper-parameters 
Ablation study: looks at adversarial trading and optimizer selection
Discussion on results: relatively little discussion about difficulties of reproduction, but does describe difficulties of reproduction
Recommendations: none given
Results beyond paper: additional implementation of methods in Python, some initial results validating MMC’s ability to train models that reject out-of-distribution inputs. However, the experiment is only on a single image, so does not provide much value
Overall organization and clarity: the paper has a lot of clarity and grammatical issues. I’ve documented a few below. I also recommend substantially updating the figure captions so that the figures are relatively self-contained and comprehensive.

2.2 I found this section and the notation presented quite hard to follow. I recommend looking at what math is actually needed to set up remarks 2-4 and then providing more explicit definitions of those terms. For example, l.89 references $N_{k, \bar{k}}$ before the definition. Consider dropping the $D_{k, \bar{k}}$ notation entirely and directly defining the term in l.89 (e.g., “is proportional to the number of points in class k when $\bar{k}$ has the highest prediction amongst other classes. Call this $N_{k, \bar{k}}$”).

2.3 I think this would be clearer if the way to compute the centers (and some of the intuition) were discussed first. For example, before l.105,  consider including some of the content of remark 5.

Minor comments:
L.5 define SCE before using an acronym
L.59 “we then present demerits of MMC loss” — > “we then present the merits of the MMC losses”
L.78 This section has several clarity issues. In particular, consider providing more context on how l.75-77 leads to l.78 as a conclusion.
L.94 “function in during the training procedure” — non-grammatical
L.97 “tend to spread over the space in an sparsely” — non-grammatical
L.127 non-grammatical (NG)
L.131 “also not losing out the high accuracies” — NG
L.133 “supervisor inappropriate supervisory signals” —NG
L.135 “this section roughly” — seems that this comes from the template?
L.201 “we validate the merits of MMC, that is” — run-on sentence
",67Q9tnozPe
45,"The authors attempted to reproduce the results from the paper ""Contextualizing Hate Speech Classifiers with Post-hoc Explanation"" by Kennedy et al. 2020.

+ The authors attempted to reproduce several claims from the original paper. 
+ they address the fact that their computational set up was different and in fact slower than that reported in the original paper
+The authors provide an explanation about the models in Section 3 before describing their reproducibility experiments. 

- It was not clear if or how the authors reproduced the train/val/test sets from the original paper. In one case, they state that ""Train and test parts were arbitrarily produced for Stormfront sentences"" and similarly for the other datasets. 
- In addition, while the original paper reports performance measures along with confidence intervals (e.g. 57.76 ± 3.9 for precision), the current paper reports the performance measure. So for the claims that are stated as not reproducible  it was not clear whether the performance measures were within the confidence intervals.

- minor issues: in several places the ""instruction text"" seems to mistakenly remain. e.g. lines 145 -147. ""Provide information on computational requirements for each of your experiments. For example, the number of
147 CPU/GPU hours and memory requirements. You’ll need to think about this ahead of time, and write your code in a
148 way that captures this information so you can later add it to this section. ""
- some minor typos in the paper that should be corrected prior to acceptance. e.g. ""Sromfront"" line 166.


",wIgGMxXAYS
46,"The authors evaluate both the method and validity of the results in their reproduction of Kennedy et al (2020). Results of  some of the experiments could be replicated. But some others did not yield comparable results with the original paper.

The authors run the method on a new dataset as well. However, it is not clear how a baseline method would perform on this new dataset.

The code is not provided! “The code was easy to run and  allowed us to verify the correctness of our re-implementation.” But all of your experiments on the code provided by Kennedy et al (2020) (this one stated in the paper as well). If you are not using or providing your code, why do you say this? How can we verify this point? 

There are many copy paste from the original paper, e.g. “We chose two public corpora for our experiments which feature the logical parts of hate speech, versus only the use of  slurs and explicitly hostile language …”.  This should be paraphrased or just summarized much better. Changing some words with their synonyms is neither paraphrasing nor summarization. In the example sentence, the word “we” causes confusion about what you and original authors do. The other example in this line is the aforementioned “implementing the code”

Number formatting, e.g., 7896 -> 7,896

Why do you not have the standard deviation for your results in Table 6? It is not clear what is happening in the paragraph you explain Tables 6 and 7. 

“have able to reproduced” -> have able to reproduce
",wIgGMxXAYS
47,"- Reproducibility Summary: the paper includes the reproducibility summary on the first page. The summary is clear, well written, and major findings (i.e. failure in reproducing part of the paper, but successful replication on the rest) are incorporated in the summary.

- Scope of Reproducibility: it is well described, specifically the authors investigate an approach to generate debiased embeddings by removing the frequency component of word embeddings. The authors list 2 reproduced/verified claims from the original paper.

- Code: the authors tried to re-use the code provided by the original authors. Unfortunately, the code was not well documented and the authors struggled in re-running it. This hampered the reproducibility effort. The code provided with this paper is a revised version of the original code, complemented with more detailed comments and documentation.

- Communication with original authors: the authors did not have any communication with the original authors.

- Hyperparameter Search: the authors do not perform any hyperparameter search, since the reproduced algorithm is a post-training algorithm which does not require to train any parameter.

- Ablation Study: there is no ablation study, but it does not make sense to have it.

- Discussion on results: the experimental results are well presented and compared with the original results. The authors describe easy parts and challenges in reproducing the original paper. Due to difficulties in interpreting the original code the authors could not reproduce part of the experiments.

- Recommendations for reproducibility: the authors do not mention any recommendations for the original authors, but they explicitly list missing details that were important for the reproducibility.

- Results beyond the paper: the authors present some results beyond the original paper: they report a qualitative analysis with some biased words comparing the results before and after debiasing.

- Overall organization and clarity: the paper is clear and well organized. I list some typos in the following:
    - Line 27: weren’t -> were not
    - Line 33: [1] highlight -> use \citet
    - Line 49: doesn’t -> does not
    - Line 71: [4] identified -> use \citet
    - Line 82: tasks for it : -> extra blank space
    - Line 82: cateogorization -> categorization
    - Line 83: Word Analogy : -> extra space
    - Line 88: Concept Categorization : -> extra space
    - Line 114: set of assumption -> set of assumptions
    - Line 115: assumptions : -> extra space
    - Caption of Figure 5: doesn’t -> does not
    - Line 137: the difference is cosine similarity -> something is wrong in this sentence
    - Line 144: doesn’t -> does not


Overall evaluation:
Pros:
- The paper is clear and well written;
- The authors could reproduce some of the results of the original paper;
- The authors provide an elaborate list of details that are missing from the original paper.

Cons:
- The authors could not reproduce part of the experiments (co-reference resolution task) due to the poor quality of the code provided, however they could try to re-implement part of the original approach.
",E4cRWiGoE3
48,"The authors attempted to reproduce the claims in the paper ""Double-Hard Debias: Tailoring Word Embeddings for
Gender Bias Mitigation"" by Wang et al. 2020

+ the reproducibility report is clearly organized.
+ it was clear in the paper how the two main claims of the original article were reproduced.
+ it was also clear how the reproducibility experiments were carried out and that the authors did not introduce any experimenter or measurement bias themselves in the process.
+ it was evident which claim was easier to reproduce and why and what was difficult.

- it was not clear whether the authors attempted to reach out to the original authors to validate some of their assumptions 
- likewise, they mentioned difficulty in understanding/executing code but it was not clear whether the authors were contacted.",E4cRWiGoE3
49,"Authors worked on replicating the paper by Wang et al. (2020) on tailoring embeddings for gender bias mitigation. Efforts were put to replicate the neighborhood metric test, however, the results were not reproducible due to lack of clarity in information/code by the original authors. Analysis on word embedding quality was reproducible within 5% of the original reported value, however, authors were not able to attempt replicating the coreference resolution experiments due to unreadable codes as they claimed. No communication was attempted with the original authors to clarify the codebase for replication purposes. Overall, while the carried out experiments were solid (additional qualitative analysis on gender bias aspect was helpful) and the shared codebase for the replicated part would be useful to the researchers, I thought the work lacked substantial materials to be beneficial to the community based on its current standings.",E4cRWiGoE3
50,"The authors provide a nice summary of the original article in the first section of the paper. Chapter 5.1. on Discrete Disparity Volume is a nice addition for further clarification. However, sometimes I believe they could have been more clear on a number of occasions, especially when it comes to the experimental results. Here are some comments that should be addressed:

- The authors of the reproducibility study coded everything from the scratch. For the purpose of the clarity they should have commented on the architecture in Figure 2 of the original paper: was everything clear and straightforward to implement? where there any ambiguities or points where a decision was to be made that could not be deduced from the Figure 2? For example, a bit more details on OCNet incorporation could be useful for the readers who would like to get a complete picture of the reproduction. This is also a part where they got help from the authors of the original article.
- In Section 3.2 on line 110 it is reported that the depth up to 100m were used for training on KITTI dataset. However, in the original article,  the authors report using depths up to 80m. Why the discrepancy and have they asked the authors of the original paper why did they decide on 80m instead? What would be the reproduced results if the reproducibility authors have used 80m as well? The results they report in the reproducibility manuscript are compared against the results from the original article,  yet there seems to be a difference in the training data. **This is not a good practice.**
- In all results reported in Chapter 4 there is always a piece of information not being explicitly stated. For example, in Section 4.1.1 (Table 2) and Section 4.1.2 (Table 3), one needs to read up to the beginning of Chapter 4 to conclude that these results are for KITTI dataset. In Section 4.2.2 (Table 5), one needs to compare results from Table 4 (Cityscape) and Table 3 (KITTI) to conclude that KITTI was used in Table 5 results. Furthermore, in Section 4.2.2 it could also be more clear what bases were used
- Furthermore, there are no results reported on the Made3D dataset, although this was explicitly stated by the reproducibility authors in Section 3.2. on line 111.
- There are no results reported for attention maps from the self-attention module.
- There are no results reported for pixel-wise depth uncertainty. 
- Chapter 2, line 87: this is not a major claim or hypothesis of the original paper. It is more of a side-result that can be easily visually verified (check 2.2 and 2.3 of the original paper).
- Tables 1 and 2: explicitly mentioning what the baseline is (Monodepth2 with ResNet18), would help the readers. 
- In Table 3: I believe Monodepth2 should be classified as ""M"" (self-supervised), while DORN should be ""D"" (supervised).
- Table 6: The reported increase in memory consumption is not as dramatic as the authors claim. Could it be that one is simpler than the other? Have they tried asking the authors of the original paper what they used for the results reported in the original paper?

Some minor comments:

- There is a few minor grammatical errors. A bigger issue is that sometimes the wording and the sentence structure is a bit harder to follow and understand. This could be improved.
- I am not sure why the reproducibility authors prefer to use the term ""atrous spatial pyramid pooling"" instead of ""dilated convolution"" (as used in reporting the results in the original paper), but they should (gently) mention that these two terms refer to the same concept.
- The authors wrote code cleanly and it is not hard to read. On a few places a few more comments would make things easier to read, but it is not a big issue.

The authors did a good coding job, but the paper itself could have been better written. There is still work to be done to make it more clear and self-explanatory when it comes to the implementation details and the reported results.",SkWyPOGL1I
51,"The report aims to reproduce and evaluate the paper [1] . The paper's main claims are that it produces nearly SOTA monocular depth estimation results through a combination of techniques - self attention and discrete disparity volume. To this end, the report attempts to reconstruct these results, borrowing from other sources and code available (e.g. Monodepth2 [2]). 

Clarity: The report is well written and easy to read.
Originality: The report and code seems to be the first attempt at reconstructing/implementing the paper. 
Significance: As monocular depth estimation is an important computer vision task in scenarios such as autonomous driving, the work is significant. 

Pros and cons: 

Pros: With regards to pros, generally, the results presented align with the paper's claims that self-attention and DDVs help in improving monocular depth estimation. The results beat Monodepth2 convincingly, but cannot match DORN [3]. 

Cons: The paper does not explain the underlying concepts properly. I would have hoped for some explanations on self-attention implementation and in particular, the discrete disparity volume ideas. Examples:

1) How is the DDV constructed and what computational/implementation challenges did it pose? 
2) How was the system tuned? 
3) Comments on the Atrouss Spatial Pyramidal Pooling?
4) A system diagram with explanation and some implementation notes on components


[1] http://openaccess.thecvf.com/content_CVPR_2020/papers/Johnston_Self-Supervised_Monocular_Trained_Depth_Estimation_Using_Self-Attention_and_Discrete_Disparity_CVPR_2020_paper.pdf

[2] Monodepth2: https://arxiv.org/pdf/1806.01260.pdf

[3] DORN: https://arxiv.org/pdf/1806.02446.pdf",SkWyPOGL1I
52,"The manuscript presents a replication study on the Lacoste, A. et. al paper “Synbols: Probing Learning Algorithms with Synthetic Datasets” which presents a tool for generating images of unicode symbols with control over parameters like font, language, resolution, background texture, etc. Paper also presents experiments showing use of Synbols in standard supervised learning setting (establishing baselines), in out-of-distribution testing, for active learning, unsupervised representation learning, and object counting. The authors of this report have been able to back up the results from the original paper. 
      
- The report clearly define and describe the experimental setting of the original paper. The overall organisation and clarity of the document is excellent.
- The authors were able to successfully implement both the proposed algorithms from the description of the algorithms in the original paper/appendix (they had a fluid communication with the original authors for testing reproducibility). Also, the implementation of all the reproduced experiments is provided in a github repository, which is remarkable.
 - The report contains a summarised discussion on the state of reproducibility of the original paper. Results obtained are very close to the values reported by the original authors.
	  
      
Weak points:

- The authors were unable to (completely) run (and check) the original experimental setting for some DNN models due computational constraints (limiting their analysis to just one shot/seed). Still, results obtained in those cases are still similar to those in the original paper.
- Not running all the datasets (e.g., 1M) when analysing the amortised times makes the analysis a bit incomplete. Perhaps Authors should have had a look for other compute resources (codeOcean, Kaggle, etc.).

Minor: 

- Typo in footnote 2.
- Typo in Methodology: ""using only seed of the same dataset""
",wMUNGLllANn
53,"The authors of this report reproduce part of the original paper (OP), and add a few experiments which test the quality of learned representations by trying different downstream classifiers. The authors found minor discrepancies between the hyperparameters reported in the OP and those contained in the code.

Format: the authors did not follow the provided latex template.
Scope: the authors reproduce some of the results from the OP
Code: at a glance the provided code appears complete. The authors mostly reuse the code provided in the OP, with some additions. It seems that the added code contains implementations copied from other open-source repositories. This is fine, but it is polite to explicitly reference those in the Readme.
Communication & hyperparameters: the authors have done their due diligence
Ablation/extensions: the authors add a few experiments which are consistent with the results of the OP. It would have been very surprising for these experiments to deviate from the existing results, and the motivation for this choice is unclear. It would have been more interesting to find, e.g. settings where the dataset generation fails, is computationally hard, or makes no sense.
Discussion: the authors make appropriate discussions and remarks on reproducibility, basically confirming that the material provided with the OP makes things easily reproducible.

- it is unusual to report the validation loss, which is used to tune hyperparameters. The test loss is normally reported instead.
- it would be good to clearly separate what was originally done in the OP from what was reproduced, and from what was added (not in the OP).
- the text is well structured and readable.
",wMUNGLllANn
54,"This work is an effort towards reproducing « Softmax Deep Double Deterministic Policy Gradients » by Pan et al. (2020). This algorithm (SD3) claims superior performance compared to TD3 by using the softmax operator during bootstrapping, instead of the conservative « min » rule used in TD3. The main finding of this reproducibility report is that there seems to be indeed a small benefit of SD3 vs TD3, but it is not as marked as in the original paper, and it was even less clear on new environments.

Overall the report is clearly written and easy to follow. My main criticism is that, as far as I can understand, the authors mostly re-used the existing open source implementation from Pan et al., and applied it to PyBullet environments (vs. the original MuJoCo ones). There is no hyper-parameter search nor ablation study. I do sympathize with the lack of computational resources, but maybe a different work should have been selected if the authors did not have enough to dig deeper. This makes the overall contribution somewhat limited, especially since the authors did not try to use the same MuJoCo environments, so it is not 100% clear if the differences (vs. the original paper) only comes from the environments themselves, or also from other (unknown) factors. The authors do mention they will provide a TensorFlow re-implementation of the algorithm, but since it wasn’t used for the report, is said to be less efficient than the publicly available PyTorch implementation, and there is no mention on whether or not it can reproduce the same results, I am not sure that it brings added value here.

In the end, it is not clear if the discrepancy vs. the original paper is because SD3 is not that good compared to TD3 on the PyBullet environments, or because hyper-parameters need to be adjusted for these environments, or because a mistake was made somewhere. This makes it difficult to draw meaningful conclusions from this report.

Minor detail: on l. 88, should max_q be max_a?",XhSN4Mm6fBM
55,"Overall, this report represents a reasonably thorough reproduction of the Pan et al. paper. However, there are some weaknesses (as outlined below), that should be improved.

Strengths
- The suggestion on how to improve the performance of SD3 (lines 160-163) is interesting, and could lead to some non-trivial improvements.
- Evaluation the results in a set of comparable open-source environments is a good goal, and aids reproducibility overall.
- Highlighting the differences between TD3 and SD3 is excellent for exposition and clarity.
- This report is written in a way that makes it reasonably clear even without familiarity with the original paper.

Weaknesses
- One reason that the authors suggest for a discrepancy between the rewards in the Pan et al. paper and their reproduction is their use of PyBullet instead of MuJoCo (lines 127-128, lines 153-154). However, this claim is not validated (or cited, other than the referenced GitHub issue).
- What do the edges in Figure 1 signify? Presumably that the algorithm uses ideas from an earlier one, but this should be clarified in the text.
- While the authors state that the results achieved are weaker than those in the original paper, there is no quantitative evidence presented, other than some graphs. What is the magnitude of the effect? What is its statistical significance?
- While the RL background section (Section 2) is appreciated, there are some inaccuracies, including ""RL is often formalized as a Markov Decision Process"": it is the agent's interactions with the environment that are formalized by the MDP, but RL itself is the process of learning a good policy in that environment.
- The report requires substantially more proofreading. Examples include: ""author's"" (line 21), ""consumption's"" (line 26), ""reprehensibility"" instead of ""reproducibility"" (line 165), lack of consistent capitalization throughout.
",XhSN4Mm6fBM
56,"Summary:
This work reproduces the results  for ""Ensemble Distribution Distillation"" which uses a Dirichlet parametrization to distill deep ensembles to preserve uncertainty. The reproducibility report shows an extensive evaluation of CIFAR-10, ablation studies on ensemble size and temperature annealing and also includes visualization for uncertainty.
 
Strengths:
* Detailed reproduction Ensemble Distribution Distillation for CIFAR-10
* Report included also all baseline models which offers also relative comparisons
* Nice simplex visualization for classification experiments.

Weaknesses:
* I only have comments about clarity of the paper below. 

Detailed comments and questions:
* Reproducibility summary: The scope of reproducibility is rather a description of the claims of the paper and is a bit vague. From my understanding, this should describe the scope of this work.
* Reproducibility summary: ""Most of the authors' experiments on the CIFAR-10 dataset"" were reproduced -> I think this should rather go to 'scope of reproducibility'. Here, it would be also good to mention any pre-trained models that you used, e.g. VGG16.
* Section 2 (Scope of reproducibility): What is not included in this work? Which experiments were not run that were included in the original paper?
* Claims 3-5: What does the original paper claim here in comparison?
* Table 3 & 4: Arrows up and down indicating ""higher is better"" or ""lower is better"" would be useful for all the metrics shown.",p1BXNUcTFsN
57,"Introduction is quite intuitive, giving a high-level context to the paper being reviewed. 
The Scope of reproducibility was well highlighted,clear and concise.
All claims identified were supported by experiments.
Although reviewer reproduced the paper using Tensorflow keras contrary to the original paper, The results obtained was still similar to the original paper, although not 100%.

The overall reproduced paper is concise, explanatory and of good quality.",p1BXNUcTFsN
58,"The authors of the reproducibility paper provide a nice summary of the original manuscript. However, there are some major issues and shortcomings that prevent this reproducibility study to be accepted for publication. I list them here:

- As authors of the reproducibility paper stated, only two out of four experiments were performed. Considering that they did not need to code anything (the code was provided by the authors of the original paper), reproducing as many experiments as possible should have been a priority.
- Even with the lack of time and computational requirements, instead of performing a full reproducibility of MNIST experiment (the simplest and the first experiment of the original paper), a full reproducibility of neuromorphic version of MNIST: N-MNIST woudl have been more appropriate. This was the second experiment in the original paper and it would have been considerably more interesting to see reproducibility results for it.
- The hyperparameter search was only performed for one of the four experiments. In addition, it was the simplest experiment and the hyperparameter search was rather brief and not exhaustive. There is no mention of feedback from the authors of the original manuscript on the hyperparameter range they experimented with. Since the code was already available, I believe this section should have been investigated more thoroughly.
- It seems that the hyperparameters used in the reproducibility study (as listed in lines 115 to 118) do not correspond to the hyperparameters used in the original paper (as listed in Section 2.1 of Supplementary Materials).
- The authors of the reproducibility study mention discovering hardcoded parameters in the source code that can not be explained. I think a list of such parameters should have been made available somewhere (if not in the paper itself, then at least on the github webpage they provided). Furthermore, there is no attempt to explain or even guess what these parameters do. I understand that just plain changing of these parameters and re-running the experiments would require additional time and resources, but that would have been in the best interest of this reproducibility study. In addition, the authors of this study do not mention attempting to clarify these with the authors of the original manuscript.

In the end I do believe that if authors of the reproducibility study were given more time and resources, they would have made a considerably better study. Unfortunately, this is not the case.",2Z29gHCr5X8
59,"Overall, this was really nicely put together. 

Reproducibility Summary: Provided, and contains a nice summary of the presented reproducibility results.
Scope of reproducibility:
- Narrowed down to two datasets (MNIST and CIFAR-10), to deal with computational constraints. I think this is well justified given the computational complexity of the reproduced algorithms.

Code:
- Original code used, with minor additions.
- Mentions checking major mathematical sections for mistakes/bugs.
  
Communication with original authors:
- Clarification and recommendations for theory communicated to authors (who reciprocated).

Hyperparameter Search:
- Conducted a separate hyperparameter search on MNIST using bayesian optimization provided by WB ML dev tools. They did the search over 3 parameters (epochs, learning rate, time window), see results beyond the paper for more details.
  
Ablation Study:
- Not applicable as far as I can tell.

Discussion on results:
- Did a good job discussing the results.
- Provided average run-times and total compute used
- Justified the experiments reproduced well.
- Discussion is well written and clear.

I think you should report which parameters are hardcoded in the original code, and what values they are set to. Possibly linking them to the actual notation of the algorithm rather than what is in the code.

It might be nice to include a concrete set of recommendations, rather than listing what is wrong. For example, ""Document all hard-coded parameters and give justification for their choice.""
  
Results beyond the paper:
- A new hyperparameter search was conducted over the MNIST dataset
- Did a nice job uncovering some potential stability concerns for future work.
  
Overall organization and clarity:
Well written and clear.

Questions/Concerns:
Did you only do a single run for the hyperparameter search? Confidence intervals for these novel results (Figure 1 and Figure 3) would be a nice addition. Maybe you did 12 runs? But it seems you did 12 different configurations. There might be high variability in the settings.

In figure 2, which line corresponds to which settings? It might be nice to try and figure out how to visualize this information, as this would be useful for making inferences about hyperparameter selection.
",2Z29gHCr5X8
60,"In this submission, the authors reproduce two of the four experiments originally performed by Zhang and Li (2020).  Confirming the original result, the authors find that Temporal Spike Sequence Learning Backpropagation improves spiking neural networks' performance to near SOA levels with reduced training time. Additional experiments were conducted to examine the influence of hyperparameters were also performed; the authors report that training times reported in the original paper may be pessimistic. 

I have no major concerns. There does appear to be a discrepancy between the original paper and replication in the CNN2/CFAIR-10 experiment. However, the authors note that they were only able to run the experiment twice, and this may reflect network stochasticity (though the delta seems rather large). The other experiments replicate nicely, and the rest of the manuscript also contains a brief explanation of spiking neural networks and the authors' rationale for choosing to replicate these two experiments, which is nice.

The authors apparently reviewed the source code (Line 208ff) and found some undocumented parameters. A slightly longer discussion of these (e.g., where they are, what they seem to do) might be helpful for future readers of both papers. However, I leave this entirely to the authors' discretion. ",2Z29gHCr5X8
61,"The author reimplements the paper ""Fairness without demographics through Adversarially Reweighted Learning"".

On one hand, the good news is the author reproduces ARL's performance in the original paper. The frustrating news is that the author finds the baseline DRO's performance is much higher compared to that originally reported.

The author says the original paper does not provide the grid search results. Fortunately, I find that the authors of the original paper publish their code and the optimal hyperparameters in their [GitHub repository](https://github.com/google-research/google-research/tree/master/group_agnostic_fairness).

The optimal hyper-parameters from this paper's Appendix B are very different from the original paper's grid search result (see GitHub).

So I think the author should revisit the experiment and try to give some insight on what are the optimal hyperparameters.",YyXrarQhX2S
62,"This manuscript provides a pytorch implementation about the paper ""Fairness without demographics through adversarially reweighted learning"". The authors do not perfectly reproduce the results, but the trend seems to be consistent with the original paper. Hyperparameter tuning is sufficient. Additional contribution of this paper is to apply this method to image data.",YyXrarQhX2S
63,"**Quality:** Overall quality of the reproduction is very good. The writing reveals the good understanding of the original work by the authors of the reproducibility report.

**Clarity:** I found the report to be written very clearly. The ease and difficulty faced by the authors is clearly mentioned in the report. The exact reproductions, understandings and explanations are also conveyed very clearly, both verbally and pictorially.

**Originality:** The submitted report is quite original on its own. The authors put their own knowledge in the domain to judge and reproduce the original work. The thought process of the authors are distinctly visible.

**Significance:** The report provides good insights on how the experiments in the original paper actually work, while also generating new hypothesis to be tested for future research, which is a positive outcome.

**Pros**

- detailed reporting of every aspect
- clear listing of shortcomings in the original paper
- thoughtfulness of the authors to present the reasons of unmatched results

**Cons**

- detailed labelling within figures found missing, making it difficult to understand the diagrams in first glance",10Fgr0kHs5
64,"The replication study pretty much stuck to the assumptions, code and interpretations of the original paper. Unsurprisingly, they found issues with context -sensitive aspects of the study -- which do not have solid theoretical foundations. They found issues with replicating the results on two datasets -- this part appears to be done correctly. All this suggests that related theoretical frameworks need more work.",10Fgr0kHs5
65,"### SUMMARY 

This work offers an in-depth exploration of the reproducibility of ""Deep Fair Clustering for Visual Learning"" which claims that:
- The proposed method ensures cluster validity and fairness on large-scale, high-dimensional visual learning while (a) seeking to find feature mappings amenable for structure discovery and (b) filtering out sensitive attributes.
- The process has been modeled as a minimax optimization problem where cluster analysis and assignment is independent of sensitive attributes (aka C(X) is statistically independent of G) with minimal utility loss, high accuracy score and fairness measure.
- The theoretical analysis has been backed by empirical demonstration on four visual datasets: MNIST-USPS, MTFL, Color Reverse MNIST, Office-31 using the following metrics: Accuracy, Normalised Mutual Information, Balance and Entropy.

The submitted report addresses the above claims as follows:

- Visualizes learned representations of encoder through t-SNE to demonstrate DFC's ten clearly separated clusters ensuring fairness.
- Considers DEC as DFC without minimax optimization (in particular without - separate subgroup clustering, fairness adversarial loss and structural preservation loss).
- Comparisons across DEC and DFC have been verified across the specified metrics (not for the following models stated in the original paper: DAC, AE, CIGAN, ScFC, SpFC, FCC, and FAlg) for the above mentioned datasets. This report further claims to extend the original code repository to include support for pre-training, different datasets, comparative methods with additional hyperparameter optimization using the Weights & Biases Sweeps feature. 

### MERITS
The report is well-written and intrinsic details of the original paper are expounded to reasonable depth. Results have been reproduced satisfactorily.

### RECOMMENDATIONS
- In Figure 1, it might be helpful to incorporate notations (For Example: feature encoder F(X) transforms data into Z) into the schematic representations as well.
- Statistics in Section 3.1 can be discussed in a tabular form. As per the [Machine Learning Reproducibility Checklist](https://www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf), I'd also recommend including links to downloadable versions of the dataset for Office-31, Color-reverse MNIST and MTFL.
- Results of the original paper on the Balance metric of Office-31 dataset has not been reported accurately: Please recheck. The reproducibility report may need to be updated accordingly. Also, re-verify your reported percentage intervals for various metrics. 
- Regarding the claim “Additional experiments aimed at validating the contribution of individual components of the DFC towards fairness”, please elucidate the experimentation as well as individual contributions of the following major components (DEC/Discriminator/Encoder). These seem to have been (a) cryptically addressed in the report and (b) contributions have been fragmented across sections (in the reviewer's opinion) and hence, do not bolster your argument. 
- I'd also recommend including the table of results into the README document on [your github repository](https://github.com/Joppewouts/Reproducing-Deep-Fair-Clustering). Optionally, you could include the list of dependencies in the report.

### MINOR CORRECTIONS
- In Appendix A, Table 2 Caption: ""meand and log variance of the distribution""; **Correction:** ""_mean_ and log variance of the distribution""
- In Appendix A, Table 3 Caption: ""The architecture was chosen to resemble te inverse of the encoder""; **Correction:** ""The architecture was chosen to resemble _the_ inverse of the encoder"".
- In Section 3, Deep Embedded Clustering, Line 89: ""A soft assignement of the datapoints to cluster centroid""; **Correction:** ""A soft _assignment_ of the datapoints to cluster centroid"".
- In Section 3, Deep Fair *Clustering*, Line 100: ""an fairness adversarial loss""; **Correction:** ""_a_ fairness adversarial loss"". (_Clustering_ instead of _clustering_ to ensure consistency of casing across headings/sub-headings)
- In Section 3, Deep Fair Clustering, Line 110:  ""In other words, they expect, given a subgroup, that the clustering result is similar when trained only on the subgroup data and when trained on all subgroups and looking only at the samples of one subgroup, and encourage the distributions to be similar""; I believe this statement can be better phrased in terms of local subgroup vs global subgroup distribution and clustering. 
- In Section 3.1, “The The Multi-task Facial Landamark (MFTL)”; Line 129: “_The_ Multi-Task Facial _Landmark_”, ""constist of 12,995 face images"". **Correction:** ""_consist_ of 12,995 face images""
- In Section 4, Experimental Setup and Implementation, Line 165: ""Some experiments require sightly different model combinations""; **Correction:** ""Some experiments require _slightly_ different model combinations''
- In Section 4.3, Centroid Initialization, Line 188: ""student's t-dsitribution""; **Correction:** ""student’s _t-distribution_""
- In Section 4.4, DFC, Line 192: ""a discrminator module and""; **Correction:** ""a _discriminator_ module and""
- In Section 4.4.1, Clustering Assignment, Line 199: ""can be use as in the Deep Embedding clustering""; **Correction:** ""can be _used_ as in the Deep Embedding clustering""
- Formatting of references can be enhanced: IEEE conference on computer vision and pattern recognition -> IEEE Conference on Computer Vision and Pattern Recognition (CVPR).





",10Fgr0kHs5
66,"The manuscript presents a replication study on the paper ""Distribution-aware coordinate representation for human pose estimation"" 

Pros: 
The authors are able to replicate the method, largely because of the available code from the original paper. 
The authors tested on an independent dataset. They also introduced additional comparison with other models. Both seem to be extension and qualitatively support the claims of the original paper. 
The authors were able to successfully implement both the proposed algorithms from the description of the algorithms in the original paper (they didn’t need to contact thus with the original authors for testing reproducibility). 

Cons: 
The missing of validation on the COCO dataset used by the original paper. This weakens the successful replication claim. 
The authors implementation is not in a github repository  
",rwTfoRdQcxW
67,"* Reproducibility Summary

  The report presents a well-written, concise reproduction study on the paper DARK. The report contains a reproducibility summary that highlights the scope, methodology, results, and what was easy/difficult appropriately as required by the challenge.

* Scope of reproducibility

  The report investigates two central claims of the original paper.

* Code: whether reproduced from scratch or re-used author repository.

  Authors re-use the repository of the original papers for most of their experiments. They also implement their own code in a novel reproducible model development kit following the author's code. I find this reproducibility kit (moai) fascinating, and a great example of a submission that introduces a reproducible paradigm to test original authors' codes.

* Communication with original authors

  The report mentions they did not communicate their results/findings with the original authors.

* Hyperparameter Search

  The authors seem to investigate a modest subset of hyperparameters for their work.


* Ablation Study

  The authors perform an ablation study on the originally proposed algorithm by evaluating the claims on a new dataset, HUMAN4D. This kind of robustness evaluation on a new dataset is very much welcome, as it adds valuable insights to the proposed algorithms.

* Discussion on results

  The report contains a limited discussion of the results on the HUMAN4D dataset using DARK. For claim 1, the authors note DARK decoding performs better for all experiments except one. It would have been better to add a discussion of why this exception occurs. The report also extends the original results by adding a new experiment with comparing with CoM. The report can be made stronger by exploring more ablations to shed light on the effectiveness of DARK, and/or adding more discussion to the effectiveness of DARK.

* Recommendations for reproducibility

  The authors highly commend the original paper on their state of reproducibility.

* Overall organization and clarity

  The paper is well organized and well written.",rwTfoRdQcxW
68,"**Reproducibility Summary**:
The authors have provided a detailed summary meeting the requirements.


**Scope of reproducibility**:
Yes, the reproducibility report has clearly and concisely stated the scope of reproducibility.

**Code**:
Yes, the authors have re-used the original author's code repository and also tried with another differential PnP (i.e EPnP) module as described in the reproducibility report.

**Communication with original authors**
Yes, the authors connected with one of the BPnP paper's original authors through their Github repo.   The authors did not reach out to HigherHRNet paper's original authors. 

**Hyperparameter Search**:
Yes, the authors have attempted to reproduce the hyperparameter search, but the $\beta$ coefficient from the original author's (BPnP)'s code did not work for the authors of the reproducibility report.

**Ablation Study**:
Yes, the authors used an alternative implementation of the BPnP module to review the results and reproducibility.  The authors tried ignoring the higher-order derivatives of the BPnP.

**Discussion on results**:
Yes, the reproducibility report contains a brief discussion on the state of reproducibility of the original papers, but does not highlight which parts are easy to reproduce or which parts were harder.  They could have mentioned the difficulty with the $\beta$ parameter here.

**Recommendations for reproducibility**:
No, the authors did not provide any recommendation to the original authors for improving reproducibility.

**Results beyond the paper**:
The authors have tried additional differentiable PnP implementation (EPnP) to verify the claim.  That is a good point.  Another good point is that the authors tried to reduce the complexity and run time of the model using a faster BPnP, then evaluated the results and provided the detail pros and cons of using the technique; bonus point to the authors for that.  The authors include significantly more quantitative and qualitative results than the original paper.

**Overall organization and clarity**:
Nicely written and coherent.

**Pros**:
Significantly more quantitative and qualitative results.

**Con**:
The authors highlight the best results in the tables using a red color.  A better choice would be green or yellow or just to leave it uncolored.",PCpGvUrwfQB
69,"Two papers are reproduced here: backpropagatable PnP & HigherHRNet, for the problem of 6DOF object pose estimation. The results are evaluated in the UAVA dataset. 
The work contains the mentioned points, including communication with original authors, and discussion of the reults. Overall, the results are meaningful. They even present results beyond the original paper, such as section 4.2. 
",PCpGvUrwfQB
70,"The report aims to verify the effectiveness of using Backpropogatable PnP (BPNP) on a pose estimation task with drones. Following the original paper, the setup uses heatmap regression, from which the object pose is extracted and refined through PnP, given 3D geometry. The incorporation of geometric constraints (e.g. the projection loss from the BPnP [1] paper) is claimed to improve estimation of keypoints. 

In addition to BPnP, the report also examines the effect of scale aggregation from the HigherHRNet paper which proposes a scheme to bottom up scheme for aggregation in stacked hourglass type setups for heat map computation. The report goes about doing this through two types of drones of varying sizes - Mavic (larger) and Tello (smaller) using the dataset UAVA which contains ground truth annotations needed (e.g. 6D pose).

BPNP scheme is compared with a reference differentiable implementation (EPnP) via PyTorch3D. 

The report was well written, and the experiments thoughtfully carried out. In general, the numbers show improvements in keypoint estimation after incorporating the the projection losses. The comparison with EPnP is also quite favourable. They also show that the 'faster' version of BPnP reduces computational time without loss of accuracy. 

Pros: Major ideas in paper explained clearly, with cogent implementation results. I would be keen on using BPnP for practical tasks. 

Cons: Hyperparameter sweep tries not touched upon in detail. In particular, how does one chose the weighting parameters? Were any stability issues encountered? I would have liked to see a more varied list of scales as in the original paper rather than just the two drones, and (please correct me if I am not reading it correctly) the numbers are not markedly better/worse across implementations (Table 4).

[1] BPnP: https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_End-to-End_Learnable_Geometric_Vision_by_Backpropagating_PnP_Optimization_CVPR_2020_paper.pdf",PCpGvUrwfQB
71,"# Reproducibility Summary : 
Complete

# Scope of reproducibility:
Clearly defined

# Code: 
Re-implemented.

# Communication with original authors
Reports one communication exchange with original authors.

# Hyperparameter Search
The report mentions the need to do some random search on unmentioned hyperparameters (such as momentum coefficient). There is no clear mention of hyperparameter search, what was the search space, budget and algorithm used for the main hyperparameters however. What I understand is that they re-used the original ones.

# Ablation Study:
There is no ablation study

# Discussion on results
The reproduction of the results is well detailed and the table help understand what was reproduced and what was not.

# Recommendations for reproducibility
There is no clear recommendations.

# Results beyond the paper
They tried applying the model on a task that was not in the original paper, the semantic segmentation. The dataset was however extremely small which makes me uncertain about how insightful the results are.

# Overall organization and clarity
The overall organisation of the paper and the presentation of the results is clear. There is many mistakes (see minor comments below) that should be corrected. Section ‘Methodology’ is hard to understand and brings no value with respect to original paper. Pointing the reader to original paper to better understand the maths beyond BayesBiNN would make a better job in my opinion.

# Comments

Algorithm 1 is difficult to understand without more explanations.

The ‘Methodology’ section was difficult to follow. The explanations of the equations are too succinct, lack motivations and explanations. Looking at the original paper, I find it easier to understand the equations based on their explanations, so I am wondering what is the value of the presentation of the equations in this report.

Line 145:  However, the results were against our intuition and the result of segmentation were a zoomed segmented image of the input with lots of noise.

It’s not clear what is meant here. What was the intuition? What is the result and what should it have been? Should it not be zoomed?

# Minor comments
Line 36. STE and BOP acronyms should be introduced.

Line 100. by a randomly -> by randomly

Line 118. reporduce -> reproduce

Line 120: but the validation split is made 0. I don’t understand what it means to make a split 0.

Line 136 Semantic Semantic -> Semantic Segmentation

Line 137 it’s full-precision -> its full precision

Line 138 it’s performance -> its performance

Line 140 we have use -> we have used

Line 167 The gave -> They gave

Line 178 in it’s total -> in its total

Line 184 that it’s -> that its",bhiGno-Cxq
72,"This paper summarizes the reproducibility of BayesBNN and gives a clear scope of reproducibility. It provides a re-implemented codebase to reproduce the results of the original paper. In addition, this paper also discusses the extended results for semantic segmentation. The paper is well-written and easy to understand. Couples questions:

1. For the case of unmatched CIFAR-10 results, I am wondering why the Batch-Norm layers may cause a large-cap of test accuracy.
2. For the semantic segmentation task, is it possible the limited size of training data may affect the model performance?
3. Line 183, what do you mean by “cleaner deep learning”? 
",bhiGno-Cxq
73,"Thank you for your great paper!

The paper successfully proves that the original paper is reproducible and it could provide the post-hoc casual explanations for black-box classifiers through the casual reference. Furthermore, the paper establishes its own evaluation system to evaluate the original paper from different aspects. Moreover, the paper extends the application domain from images to texts, which is great for generalization. However, I think it would be good if you can add more details about what is different between your implementations and the original paper's, like the solution to reduce algorithm complexity. Last but not least, adding a few figures about the models' architecture would be great for readers!",Vqtf4kZg2j
74,"Good job in reproducing the results. The original paper carries out methods for generating causal post-hoc explanations of black-box classifiers based on a low-dimensional representation of input data. This paper tries to reproduce those results in detail and provide a more efficient implementation. While reproducing the results of the original paper, the authors of this paper take a further step ahead:
1. They provide a higher resolution transition for the first causal factor for MNIST 1/4/9 classifier.
2. They also dive into hyperparameter search for the Principled procedure for selecting (K, L, λ) as explained by the original paper.
3. They have also tried out the proposed method on the SST text dataset and tabulated the duration for both text as well as for image datasets.

Also, the resulted figure from this implementation is similar to the figures reported in the original paper. 

They have also mentioned that they have not found this method to be scalable in contrary to the original paper but they have not mentioned any ideas on how to scale up but they are relying on future papers to do so.

On a final note, this is a solid work and will be very helpful to gain insights if the original paper is reproducible or not and to what extent can the algorithms mentioned in the paper can be used to solve the explainability problem of black-box classifiers.

",Vqtf4kZg2j
75,"Thank you for your great paper!

Summary: The authors tried to reproduce the original paper which claimed that complex-valued DNNs effectively increase the difficulty of inferring inputs for the adversary attacks compared to the baseline and the proposed privacy-protecting complex-valued DNN effectively preserves the accuracy when compared to the baseline, but did not get the satisfying results. Moreover, the authors proposed certain good points to question about the original paper.

- Pros: 
1. The authors are really rigorous to provide certain good points to doubt the original paper (Section 5 Discussion in this paper). It proves that the authors read the papers carefully and devotes themselves to designing the experiments.
2. The authors describe the details of the experiment very carefully, and the idea is clear to me.

- Cons: 
1. The authors haven't carried out the whole experiments (such as the authors did not successfully implement VGG-16 / Alexnet and the inference attacks)
2. The result section is confusing to me as I can't figure out which one is the new results without ReLU in the generator

Overall, I really appreciate the discussion section, so I would clearly recommend the paper to be accepted!
",P30M7d9DyXw
76,The authors did well by reproducing the original work even though there was no readily available data.,P30M7d9DyXw
77,"Summary:
This work reproduces the main results of ""Fairness by Learning Orthogonal Disentangled Representations"". This includes implementing the model and evaluating it with different image datasets on downstream tasks.

Strengths:
* Evaluation of approach evaluated on 5 datasets

Weaknesses:
* I found the scope of this paper limited to the main results of the paper. At least, the authors should mention what is not included in the report. Further, it would have been good, if the authors had added an ablation study or the sensitivity analysis.
* It was not clear whether the hyperparameters used (3.3) were the same used in the paper or not.",l0PyfnGP1L
78,"**Quality:** It is a fair quality report, which lacks on crucial details. Understanding of the authors on the original work was absent.

**Clarity:** The information presented is clear but the amount is less.

**Originality:** It is a normal report, nothing to fancy to be rated original.

**Significance:** The report currently is low on significance. It would have been better if the authors of the report provided some more details on how the original paper presented their works, and how the authors of the report see it and understand from their perspectives.

**Pros**

- Report covers all aspects of the reproducibility
- report of successful communication with authors of original work

**Cons**

- far less detailed to be called a report
- authors understanding and perspective of original work missing
- authors of the report just say that the results are dissimilar, however a much detailed discussion regarding the same giving reasons was felt missing
- overall, the report could have been improved with diagrams and figures added to enhance the information presented",l0PyfnGP1L
79,"This report reproduces the paper ""Parameterized explainer for graph neural network."" [1]. A reproducibility summary is provided but not well-structured according to the guideline provided. First, the scope of reproducibility primarily discusses the experimental results consistency obtained without mentioning their implementation, difference compared with the original paper etc. This section seems more suitable to be renamed as ""reproduced results"". Second, the methodology requires a more detailed discussion so as to better discuss the implementation differences compared with the original paper. The authors mentioned that there was no need to contact with the original authors but the implementation differences/difficulties should have been communicated with original authors. What's more, AUC is the only evaluation metric. Although the authors discussed the suitability of AUC score for this task, additional evaluation metrics could be considered to go beyond the paper. So could the due diligence in hyperparameter sweep, discussion of the results, datasets other than the ones used in the original paper. Overall, this reproduction provides some findings for reproducing the original paper but not comprehensive and could have gone beyond the original paper. 

[1] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. Advances in Neural Information Processing Systems, 33, 2020.",tt04glo-VrT
80,"This paper is well written, and the results are compared with those of the original paper. The authors have reimplemented the code using a different library and have made minor changes to their code to reproduce the results. The results are not in the same threshold as the original paper and the authors have not studied the reason behind it and have just hypothesized what might be the reason which is not enough at all. Besides, I do not see any novelty or additional contribution beyond the original paper.

Overall, my evaluation of the paper titled “Reproducing: Parameterized Explainer for Graph Neural Network”” is as below:
•	The authors provide a fair representation of the objectives of the paper, its results, challenges, and contributions. (Reproducibility Summary: 10/10) 
•	The authors provide the areas of the original paper where they have tried to reproduce the results and adhere to them along with the paper. (Scope of reproducibility: 10/10)
•	The code from the original paper has been used and reimplemented using PyTorch. Overall, there are enough documentations, and the code sounds clean. (Code: 10/10) 
•	Scope of the communication with the authors of the original paper is not mentioned, and in fact, the authors did not see any need to contact them (Communication with original authors: 0/10)
•	The code is implemented using a different library (PyTorch), while the original implementation is in TensorFlow. The results from the new implementation do not exactly match with the original paper and the authors have not really studied why this is the case. Studying different aspects of the two implementations and detailed comparison will be very helpful for the community. But unfortunately, the current study does not make such an effort to study the underlying reasons behind the differences. (Hyperparameter Search: 3/10)             
•	The authors have just reimplemented the code using a different library. This study does not introduce any novelty and does not go beyond what the original paper introduces. (Ablation Study 0/10)
•	There are detailed discussions on the study, what has been done, the outcome, and fair comparisons of the results from two libraries is provided (Discussion on results 10/10)   
•	The detailed illustration of the results is very helpful, but the authors do not provide any recommendation on future work for reproducibility. They mainly summarized what they have tried and the differences in the outcome with respect to the original paper.  (Recommendations for reproducibility 5/10)        
•	I did not see any new novel contribution from this paper. (Results beyond the paper   0/10)
•	The paper is organized, coherent, clear, and well structured. (Overall organization and clarity 10/10)
        
I recommend reject of this paper.
",tt04glo-VrT
81,"This work aims at reproducing the iFlow method proposed in ""Identifying Through Flows for Recovering Latent Representations"". The original paper made some claims against the baseline approach iVAE, and this work reran the experiments and validated some of its claims, but not all: in particular, this work showed that in the 2D case, iVAE does not have mode collapsing issues as claimed by the original paper and actually outperforms iFlow in terms of MCC. Other than this, this work is able to reproduce the major claims by the original paper.

Pros:
1. This work conducted many experiments and was able to rerun most of the experiments in the original paper despite some experiments needing changes to the code. 

Cons:
1. In section 6 the authors suspected that ""One possible explanation is the bug regarding the batch size, which could mean the paper used a batch size of eight rather than the reported 64."" Is there evidence that under batch size 8 VAE gets worse performance and occasionally gets mode collapsing?

2. I like the extension to the MNIST case, but unfortunately, this work didn't propose a feasible implementation.

3. In terms of presentation style, the captions of figure 1 are too small, and it's not very clear what are the two rows of figure 3. Is the top row iVAE and the bottom iFlow?

Overall, this work has reproduced the major claims of the original paper, and also showed some potential bugs of the original paper in the 2D case. I am leaning towards its acceptance.",CoKdaIQzeRx
82,"Reproducibility Summary

The authors did an excellent job of explaining the summary of the report. They also mention a detailed description of the reproduced results and the difficulty faced during the reproducibility of the original paper.

Scope of reproducibility

The authors have clearly stated the scope of reproducibility with clarity in the claims they learned from the original paper with further details explaining the clams.

Code

From my understanding, the code was provided by the original authors with detailed information about the hyper-parameter search. But the authors do a commendable job in preparing the code-base for figures and thoroughly checking the code for any possible scope of improvement.

Communication with original authors

As mentioned by the authors they were not able to get clarification from the original authors. It would be great if they could share the doubts they had during the reproducibility which were not clarified by the original authors. 

Hyperparameter Search

The original authors have described the hyper-parameters needed for the successful replication of the numbers. But the author’s effort of verifying every seed and python version to get the closest results is really commendable and also making the code publicly available is a crucial step for advancing the current work.

Ablation Study

The authors did a great job of showing ablation for both iFlow and iVae methods.

Discussion on results

There is a discussion section mentioned in the paper that repeats how the experiments support the claims mentioned in the original paper. It would have been great to see if the authors can provide a thorough description of the method strengths and weakness and why the experiments can support the claim. 
 
Results beyond the paper
 
The authors have mentioned a section in which they mention the experiment with a real dataset to check if the claim still holds. It is also nice to see the difficultly faced while experimenting with a new dataset. It would have been great to see if both the paragraphs (sec. 4.4.7 sec. 6.2) would be structured more appropriately for easier readability.

Overall organization and clarity

I found the report to be well organized and extremely easy to read. The plots, however, could be slightly updated to improve readability.
",CoKdaIQzeRx
83,"## Overview
This report aims to investigate the reproducibility of ""Towards visually explaining variational autoencoders"". The reproducibility summary is insightful and details the step necessary as well as the difficulty they encountered while trying to reproduce the results of the paper. It shows that even if more and more authors are now making code available through different means (paper with code, GitHub), there are still some ways to go before reproducibility in AI is the norm. 

## Quality:
The quality of the report is mixed.
### Pros
- The methodology is well explained and the plan to reproduce the results is solid and well thought out. The authors did an extensive effort to reproduce and also re-implement the results from the original paper. The code they made available online to reproduce the results from the report work well, they are sufficiently commented and I could reproduce the results from the report. Contrary to the original paper where a large proportion of the code was missing. 
- The dataset, parameters, and setup for the experiments are well presented. 
- The results are well presented and show that the implementation is functional. 

### Cons
However, the paper is not well structured in my opinion:
- the last paragraph of the introduction should absolutely state the results of the report upfront. It should be written in the introduction what was achieved by the paper, the reader should not have to read the whole paper to have the most relevant information. The report presents the three major claims that are made by the original paper and should answer them in the same paragraph. 
- the methodology section is missing a high-level explanation of the concepts they are aiming to implement. As it stands, a reader not familiar with the original paper could not follow the experiments that are being presented. For example, the disentanglement problem is not defined nor explained in the report. 
- section 4.1 ""Discussion of results"" should have been included in the section results. This makes the results and the figure hard to follow. 

## Clarity
The paper is well written and the article is easy to read. However, it would need more proofreading. 
- What are the equation referring to at line 87 and line 130 ??. 
- Why in table 1 (a) line ""conv 2"" Our reproduction ""0.644"", our best ""0.320"" the best results is lower than the reproduction? 
Also, the references are inconsistent, for example, citation lines 320-322 and citation lines 339-341 are citing the same conference and are not formatted in the same way. 

## Final words
The authors of the report did an excellent work reproducing the results from the original paper and had an interesting take on why their results are not consistent with the original paper, however, the paper is poorly presented and is missing crucial high-level information on the main concept they are trying to present. Finally, the review is supposed to be double blind, however, the GitHub repository where the authors made the code available was not anonymized. ",Lwb6qIpEW9-
84,"The report clearly summarizes the problem statement of the original paper ""Towards Visually Explaining Variational Autoencoders"". 

The submission has covered all experiments in the original paper. Particularly, when the original implementations are missing, the authors of the submission use good reasonings and additional material to decide their own setting. 

The authors also use good reasonings to discuss why their results are different from the original paper, especially on the difference for the UCSD Ped1 dataset

The submission itself has covered essential implementation details. After reading this submission, I don't see any further details that are needed for the implementation. 

Minor comments: 
1) It would be helpful to provide a learning rate scheduler in supplementary.
2) Do the authors try to choose two latent dimensions not randomly, but through a search of which two dimensions are the most informative (either for the overall entropy, or the mutual information between the input and the latent encodings?). Is it possible/feasible to compute these metrics without exhaust the GPU? 
",Lwb6qIpEW9-
85,"Reproducibility Summary: Included, covers the relevant topics

Scope of Reproducibility: Re-used existing code for MNIT experiments; custom implementation for the other datasets

Code: linked to github repo

Communication with Original Authors: the authors were contacted, but no response was received directly. It seems like a reasonable and fair effort was made.

Hyper-parameter Search: The reproduction largely relies on its own implementation

Ablation Study:

Discussion: Clear discussion of ways that the original paper is hard to reproduce. Including hard to follow code and mismatches between the implementation and the description in the paper. 

Recommendations for Reproducibility: Well described in section 4.2

Results Beyond Paper: New baseline proposed and experimented with

Overall Organization/Clarity: well written paper, with a few minor errors in language. 

Errata

L.87 equation reference is broken",Lwb6qIpEW9-
86,"This report reproduces the paper ""Explaining Low dimensional Representation"" [1]. A detailed reproducibility summary is provided summarizing the results obtained using the implementation based on the codebase of the original authors along with a new PyTorch implementation from the efforts of this reproduction report authors. Communication with original authors (how to choose the ε hyper-parameter), hyperparameter search, discussion on results, recommendations for reproducibility and results beyond the paper (replicated the algorithm in PyTorch, new datasets and the use of more complex transformations to explain differences between clusters) are also properly constructed. Overall, this reproduction is well-constructed. Possible stretches could be the discussion of the evaluation metrics being used with respect to their suitabilities, additional metrics for a more comprehensive report and providing recommendations to the original authors for improving reproducibility.
   


[1] Gregory Plumb, Jonathan Terhorst, Sriram Sankararaman, and Ameet Talwalkar. Explaining groups of points in low-dimensional representations, 2020.
",cqAHExg2f
87,"This paper checks the original paper's four claims (listed in lines 97--103) on the algorithm TGT (Transitive Global Translations). As pointed out (lines 105 - 107) by the authors, the original paper's claims are investigated via the original paper's original code. The authors of this report use new code instead to verify some extended experiments. We doubt whether this is a well-focused reproducibility investigation.

We list our concerns below.

1. Language. There are quite a few grammar problems, and the paper appears carelessly composed.

2. In the 'methodology' paragraph, by putting 'We also replicate their findings by re-implementing the authors' method in PyTorch...' does it mean that all or only a part of the computing in the original paper is re-implemented?

3. Given that the communication with the original authors does not yield meaningful result and is only because of misunderstanding, why is that paragraph still listed?

4. Could the problem with the deprecated software package be solved by Docker or Anaconda? Note that any software package shall be deprecated at some time point in the future.

5. For the clustering's uncertainty, the authors reported in the 'What was difficult' paragraph, is it because of the random seed or different implementation techniques? Is the final result sensitive to different initialisation of random clustering? It seems to us that the paper only reports the observation but has not paid any effort to uncover the cause.

6. What does 'GCE' stand for? (lines 80 & 83)",cqAHExg2f
88,"The report is on reproducing the paper ""Explaining Low dimensional Representation"" by Plumb et al. Overall, the submission does rather thorough reproduction and even go beyond the original paper to add some extension (though missing parts such as hyperparameter search). However, writing of the report can be largely improved as currently it looks more like a student's report rather than a work that can be published in the special issue of a journal.

Reproducibility Summary: is provided and adequately reports the major finding of the submission

Scope of reproducibility: is clearly stated and followed later

Code: the authors of this submission both used the code provided by the original authors and then re-implemented it from scratch. The github link to the code gives an error (potentially because the repository hasn't been made public yet). In anyway there was no opportunity to check the code of the submission or its docs.

Communication with original authors: the reports mentions communication with original authors regarding a choice of epsilon hyperparameter, but the authors emphasised the difficulty of selecting clusters, but no mentioning whether this question was attempted to clarify with the original authors.

Hyperparameter Search: in the experiments with the original code the authors only used the same hyperparameters as in the original paper. No hyperparameter sweep has been performed.

Ablation Study: No ablation study has been done

Discussion on results: the discussion at the end is rather well done with discussions which claims from the original paper were confirmed and which were questioned during reproduction. However, presentation of the results themselves is the poorest part of the report from the presentation point of view. The results sometimes not thoroughly discussed but the text just refer to corresponding tables and figures, which in itself is not satisfactory, moreover, those tables and figures are also missing some details and can be unclear.

Recommendations for reproducibility: the only recommendation left is in terms of this selection of clusters, which seems to cause the main issue with reproducibility

Results beyond the paper: the report provides results beyond the paper when the authors investigate a more expressive transformation than considered in the original paper. Though the results show that this extension does not actually bring significant benefits it is interesting to see these results and negative results are also very useful for the community

Overall organisation and clarity: the part that can be mostly improved. As mentioned before, the results section is the poorest here lacking the good presentation.

Some particular points (mostly on organisation and clarity):
1.	Lines 5-6, “They show their method …” – the sentence is not grammatically consistent
2.	Line 43, x_s is not defined
3.	Notation x for a low-dimensional representation in Introduction is not very suitable as it is later used to denote a point in the input space in Methodology
4.	Lines 197-198, “Since the clusters in the Tensorflow…” – the sentence is not grammatically consistent
5.	Section 5.3.2 and 5.3.3, “deltas” and “logit gammas” appear without introduction (translation and scaling were not called like this before). They are being explained only later in Section 5.3.4
6.	Section 5.3.5, G is not defined and j is reused here as it denoted the group in lower dimensional space before
7.	Line 252, “Based on the reproduction…” – unfinished sentence 
8.	Section 6.1, first paragraph – translation of jupyter notebook into python scripts doesn’t seem to be a problem
9.	Line 268, “This also means …” – something wrong with this sentence
10.	Section 6.2 is redundant as this idea has been discussed at the beginning
11.	Figures 5-10 require more explanation, axes are not labelled, missing legend and overall explanation what is going on. “0th dimension” does not look good in the text

Minor:
1.	Line 171, there seems to be a typo and the second x_2 should be x_3
2.	Figures 2 and 3 – make all subfigures of the same size

",cqAHExg2f
89,"This reproducibility review is of high quality: the authors were successfully reproducing the earlier results in another modeling ecosystem and the reporting is clear. 

If there would be anything to improve, it would be about finding some examples that pose potential problems for reproducibility; even with good reproducibility there can be potential pitfalls and reporting what has been done to identify and point out the weak points would be informative if such examples can be found.
",kNqh-T-OJc
90,"The authors have done a fine job in reproducing the original paper. 
* The reproducibility summary is provided. 
* It seemed fairly easy to reproduce the results from the paper. 
 ** The authors converted the original code to Pytorch and started their analysis from scratch. Their results tallied with that of  the original paper. 
 ** Further, the authors ran the code on a new Crop dataset checking code/idea generalizability. This is a great move.

* Even though no communication with the original authors was needed, some areas to tidy up in the code and notebooks are provided by the authors but these are minor.
* Overall approach to reproduce the results and the paper are clear. ",kNqh-T-OJc
91,"Authors attempted to reproduce the main experiments in the original paper, namely (1) per-image attention maps on MNIST, (2) anomaly detection explanations on MNIST, (3) anomaly detection on UCSD Ped1 and MVTec-AD, and (4) latent space disentanglement on dSprites. The authors of the original paper only provided code for generating visual explanations on MNIST.

Authors distinguish two visualization methods: (1) visualizing explanations for VAEs by calculating the attention maps per latent dimension and aggregating these, and (2) generating attention maps from the sum of the inferred mean vector in order to visualize anomalies. Authors then compare the two methods for anomaly detection, which I believe is incorrect. (1) aims at highlighting the areas of the input image that explain the data (e.g. if the VAE was trained on 1's, which regions of the image look like a 1?). On the other hand, (2) aims at highlighting the regions of the input image that are not well explained by the distribution of the training data (i.e. if the VAE was trained on 1's, which regions of the image do *not* look like a 1?). Therefore, the first method should be used to reproduce Figure 1 in the original paper, whereas the second method should be used to reproduce Figure 4.

Authors used the provided code for the aforementioned experiments, where they had to remove an input parameter to make it work. There is an issue on this regard in the repository, where the first author of the original paper wrote the following: ""[...] To me, it definitely has to do with the new updates of PyTorch. But unfortunately I'm not exactly sure how they affect auto-gradients in our case."". I would encourage authors to double-check that the code behaves as expected, or run the provided code using version 1.0 of PyTorch as suggested in the repository.

The remainder of the submission attempts to reproduce the rest of experiments, but all results (including baselines) are quite far from those reported in the original paper. It is unclear why this happens and, while authors suggest that it might be due to shorter training runs, I suspect that this might also be related to some mistakes in the implementation of the attention maps.

In general, the submission failed at reproducing the original results. It is unclear whether this is due to a difference in the experimental setup or due to implementation errors. For this reason, I lean towards rejecting the paper and encourage authors to investigate why the results on MNIST could not be replicated before moving on to the more complex experiments in the paper.

Minor comment: captions for Figures 1 and 2 are very short and it is difficult to understand what is being shown.

Some typos:
- Reproducibility Summary: papers -> paper's, its' -> its
- Introduction: models -> model's (or models')
- Figure 2: Repliction -> Replication
- UCSD Ped1 Results: preform -> perform",b9wHFv2V_j
92,"This review is detailed and high quality. It aims to reproduce VAE results on multiple data sets. 

The reproducibility report is robust since the authors are able to replicate results in some of the data sets (MNIST). This, together with the detailed reporting, demonstrates that they had acquired the necessary understanding of the methodology in order to reproduce the original analyses. Reproducibility efforts fail in other data sets, and plausible explanations are provided. Communication with the original authors has taken place appropriately.

The work is significant as it robustly highlights potential problems in the original publication in terms of reproducibility and includes thorough reporting and discussion about the contributing factors.
",b9wHFv2V_j
93,"This paper provides a detailed description of the setup for reproducibility and well-discussed experiments and results. Given the limited resources and lack of support from authors of the original paper, it shows the difficulties for reproducing the main results for anomaly detection. 

It is not very clear about the scope of reproducibility based on the description from the reproducibility summary on page 1. It sounds more like a description of the main results in the original paper. 

“To produce attention maps to localize anomalies for the MNIST dataset the repository of the authors could be used.” It is not clear here whether reproduced from the reused author repository or not.


The writing of this paper needs to be polished. Examples below show the sentences that may need to rephrase: 
“The papers primary claim is that ...”
“ ..., its’ supplement and external source code.”
",b9wHFv2V_j
94,"The authors have attempted to reproduce the results of the article ""Towards Visually Explaining Variational Autoencoders"" by Liu et. al. (CVPR 2020). They have presented a summary report which has all the required elements. 

The authors have made a significant effort to reproduce the results of the original paper. They mentioned that they obtained some of the code from the authors and sourced the rest from other github-repositories. They also report having contacted the authors to clarify information in the original paper.  

The report is reasonably well organized though the explanation of the original paper in the Introduction could use improvement. They could include some more details about the methodology so as to enable understanding it without reading the original paper.

Some of the other concerns I have with the report are as follows: 

1. They report that the original paper's authors' code was incomplete but do not clarify what additions/adjustments they had to make to the code. They have not provided their modified codebase.
2. The authors' primary focus is to reproduce the anomaly detection results and report that they were unable to reproduce all the results of the original paper. They also report that their results for the quantitative metrics do not match those of the original paper. However, the authors have not tried to explain why  The code for quantitative metrics used to evaluate the results was written by the authors themselves. Is it possible that there were errors/differences in the evaluation code? There is no mention of any correspondence between them and the original paper's authors to discuss the differences in the results. Their final conclusions do not include any analysis of the differences in the results. 
3. It is impressive that they have reported the training time for each of the data sets that they have tested on but they have not due diligence on the hyperparameter sweep. The only hyperparameter they have performed a search on is the number of epochs. They have used the default values  (as specified by the original paper's authors in their supplementary material) for the other hyperparameters. 
4. Though the reproduction of the results of latent disentanglement using the dSprites dataset is included in the scope of the reproducibility they have not performed these tests. 

In summary, though the authors have made a significant effort and shown the difficulties in reproducing the results of the original paper,  I believe that this report could use a lot of improvement. 
",mXpKOalTtN8
95,"The original paper was difficult to reproduce because of the lack of certain parts of the code and poor documentation, however, the authors did a good job in reproducing and verifying various claims. As per the results, the quantitative values reported here are lower than expected. The paper is very well written. What is lacking is a thorough hyperparameter exploration. 
 ",mXpKOalTtN8
96,"The authors provide a very nice summary and concrete that is being elaborated in the document. They cover fully the reproducibility of the paper in terms of scope where they reused the original author's code. It would be interesting to perform a hyperparameter search in order to verify the claims of the authors and maybe find new hyperparameters not experimented with the authors. 

In general, the report is well written and has a clear structure so I recommend it for being accepted. ",mXpKOalTtN8
97,"*Scope of reproducibility:*

The authors clearly state the scope of their experiments (reproducing the embedding experiment, restricted-self attention with BERT and the sequence-to-sequence experiments), and cleanly execute on them. 


*Code:*

 In some instances, the authors re-used code from the original authors, and in some instances, they wrote their own. I did not see a reference to the code written for this paper. 


*Communication with original authors:*

The paper authors reach out to the original authors. It looks like a good discussion took place, and the original authors were helpful in clarifying some of the ambiguities that arose through the paper (and also in providing code + datasets). 


*Hyperparameter Search:*

Neither the replication study nor the original paper used a hyperparameter search. However, the replication study included results on the variance between the 5 random seeds (original paper reported the mean). 

*Ablation Study:*

I don't believe the replication study performed any ablations. 

*Discussion on results:*

The replication study presented an excellent description of the reproducibility of the original paper and made clear when the results reproduced and did not reproduce. They clearly stated that some details were ambiguous, but that they were ultimately able to resolve those details. 

*Recommendations for reproducibility:*

The replication study authors and original paper authors seem to have clarified some ambiguities during their discussion. Those would be useful to add to the original paper. 

*Results beyond the paper:*

The replication study investigates the idea that under-parameterization of the models could lead to a decrease in test accuracy when the hyperparameter controlling attention mass on impermissible tokens increases (less able to pay attention to impermissible tokens). They investigated this by increasing the embedding dimension and found it did not improve test accuracy. 

*Overall organization and clarity*

* I appreciated the reproduction of the tables from the paper with the author|reproduced! It made it easy to follow along with. 
* Thank you for including the breakdown of the computational requirements for running each task (table 2). This is great. 
* I think the explanation of the seq2seq tasks makes sense if you have read the original paper, but could be confusing if someone has not. Please try writing more on this!

	
",-rn9m0Gt6AQ
98,"The authors follow the Reproducibility Summary very well.
The authors re-used the original code repository. They also ported the code to PyTorch Lightning to make it easier to reproduce the research in the future.
They did not change any hyperparameters and used the same value in the original paper.
The authors had fair contact with the authors of original papers and they had discussed minor issues.
There are no changes in hyperparameters. The implementation results are the average of five-run times of training the model. They used all datasets except the Reference Letters, due to privacy concerns.
The authors added two Blue scores in the seq-seq model for translation.
They do not propose any beyond results or improving the original paper, but they have a good discussion that can describe the deep understanding of the paper.
There are minor grammar typos.",-rn9m0Gt6AQ
99,"The paper is well written and easy to follow. Authors seem to understand the original paper very well and have done a good job in organizing the content. Given one of the classification dataset was not publicly available, reproducing for one task was not feasible. Apart from which authors have experimentally verified the claims on other tasks/datasets.

Authors have been in constant touch with original paper authors and it is appreciable that original authors have helped in reproducing the experiments by providing data/code as requested by authors along with providing more details/clarifying queries.

It is good that the authors had re-implemented BERT model (as the source code was not available initially) and discussed the challenges in re-implementing with information provided in paper which is important for Reproducibility challenge. However, they were not able to replicate the results. Once the source code for BERT experiments was provided by original authors they were able to reproduce the results and verify authors claims. It would have been interesting if the authors could have identified the reason for performance drop in their re-implementation setting. 

In table 4, it is interesting to note that there is no change in accuracy for 'Embedding model' on occupation prediction task which is not expected as removing impermissible tokens negatively impacts performance. Any explanation for this behavior w.r.t impermissible tokens ?
Table 5 includes accuracies from paper and reproduced results, however the original paper does not report accuracy for the task 'En-De MT' - where do these numbers come from ?

Additionally, authors experiment with different embedding sizes hoping to counter the performance drop as λ increases - however it was inconclusive. Having said that authors provide another insight into why they believe could be the reason for performance drop. It would have been interesting to see the impact of the way to remove impermissible tokens on performance trend.

Overall, the paper provides good discussion points and clearly outlines what was provided, what was challenging to infer based on the information in paper/code. They perform all the experiments mentioned in the original paper and verify the claims along with providing insights and implementation aspect. This work helps in understanding the internal details required to reproduce the original paper. Hence, I would recommend to accept this paper.",-rn9m0Gt6AQ
100,"***Reproducibility Summary***
The authors provide a complete and useful reproducibility summary.

***Scope of reproducibility***
The authors clearly state the experiments they are trying to reproduce from the original paper and their setup.

***Code***
The authors reimplemented some code and referred to some parts of the original code, which is not complete and is missing parts to reproduce certain experiments.

***Communication with the original authors***
The authors of the report clearly indicate that they have had limited communication with the original authors of the paper through GitHub issues. Since there were some issues in reproducing some of the results, more communication is encouraged to further investigate the issues.

***Hyperparameter search***
The authors coducted reasonable hyperparameter searches considering their computational resources and the amount of details provided in the original paper.

***Ablation study***
The authors have conducted additional ablation studies on the idea proposed in the original paper as an extension of that work.

***Discussion on results***
The discussions of the results is through, highlighting what was reproduced, what was not and possible causes.

***Results beyond the paper***
The authors have included ablation studies as additional results beyond the original paper.

***Overall organization and clarity***
The report is well organized and it is quite clear.

Overall this is a good report, with reasonable experiments that highlight issues in reproducing some of the experiments in the original paper. More communication with the authors is encouraged to try to understand better whether the original results are not correct or there are missing details in this reproduction.",eAy3DVh0xp
101,"The authors reproduce the paper ""Towards visually explaining variational autoencoders"" [Liu et al. 2020]. The authors provide a brief summary of the paper and try to reproduce all the experiments mentioned in the paper, however for many of the critical experiments the results reported are different from the original paper. For instance, for the MVTec-AD dataset, the authors report numbers in table 2 and for some of the instances, they have double-digit differences in their AUROC measurements. 
In addition, some of the provided figures are not very informative. For example in Fig.2 and Fig.3 qualitative results of the reproduced model as well as the original model are provided however since the frames are not exactly paired it is sometimes hard to do a fair comparison. Furthermore, for some frames that look similar, the attention amps look vastly different.
Furthermore, the language of the report can also be improved.
Also an extension of the original paper, authors investigate applying this method  to stacked RBMs but as they claim they could not obtain any meaningful and explainable results.",eAy3DVh0xp
102,"In this report, the authors try to reimplement, reproduce and extend results from the paper Liu et al. (2020) - Towards visually explaining variational autoencoders. The original paper takes a step towards visually explaining generative models by using visual attention maps conditioned on the latent space of a variational autoencoder (VAE). 

The authors of the reproducibility report some parts of the original authors’ code and did slight hyper parameter tuning. Most of the results are claimed to be reproduced for parts of the original paper for which the code was available, while for some other parts, reproducing results was hard. The authors of this report also did not have an extensive communication with the original authors, except minor ones on Github by opening an issue. 

The report not only tries to reproduce the results from the original paper, but also tried an extension of the work by using Restricted Boltzmann Machines. The authors of the report provide a clear description of the datasets, and include a couple of ablation studies which are useful.

Some of the parts which seemed missing or confusing were:
- Include appropriate citations - for example, for Restricted Boltzmann Machines
- Line 72 - Figure 9 and 8 in the appendix are referred to. But these don’t exist in the appendix
- Notation in the Equation 1 can be clarified
- Line 138 - Could be made clearer on which parts of the original code are being referred and including more details directly from the paper here would also help.
- Formatting of Figure 1
- Line 188 - …similar to that of the paper (see 2). —> Seems the referred number “2” is Table 2? This can be properly labeled and linked


Overall, the authors seems to reproduce the results and make an educated hyper parameter selection. Ablation studies and a couple of insufficiencies from the original paper are also highlighted. If the manuscript can be proof-read and typos and missing figures can be fixed, it can be taken into consideration in the final score. ",eAy3DVh0xp
103,"Even though the authors failed to reproduce results regarding performance, they find that the privacy claim is valid. 

They provide a detailed discussion about what was easy and what was difficult when they prepare the reproducibility report. The report is well-written and easy to follow. ",zLzuHtQopDp
104,"The authors did well. Though the codes for the original work was not available, they reproduce the work by coding from scratch.",zLzuHtQopDp
105,"This paper attempts to replicate the work in Interpretable Complex-Valued Neural Networks For Privacy Protection which in general was not possible due to missing details in the original paper.

The author provided a good description of the work that they have done to replicate the original. The authors identified the details that were missing in the paper. They also mention time constrains, which did not allow them to pursue for example some further hyperparameter tuning.

The authors tried to contact the original authors without any success.

What would have been interesting is to provide a full summary of what would have made the reproduction easier, besides providing the pseudo-code.

All-in-all, it is a good paper.
",zLzuHtQopDp
106,"This paper tries to reproduce the results provided in ""Identifying Through Flows for Recovering Latent Representations"". the authors reproduce the experiments in the original paper obtaining results, which are reasonably close to the original paper's reported numbers, diverging by only about 2.5%. The reproducibility report is written in a clear way having details of the experiments conducted. Furthermore, the authors investigate further experiments comparing the model with additional Flow-based models to give context to the devised model. Furthermore, further discussion is done on the applicability as well as the practicality of the iFlow model. 

I have a few minor questions:
Why are the MCC results so different on different seeds? [Figures 1-3]
In figure 4, the data samples on which iFlow and Flow are compared look actually different and for the iFlow the data itself is more disentangled. Can you please provide some more comparable results for this?",1E7p3r8-zxV
107,"This reproduction report does extensive works on different aspects to verify the original paper iFlow. The authors give a comprehensive study on three objects: 1) reproduce the results of MCC-scores presented in the original paper; 2) they elaborate on the fairness of the metrics used in the original paper; 3) they also evaluate the usability and practical advantages by working on different commonly used machine learning/presentation learning datasets. The authors of this report find some interesting points which are not presented in the original paper, and they also give a clear presentation of the experiments, analysis, and conclusion. Therefore, I feel this report is a good reproduction of the iFlow paper, which gives us a better and confidential understanding of the paper.  ",1E7p3r8-zxV
108,"This paper discusses reproducing the work included in Learning to Deceive with Attention-Based Explanations by Pruthi et al. Pruthi et al. claim that attention weights can easily be manipulated without significancy accuracy loss and that human subjects can be deceived by this attention weights. The paper focuses on reproducing the first part of the claim as the second part requires a user study. 

The paper focuses on three models: embedding, biLSTM, and BERT. There are two evaluation tasks: classification (occupation, pronoun-based, sentiment analysis), and sequence-to-sequence. The paper shows that the results do reproduce the results fairly well, and the reproducibility aspect was acceptable (with quick responses from the authors).

Overall, the writing style in this paper needs to improve. There are many grammatical mistakes, and the paper does not flow very well. The first half of the paper did a good job of explaining the problem and the models used. However, the second half of the paper, including the results and reproducibility portions, needs more work. I would have like to see the differences in training time / evaluation time, better explanation of Table 4 (especially what the column names are), and more information on what correspondence occurred with the original authors (did they give a more thorough code? define the anonymization functions better?) ",VDlDzmfZaxg
109,"The report describes the efforts in replicating the results of ""Learning to Deceive with Attention-Based Explanations"", where it is shown how 1) attention weights can be manipulated without loss in performance and 2) humans can be deceived by the obtained attention weights. The report describes the efforts in replicating the first claim. The authors of the report used the code of the original paper to replicate the results, adding the transformer part and missing anonymization functions. Overall, the results have been extensively replicated and the experiments well described. There are however some parts not well written (e.g. typos, half-sentences, missing links),  with missing explanations (e.g. multi-class sentiment analysis, transformer, anonymization) and with critiques to the claims of the paper (e.g. importance of the user study, capabilities of deceiving of the attention weights) not supported by experiments. Overall, I think the reproduction effort has been well conducted, but the report needs to be improved (see weaknesses below) to be accepteded. Below I detail what I think are the strengths and the weaknesses of the report. 

Strengths:
+ The authors managed to replicate all the quantitative results of the main paper (on public datasets) with fair efforts for the missing components they added.
+ The experiments are very detailed, from the data splits to the hyperparameters used.
+ Discussions on the difference between the reproduced and the original results are also thorough and well justified.

Weaknesses:
- Maybe it is due to the lack of time, but the report misses careful proofreading. There are Tables not well linked (e.g. lines 136,172) wrongly cited equations (Eq. 0, line 74), typos (e.g. ""Reproducability"", line 207), missing end of sentence points (line 182), upper case start of sentences (e.g. line 161), half-sentences (line 202). Proofreading the report is necessary to ensure its quality. 

- In Table 3 multiple numbers have an empty standard deviation for the A.M. column. Why is this the case? The table looks a bit weird with all the empty space after some +-, thus it would be good to add those values (preferably) or eliminate the +-.

- Some parts and definitions are not clear. For instance, I was not able to find definitions for A.M. (Table 3-4) and I (equation ""0""). Similarly, the report mentions an extension of the paper by performing multi-class sentiment analysis (line 89) that however is not detailed in the following section and is not reported in the results (since the comparison is with the values reported in the original paper). These details should be included to avoid misconceptions.

- The attention weights \alpha are computed from the dot product between QK^T  ""softmaxed"". However, line 54 reports the non-softmaxed version of the dot-product to compute the attention, which is inaccurate. I would suggest the authors
to re-define the attention A by explicitly showing the contribution of \alpha and how it is computed.

- The difficulty of the reproduction due to 1) missing transformer code and 2) missing anonymization (lines 209-210) are not extensively described in the report. Since the report should highlight the encountered difficulties (if any) in reproducing the original results, I would extend section 2 to include a discussion of any components the authors needed to add to reproduce the results + every effort (e.g. missing libraries, dataset set up) that was required to reproduce them.

- The scope of the report is to reproduce the quantitative results of the original work, without reproducing the user studies. While I agree that the three subjects of the original paper do not constitute a large set allowing drawing general conclusions, I do not think it is fair to say that 1) the human study is unnecessary and unfounded (lines 213-215) and 2) raising doubts on the capabilities to deceive of the model (line 218). These are personal thoughts with no scientific/experimental grounds, since not results are shown to support that humans might not be deceived. I strongly suggest removing any claim not supported by 1) experiments 2) experience in the reproduction.

- The sentence in lines 214-215: ""it added bulk to an unstructured paper, which in our minds, would have benefited from less individual projects""  is non-sensical and not founded anyway since 1) the unstructured paper is the original one? (peer-reviewed and accepted to ACL 2020) 2) what are the individual projects?",VDlDzmfZaxg
110,"The authors did a good work, they joggle round missing data and came up with same result as the original authors.",VDlDzmfZaxg
111,"Reproducibility Summary:
- The summary did a nice job outlining the report.

Scope of reproducibility:
- Makes clear what the scope of the report is, and provides nice justification for why certain parts were left out (i.e. missing data sets).

Code:
- Uses the original code
- Does an extensive look and dissection of the code base to find some issues worth noting. For example:
  - orthogonalization applied to both the P-path and Q-path), which wasn't obvious from the original text. They do a small experiment to test the model's sensitivity to this change.
  - The final prediction is calculated differently than reported
  - Unreported fine-tuning of the embeddings for the two paths independently
  - An odd choice in how the dev set is chosen from the training set.
  
Communication with original authors:
- Mentioned sending email to clarify parts of the code with no response.
  
Hyperparameter Search:
- None done in the reproduction, and none done here.
  
Ablation Study:
N/A

Discussion on results:
- Did a nice job raising concerns about the original paper's findings.
- Raised many concerns, gave evidence for why these concerns would be problematic (even testing what the changes would be), and gave concrete steps for improvement.

Results beyond the paper:
- Expand the set of evaluation metrics
- Test claim's generality on different architectures. Specifically, the authors applied the metrics to a Bi-directional LSTM model.

Overall organization and clarity

Overall, the report is well written and relatively clear. Some things I would suggest/some questions:
- How many runs did you do for each experiment? Is this similar to what the original authors did?
- table 6, how were the confidence intervals calculated?
- The sub-section ""other attention mechanisms"" in section 7 is a bit out of place. Maybe you can remove the this entirely, and add it to the final discussion? Because you don't run more experiments here, it just doesn't seem to fit. If you do run experiments here, this needs to be made much clearer.
- The original paper does not seem to do a parameter sweep of any kind, so this would have been a nice inclusion in the report. Event if it wasn't for all the data sets.
- The original paper chooses the best model (out of a set of unknown size) from the validation accuracy and reports the test accuracy. I'm not apart of the community this paper is aimed at (i.e. NLP/Explainability), so I'm not sure how common this is as a practice. Usually, I would prefer to see multiple runs and either a median or average reported with confidence intervals. Of course this can be ignored if this isn't standard practice in this community.

Again. I think this report is well put together and the biggest weakness is the lack of hyperparameter search and ambiguity around how the models were reported (i.e. best of, mean, median) w/o confidence intervals.
",lE0wqKGROKa
112,"* Reproducibility Summary

  The report contains a well-defined and articulate reproducibility summary as prescribed by the challenge.
* Scope of reproducibility
  
  The report contains well-defined scope involving two central claims of the original paper - attention weights not being faithful in plausible explanations in LSTM, and methods to reduce the conicity in order to increase the plausibility of the explanations.
* Code: whether reproduced from scratch or re-used author repository.

  Authors provide their own codebase link, which consists of the code re-used from the original repository. The codebase is well structured with proper README in the appropriate places.
* Communication with original authors

  The report mentions that they have contacted the original authors, but they did not hear back from them. This is unfortunate but sadly happens quite frequently. I applaud the author's effort to reach out to the original authors despite the no reply.
* Hyperparameter Search

  It does not appear that the authors performed an additional hyperparam search than what was reported in the original paper.
* Ablation Study

  The authors compute several extra experiments as part of the ablation of the original work. They add a new evaluation method to clarify the conclusions of the original paper further using LIME. This is a splendid idea, and the correlation results with Pearson correlation (funny it's a correlation of a correlation!) and JS divergence shows the need for such study. The results are mixed, as the main selling point of the paper (Orthogonality and Diversity) does not correlate well with LIME. I would be curious to hear from the authors if they read this review.
  The authors also tested for generalization using Bidirectional LSTM to test the author's claims further (and add a note on why other mechanisms, such as Transformers, are not straightforward to evaluate in the same setting). The authors find the proposed methods do not unconditionally improve the explanations. These kinds of cross-architecture robustness experiments add tons of value to the original paper, and I commend the authors for doing the same.
* Discussion on results

  The report provides a clear and concise discussion of their findings. The authors provide faithful results, both of which experiments worked and which did not. The authors summarized their findings on the original paper and conclude Orthogonal LSTM does clearly leads to lower conicity than Vanilla LSTM, however, the results are mixed. Table 4 is a great summary, clearly defining how each of the claims is supported or not by their reproducibility study.
* Recommendations for reproducibility
 
  The report goes above and beyond to conduct a thorough code review of the original paper, which is a stellar contribution to both reproducible research and to the understanding of the code provided by the original paper. The authors further provide ample discussion and conclude the benefits of orthogonality and diversity training are more relevant for simpler tasks.
* Overall organization and clarity
  
  The report does not have any significant typos. It is well organized into appropriate sections.
",lE0wqKGROKa
113,"1. The authors have clearly identified the following claims in the paper for reproducibility:
   1. Superior privacy preserving properties of the complex-values networks compared to the standard networks.
   2. Similar classification performance as real-valued networks on both datasets.
2. The authors of this report have implemented the code by themselves as the original implementation wasn't open-sourced. Despite the lack of information, the authors have also done hyper parameter sweep. The authors have clearly provided the implementation details in their report.
3. Due to lack of time they couldn't communicate with the original authors and have mentioned it in the report.
4. In their discussion and results section, the authors have clearly identified the parts of the paper that were easy to implement and those that weren't. Through extensive experiments, the authors have shown that while the privacy preserving claims of the complex networks hold, their classification performance is impacted compared to the baseline model. This is in contrast to the claim reported in the paper. However the authors have also considered if the difference in the accuracy might boil down to implementation of underlying architecture. Overall the authors have suggested that to further support the the main claim of privacy preservation, the paper can be augmented by adding pseudo-code and figures.",U2E22LewEX1
114,"The reproducibility of this paper involved coding the experiments since no code was available from the original authors. Considerable effort has been taken to recreate the experiments as closely as possible but no effort was taken to contact the original authors for hyper-parameters to test the reported metrics. The reproducibility report, however, outlines the main claims made by the original authors and test these claims in their experiments. They have contrary results to the original claim that complex valued networks perform similar or better than real-valued networks. However, this is difficult to compare without training the network on the original parameters with the right optimiser. Moreover, the models have only been trained once which is not adequate to obtain optimal model performance and hence not adequate effort to report believable results.
The reproducibility report is able to verify that complex valued networks are less susceptible to inversion attacks but with weaker results. This could again be attributed to the difference in model parameters and optimisation technique used and hence an expectation of identical results is not reasonable.",U2E22LewEX1
115,"This report does reproducibility study of Xiang et al. ICLR 2020 Interpretable Complex-Valued Neural Networks for Privacy Protection.

Reproducibility Summary: present with adequate and sufficient summary of the performed reproducibility study.

Scope of reproducibility: is clearly stated and the report follows it

Code: the authors of the report implement the code from scratch as the original authors didn't make their code publicly available. The link to the repository is provided. All the results from the report seem to be able to be reproduced by following the easy to follow jupyter notebook. Although the code looks clean and readable from a quick glance, the documentation for the code is very limited or absent.

Communication with original authors: the authors of the report claim that they didn't communicate with the original authors due to lack of time. This claim without any further clarifications looks a bit odd as communication with the original authors may resolve some issues which the authors of the report mentioned as required lots of time, such as some missing implementation details in the original paper.

Hyperparameter Search: no hyperparameter search has been performed with discussion that this was due to limit of time. Considering the amount of experiments performed and that the implementation was done from scratch, the claim looks reasonable.

Ablation Study: no ablation study is performed beyond the original paper.

Discussion on results: The report explicitly discusses which parts were easy and difficult to reproduce and which claims from the original paper were confirmed in their experiments

Recommendations for reproducibility: though there are no explicit recommendations the authors emphasise which implementation details were missing in the original paper that caused the most problems in the reproducibility study

Results beyond the paper: No results beyond the paper, moreover some of the more computationally heavy results from the original paper were not reproduced

Overall organization and clarity: The report is very well written and easy to follow",U2E22LewEX1
116,"This is a well-written and faithful replication of the original work. The authors describe the methods well and do their best to reproduce and extend studies of the original methods to multilingual settings, providing their own code along the way. Though some of the original datasets were not accessible to the authors, they swapped these out for extended studies which seem to show consistent results.

Overall, I think the paper was well-written and met the expectations of a great reproduction. Perhaps, some additional discussion on the intuition behind adding multilingual experiments and the potential connection to conicity might be worthwhile to motivate the additional experiments a bit more. In general, additional discussion of findings might have been helpful to the reader, but not necessary. The description of the original method served as a nice summary of the original work as well.

Typos:

In the conclusion: ""Majority of..."" -> ""The majority"" 

Appendix 1, citation for adam is broken.",r3R_osip5G
117," I can confirm that the authors included a clear and concise reproducibility summary, noting that they reused some of the original code and data and made a few modifications to original code base to extend the experiments. The authors highlight that there were some challenges working with the data where licensing was involved and in other cases where the datasets were large.  The authors do not try new hyper-parameters, but re-use the hyper-parameters from the original paper. Ablation or recommendations to the original authors are not provided, however the authors do provide some results for independent experimentation on the CLS dataset. The paper is otherwise well written and easy to read. ",r3R_osip5G
118,"* Reproducibility Summary

  The report contains a well-defined and articulate reproducibility summary as prescribed by the challenge.
* Scope of reproducibility

  The report contains well-defined scope involving six central claims of the original paper. The paper also investigates additional claims on Transformers and multilingual data.
* Code: whether reproduced from scratch or re-used author repository.

  Authors provide their own codebase link. However, I was unable to open it - seems the link https://github.com/KacperKubara/Transparency does not exist. Neither did I find any code in the appendix. I'll be happy to increase my score if this is fixed.
* Communication with original authors

  The report mentions they did not communicate their results/findings to the original authors.
* Hyperparameter Search

  The authors did not perform any additional hyperparam search, they seem to have only run the default ones provided by the author. This is a missed opportunity in my view, as the authors could have explored various hyperparam choices to see if the results hold more robustly.
* Ablation Study

  The authors perform an ablation study by introducing Transformers and multi-lingual data. This kind of ablation study is perfect and highly appreciated for a reproducibility report.
  It's interesting to find the conicity of Transformers to be much smaller. However, that doesn't totally imply Vanilla Transformers to have a more plausible explanation of attention weights. Also, the choice of evaluating the same on Transformers is tricky, as multi-head attention systems it is generally difficult to pinpoint the attention contribution of a single word.  The authors could have compared their results with that of what BERT looks at https://arxiv.org/abs/1906.04341, but in any case, this additional set of experiments are quite welcome.

* Discussion on results

  The report provides information on which parts of reproduction are easy and which are difficult. Not surprisingly, certain datasets are difficult to procure and some are harder to train. The report does a good job mentioning these.
* Recommendations for reproducibility

  The authors highly commend the original paper on their state of reproducibility.
* Overall organization and clarity

  Overall, minor typos but not that significant. In Section 4.1 the authors might have forgotten to comment out the line ""Logically group related results into sections""
",r3R_osip5G
119,"The report aims to reproduce the results from the following paper ""CNN-generated images are surprisingly easy to spot... for now"". The report is very complete, well written and well executed and although I have not read the original paper I was able to understand the ideas behind the original paper just by reading the report.",HhcM-JXyhl3
120,"This report reimplements the algorithm proposed in the original paper and confirms the validity of the experimental results in the paper. The pre-trained model used in this reproducibility assessment paper can be accessed in an anonymous git repo and the report gave details about how the reproducibility test is organized. Additionally, the report also points out the performances in the original paper depend on the choice of the training data set and the choice of the data generator.  This is the limit of the work in the original paper, which helps practitioners to better use the proposed algorithm. 

",HhcM-JXyhl3
121,"The paper seeks to reproduce the results of the paper titled ‘CNN-generated images are surprisingly easy to spot... for now’. They sought to validate two  main claims in the original paper – that data augmentation and data diversity helps with generalization in the context of real/fake image classification. They don’t validate the third claim that data augmentation aids robustness. In addition to the above, the paper also investigates the situations then the training data generator and classifiers are changed. 

Overall, the reproducibility study is reasonable. And, despite the shortcomings mentioned below, it should be useful to the audience of this challenge as well as the wider AI/ ML community interested in the original work. However, due to the concerns below, I recommend rejecting this paper with a rating of marginally below threshold (which I’m willing to revise based on discussions with the authors).

In the following, an evaluation of this paper on the metrics suggested by the RC 2020 challenge is presented:
Reproducibility Summary: 

The authors have provided a brief and clear summarization of the problem statement and the proposed approach and have reported their major findings.

Scope of Reproducibility:

The authors clearly enumerate the scope of the reproducibility study to validate two claims from the paper with a partial recreation of the original experiments as well as several additional ones. The study is designed and conducted accordingly.
Code: 

The code for the original paper is publicly available. However, the authors have re-implemented the code for the experiments performed and use the original implementation sparingly. The code base is submitted with readable code and docs. 
Communication with original authors

The authors mention that most experiments could be reproduced using minimal communication with the original authors, given the details in the main paper and the well-documented code repository. There were a few experimental settings that lacked clarity (or were misunderstood by the report authors) which were resolved by communicating with the original authors. 

Hyperparameter search 

For the reproducibility experiments, the authors use the hyperparameter details provided in the main paper and do not perform any parameter tuning. However, the authors perform data augmentation (Blur + JPEG) differently from that of the main paper. The variation in the implementation is also hypothesized as the reason for the discrepancy between the obtained and original results. Thus, a hyperparameter tuning for the same is also expected. 

A discussion on the hyperparameter search for the additional experiments performed is missing. 
Discussion of results 

I have the few concerns: 

Data diversity and generalization: These results are presented in Table 1. The authors conclude (line 154) that these results are similar to the original reported results. This is clearly not the case when differences as high as 24% in absolute terrms (Deepfake 20-class: 66.3% --> 93.7%; SAN 2-class: 52.9 --> 72.3) are seen. Even when the differences are low, they seem statistically significant (StarGAN 2-class: 87.3 --> 83.5). However, the trend that diversity leads to better generalization seems to hold.

It is speculated that the difference in results is due to the different blurring function used and the way blurring, and JPEG compression is applied to the data. This could be the reason but doesn’t seem to be verified. 

The authors also demonstrate via Figure 1 that the correlation of improved performance with more diversity is stronger when accompanied by data augmentation (blur, jpeg compression) than without.

The observation “dependent on the data set” in lines 173-174 is concerning. Since this reference to the data set is test data, all it says is that the results are not expected to hold across different test scenarios.

Data augmentation and generalization: This analysis is reproduced in Table 2. We observe statistically significant differences even in this experiment, even for the ‘No Aug’ scenario where no blur or JPEG compression is applied. For example, (SAN No Aug: 93.6 --> 87.2). Secondly, the trend of improved performance with data augmentation is really mixed with 4 settings out of 10 -- StarGAN, SITD, SAN and DeepFake bucking the trend. Whether this can be entirely attributed to variations in implementation is not investigated 

However, the overall trend that diversity (when accompanied by augmentations) shows an improvement in performance seems clear and stands validated by the report. 

The authors also make a key observation that data diversity alone (without augmentation) is insufficient to achieve generalization which was not clear from the experiments in the original paper.
Recommendations for reproducibility 
The authors were able to reproduce two major claims of the paper and obtain better performance than originally reported for some experiments. They suggest that performing the data augmentation – “blur + JPEG,” simultaneously rather than sequentially helps improve overall performance.  
Results beyond the original paper:
Two additional investigations are carried out that go beyond the original paper: (a) changing the generator used for training (pre-trained StyleGAN2), and, (b) training a different classifier (VGG and DCT-ResNet).

While some trends are clear (ProGAN better than StyleGAN2 for training), others not quite and it seems that the authors are not careful in making deductions.
-	Line 206: it should be 87.6 --> (86.3, 87.1, 89.2, 97.1}
-	(l. 209-212): For StyleGAN2, other proposed augmentations don’t worsen results for StyleGAN2: not for GauGAN, CRN, IMLE, SITG, SAN. For ProGAN: blurring doesn't hurt for StyleGAN, StyleGAN2, BigGAN, StarGAN, SITD, DeepFake. Other's don't improve for StarGAN, SAN, DeepFake.

The question of whether the performance improvement and generalization trends hold for other classifiers is not properly discussed. It also lacks a detailed discussion on the implementation and hyperparameter tuning for the additional experiments performed.
Overall clarity and organization 

The report is well structured in general and has a reasonable clarity. The readability could have been improved by organizing the experiments in the report in the same order as that of the main paper (where discussion on data augmentation precedes diversity). 

The authors should address the following issues in their draft:
-	Typos in specifying the learning rates (lines 124-126): 1e-3 or 10^-3 instead of 1^-3 etc.
-	(line 10) It’s not clear what is meant by - “if the results extend beyond the original contribution”.
-	The authors don’t clearly mention whether the data augmentations were applied only to the synthetic images or the real images as well.
",HhcM-JXyhl3
122,"This report is reproducing most of the experiments from the paper ""On Warm Starting Neural Network Training"". While the general trends are visible in the reproduced results, there are many details that are not the same. As the reproduced paper doesn't provide confidence intervals in their figures (which the original paper does) and they show test instead of validation results, it is not very easy to compare results. Equivalent experiments in most of the examples achieve different max accuracy. Even more concerning is that gap that is shown in the very first experiment in order to motivate the work, is almost not existing in Table 1 of reproduction. Other experiments also show unexpectedly good performance of warm start models in the reproduced results. Overall, experiments results within the report show inconsistent behavior and instability.

The report is well written and organised and it contains a summary section at the beginning. Report authors clarified questions with original authors. The link to the implementation doesn't work. I've searched the repository and that project is deleted. 

I like the report, but I am giving grade 5 because the results are not matching the original paper results, and since they are less extensive and elaborative and with no confidence intervals, I have more confidence in the results of the original paper. Additionally, the link to the code doesn't work, so it's not possible to see the setup of the reproduced experiments.

Below are comments related to different experiments in the order in which they appear in the report:
- Experiment in Figure 1: Report authors use only 200/400 epochs instead of 350/700 epochs used in the original paper, but they are able to show the same effect. It is good that even with fewer time resources, we can show the same effect.
- Table 1: While the original paper gives us validation accuracies, the report authors give us training and test accuracies, so it's not comparable. For LR, there is no gap according to Table 1, but similar is true for original Table 1. However, while in the original Table 1 gap is obvious between random init and warm start, in the reproduced study, that's not true. Most of the gaps are within one standard deviation and gaps are inconsistent, meaning that in some cases warm start is better. Also, test results here are much worse than original validation results. Did they train for the same number of epochs? The difference is too big to be explained by validation vs test set difference.
- Figure 2: there is no confidentiality interval that exists in the original paper. As authors of the report saw that SVHN behaves differently, they added additional experiments here, which show slightly different behavior and require a convergence threshold of 99.9%. I would suggest changing the scale of Figures 2 b) and c) to show only information above 80% or 85 % of accuracy so that it is easier to see the gap.
- Results in Figure 4 are different than in the original study. In this report, warm start models are performing consistently better than in the original paper, so we could see there was almost no gap visible in Table 1, and in Figure 4 performance of warm start is better than the random start, even though that's not true in the original paper. Again, the report deals with test errors while original papers worked with validation errors, but that shouldn't affect the conclusion. It is also interesting to note that the report is able to achieve better results on the test dataset with a few warm start experiments than the original paper achieves on the validation dataset with any model. That is suspicious to me.
- While the original paper has confidence intervals for Figure 4, that is not given in reproduction (Figure 3) and the patterns show small but visible differences, which might be explained with confidence intervals. Again, the test accuracy that is achieved by the report is up to 5% higher than in the original paper.
- What is the x-axis of Figure 5? Is it the number of epochs?
- Figure 6 (matches original Figure 7): Behavior is inconsistent for different lambda values. No pattern can be spotted in this figure, except that for lambda=0 train time is strongly the highest. It is very strange that for lambda=0 training time is twice bigger than expected for around 25 thousand examples.
- In general, results reported in those figures are less smooth, which is probably because the information is displayed in more coarse grain, but it may also be due to the instability of the models or issues with reproducibility. However, it is not clear from the report which one it is, and it seems it's rather from the former.
- Authors add experiments on warm start with augmented data in which warm start slightly outperforms random initialization. However, in some of the repeated experiments, we can see similar behavior in the report, so it looks like warm start performs better throughout this report and not because of the data augmentation.
- Figure 11: Why does shrink-perturb go down after on the graph in the bottom left corner when x > 0.6? Most of the results in this figure are inconsistent with the original paper as they show that fresh has the best performance, but results are inconsistent. Also, on the contrary from the rest of the report, here we can see that warm has the worst performance in all the cases.",N43DVxrjCw
123,"In the proposed paper, one of the main pros is that you've plotted and calculated each relation that was measured in the original paper and obtained mainly near or the exact values toward the original paper. But some minor issues showed off during reviewing your paper.
First of all, you've forgotten to blind your names and affiliations, not a major problem but it might affect reviewers' ideas sometimes. After that, you've also forgotten to numerize lines, so I have to mention them using paragraphs' titles. 
According to the ""What was easy"" paragraph, you'd better re-write ""since many of the parameters are reported in the original paper"" into ""since many of the parameters ***were*** reported in the original paper."" There also was so good that you've explained all the conditions you've put your dataset under test, like the optimizers, loss functions, etc. and I guess you've tested as far as I know enough conditions to explain your model and results.
As it seems, the original paper didn't do the ***data augmentation***, but it's so good to multiply the amount of data to exploit enough accuracy.
In the ""effect of hyperparameter"" paragraph, you've made a typo and wrote ***vales*** instead of ***values***, in the ""We iterate over all pairs for these vales"" sentence.
In the ""effect of data augmentation"" paragraph, you've mentioned that ""However, because the learning rate is low, the models are not fully converged even after 350 epochs"", although you've previously mentioned that your model converged to 99% of accuracy, so it wasn't crystally clear for me. In the sentence ""the original paper’s authors look at the difference"", it would better to re-write ***look*** to ***looked***, and ***difference*** to ***differences***. In the last sentence of this paragraph ""Due to the limits of this report, we also leave the careful comparison between data augmentations and Shrink & Perturb as future research in this area"", it would be better to explain ***limits*** more.
The link you've attached as ""https://github.com/CS-433/cs-433-project-2-fesenjoon."" didn't work properly, I couldn't find your project, according to the link you've attached in the ***Experimental setup*** paragraph.
But in total, you've represented your method very well.",N43DVxrjCw
124,"1. In this report the authors have tackled the following reproducibility claims from the original paper:
  - Warm-starting a Neural Network training has poorer generalization compared to training with new data + old data from scratch
  - Application of Shrink+Perturb technique proposed by authors to rectify and improve the warm-starting generalization.
2. The authors have implemented the code by themselves and tried various hyperparameters as mentioned by the original authors. They communicated with the original authors to clarify some of the implementation details mentioned in the paper like when to stop for training convergence, hyperparameters to tune, etc. However, the public link for the code implemented by the report's authors is not accessible.
3. In addition to implementing the code, the authors have also tried other techniques that might help with the warm-starting problem such as data-augmentation, early-stopping, regularization etc.
4. In the results and discussions part of the report, the authors have described the implementation details of the project. The authors have reported that warm-starting definitely has a generalization gap, shrink and perturb method is effective in reducing the gap.
",N43DVxrjCw
125,"This paper aims at evaluating the claims made by ""Towards Transparent and Explainable Attention Models"" by trying to reproduce the experiments in the original paper. Although the authors were not able to run all the experiments in that paper due to some dataset links missing, they have covered a decent amount of datasets and conducted extensive experiments, including an additional LIME experiment to validate their results from a different perspective. This paper is eventually able to validate claims 1 & 2, claim 3 to some extent, but not claim 4: in fact, on some datasets and some metrics (such as JS divergence), this paper observes opposite results.

Pros:
1. This work notices some issues of the presentation of the original paper, namely the fact that the original paper ignored one of the classes when plotting Figure 2 for some reason, which if included seems to cast some shadows over the desirable outcome.  I think this insight is important for future researchers to get a complete understanding of the claims made by the original work.

2. This work runs each experiment multiple times to report the central tendency, which is useful to have.

3. After finding inconsistent results compared to the original work, this paper conducted a further LIME experiment which provides further evidence of the validity of their experiments.

Cons:
1. Given the different observations and the fact that this work used the code from the original work, I think it'd be nice to contact the authors of the original paper.

2. Not being able to evaluate all datasets seems to be a limitation, especially for reporting the mean statistics. 

3. In table 3, sometimes both the Pearson correlation and the JS divergence improve. This seems to warrant further investigation: why do they both improve? Is it because the attentions are so spiky that they incur a large penalty when measuring JS?

Questions:
1. It's a bit surprising that in table 2 the vanilla LSTM needs so many words as its rationale, especially for classification tasks with so few classes. Did you check the extracted rationales and see if those are words with strong sentiments? Could it be a bug?

Typos & Presentation Issues:
1. 3.1.3 p -> log p
2. figure 1: caption too small
3. sec 4.4: not unsurprising -> not surprising

Overall, this paper has conducted extensive experiments and repeated most of the experiments in the original paper. This paper noticed some results that seem to be neglected by the original work, and showed some contradicting results from the original work. I think this paper is valuable to researchers who are interested in the original work, and recommend its acceptance. ",gEUo6MXTjpH
126,"The submission is a nonanonymous report, which violates the policy. 

As for the detailed reports, it is good and it gives a good reproduction of the original paper. 
The authors clearly show what questions they want to answer through this report. According to these lines, the authors conduct experiments and analyses to provide results. With different empirical studies, the authors verify that 1) the prediction performance is almost the same as the original attention mechanism; 2) diversity-driven LSTM indeed gives more transparency attention. However, the other two claims from the original paper are not so convincing or hard to be investigated. 
One slight suggestion is the presentation, the last section, Sec 5, the authors could remove the 5.2-4 since they have been shown in the first section. 

The score is only raised by the nonanonymous policy, I am sorry for this. ",gEUo6MXTjpH
127,"This paper provides a reproducible test of the paper Mohankumar et al. (2020) which claims that current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions in Natural Language Processing tasks and develop new LSTM units to achieve the goal.

In the reproduction, the authors first give sufficient background information about the paper. Then provide enough experimental details about the dataset used, the training model, and the hyperparameters for optimization. The authors also discuss each experimental result separately and compare the numbers/observations with the original paper.

I feel this piece of investigation is above the acceptance bar and I recommend acceptance.",gEUo6MXTjpH
128,"This report confirms most of the claims in the original paper and points out some minor issues of the proposed iFlow method that are not fully supported by the experimental observations. Furthermore, the report also mentions that the MCC scores of the iVAE
reported by the authors are significantly worse than those in the iVAE paper. The limit of the report is the lack of experimental evaluations traversing different hyperparameter values due to the computation bottleneck.  The report states the coverage limit clearly. ",8jmIpypMzE
129,"The overall paper presents good ablation with a clear introduction to the problem statement in the reproduction summary.

It was good to see a detailed hyper-parameter search over the baseline model. 
But, it would have been great if the authors would have extended the work over real datasets in support of the points mentioned in the discussion section. 
Also, if the authors can share more details regarding the time complexity for running the experiments which would help future researchers to tackle this problem first.
The discussion section mentioned by the authors listing down the strengths and weakness of the method which is a great reference point for future work. 
From the paper, it seems that the authors may not have any direct communications with the original authors. They mainly obtained information from the original paper and the original codebase. 
The authors have clearly stated the scope of reproducibility with clarity in the claims they learned from the original paper with further sections explaining the clams.
Small suggestion: In page 9: section-""Baseline improvement experiments"" if the table can be written in a more representable manner.

",8jmIpypMzE
130,"The report described the authors' efforts in reproducing the work ""Learning to Deceive with Attention-Based Explanations"". The report is well written in general. The authors have reproduced most of the experiments from the original paper, except for the ones requiring a private dataset, which is understandable. In place of the missing experiment, the authors added a new result with a multi-class classification problem, which supplements the original results. In addition, the authors reported and analyzed the variance of the proposed methods, which were not present in the original paper. The discussion of the report is also insightful.

Though I think the report did a good job in overal, one aspect where it can be further improved is that the hyper-parameters used in the experiments were not discussed: did the authors used the same ones from the original work? is the proposed method sensitive to the selection of different hyper-parameters? Some investigations in this  direction would be valuable.",DivtnNApCJY
131,The authors performed well. They had good communication with the original authors and was able to substitute for missing dataset.,DivtnNApCJY
132,"As a first remark, the report seems to repeat content. This is probably not the author's fault, I noticed the same problem with other reports. That is probably a problem with the instruction template. This is something for the organizers to take into consideration.

General remarks
The report follows the prescribed format. The authors reproduced the experiments in pytorch. The original implementation was in tensorflow. The author found errors in the implementation and a lot of gaps in the parameter tuning. In some cases, they get results opposite from the claims of the paper. The biggest problem of the paper they evaluated was the fact that the text and code were not in agreement. In fact, the code was convoluted and it was difficult to actually get the knowledge out of it. 
The report is well organized and there is a clear correspondence between the code and the issues the report addresses

Problems of the report
The report has a lot of references to nowhere, this is probably a problem with their latex referencing system. There are references to sections, tabs, and an appendix. While this is a technicality that can easily be resolved for the tables and sections, I couldn't locate the appendix. As mentioned in the comments the authors point that the pytorch function for the node convolution in pytorch is different from the one used in tensorflow. The original paper authors attribute the differences to that. In my opinion, this difference can not be responsible for the discrepancies. ",8JHrucviUf
133,"The report is well written and has a concise explanation. Additionally, the authors provide a great summary at the beginning. I would like to know if the numbers from the TensorFlow implementation (authors' paper) match the results reported in the main paper. It would interesting to perform a hyperparameter search and provide code and docs so it would be reviewed before accepting the paper. After the authors of the report make some claims and report an improvement for their own implementation. I suggest accepting the report",8JHrucviUf
134,"As noted in the reproducibility summary the goal of Adversarially Reweighted Learning (ARL) is to improve the fairness of a classifier for disadvantaged groups. In contrast to other approaches the group label is not available, but has to identified from the examples.

The authors use the original code to reproduce the experiments as well as reimplement the software from scratch based on the information in the original paper. Communication with the authors of the original paper was attempted, but failed. Reproduction of the experiments (using the original code) showed large differences to the results in the original paper. Replication with the reimplemented software yielded comparable results instead. Using the optimized hyperparameters from the replication improved the results of the reproduction, too. These results are discussed in the reproducibility report in detail and the missing information for full reproducibility was pointed out there.

The report is well written and clearly shows the easy and difficult parts of reproducing and replicating the original paper. Additionally, the authors analyzed if the improved accuracy obtained by ARL was significant. But here (in table 4) the AUC values for ARL are for different groups, AUC(avg) for Adult and COMPAS data sets, AUC(minority) for the LSAC data set, and those for the baseline cannot be found in tables 2 or 3. I recommend to correct table 4 and reevaluate the significance. It may be that the improvement for AUC(avg) is not significant, but the goal of ARL is a significant improvement for AUC(minority) or AUC(min).",P6-9f50PuMY
135,"This work replicates the paper ""Fairness without Demographics through Adversarially Reweighted Learning."" The replicating authors try the original code and implement new code to verify the results of the work and find that the results generally do not hold under a significance test. This is important to thoroughly check and replicate because such fairness mechanisms may be relied upon in real-world settings.

Pros:
+ I think the testing of both a re-implementation and original code is hugely important and well done by the authors.
+ I also appreciate the use of statistical robustness checking to make sure that results hold (or in this case, might not hold). This is the strong point of the paper to me as it expands upon the original paper and checks the quality of the results.
+ It appears that the original codebase has been updated to include hyperparameters, so this work may have done its job! That being said, a note of this should likely be made in this paper to reflect the update.

Cons:
+ While I agree that the original work should have included hyperparameters and made the code able to reproducible the exact results, I'm not sure that I follow why the authors couldn't do the exact same thing as with their pytorch implementation and run the grid search per the original paper specifications. It seems that with the replicated PyTorch code, they were able to get quite close to the original results with their grid search so it seems feasible to have done the same thing with the original tensorflow code. As a result, I'm not sure it's fair to call the original paper not reproducible in such strict terms. 



Typos/Style:

Overall, this work could use another pass to clean up typos/style, some of which are below.

In the paper “fairness without Demographics through Adversarially Reweighted Learning” [1], Lahoti et al. ask themain research question: “How can we train a ML model to improve fairness when we do not know the protected groupmemberships?”. --> Fairness should be capitalized in the paper title, there shouldn't be a period after the question mark in quotes. 

 original TensorFlow implementation by Lahoti et al.1. --> don't need a period after the footnote and in general this list should be separated by semi-colons not periods

 approach should match the results presented in Lahoti et al.. --> two periods

of all differences we refer to Table??in the Appendix. --> Table isn't linked correctly 

as tested on their openly available implementation onGitHub2.  --> footnote usually goes after the punctuation 

""Many scientific research proves"" -> ""Much scientific research proves"" or ""Many scientific research papers prove""

""Note that since the P-value cannot exceed 1, it is customary to report it without 0 before the decimal point (e.g.P=.031)."" --> If this is customary, the statement can likely be omitted.",P6-9f50PuMY
136,The optimal hyperparameter of the original paper is now available in their Github repository. Can you reproduce their original results for both the baseline and ARL. Can you provide reasons why the original paper has a bad baseline result? Is it because they were not choosing proper hyper-parameters?,P6-9f50PuMY
137,"This paper provides a holistic reproducibility report of the original paper on the algorithm named 'deep fair clustering' (DFC). The following issues are examined.

1. On the data sets 'Color Reverse MNIST' and 'MNIST-USPS' (they are the two data sets among four used in the original paper), DFC is tested against all the four metrics used in the original paper: 'Accuracy', 'NMI', 'Balance', and 'Entropy'. The mathematical details of these metrics are provided. In particular, a part of DFC and the data pre-processing are re-implemented since the corresponding code for the original paper is not accessible.

2. Extension by using no pre-trained cluster centres. It is shown that by discarding pre-trained cluster centres, DFC has a noticeable drop in performance.

3. Extension by using different divergence functions for regularisation. In particular, JS and CS divergences are used to replace the KL divergence used in the original paper. The comparison of performance is well documented.

4. Extension by using non-binary/corrupted sensitive attributes. It is shown that with corruption, DFC has a noticeable drop in performance in all the four metrics.
The paper is well organised, and the clear structure makes the report easy to follow.

Possible problems:

1. The words' fair' and 'effective' have both general English meaning and context-specific definitions, and both are tightly connected to machine learning and artificial intelligence. Using such words without giving a brief introduction leaves the readers confused. The confusion is not reduced, e.g., even after authors write 'feature representations are considered fair if they are statistically independent of sensitive attributes.'

2. The point of 'sensitive attributes' in section 2 is confusing. I am particularly lost at how this is related to turning the background of images to white or black.

3. Usually, the change of regularisers would significantly modify the performance of an algorithm. Examples include enhancing sparsity by l1 or l0 penalty, enhancing prediction accuracy by l2 penalty, and so on. I am not sure if it is fair (see, this word 'fair' has a different meaning than it has in the paper) to test the claims' robustness by changing regularisation.

4. How is the 'background' of an image or the 'background colour' defined?

5. A typo in line 149, p(x=q(x)) should be p(x)=q(x).
",DXVAJGohUKs
138,"The author(s) provide precise details in their report regarding the proposed algorithm and data set (MNIST) details. The objective function and model description are well defined. They provide every detail of the code and algorithms that have been used by other papers and provide the citation. The evaluation criteria; accuracy and NMI were used to evaluate cluster validity. They didn't discuss details of their result in Tables 3, 4, and 5, but they were understandable.",DXVAJGohUKs
139,"Reproducibility Summary : The report has this summary that contains major findings.
Scope of reproducibility: The report states the scope clearly, and follows it.
Code: The authors reproduced the code in PyTorch.
Communication with original authors: Fair communications are mentioned, but I am not sure whether the original authors have evaluated the results in this report.
Hyperparameter Search: It is mentioned in the report that hyperparameters were varied, but not all results are included in the report.
Ablation Study: The ablation study is not comprehensive.
Discussion on results: The report discusses the state of reproducibility of the original paper, and mentions the easy parts and difficult parts. The numerical results of this report are consistent with those in the original paper. Only the results on ResNet-18 deviate.
Recommendations for reproducibility: It seems the report does not discuss a lot on how the original authors can improve reproducibility.
Results beyond the paper: The experiments in the report are almost the same as those in the original paper.
Overall organization and clarity: The overall quality is good.
",eNj0zqNUkBU
140,"The report clearly presents its scope and reimplements SAdam on pytorch where they compare their implementation with the version obtained from the authors which is in tensorflow. The report has multiple positive aspects:
- communication with the authors
- reimplementation of the code
- clear reproduction of the original paper
- clear description of the problem and the presentation of the report 
- additional experiments on the resnet model

Major shortcomings of the paper include:
- lack of a range of hyperparameter search for the robustness of results
- testing the algorithm in contexts other than simple vision problems

The area of algorithmic improvements is a tricky subject where it is hard to confirm with strong confidence that the results are robust across wide variety of hyperparameters, and confirming the validity of the results in a way that translates to different structures of problems. I think one of the strongest points a reproducibility can make is to show that this is the case. That said, the present report does a great job at reproducing the core components of the investigated paper; I encourage the authors to extend their hyperparameter search for robustness and comparisons as well as applying the algorithm to wider (different) sets of optimization problems.",eNj0zqNUkBU
141,"This manuscript provides an pytorch implementation and reproducibility report of ""SAdam: A Variant of Adam for Strongly Convex Functions"". Most of the claims are consistent with the original paper. Experimental results include L2 regularized softmax regression, 4 layer CNN, ResNet 18 on MNIST, CIFAR10 and CIFAR100 datasets with exhaustive hyperparameter tuning. It would be interesting to see the results on ImageNet.",eNj0zqNUkBU
142,The report is interesting. I want to know how this approach is different than the approach mentioned in Keras-vis (https://github.com/raghakot/keras-vis). A comparative analysis with the Keras-vis approach will be beneficial. It is also required to show some failed cases as well.,DmNeiy8i5lu
143,"The authors do a commendable job in reporting the state of reproducibility of the original paper and provide a concise summary of their findings. However, I feel that the current report can be significantly improved with additional experiments to further understand the results obtained by the authors. I have presented a detailed evaluation of the reproducibility report with the associated metric below.

Reproducibility report: The authors present a comprehensive summary that outlines their results and contribution accurately. 

Scope of reproducibility: The authors present a clear enumeration of the reproducibility scope, that is derived directly from the original paper. 

Code: The authors reused the code from the original authors, citing difficulty in writing the code from scratch from the paper itself. However, they fail to provide explicit details of the issues they faced. It would be helpful if they could outline the problems they faced or propose possible additions to the original paper that can be added to the supplementary information in order to facilitate replication by the community. The github repo submitted by the authors is well-organized and serves as a good codebase to reproduce the results of this paper.

Communication with original authors: The authors present a clarification except citing their proactiveness to contact the original authors and clarify the discrepancy in their obtained results. They also present a discussion pertaining to the colors in the visualization results. However, it is unclear if the authors experimented with different batch sizes or having the same image with different images in the batch to observe if the visualization results changed, as alluded to by the original author.

Hyperparameter Search: I feel that the authors lacked in performing a hyperparameter search and evaluating the robustness of the results. Given that the results are susceptible to the statistics of input batch, I would have liked to see experiments to study the impact of batch size or different seed (allowing for different images in the batch) on the visualization results for each image. This issue stands as a significant weakness of the report and therefore the robustness of the results of the original paper remain undetermined. 

Ablation Study: The authors' description of the original methodology is not convincing enough and thus it is difficult to comment if the authors understood the original methodology well. The absence of ablation experiments further raise questions about this issue. It would be helpful if the authors could explore some ablation experiments to comment about the different aspects of PFV functioning. For instance, what is the role of the element-wise multiplication with the activation map? Does it help in making the PFVs more focal?

Discussion on results: The authors were able to explore the scope of reproducibility mentioned earlier in their report. However, the dicussion is not very clear. Specifically, what parts of the paper did they feel lacked clarity thus preventing them from writing the code from scratch? Also, how did the code base from the original authors help clarify those ambiguities? The current report lacks details beyond mentioning ""required a bit of advanced knowledge in the CNN architectures"". I believe adding this detail is crucial to understand the drawbacks of the original paper with respect to its reproducibility. 

Recommendations for reproducibility: The authors provide no additional recommendation to the original paper authors based on their experience. A good reproducibility report often entails specific recommendations that could help make the original paper easier to understand and reproduce. 

Results beyond the paper: The authors perform additional experiments using different model architectures. I feel this is very helpful and extremely commendable. However, they do not provide further insight into the results. For instance, why does PFV work fine for Resnet but not InceptionNet even though both of them consist of skip connections (which is cited as the drawback of PFV). Similarly, I am not clear why PFV fails to visualize objects correctly. The results seem to portray that PFV was able to locate these objects in Section 4.4. These ambiguities in the paper need to be resolved and currently serve as severe drawbacks. Furthermore, did the authors explore visualization results for images where more than one entity in present? Is PFV, being a class agnostic unsupervised method, able to identify all entities in the image or does it focus on one of them? Does this result change with changing batch statistics?

Overall organization and clarity: I found some grammatical errors and typos in the paper. I believe the overall presentation of the paper could be improved as well. 

Overall, I think the authors have made a good preliminary attempt at replicating the paper and I hope this experience has provided them valuable insights into the method and the field in general. However, the report in its current state has severe drawbacks as mentioned above. If the authors can address these concerns, I am happy to change my evaluation. ",DmNeiy8i5lu
144,"The paper reproduces Principal Feature Visualisation for CNNs by reusing the provided code of the original authors and running it on a chosen set of images. The authors build their study on four claims extracted from the original paper (contrast, lightweight, ease of interpretability, unsupervised) and use a dataset comprising images from Open image dataset v6, example images of the code repository and ""some images online"". The authors state the the four claims can be verified.

While the paper shows interesting results of running the reused code on the chosen set of images, the paper neither critically assesses the implementation of the approach nor follows a structured approach to reproduce experimental outcomes and findings. 

The original paper evaluates the approach for the debugging classification errors (for dog breeds) and transfer learning (using the Pascal VOC2012 dataset for fine-tuning). It would be highly interesting to design and conduct experiments for other target classes / target datasets or use other CNN architectures. While the authors actually vary the latter (and use established pre-trained models, such as AlexNet or MobileNet), they only show the results on five images and do not further assess the applicability for debugging or transfer learning.",DmNeiy8i5lu
145,"# Reviewer Guidelines Rubric

---

* *Reproduciblity Summary:* Followed template
* *Scope of reproducibility*: Is stated and adhered to.
* *Code*:
  * Used original authors code
  * Additional experiment code is included as supplemental material
  * Code looks reasonable, `README.md`  and   `requirements.txt`  included.
* *Communication with Original Authors*: mentioned briefly, sounds cordial, only appears on summary.
*  *Hyperparameter Search*
  * Reused many parameters from appendix of original paper.
  * Did search over $\lambda$ 
  * Also experimented with centroid size and penalty constant $c$ as hyperparameters 
  * Only tested one hyper parameter at a time instead of simultaneously
* *Ablation Study*:
  * I believe some parts of the hyperparameter search accomplish this (eg  $\lambda = 0$) but otherwise not discussed. 
* *Discussion on Results*:
  * Reproduced results closely on several standard data sets
  * Reproduced the baseline model (Deep Ensemble) as well DUQ
  * No side-by-side comparisons with original paper
  * Other than on summary page, does not really describe which parts were easy or difficult to reproduce.
* *Recommendations for reproducibility*:
  * Not provided
* *Results beyond the paper*:
   * Introduced extension E-DUQ and some preliminary exploration
* *Overall organization and clarity*
  * Looks fine in general
  * LaTex is annoying sometimes, but would be good to get plots/tables on same page they are discussed somehow.

---

### Summary

This paper does accomplish the goal of reproducing the original paper to a large degree, but the discussion of reproducibility is short - it would be stronger if there were more in-depth explanation of what was easy, what was hard, and what the original authors could improve. 

It is also difficult to judge how successful the reproducibility exercise is because the authors do not provide a side-by-side comparison. The text could quote the original results directly rather than alluding to them ambiguously.

This paper does reproduce the baseline model used for comparison, which strengthens its results and is appreciated.

The extension E-DUQ could be useful, but isn't explored enough in this paper and is only tested on one example. It might be better to expand it as a stand-alone paper instead.
",Gsj8MmNkA6L
146,"__Summary__

The original paper (OP) proposes a deep learning  architecture based on a single deterministic neural network that produces uncertainty estimates. The OP proves the performance of the method in a toy setting (2 moons) and 2 usual OOD classification problems (MNIST and CIFAR10). The OP also studies slight modifications of the proposed loss function and the consequences on performance of their method.
The reproducibility report (RR) reproduces most of the experiments of the OP. Additionally, the RR performs an additional experiment to better illustrate one of the stated limitations of the paper (namely its inability to separate espistemic from aleatoric uncertainty).
Finally, the RR proposes an extension to the method from OP by learning a parameter previously fixed.

__Positive points__

- The RR reproduced most of the experiments from the OP and managed to achieve similar results. As a consequence, the RR's authors confirmed that they were able to generally reproduce the results from the OP and validate its claims.
- The RR's authors made the effort to contact the OP's authors to show them their report, which is very positive.
- The RR added a figure (Fig. 1) of the architecture which was not present in the OP. This is helpful in understanding the neural network studied and the mechanism proposed.
- The RR discussed clearly what kind of hardware was used and the time required to perform the computations, which is of great use to the reader trying to reproduce the results of the OP.
- The RR performed an additional experiment on both the MNIST and CIFAR10 datasets, showing that DUQ was not robust to a noise perturbation of the input. This experiment clearly and explicitly confirms a limitation mentioned by the OP.
- The RR proposes an extension to the original idea to try to alleviate the limitation mentioned above. The RR then performs experiments illustrating the performance of this extension and comparing it with DUQ. By doing so, the authors of the RR have shown their understanding of the paper. Proposing a new method is a great initiative.


__Negative points__

- The RR claims that all reproduced results are matched within 1% accuracy, but these quantities are never computed during the paper. Worse, this claim appears to be false (Tb. 5 of the RR vs Tb. 1 of the OP). This makes trusting the conclusions of the report difficult.
- A clear conclusion on what was being reproduced and the state of reproducibility is often missing. Sec 4.1: what are you trying to reproduce? Are you satisfied with the result? Sec 4.2: the RR mentions that the trend is similar: can you ground this statement with a quantified metric? Moreover, at l.122, the RR mentions that ""[their] analysis supports the claims made by [OP] authors"". It would be useful to precise which claims exactly are being validated by the RR analysis. Same at l.173. In addition, some statements are made regarding the reproducibility but are often vague (Sec 4.2, l.122: ""slightly more accuracy per rejection"", Sec 4.3: similar rejection classification plot, similar performance, etc).
- The RR could have been better organized. The order of the figures does not match the progression of the discussion (for instance, Fig.8 is reference directly after Fig.1). The same can be said for tables. Additionally, the RR could have benefited from better formatting (Sec 3.1 especially). This overall makes the RR difficult to read. 
- It is sometimes unclear what is the contribution of the RR and what is the contribution of the OP. For instance, is Tb. 3 (Sec 4.2.3) trying to reproduce the OP results or is it an additional results? The same can be said for the experiment on c from Sec 4.2.3. In Sec. 4.2, you compute the computational time taken by the deep ensemble and DUQ. Is it your own experiment? Is it a reproduction from the OP? If then, what is your conclusion on the reproducibility?
- Limited details on the training procedure for the additional experiments is given. A description or at least a pointer to the RR code would have been appreciated. Moreover, background on some choices (for instance, why the value of sigmas in Fig.6 and Fig.7?) would have helped understand the design choices.


===

Overall, I find that this was an encouraging report that addressed the general reproducibility of the paper. I appreciated the initiative to propose new experiments. 
Reorganizing the report would greatly improve its readability. While the new experiments were interesting, I found that the report would benefit from a deeper discussion on the current state of reproducibility. Adding details on the methodology of the experiment, what the RR authors set to reproduce in this specific experiment, and a clear conclusion on the reproducibility of the experiment would also improve this report to help it make a more grounded statement on the status of reproducibility of the paper in general.

===

__Additional comments__

- Fig. 1 is very helpful, but takes too much space in the RR. Consider making it smaller.
- Is it not clear what is i in the equations of Sec 3.1. The summation signs do not indicate what variable is being summed.
- In Tb. 1: is the training time a new result? A reproduced one? This is not said clearly.
- The authors study changing the constant number in the two-sided gradient penalty (Sec 4.2.3). At the first reading, it is not clear what exactly is changed since the letter c is already used in the precise equation of the gradient, but not with the same meaning as in Sec 4.2.3. Additional precision on the exact quantity that is being changed and additional care to the notation could improve the understanding of what the RR tried to do.
- The term ""sensitivity"" in Sec 3.1 is not defined.
",Gsj8MmNkA6L
147,"The authors test two claims made by the original paper: the ability of the proposed approach to detect OOD, and the role of different hyperparameters as an ablation study. In addition to that, the authors explore sensitivity of the proposed algorithm to noise, and propose an extension for explicit detection of aleatoric and epistemic uncertainty.

* Re noise sentivity of the algorithm (Figure 6), it is quite surprising that Deep Ensemble or E-DUQ perform so much worse that DUQ, even when we reject 95% of data. Can the authors give more details and better explain why that is the case?
* In Table 1-5, how many splits (or randomly generated train datasets) were used?
* Please provide confidence intervals in tables (e.g. 5-95% quantiles) for performance metrics. Otherwise, it is hard to assess what is significant. For AUROC, you need to report bootstrapped values.
* More details about lengthscale prediction would be needed (section 4.2.2)
Minor:
* The authors make several grammatical mistakes, e.g., in the usage of articles ""the"". The abstract has no subject in their sentences, periods are often separated from the final word. Overall, although the text is understandable, the writting would need some polishment, and it would be nice if the English could be checked by a spell checker.
* Eq. 54-55, what is m_{c,t}? Not defined in the text.
* The explanation about the gradient penalties is not self-contained, that is, it is not understandable without having to go back to the original paper, so either remove it or make it self-contained.
* Because this is a reproducibility challenge, the authors should provide more details on experimental setup to generate Tables 1-5 (what hyperparameters, setups, splittings, number of runs, etc, etc).",Gsj8MmNkA6L
148,"The authors of this report provide a complete open-source reimplementation of the original method as well as the figures provided in the original paper. They find some discrepancies and variance in the results. The authors do not go beyond the original content of the paper in terms of understanding the method; they do superficially report some unsuccessful attempts to improve it.

Format: the authors correctly follow the format required for the challenge.
Scope: the authors (attempt to) reproduce all the results from the paper
Code: at a glance the provided code appears complete. It was written from scratch since no code is provided in the Original Paper
Communication & hyperparameters: the authors have done their due diligence
Ablation: the authors perform the same ablation study as in the OP, finding some differences, but do not report further ablative modifications. They also perform a similar sensitivity analysis.
Discussion: the authors make appropriate discussions and remarks on reproducibility

Comments:
- there seems to be a typo in eq (4), \lambda was replaced by some text
- broken reference on line 171
- Figure 1: the labels of the plots are pretty small, in academic papers labels should have the same font size than the main text 
- “We first defined performance as train target accuracy, [then test]”, this is a nice insight, but not the correct way to do this. If using the training data, then you are only measuring memorization, if using the test data, then you’ve “cheated” and your test data is no longer test data, and no longer gives you an unbiased estimate of the performance of your method. The correct way to do this would be to split your training data in two, i.e. create a validation set, and use this validation set solely to determine early stopping. 
- missing Figure reference on lines 224 and 236
- the authors critique the OP for not having the same number of random seeds per hyperparameter setting. Is this referenced somewhere in the OP? Or is this from an email exchange? These numbers should be made explicit.
- Figure 3 & sensitivity analysis: the authors note that “there is very little similarity to be found in any of the accuracy landscapes”. Experiments have natural variance, especially since different random seeds are used (and small code differences probably still remain). Considering the closeness (there isn’t much difference between 0.71 and 0.73) of the bounds of the colorbars it would be quite unexpected for the same peaks and valleys to show up. This shows that this method is robust to these two hyperparameter choices, or alternatively, that these two hyperparameters do not influence the results.
- stylistic comment: the authors use the “we” voice in a lot of contexts, this makes it hard to distinguish between what is something that they contribute (e.g. novel measures, novel hyperparameters or other choices) and something done by the original authors. Alternatively, the passive voice can be used, for example:
  - “We combine these two loss terms into a single term” -> “These two loss terms are combined into a single term”
  - “after completing training, we train two classifiers” -> “after completing training, two classifiers are trained”
- on language & grammar: the text is very readable, but contains some typos and grammatical mistakes and would benefit from additional proofreading.
",vOIGINzuJR6
149,"This report presents an attempt at reproducing results from « Fairness by learning orthogonal disentangled representations » (Sarhan et al., 2020). In that paper, a representation learning algorithm is proposed to learn « fair » representations (i.e., representations from which sensitive attributes cannot be easily predicted) by learning two separate representations (one for the class of interest and one for sensitive attributes) that should be both disentangled and orthogonal to each other. This reproducibility work is only partially able to reproduce the original paper’s results, in part at least due to the lack of open source implementation and some missing experimental details.

The report’s authors clearly state their objective, explain what they have done (and what was easy/difficult), and compare their results to the original paper. This is a significant effort since they had to start from scratch, and they attempted to reproduce more than just the main results (they also include an ablation study and a sensitivity analysis). I also appreciate that there is a well documented open source repository to easily re-run these experiments.

My main concern is that it remains unclear why some of the results are so far off from the original paper. In particular, the YaleB and CIFAR-100 results are worrying, since the model is clearly unable to learn the task at hand. I would have expected the authors to dig deeper on that, for instance by doing more ablations / hyper-parameter search to at least identify which parts of the loss / model are responsible for this issue. Without such an analysis, there remains some doubt regarding the correctness of the implementation.

Another potential concern is that it is not clear from the descriptions of the datasets in 3.2 that the definitions of the sensitive attributes match what was done by Sarhan et al. on the YaleB and CIFAR datasets. Could you please clarify that point in the report?


Small remarks:
- On .81 should « based on z_T » be « based on z_S »?
- I find eq. 3 a bit confusing because q_phi_S is applied with z_T as input instead of z_S: how is that possible?
- In eq. 4 there is probably a missing \ in the Latex code
- l. 171 has a missing reference
- What are the horizontal dashed lines in the plots?
- References to figures in the Appendix seem broken (ex: l. 224, but there are more)
- l. 360: « We should check whether these are in fact the last hyperparameters we used » => was this sentence supposed to be included?
",vOIGINzuJR6
150,"The paper provides a thorough explanation of the reproducibility efforts. It could possibly be improved by providing more experiments, especially on the attempts to apply the approach to new examples. (Hyper)parameter choices are mostly adopted from the paper and could have been experimented with, too.

The summary is somewhat vague in some places. For instance, the authors state: ""The accuracy, however, when running Experiment 3 (38%) is much lower than in the paper. This is because we divided the amount of Monte-Carlo samples by 5"" This needs some motivation as to why the number of samples was changed and whether this is a problem or not.

Great to see, authors have been in touch with the original paper authors.",OownuYG0SOC
151,"This work aims to reproduce and examine the claims made in ""Generative Causal Explanations of Black Box Classifiers"". The authors do so by reproducing the original results presented in the paper and additionally evaluating against imagenet to test the behavior on a more challenging dataset. The authors remark that they are unable to fully reproduce the results, though this is likely because they did not evaluate using the exact same procedure due to a reduction in the number of Monte Carlo samples. They also evaluate using the image net dataset and show worse behavior. However, it isn't clear to me if this degraded performance can also be attributed to the same issues that prevented reproducibility on the original experiments. 

Overall, I think the authors made a nice attempt at reproduction (MC samples aside), and do well to consider an additional dataset. I found the rationale behind the extension to be solid, and very important when thinking through considerations that go into applying a model in more realistic settings. 

I have two core issues with this reproduction:
1. The paper is generally difficult to follow. The paper reads closer to an outline than a finished report. I would encourage the authors to spend some additional time on organization, making sure that the key takeaways are made plain and that the report reads fluidly throughout. 
2. This reproduction tests one additional dataset is commendable, but I would have liked to see some examination that gets closer to the behavior of the method. For example, robustness to hyperparameters, model complexity, and other behaviors. 
3. It is not entirely clear to me why the authors chose to use less MC samples, since it hampered the ability to fully evaluate the claims of the paper. ",OownuYG0SOC
152,"In this paper, the authors reported results from the reproducibility study of [1].

Pros: 
1), successfully reproduced the results reported in the original paper
2), went beyond the empirical study in the original paper by testing the method with networks of more complicated architectures and additional datasets of increased learning difficulty to test the scope of the usability of the method. 
3), performed study of searching for hyperparameters: K, L, \lambda
4), good communications with authors of original paper

Cons:
1), the writing needs substantial improvement
2), study of using more complicated generative model is needed for more complicated classifiers or learning tasks. The proposed method in the original paper may still perform well using more complicated generative models for cases where there is increased complicity in the learning task and classifier.  
",BHqmQ6tnb3N
153,"The report details reproduction of the paper ""Generative causal explanations of black-box classifiers"" by O’Shaughnessy et al. The authors tried to do all the right things and even go beyond only reproducibility but also to extend the original experiments but some parts are rushed and some conclusions drawn are questionable.

Reproducibility Summary: is provided but it is quite short and cover the main points of the reports only at the very high level. It could have been extended to include more specific details.

Scope of reproducibility: is clearly stated and is followed in the later text.

Code: the authors used the code provided by the original authors of the paper. They mentioned several issues with the code which they had to overcome (sometimes with the help of the original authors). The release code by the authors of the report looks quite readable and well organised.

Communication with original authors: The report mentions communication with the original authors but mostly in terms of working with their code. However, the authors emphasise that the main issue with reproducibility was selection of hyperparameters and no discussion of that with the original authors is mentioned.

Hyperparameter Search: the authors emphasise that hyperparameter search method provided in the original paper is not very clear, nor it is well motivated, nor different values are checked in the original paper. Moreover, the authors claim that the method from their experiments is quite sensitive to this choice of hyperparameters. However, although claimed to conducted experiments on hyperparameter search, the authors only report results for tuning only 1 of the 3 main hyperparameters. Even with these experiments the provided results do not seem to confirm the authors' claim on high sensitivity. I may be missing something but the results of figures 4 and 5 look consistent enough for me. 

Ablation Study: no ablation study has been conducted

Discussion on results: the report does have this discussion and clearly states which parts of their reproducibility study was easy and difficult. However, some drawn conclusions by the authors are questionable. For example, the authors claim that from their experiments the model is highly sensitive to selection of hyperparameters. Considering that this is true (questionable on its own (see above)), then the authors claim that the model is not generalisable to a more difficult dataset, however, they do not report at all on hyperparameter selection for this dataset. From this it might be the case that this is still the same issue of sensitivity to hyperparameters rather than the additional issue with generalisability. 

Recommendations for reproducibility: are provided to some extent. The main recommendation is considering the hyperparameter selection procedure. 

Results beyond the paper: the authors go in several directions beyond the paper. They explore different base classifiers that the proposed method from the original paper should explain and they test the model on a more difficult dataset. 

Overall organisation and clarity: the report is mostly well written and easy to follow. There are a number of minor typos. 

Some specific points:
1.	Line 29, “parameter selection of the paper” – bad wording
2.	Line 47, DAG is not introduced
3.	Lines 118-119, “we introduce the following two variants” which follows only by one option
4.	Lines 135-136, “These values …” should better be placed for the sentence “As described in Section 2.1.3” as the method from Figure 1 only determines values for K, L, and lambda and not the number of samples N_alpha and N_beta
5.	Figure 2 requires more explanation about set up of the experiment and what different colours mean
6.	Section 4.3 – everything should be adjusted to 3 input channels at once and there is no need to compare results without doing so

Minor:
1.	Line 10, “of the of the” – the second of the is redundant
2.	Line 83, “traditioanally” -> “traditionally”
3.	Both “grayscale” and “greyscale” is used in the text. It should be consistent
4.	Line 117, “i.e.” is redundant
5.	Line 123, “. Which” -> “, which”
6.	Line 186, “In this table becomes” -> “From this table it becomes”
7.	Line 196 – missing full stop at the end of the sentence
8.	Line 202, “Posterior to redesigning” -> “After redesigning”
9.	Line 206, “Especially for the horses” – unfinished sentence
10.	Line 217, “A number” -> “a number”
11.	Line 240, “pragmatic” – it seems the authors meant something else
",BHqmQ6tnb3N
154,"This paper aims to reproduce and examine the claims made in the ""Generative causal explanations of black box classifiers"" paper. The authors approach this problem by examining three aspects of behavior of the proposed model:
1. Reproducibility of the results presented in the original paper. 
2. Sensitivity to alternative black box models. 
3. Sensitivity to the choice of hyperparameters. 
The authors find positive evidence of (1). However, they find a decay in performance with more complex models and a lack of robustness with respect to the choice of hyperparameters. 

Overall, I felt this was a solid reproduction. On the positive side: the authors do well to push the proposed methodology outside of the aspects presented within the paper, and I found the findings to be informative. 

On the less positive side: (a) Ideally, I would have liked to seen at least a set of hypotheses summarizing the explanation of the degradation of performance. (b) Some of the writing could use a little more fleshing out, the descriptions in the beginning of the text are spars and make it difficult to have full context without having read the prior paper within a very short window.   ",BHqmQ6tnb3N
155,"The authors of this report have tried to reproduce the results of the paper ""Generative causal explanations of black-box classifiers"" by O’Shaughnessy et.al. They have written up a Reproducibility Summary which has all the required elements. The scope of reproducibility is well-defined, and the report is well-structured and well-written. 

The authors have reportedly re-written major portions of the code themselves and have not contacted the original paper's authors. They have shared the link to the new codebase in the report which seems to have sufficient documentation. They have explained the idea in the original paper well in their Introduction. It is impressive that they have not just tried to reproduce the results in the original paper but have also tested the framework on the entire MNIST dataset and the CIFAR-10 dataset, and included a discussion on the results on these datasets. They have even tried to find causal relationships in a low-performance classifier on the MNIST dataset to extend the usefulness of the explanatory framework. 
They rightly question the usefulness of the explanations as some of the latent factors cannot be easily interpreted, especially on complex data sets. They also try to explain the scenarios when the framework doesn't work as expected in disentangling the causal and non-causal latent factors. 

A few suggestions to improve the report:
1. The authors should include the range of hyperparameters that they have tried before landing on the ones that they have used.
2. The authors should include the training times and the hardware they have used. 
3. The authors rightly mention that it was difficult to quantify the results and the judgment of the results was based more on intuition, subject to personal biases and interpretations. Do they have suggestions on what kind of metrics can be employed here? Consulting with the original paper's authors may have led to some good discussions about this aspect and possibly some useful metrics. 

In all, this is an impressive reproducibility report. The authors have done due diligence in attempting to reproduce the results and have gone beyond by experimenting on more data sets and models. ",sacRaG5zt_m
156,"The authors reproduced the results in the paper ""Generative causal explanations of black-box classifiers"". Though source codes are open-sourced by the original authors, they claimed difficulties of using the original codes and re-implemented most of the codes. Beyond that, they experimented with more datasets and analyzed the experiment results. They also proposed to evaluate the proposed framework to check its ability to examine poor models.

The report is clearly written with detailed analysis. The authors did substantial work to re-implement most of the framework. They discussed the challenges in using the hyper-parameter search methods proposed in the origin paper, i.e. it's too expensive for large models. 

Overall, this report clearly described the effort they made to reproduce the results. The ablation study is well motivated with reasonable results. ",sacRaG5zt_m
157,"Summary: This is a replication of the NeurIPS paper ""Interventional Few-Shot Learning"" by Yue et al. (2020). This replicability-report focuses a lot on replicating four specific experiments from this paper: Resnet10 Baseline, Resnet10 IFSL using both the SIB and MTL algorithm for each (4 configurations). The report seems to corroborate the results for the SIB algorithm and obtain close but slightly different results for the MTL algorithm using different computational resources but the same code provided by Yue at al. (2020).

With respect to the pros, this replicability study is clear and easy to read. It is also very succinct. The report also contain all the main minimal expected elements for the Replicability Challenge, including the type of contact the author of the report had with the authors of the original paper. This report has information of what to expect when running the code provided by Yue et al. (2020). 

On the other hand, a better detail of why the replication focused on only the four experimental cases could have improved the report. The original NeurIPS paper had a large evaluation protocol with various assumptions tested. Thus, this report provides a valuable but limited information about the original paper. Also, since the same code provided by the authors was used, it could have improve the replicability study if more information about the hyperparameter sweep, and may be other IFSL model assumptions, were added. This could have been done, for instance, by including a discussion of the parameters used to replicate the experiments. Finally, the report reads more like a summary of the study performed. In fact, there is really not distinct separation between the summary and the body of the report.",iwAC32Fov_3
158,"This short submission describes the author's attempts to reproduce Yue et al. (2020)'s ""Interventional Few-Shot Learning"" results. It focuses on one dataset (miniImageNet) and two methods (meta-transfer learning and simulated information bottleneck) that the original paper proposes can be augmented with the interventional strategy (IFSL). Using author-provided code, this submission reports that including IFSL does improve the model's performance, though not to the same extent reported in the original paper. A baseline discrepancy on the meta-transfer learning experiment was also observed.

Unfortunately, the submission includes very few details--it is essentially just the reproducibility summary--which limits the potential value of this work. The author obtained the best results using Yue et al.'s  hyperparameters; for all runs ""the results were much worse"" when these parameters were optimized by the author. However, the significance of this claim is tough to assess because no methodological details or quantitative results were reported in this submission. The only results are reported in Table 1 and 2 and these are unclear: over how many runs was the average taken? What is indicated after the ± (SD, SE, etc)? 

The manuscript does not include any exploration or explanation of the results reported here, nor is the original method tested outside of the original framework. References to the broader literature (and the original paper!) are missing, and the writing could be more polished.

I regret that I cannot be more positive, but I do not believe the work is ready for publication. ",iwAC32Fov_3
159,"
Thank you for this excellent work!  

The authors clearly explain the Scope of Reproducibility.

The authors successfully conducted experiments to reproduce the paper. I would love to know if they use the same batch-size as the original paper? 

I like that the authors conducted experiments on two different settings (MTL and SIB). In both experiments, I can see that the proposed approach improves baselines.

Issues regarding the report: 
It would be better if authors can tell why there is a large gap between their numbers and original paper numbers for the MTL experiment.
It would be better if authors report experiment results with different hyperparameters. 
Authors should try contacting the original paper author via their emails. 

Overall, this is a nice work to reproduce the original paper and I recommend it for publication

",iwAC32Fov_3
160,"Limited scope
--------------------
The authors clearly describe the scope and adhere to it, however, the scope is extremely limited:
- one dataset only (CIFAR-10), when the original article used 2 other small-scale ones (MNIST and CIFAR-100) and a larger-scale one (mini-ILSVRC12)
- one loss combination (NCE+RCE), when the original article tested 3 other ones extensively as well as 5 baselines, and has additional results for 2 other variants
- one noise setting (symmetric with a noise rate of 0.4) when the original article reports most results on 9 (1 clean, 4 symmetric, 4 asymmetric)

The authors also did not justify why they chose that specific combination, out of the 48 bolded ones in original tables 2 and 3 (among 972 entries when including baselines and ""clean"" experiments).

The only source of variation envisioned in the experimental setup was running the code on different hardware and environments: local CPU, Google Colab, and AWS GPU, all using the same software.

Limited effort
-------------------
Of the different hardware settings, only one result was reported, and there was no detailed analysis of failures, or mention of mitigation strategies. For instance:
- ""the code failed to run over certain epochs in CPU machine"": in what way? Could it have worked on a smaller dataset (e.g., MNIST), or maybe with a smaller batch size?
- the runtime exceeded the 12 hours limit on Colab: maybe the authors could have implemented a checkpointing strategy, and restarted training a few time.

In addition, the report contains:
- no summarization or explanation of the original paper, methods, or experimental setting
- no ablation studies: only one model was retrained as described, the baselines were not retrained
- no hyperparameter search: only the values found in the code were used
- one single run with a given random seed, even though the original article reported mean±std over 3 runs for most results

Confusing reporting of results
------------------------------------------
The symmetric noise setting with a parameter of $\eta = 0.4$ is characterized as ""noisy labels of 4%"" when the original article indicate it would be 40% of corruption ($1 - \eta$ or 60% of labels would stay the same).

The original result is reported as ""86.02%"", omitting the standard deviation of ""± 0.09"".
The reproduced result of 85.97% is reported as ""within 1%"" of the original one, which is technically correct as $0.05 < 1$, but confusing. Why mention a 1% threshold? Was it determined in advance?

Conclusion
----------------
In the end, this report does not shed any additional light on the original paper or code base, except that the code runs and did not produce an unexpected result.

The [task description](https://paperswithcode.com/rc2020/task) for the challenge mentions: _""Just re-running code is not a reproducibility study""_, and I don't think this reports clears the bar, I recommend to reject it.

",XLn0zjYiRQj
161,"Issues regarding the report: 
Authors test the code only on the CIFAR-10 with 0.4 Symmetric Noise Rate. I believe it would be better to show at least one more experiment (maybe with 0.8). 

Authors need to give a better explanation of how hyperparameters are selected.  Further hyperparameter search can be useful, authors can change model optimizer and learning.

Another important point, there is no information regarding the stopping criteria?. It will be better to plots how training loss changes during training.  It can be also better how to test accuracy change during training,

If possible authors can train and test one of the competing models as well. We can see how improvements happen.  Maybe improvement related to the optimizer or something else?
",XLn0zjYiRQj
162,"The author(s) provide very details in their report regarding the proposed algorithm and data set (ZINC) details, the report is 17 pages which I believe is too long. The model description is well defined. They provide every detail of the code and algorithms that have been used by other papers and provide the citation, but I believe it could be abstracted. The evaluation criteria; logP score were used to evaluate the optimization.",ntxHNb7y429
163,"In the very early sentences of your review paper, In line 4, ""an adaptive, neural network-based, penalty that is supposed to"" it's better to put a comma, right after the ***penalty***, ""an adaptive, neural network-based penalty, that is supposed to."" The link you prepared in line 69 as your GitHub repository, did not work properly.
In my opinion, it was better to test your algorithm with other optimizers rather than Adam, and other Loss functions except for binary cross-entropy. It helps you obtain more accurate results by searching in different conditions.
In line 94, you made a typo and wrote ***pratical*** instead of ***practical***.
What is the y-axis in Fig1 plots?! What does it refer to?
You'd better add Legend to each of your images, even if you're partializing them to a) b) c).",ntxHNb7y429
164,"Overall, my evaluation of the paper titled “A reproducibility study of “Augmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space” is as below:


•	The authors provide a comprehensive summary of the objectives of the paper, its results, challenges, and contributions. (Reproducibility Summary: 10/10) 
•	The authors provide the areas of the original paper where they have tried to reproduce the results and adhere to them along with the paper. (Scope of reproducibility: 10/10)
•	The code from the original paper has been used and modified for the hyperparameter search performed on the paper. Overall, there are enough documentation, and the code sounds clean. (Code: 10/10) 
•	Scope of the communication with the authors of the original paper is mentioned (Communication with original authors: 10/10)
•	Effect of different hyperparameters that change the results has been studied in detail which complements the results of the original paper. (Hyperparameter Search: 10/10)             
•	Although the authors have performed comprehensive hyperparameters search and have quantified the effect on the result, the level of novelty in the paper is limited. Maybe the authors could perform some study on the neural network-based penalty and see if they can add additional improvement to the original paper. (Ablation Study                                           6/10)
•	There are detailed discussions with graphs on the effect of each hyperparameter on the outcome. The authors have been very meticulous. Have examined the method in detail and have suggested valuable improvement to the original paper (Discussion on results 10/10)   
•	The detailed illustration of the results is very helpful, but the authors do not provide any recommendation on future work for reproducibility. They mainly summarized what they have tried and the differences in the outcome with respect to the original paper.  (Recommendations for reproducibility 5/10)        
•	I did not recognize significant novel contributions from this paper. (Results beyond the paper                          5/10)
•	The paper is organized, coherent, clear, and well structured. (Overall organization and clarity 10/10)
        
",ntxHNb7y429
165,"I would like to congratulate the authors on their commendable effort in preparing this reproducibility report. I believe the authors' effort will significantly contribute to the improvement of the reproducibility of this work. I have presented a more detailed evaluation below.

__Pros:__
1. The authors did a great job in presenting the reproducibility summary, especially the easy and hard parts of their reproducibility effort.
2. The introduction and scope of reproducibility are clearly presented and nicely setup the rest of the report. I would also like to appreciate the authors' effort to present the mathematical foundations of the method. However, I feel the grammar and writing style could be improved for better readability. 
3. The authors built up on the original code base which only had the codes for the Prediction task with memory. I believe this is a significant achievement and hope that this experience was highly educational and rewarding for the authors. However, the original code repo has now been updated. So, it would be helpful if the authors could comment on discrepancies (if any) in their implementation and the one provided by the original authors in their final version. 
4. The authors mentioned performing a hyperparameter search and reported finding different set of hyperparameters than originally reported in the paper. Having a plot of the performance variance for different hyperparameters (in the supplementary) could add value to this effort.
5. I would like to appreciate the several ablation studies performed by the authors to evaluate various components of the proposed methodology. The inferences from these studies though could be presented better. 
6. The authors performed useful additional experiments beyond the ones performed in the original paper. These experiments definitely add to my understanding of the method. Although I could follow the results of these experiments, I was slightly confused about the inferences presented by the authors. However, since this is a reproducibility report, I gave more weightage to the rationale behind the experiments rather than their interpretations.
7. The authors allude to some reproducibility recommendations but it's not very explicit. I think these recommendations are extremely useful and it would help to have them clearly mentioned. 


Despite the several merits of the report, I have few concerns primarily pertaining to the presentation and writing/organization style of the report. I have detailed them below.
__Cons:__
1. It would help to have a figure/flowchart that outlines the several experiments that the authors tried to replicate and/or investigate in detail with additional experiments. I would strongly suggest adding this figure in the introduction or scope section. This figure would establish the context and methodology of the original paper as well as give the reader an overview of the rest of the report. 
2. With the original code repo being updated recently, the authors may have to briefly comment about the discrepancies (if any) in their implementation. Additionally, they need to update the report to incorporate the information that the original repo now contains the codes to both tasks. I don't think this needs to be very detailed but updating this information will add to the completeness of the report. 
3. Although the authors report that they communicated with the original authors, it is slightly unclear as to how this communication served to improve the authors' understanding and/or implementation of the methodology. Also, I am curious to know if the authors communicated the observed discrepancy about the ShanghaiTech dataset and the engineering fix they had to deploy. 
4. It would help to have a plot of the performance variation with changing hyperparameters. This result would indicate how sensitive the results are to hyperparameter values. Also, did the authors try different seeds? I understand this could be difficult to do on complicated dataset but it is an essential component to establish that one method statistically outperforms another method. As a recommendation, I would propose the authors to run their ""light weight"" experiments on multiple seeds and report the standard deviation in metrics. 
5. I think the overall presentation style needs to be improved. The current report has very interesting experiments and excellent insights. These insights need to highlighted properly. For instance, it will be helpful to have the results reported by the original authors alongside the results obtained in this report to clearly compare and contrast the discrepancies or agreements. Furthermore, it would help improve the readability if the authors could add a flowchart or table that summarized their findings in terms of which modules turned out to be redundant for each of the tasks. Having such a figure or table will significantly improve the impact of their findings. 
6. The discussions and implications presented by the authors don't seem very convincing. For instance, the authors mention _""Figure 9 shows both models having trouble reconstructing the input frames, but the Prediction model reconstructs the image with considerably more artifacts making it easier to classify correctly.""_ It is hard to conclude this without definite reconstruction error (MSE, perhaps?) values. 
7. Furthermore, did the authors try an epsilon-greedy type of approach to mitigate the problem of mode collapse in memory module? This would entail using the memory module as proposed with probability of (1-epsilon) and using a random combination of all memory items with epsilon probability. The value of epsilon could become another hyperparameter, but setting it to some constant value (eg. 0.1) could perhaps serve as an acceptable solution. However, this would be a minor comment and mostly out of my own curiosity. I have not used this concern to judge the merit of the reproducibility report. 
8. Finally, I would encourage the authors to add clear reproducibility recommendations. I believe the authors are at an appropriate stage to provide specific recommendations for this work and a good reproducibility report should have them.

Once again, I would like to congratulate the authors on their work and I hope they can address some of the concerns mentioned above to improve the impact of this report. I believe that if the authors could add the aforementioned summary figures and/or tables and improve the overall readability of the final version of the report, this work could be extremely useful to the field in general. ",vvLWTXkJ2Zv
166,"The authors reproduced the work of Learning Memory-guided Normality for Anomaly Detection. 

In this work, the authors have:
- Reused part of the original code and scripted some parts that were not available
- Have communicated with the authors regarding the changes
- Did a thorough ablation study

And finally, went beyond the scope of the reproduction and suggested further improvements.
In the reviewer's opinion it is a thoroughly conducted study.

Small typos: Space missing line 45
",vvLWTXkJ2Zv
167,"This paper tries to reproduce the ICLR2019 paper ""Learning to learn without forgetting by maximizing transfer and minimizing interference,"" by Matthew Riemer et al.

The authors present an straightforward 1to1 reproduction of the original paper experimental setting. They ran the code according to the instructions given in the official implementation and got very similar results. This is OK for the the ML Reproducibility Challenge. However, the authors failed to provide meaningful results/insights for a majority of the analysis performed. In this regard, many components of the proposed results, ablation and running times studies in the manuscript are incomplete or limited, without offering clear descriptions and/or insight into the outcomes.


Itemised comments:

- Introduction is too short (seems a bullet list) and non-informative about the problem addressed, its importance, motivation, etc. Also, La-MAML should be better introduced and described.
- Repetitiveness: Methodology and Computational requirements sections are mostly the same. Also Section 5.2.
- Pseudocode in Section 3.3 shouldn't be a low-resolution figure.
- Tables 3 and 5 shouldn't be low-resolution figures. Lack of consistency wrt other tables.
- Sections 4.1.3 and 4.1.4 placed after section 5.
- Ablation analyses are not explained. The reader cannot get any useful insight from section 5. 
- Results and additional results are vaguely described. This makes it very hard for the reader to follow their interpretation.
 
",d0svLMnvzWK
168,"This report repeats a large number of experiments from the original article, using the original code, confirming the claims in scope. The authors spent time and ingenuity adapting and packaging the code to run on 4 different environments, to satisfy runtime and memory requirements of different experiments. However, repeating the original experiments does not shed much light on the original paper, and this work does feature re-implementation, new experiments, ablation studies, or hyper-parameter re-optimization. The small discrepancies that appear in some figures are not discussed.
As the [task description](https://paperswithcode.com/rc2020/task) of the challenge says, ""Just re-running code is not a reproducibility study"", and this submission does not provide additional insight on the original code or paper.

Reproducibility summary
--------------------------------------
The summary is filled up correctly, and summarizes well the report, although there are inconsistencies:
- the ""Scope of Reproducibility"" paragraph mentions only 1 claim, when Section 2 mentions 3, though they broadly overlap
- the ""Methodology"" paragraph only mentions one runtime environment (Google Colab with T4 GPUs), when Section 3 (Methodology), especially 3.6 (Computational requirements), expresses the need for using 4 different systems and says most experiments were using a different one (Kaggle Notebook with P100 GPU).

Scope of reproducibility
---------------------------------
The claims are clearly stated, and reproduced results connected to them. There is not much justification of how the results support the claims, except for the explanations in the original article.

Code and experiments
----------------------------------
The authors re-used code and hyper-parameters from the original repository.
No additional hyper-parameter search was reported.
No additional ablation studies were reported.
The report provides timing information for all experiments, and discusses speed differences and trade-offs of different systems. It could have been more clear which reported durations were obtained in which setting (Table 6 tags some results with ""\*\*"", and Section 3.6 suggests it may be ""3. Institute GPU"", but it's not mentioned in the caption).

Discussion on results
------------------------------
The submission reports accuracies ""within 4%"" of the original values, but does not elaborate on which experiments were further off from the originals, or how that ""4%"" (presumably 4 percentage points) was computed.
In fact:
- is not so meaningful when reported values for BTI can be ""-2.73 ± 0.45"", and
- In Table 3/4, the RA metric for ""Sync"" on ""Permutations"" went from ""70.54 ± 1.54"" to ""59.3 ± 2.3""

Most of the results do seem to line up, and consistent with the provided confidence intervals, and there are reasonable explanations (one bad run can become likely when running many experiments, the original authors might have had a lucky random seed, this might go away when re-tuning hyper-parameters...), but this was not acknowledged or discussed in any way.

Similarly, Figure 3 shows a similar effect of the learning rate value as Figure 2 (original), but the whole graph is 5 to 10 % lower in absolute accuracy, without it being acknowledged or explained.


",d0svLMnvzWK
169,"The paper is poorly written with an open-ended conclusion and the results are not exhaustive. Also, there is a lot of ambiguity around reproducibility since the author found a bug in their code. They state to provide the full results after resolution.",7NDziwkaDm6
170,"The work of RetinaFace is reproduced here, to address the long standing problem of face detection, and evaluated on the dataset of WIDER. The reproduced method achieves on average 11%-18% worse than the original reported results, which seems to due to a bug in the reproducing effort?? This important aspect is not clear.   The bright side is this work comes with detailed implementation description.
",7NDziwkaDm6
171,"The submission implemented RetinaFace using Knet framework. It contains most of the features from the original paper. In particular, the author conducted some ablation studies focusing on landmark detection, context module, and cascaded structure. The results are discussed in detail in the report. 

The strength of the submission is its solid, from-scratch Julia implementation. Julia has gained lots of popularity in the data science community. But there are relatively fewer off-the-shelf models for state-of-the-art computer vision models. The submission is a good example that can be referenced by people who want to study implementation object detectors in Julia.

The performance is lower than the numbers reported in the original paper. The author attributed this to a bug in the Knet framework. The author observed some progress in training after the bug is fixed but could not finish the training in time. Due to this reason, I feel reluctant to give higher scores, but at the same time encourage to have this submission accepted due to its language & framework choices and in-depth discussion about the experiments.",7NDziwkaDm6
172,"The proposed paper reproduces the result of ""Fourier Domain Adaptation for Semantic Segmentation (CVPR 2020)"", by primarily utilizing the code released by the original authors. The paper has clearly outlined the details of the algorithm and also compared the results with those presented by the original authors. However, the following points might improve the quality of the work

1) There seems to be a difference between the result obtained in the paper with that presented by the original author. A detailed discussion of the possible reason for this difference is expected. 

2) Although, we understand that it might be difficult to secure access to computing resources. Nevertheless, it is expected to perform a thorough hyperparameter search. ",MBIIiRE0EXv
173,"The paper is well structured and easy to follow. The authors provide a good summary with well defined scope of reproducibility. However, the authors could not verify claims on Synthia dataset but instead try to debrief the experimental setup and overall method. 

Overall, the authors are able to evaluate approach on GTA5 dataset along with providing computation details for training time.  The authors perform experiments with multiple backbone and are able to reproduce within decent error rate. The limitation is authors do not show any qualitative evaluation for comparison with paper which is crucial for semantic segmentation analysis. Effect of the size of the domain β is not analyzed qualitatively. Authors provide detailed experimental setup along with summary of code flow which helps in navigating the source code.  The authors had communicated with original authors to resolve queries. They do not provide any additional/extra experiments

It is not clear in sec 4.3.1 authors mention 'we could not find the author’s approach to generate pseudo labels in the paper' but in sec 4.3.2 they claim to improve the original pseudo label approach ? Section 4.3.3 may not be relevant here.

",MBIIiRE0EXv
174,"The authors have provided a summary of their experiments regarding the reproducibility of the paper - FDA: Fourier Domain Adaptation for Semantic Segmentation which was published in the CVPR 2020. The central claim of the original paper is that a simple Fourier transform can be used to achieve state of the art performance when tested on the semantic segmentation task. The authors of this reproducibility report were able to reproduce most of the results from the original paper (except on one dataset) and they optimized the original paper’s code to enable an easier loading of the model weights.

Pros:
Including a codeflow in the paper is very nice. It tries to show how the overall structure of the codebase is to make it easier for anyone trying to implement and reuse code.

Typos et al for improvements:
The authors can definitely benefit from cleaning their document and making sure tables and references are hyperlinked appropriately. Some of these typos and pointers are mentioned below:

- Line 43: (include a space at new line start) … methods. In the …
- Hyperlinks in some equations are missing - for example, in Eq1 (line 64) and Eq 3 (line 68-69) . Authors might include those as it helps in easier readability and navigation.
- Some of the citations are missing / wrong. For example, citation numbered [13] in Line 65 does not lead to anywhere. There are only 10 citations under references. 
- Hyperlinks for tables - for example, for Table 3, 4, 5, 6
- Table 1 and 2 are not linked anywhere (though they appear in the vicinity of the related discussion)
- Typo: varified -> verified (line 88)
- table 6 -> Table 6 (line 120)
- However in the last round -> However in the last round (line 100)
- MBT mIoU. . -> MBT mIoU. (Line 101)
- training of the mordels -> training of the models (line 110)
- via mail -> via email (line 157) —> Unless this was really done via snail mail

The claims listed under the ""Scope of reproducibility"" section can be improved to reflect what the authors of the reproducibility report did and what to expect in the rest of the document.

The ""Method descriptions"" section can be improved. The equations and used notation is not very clear. If this section can provide a good overview of the methods which is self-sufficient to understand, it will be helpful in better readability.

The authors can add some additional ablation studies to dig into and understand better the ideas from the original paper.

Overall, by addressing these issues and cleaning the document, the authors can improve their writing significantly and can be taken into consideration while making the decision.
",MBIIiRE0EXv
175,"# Reproducibility Summary

Yes it is provided.

# Scope of Reproducibility

Train ELECTRA with a similar dataset (OpenWebText) and evaluate on GLUE. Done a single GPU, this should reflect the claim of the ELECTRA paper of its effectiveness and relatively low computational cost to baselines such as BERT.

# Communication

The reproduction authors communicated with one of the original authors (partially through github) to answer most questions.

# Hyperparameter Search

The authors perform thorough hyperparameter search and it is reported clearly in the paper.

# Ablation Study

The authors perform some useful ablation with respect to discriminator and generator sizes. I think there is much to explore here such as other techniques that are used in GANs (e.g. different training schedules for the discriminator and generator).

# Discussion

It's clear from the discussion that ELECTRA is easy to reproduce. This seems attributed to the nature of the work — the ability to train on a single GPU — and the clarity and documentation associated with the original paper.


# Recommendations

No clear recommendation is made to the original authors, although the ablation experiments about generator size could be relevant.


# Results beyond the paper

The experiments and results are primarily in the spirit of the original work.

# Overall Organization and Clarity

The paper was clear and easy to follow. That being said, if needing to save space then some figures and tables could move to the appendix with only the main results in the main text.
",Or5sv1Pj6od
176,"The reproducibility report shows ELECTRA can be reproduced quite well. The report is detailed and coherent. An additional insight is provided which is “sensitive to capacity allocation between generator and discriminator” by the authors. Finally, figure 3 is great.

The code had to be shared with the reviewers.

There are some adjustments in this re-implementation. This is both a risk to the reproducibility as the difference increases and a strength to demonstrate the core idea of ELECTRA works.

“can also influenced the” -> can also influence the
",Or5sv1Pj6od
177,"This was a clearly written paper reproducing the original ELECTRA paper. The replication study authors do a great job of discussing what was difficult and easy to replication about the original paper (including the code that was released with the paper). I enjoyed reading it and thought they did an excellent job!


*Scope of reproducibility:*

This paper reproduces the GLUE scores from ELECTRA in PyTorch and on a single GPU. 



*Code:*

The paper authors reimplemented ELECTRA in PyTorch (originally in TensorFlow). The authors state they will release the code, but I did not see it. 


*Communication with original authors:*

It appears the replication study authors communicated with the paper authors to clear up minor details, but that the majority of questions were answered by the Github repo or the code. 


*Hyperparameter Search:*

Neither the replication study nor the original paper used a hyperparameter search.

*Ablation Study:*

I don't believe the replication study performed any ablations. 

I was surprised to see such similar results for ELECTRA even though the replication study did not use the WikiBooks dataset (and only used OpenWebText).

*Discussion on results:*

The replication study presented an excellent description of the reproducibility of the original paper and made clear when the results reproduced and did not reproduce. They clearly stated that some details were ambiguous, but that they were mostly able to resolve those details. The scores in the original implementation and in the new implementation mostly match. 

*Recommendations for reproducibility:*

The republication study pointed out areas where the original paper did a great job of giving enough details for reproducing the work and areas where the original paper could revisit (such as the discrepancy in the WNLI tasks. 

*Results beyond the paper:*

The replication study aggregated information from several sources to give additional context to the paper (such as the efficiency results). This was great to see. 

*Overall organization and clarity*

* The replication study authors were very clear about where their implementation differed from the original work and included a table making it very easy to read (table 5). Thank you!",Or5sv1Pj6od
178,"This reproducibility report is relatively easy to follow. Their re-implementation supports the claims in the original paper. 

At the time of writing this review (2021, Feb 8), the authors of the original paper released the code. This could make reproducibility easier. ",0z31bGAM8Hr
179,"In this paper, the authors discussed their attempt to reproduce the results reported in [1]. 

Pros:
1), implemented the method proposed in the original paper from scratch using a different framework, i.e., Tensorflow. Pytorch was used in the original paper.
2), successfully demonstrate the value of adding the Gaussian filtering layer during curriculum learning comparing to the baseline model. One thing that the authors should make it clear is that whether the baseline model was also trained with curriculum. 

Cons:
1), only performed reproducibility study of a small part of the results reported in the original paper.
2), the reported performance even though demonstrates the benefit of CBS, but does not reach the level reported in the original paper. 
3), very limited study of hyperparameter search
4), no ablation study
",0z31bGAM8Hr
180,"Summary: This is a replication of the ICLR paper ""Your Classifier is Secretly an Energy Based Model and You Should Treat it Like On"" by Grathwohl  et al. (2020). This replicability-report focuses on studying the results both on accuracy and model calibration of the original paper. The report seems to corroborate the results of the Joint Energy-Based Model (JEM) and provides interesting results on the inception scores obtained.

With respect to the pros, this replicability study is clear, well organized, easy to read, and seem to study the paper in great detail considering the limited time. The report also contains all the main expected elements for the Replicability Challenge, including the type of contact the author of the report had with the authors of the original paper, scope, code, computational platform, difference of the hyperparameter in the original paper vs. the replicability study, the implication (discussion) of the results, limitations, and additional insights. This report has a clearly delineated summary.

On the other hand, there are a few elements of the report that could have been clarified a little more. For instance, the authors claimed they performed additional experiments with uniform noise and computed its scores - in the words of the authors this evaluation went beyond what was reported in the original ICLR paper. However, the original ICLR paper did consider uniform noise -- see section F.2. Clarification on this can improve the report's clarity. ",ShrPBsjByVa
181,"I am puzzled by the authors' choice to reproduce the results of the paper without looking at their code while only looking at their code when needed. Why not just use the provided code and build upon it?

The authors state that the limited computational resources were a bottleneck in their reproduction, which is fair. However, given such a limitation one would expect experiments to be chosen more carefully with a directed purpose, but this does not seem to be the case. For example, Table 2 seems pointless to me. The focus of the paper is the joint energy-based models (JEM) and the 95.8% purely discriminator accuracy was presented only to provide a point of reference. Why spend time on this?

Also, Table 3 is puzzling. The authors achieve a better inception score, far better than everything else (including purely generative models) listed in Table 1 of the original paper. This would be a surprising result that requires an explanation or a discussion, but the authors do not acknowledge it at all. Well, the inception score with CIFAR10 (10 classes) is by definition <=10, so the reported 19.39 must be an incorrect number. In fact Tables 4 and 5 have the same issue.

I also do not find the other experiments to provide much new information. For these reasons, I recommend the reproducibility report be rejected.

Errata:
1. line 52, Due 'to' issues with training time
2. The reference [3] https://www.cs.toronto.edu/ kriz/cifar.html. is not accessible.
3. line 101, the right parenthesis of max_y p(y|x_i)
4. The reference for wide resnet should be included.",ShrPBsjByVa
182,"1. The authors clearly state the scope and limitation of the reproduction experiments.
2. The results are “reproduced"" using a scaled down architecture (Wide ResNet 28-2 instead of Wide ResNet 28-10). The authors claim that the disparity in reported training time in the original paper and the authors’ own estimate is the reason for scaling down the architecture. 
3. Only one aspect of the results presented in the original paper is reproduced. However, the authors present fresh generative samples from noise and show how the generative samples evolve over SGLD steps. 
4. The authors have used exactly the same hyperparameters used in the original paper. The authors mention that the hyperparameters may not have been optimal for the downgraded architecture used for reproduction of results. 
5. The authors have written the code from scratch (repository made public), but refer to the original code only for a few pre-processing steps that are not clear in the original paper. 
6. The authors have communicated with the original authors for clarifications on training instabilities and training time.


7. Typos:
	* Line 101: Missing closing parentheses
	* Line 108: a buffer, instead of an buffer
	

",ShrPBsjByVa
183,"The report aims to reproduce the following paper: ""Multi-scale Interactive Network for Salient Object Detection"". 

The report is based on re-running the existing code-base. The report adds additional 3 datasets.

On the positive side, the report is clearly written and easy to follow and contains enough details to understand the original paper.

On the negative side, the experimental evaluation does not add new hyperparameters to test limiting the scope of the report given that it just re-runs the original paper implementation. Moreover, I'm not convinced about the F-measure argument, that it is not suitable to handle properly ""black images"", since bot precision and recall should handle properly images without silent objects. Maybe the issue is in the implementation of the F-measure?",whqqXkXlwYQ
184,"Overview:
The authors reproduce the paper using officially released github repo for MINet. They tested the approach extensively by trying it out on 3 additional datasets with necessary dataset-specific metric fixes.

Positives:
1. The authors test the method on three additional datasets.

Negatives:
1. The authors should try to optimize hyperparameters on the reported datasets since the code is already available. I do understand that there might be time/resource constraints. (Minor)
2. The numbers on the additional datasets in itself do not reveal much unless compared with other approaches. It would be great to test EGNet or SCRN on the additional datasets (whichever is less effort) and include in the report. If the results are publicly available in a different paper, it would be best to just add it to the report for completeness.",whqqXkXlwYQ
185,"
The scope of the report is stated in the paper, however, it does not break down the scope into detailed items. It would be better to have a more detailed scope definition.

The code is from the original authors. The authors of this paper made some efforts in trying the proposed approach in different datasets.

Not much communication with the original authors other than reuse the GitHub code from the original authors.

There is a hyperparameter described in the paper, but there is not much hyperparameters search in the report.

It is interesting to see the comparison between different computer specifications to run the experiments, though they are not motivated well. I think it would be better if the authors could find out which salient region detection approach works the best for the retrieval task. Specifically, can you tweak the hyperparameters of the proposed approach? Would that change the retrieval performance? 

I recognize that the paper has done a bit more experiments to check the various versions of the proposed approach on different datasets. There is a detailed discussion.

The original paper is reproducible. However, according to this paper, it is better to provide details about the time spent on the training. 

According to Table 2, it seems that the reports have attempted some backbone models. I am wondering why we skip efficient networks like MobileNet v2 here if we are looking for a bit more efficient way in training and testing. 

It is a bit confusing which computer in table 3 is used in reporting the training time and testing time in table 2. Also, it would be better if the two machine has the same CPUs or the same GPUs, so that we can have some control groups. Right now, there are too many differences in the two computers. ",whqqXkXlwYQ
186,"Overall the authors have done a very good job on reproducing the ECCV 2020 paper ""Autoregressive Unsupervised Image Segmentation"". As stated in the report, it was relatively straightforward implementing the architecture described in the original paper with only two hurdles; one relating to the loss function, which since it was taken from a previous paper, the authors were able to use information from that paper to reproduce the ECCV paper; second relating some issues around the attention layer, for which they contacted the authors, hence resolving it.

All the steps undertaken are explained concisely and appropriately, including what was easy and what wasn't. Authors managed to approximate results in one of the two datasets used in the original paper. Bottomline is that all the results presented do not match those of the original paper - more or less. The reason is that the authors were not able to use the same resources as the original paper, leading to different batch size and also architecture (4 vs 5 layers). Such changes can have an unpredictable effect to the final results therefore I am not sure one can be certain that the results obtained match - or not - the ones presented in the original paper.

Having said that, there is only so much that can be done if the resources are not available; nonetheless the results are affected.

Overall it is a very good effort and the authors have done a great job in reproducing the paper. It seems that the original paper has had enough information, allowing a relatively straightforward reproduction with two caveats; one being having to contact the authors; and second resorting to a previously published paper to get some information on the loss function.",gtudcKh8dBW
187,"**Strengths:**

1. Further hyperparameter investigation on the Potsdam dataset.

The reproduction does some further hyperparameter scans on the batch size and output stride. They show how some of the accuracy lost from other necessary changes can be regained with some tweaks. This is a good contribution to increasing data on how well the underlying segmentation method works.

2. Good discussion of implications of the experiments on reproducibility success.

While there are some gaps in the completeness of the reproduction, as in the following, these are for the most part clearly described by the submission. Section 2.1 lists specifically which experimental results were replicated. The fact that unsupported claims are possibly unsupported due to differences in the reproduction are clearly described in other parts of the submission.

3. The submission clarifies details in the original significantly.

Some of these are specifically listed in the Section 5.3 (describing the communications with the authors). Architecture specifics such as the number of residual blocks and strides are also given in a clearer and more complete way in this submission than the original paper.

The original paper has some references to supplementary material that no longer seem to be available in the final published form, as far as I could find. Based on the references, some of these details may have been in that document.

This submission collects (from the code, discussion with authors, and possibly other sources) and describes some of them in a clear way that looks like it'd be a really good resource for any reproduction or follow-up work on the original paper.

**Weaknesses:**

4. Reproduction trains fewer epochs for reasons not described.

The command-line arguments in: \
https://github.com/Max-Manning/autoregunsupseg/blob/master/run_experiments.sh \
seem to specify 20 epochs for all trainings.

Section 3.4 of the repro says: ""All experiments were run for 10 epochs.""

This discrepancy and its effects isn't discussed further, unless I've missed it. Some clarification in the rebuttal may help.

5. Experiments on COCO-stuff are on a smaller model.

The reproduction reduces the number of residual blocks in the auto-encoder. This is done due to limitations on available compute resources.

Hard to draw any conclusion at all from the smaller model on COCO-Stuff. The observed drop in accuracy could be from either the smaller model, or a failure to reproduce. Results on Potsdam with the smaller model do not disambiguate between the two, as the tradeoff for model size vs accuracy can differ between datasets.

The authors of the repro acknowledge this and explain well how it affect their conclusions, so this is a not a strong negative for the score.

It might help to note the comparison to baseline accuracy numbers. E.g. the comparison to other unsupervised ssemantic segmentation methods in Table 3 of the original paper.",gtudcKh8dBW
188,"Positives:
1. The authors implement the method using pre-existing well tested code released for a previous paper.
2. The authors are able to reproduce the results on Postdam dataset to within 1%.

Negatives:
1. The authors take a rather narrow outlook in reproducing the paper. Due to computational limits, they reduce the batch size and use a smaller model which makes verifying the claims rather difficult. The authors report that they ""do not find evidence supporting this claim.."". It should be mentioned that due to computational constraints, they were unable to run the full model in this summary line itself.
2. The authors apply their judgement and refute the claims made by the original paper. They mention that their models couldn't differentiate edges well. But, this could very well be due to a training issue or just to architectural simplifications. It would have been better if the authors also communicated with the original authors to determine the reason for the wide gap in performance on COCO-Stuff.
3. The authors could have summarised the original paper better in Introduction section.",gtudcKh8dBW
189,"Instead of reproducing exact results from the original paper, the submission implemented their own version and verified that the assumptions and results of the original paper do hold. In general, this is a good complementary study for the original paper.

The subject of study in the original paper is a new multi-resolution classification model called RANet, aiming for speeding up image classification under the assumption that some easy images can be classified at the lower resolution. The submission has an implementation of the RANet model in Tensorflow (with Keras), and the code is open sourced. This enabled Tensorflow users to try out RANet in the future, since the original authors implemented RANet in PyTorch.

2 main assumptions from the original paper are verified. The first is the existence of such distribution from easy- to hard-to-classify images, which is verified by showing that different subnets have different classification errors. The second is that RANet have an effective reduction in computational cost for image classification, which is verified using a classification task and a new earth quake detection dataset.

Being a good complementary study already, the submission could offer improvements in the following ways: 1) the submission should discuss how easy it is to reproduce the exact results in the original paper using the original authors' PyTorch implementation. It will form a better story for the current submission. 2) The authors should remove affiliation information on the computational resources used. This may run into anonymity troubles for more serious venues.",_TGQQdIBhEv
190,"**Strengths:**

1. Illustration of performance during training.

Table 1 is a nice result for reproduction purposes. The training dynamics of networks with conditional/adaptive components are known to be unstable sometimes. This reproduction shows, independently, how different subnetworks train within a CIFAR-10 experiment.

2. Considers an additional ""spalling"" dataset not in the main paper.

While not central to reproducing the claims in the original paper, it is good to investigate whether a method works on new data. One generally hopes that AI & CV can generalize beyond the standard datasets such as ImageNet and COCO, and the concrete spalling dataset in the reproduction has some major difference in the domain considered.

3. Full re-implementation of the RANet architecture.

The experiments in the reproduction were done in Keras, while the original RANet implementation is in PyTorch. Since there are still small differences in behavior between the two, it is good to know that the RANet architecture is not so implementation-dependent that it doesn't generalize beyond this.

**Weaknesses:**

4. Only reproduces some of the smaller-scale experiments.

The only dataset used in both the original paper and this reproduction is CIFAR-10. It is reasonable to cite computational requirements as a barrier to reproducing the experiments. However, it is a particularly major limitation when reproducing RANet, as the purpose of RANet is to adaptively reduce the resolution at which inference is done. CIFAR-10 images are already very low-resolution (32x32), having been selected from TinyImages.

If I understand correctly, the architecture in RANet is also adaptive w.r.t. to the number of layers processed (so some input-dependent adaptive behavior can be observed with this too), but this is also in previous work such as Veit & Belongie. The central new claim in RANet is in the handling of scale/resolution, that is better illustrated by ImageNet experiments (such as Fig 6c of Yang et al).

5. Some lack of clarity in dataset construction.

For the spalling dataset: were the images, or the patches, split between train and test? It is unclear to me based on my reading (around line 82). In Fig 3, it looks like some of the patches can overlap? Is it possible in the dataset contruction that there could be two overlapping patches, from the same image, for which one patch is in the training set and the other patch in the testing set? It would be less than ideal for train and test to be correlated in this way, due to having examples that share pixels.

**Misc Comments:**

  * 46: Typo ""Tensoflow""
  * 46: ""train own"" -> ""train our own""
  * 47: Is ""be"" meant to be ""by""
  * 47: Should capitalize ImageNet",_TGQQdIBhEv
191,"The report did a reasonable job in reproducing the research work on predicting future trajectories from past data by leveraging a goal-prediction generative model. The experiment demonstrated that the original work is quite reproducible and supports the claims made by the authors of the paper. It is also greatly appreciated that additional experiments are performed (reference shift and recurrent architecture) to further investigate the proposed idea. However,  I do have a few concerns regarding the report:

1. The majority of the experiments presented seem to be running with the existing code from the original authors. The report did mention that some changes are required for certain experiments but didn't provide more details. It would be helpful if the authors could provide more details regarding which experiments in the report required non-trivial changes to the codebase to better judge the effort required for the work.

2. Writing could be improved. For example, the definition of ADE and FDE should be made clear in the report (They are defined in Eq (1) and (2), which aren't referenced in the text.).
",5M4oJ5b6Dc_
192,"While there are some strengths, this report needs substantial improvement in a few areas.

Strengths
- The GitHub repository containing the code for reproducing the experiments appears to be thorough (although it breaks anonymity, and should have been anonymized before submission)
- There was an extra experiment on using an LSTM architecture instead of an MLP.
- The reproduction of the results from the original paper appears to be thorough.

Weaknesses
- If using the code from Mangalam et al., how do the authors account for the differences (albeit small) in the results?
- It is not clear how the GitHub repository for the paper reproduction differs from the original repository provided by Mangalam et al. without going through the commits. It appears that there are more comments and some additional experiments, but it would helpful if this were summarized in the report and the additional experiments were separated from the original repository in some way. This is made even more confusing by the fact that the README on the main page of the repository is unchanged from the original one (other than the ""About"" messsage on the page).
- The extra experiment should be clarified. Why did the authors chose to run it? What could it demonstrate?
- Figure 1 and Table 1 are directly from the Mangalam et al. paper, but this is not acknowledged.
- Notation (such as in Table 1) is not explained. Generally, this report is very difficult to read without cross-referencing the original paper.
- The same holds for key abbreviations used in the report but not explained, including FDE and ADE.",5M4oJ5b6Dc_
193,"This paper attempts to reproduce the work published in ECCV 2020, i.e., On Disentangling Spoof Trace for Generic Face Anti-Spoofing. The authors take help from the original paper, the original authors and the official implementation, which took them around a month. They also verified different segments of the original implementation and propose several improvements over a few limitations of the original paper.

Strengths: 
The writing is good and easy to follow. The summary of the original paper is clarify and detailed. The authors provide figures to demonstrate whether they have succeeded to reproduce the original results.

Weakness：
1)	As argued in the report, the reproduction took help of the authors and the official implementation. Therefore, it is significant for the authors to put emphases on what are challenging to reproduce the results of the original paper. The FFT adopted in the final decision is confusing. I cannot make it out how it was injected in the original method. 
2)	Typos, like the last sentence in the results part in Page 1. It is suggested to go over the paper carefully to improve the quality of the paper.
",4PKKAvEAE-s
194,"The study tried to reproduce the paper by following the official implementation. This report can be improved in following aspects:
1. Language could be improved, e.g., line 149 contains typo, some sentences can be shortened.
2. When referring to equations in the original paper, the authors do not explain the context. This makes the study hard to read.
3. The descriptions for figures were not clear to me.",4PKKAvEAE-s
195,"# Summary and overall assessment

The authors analyzed the reproducibility of ""On Disentangling Spoof Trace for Generic Face Anti-Spoofing"" which proposes a GAN to identify spoofing in images for robust face recognition. The strengths of this report is that they prototyped the paper ideas from scratch and proposed modifications to improve the methodology. However, the overall report is very vague in writing (see comments below) and I was missing details to understand what has been done, e.g., scope of the reproducibility, description of the approach. Further I do not believe that the contributions to improve the approach is necessarily the scope of a reproducibility report. This can also change the overall results and make them not comparable to the original results.

# Detailed comments on each section

Reproducibility Summary/Scope of reproducibility:
* This is rather a summary of the work itself. I would have expected this paragraph would summarize the content of this report. * ""Based on that, this paper suggests"": the paper should be citied.
Reproducibility Summary/Results: 
* ""We succeeded to match the ACER of the original paper to within 0.53%""/ For OULU NPU Protocol-1 the numbers are 1.9% vs. 1.195%, which would result in a difference of 0.705% > 0.53%?
* Last sentence ""Later, we proposed a few techniques to"" is incomplete.

Introduction:
* ""Of all bio-metric authentication technologies, face recognition is the most intuitive and effective."" Can you cite the source or elaborate?
* ""In the past, hand-crafted features like HOG and LBP were used to tackle the problem of face anti-spoofing."" Please cite relevant works.
* ""In recent years, CNNs have been adopted as the preferred solution for this problem."" Please cite relevant works.
* The introduction could have given more details about solutions to spoofing. Is it a classic classification problem solved with hand-crafted features or CNN features?

Scope of reproducibility:
* Equation 1: Can you explain this equation?

Methodology:
* Can you explain the methodology in details? What is used as input, what problem is optimized, what is the loss function?

Experimental setup:
* What model (architecture) was used here?

Implementation:
* ""However, there is a functionality which is not available in PyTorch,"" Can you be concrete and say which functionality?

Resources:
* ""During the implementation phase, we found inconsistencies between the paper and the official implementation."" Which ones?
Other:
* ""The following section formatting is optional, you can also define sections as you deem fit."" I believe this can be removed.",4PKKAvEAE-s
196,"This submission reproduces the self-supervised learning results of FixMatch on CIFAR-10 and studies how the interaction between supervised and unsupervised learning objectives might lead to confirmation errors.

**Reproducing results.** The submission successfully reproduced SSL results on CIFAR-10, with error rates within the ranges provided in the original paper (c.f. Table 1). Authors reimplemented the method using Pytorch (the official code used TensorFlow), following the method description in the paper and checking the official code when something was unclear in the manuscript. I believe that reproducing FixMatch using a different deep learning framework is an important contribution to the community.

**Beyond reproducing results.** Authors devote an important part of the submission to their hypothesis that FixMatch might suffer from confirmation errors, including an exhaustive literature review. This goes beyond reproducing results and changing some hyperparameters, and can be seen as an improvement to the original method. Unfortunately, it is unclear whether the reported gains (e.g. Table 3) are statistically significant due to the lack of cross-validation or additional random seeds. The method has potential for a future workshop or conference submission if more experiments to back up the hypotheses are reported, thus I encourage authors to pursue these ideas.

Given that the submission not only reproduces some of the results of FixMatch, but also explores some of its potential limitations and proposes improvements to the method, I recommend its acceptance. 

Minor comment: the first half of the paper provides error rates, but then accuracy becomes the metric of reference. While both are essentially measuring the same thing, I would encourage consistency by using the same metric throughout the entire manuscript.",3VXeifKSaTE
197," I can confirm that the authors include a clear summary of their work. They perform all their experiments using the CIFAR-10 dataset and highlight that the original paper was missing some implementation related detailed, which was later clarified. The authors use the same hyper-parameters, as the original paper and perform some ablation studies for the confidence threshold. The authors reproduce the experiments using pytorch and do not provide any recommendations to the original authors. The paper is otherwise well written, though slightly confusing at times. ",3VXeifKSaTE
198,"The report aims to reproduce the results of the paper 'FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence' The report gives a summary of how the reproduction is conducted and briefly introduce the original paper. The paper also gives the details including the hyper-parameters and computational infrastructures. 

The authors provides the reproducing codes with detailed documentation.",3VXeifKSaTE
199,"The proposed work reproduces the result from ""Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention (CVPR 2020)"". The proposed work was able to reproduce the result (except for different dataset) from the original paper. However, the consideration of the following points might improve the quality of the proposed work

1) A thorough hyperparameter search and a corresponding discussion is expected.

2) The original paper also demonstrates that the processing time and memory requirement is considerably decreased. It would be better to comment/validate this statement.",r87dMGuauCl
200,"This report aims to reproduce a paper on classification of time sequences of satellite imagery using transformers. The report describes the attempt to reproduce the original paper from scratch, then using the provided code. In addition, the report adds on more investigation on another dataset.

In general, the report is very cleanly written. It was easy to follow along. It is valuable that the authors include their misconceptions and how they fixed it. The report clearly states the scope of reproducability and follows it. In their scope, the authors attempted to reproduce the paper from scratch. Then, they discovered that the original paper falls short on the explanation of the exact architecture being used. The authors consult the the sources included with the original paper and contact the original paper authors. Overall, this shows a healthy and admirable approach to scientific investigation and collaboration.

The report examines the architectural choices made in the original paper. The report experiments with the proposed approach and compares with a more standard architecture (Vaswani et al., 2017). This part of the report reveals insightful results that the standard architecture yields similar or even better results.

The report extends the original paper with an extra experiment on a new dataset.

Suggested improvements or extensions:
- As far as I understood, the report does not conduct the hyper-parameter search. Nor it tries to determine the stability of the hyper-parameters. This investigation would greatly improve the report.
- With an exception for the case study mentioned above, the report doesn't provide any ablation studies. 
- The figures are almost impossible to read in black and white. I encourage to modify the figures to make them more accessible to people with black and white printers and the color blind.

Overall, this is a well-structured, well-written report that analyses the original paper. Furthermore, the report extends the original paper with the results on an extra dataset. I recommend accept.",r87dMGuauCl
201,"The authors of this report aimed at reproducing the method presented in the paper ""Satellite Image Time Series Classification with Pixel-Set Encoders and Temporal Self-Attention"" published at CVPR 2020 by Garnot et al.
The authors not only did they attempt to reproduce the code and evaluate it on the dataset used in the original paper, but they also went on to use another dataset to expand upon the evaluation process, including some changes to the way the test set was selected in the original dataset.
Given that the authors of the original paper have made their code available allowed for a direct comparison between the reproduced code and the original code. In fact, the authors of this report have had some issues on some aspects of the use of the original transformer implementation, but that was resolved in a communication with the authors of the original paper and of the Transformer one. Minor discrepancies and issues with the implementation were also resolved via checking the provided github code of the original paper, hence confirming the reproducibility of the original paper.
Finally, although the report is well written overall, I would have expected the conclusion to be a bit more elaborate on what was easy and what did not work as expected; but that is a very minor issue.",r87dMGuauCl
202,"***Reproducibility Summary:***
       The authors have provided a detailed summary meeting the requirements of a reproducibility report.

***Scope of reproducibility:***
        Yes, the reproducibility report has clearly and concisely stated the scope of reproducibility.

***Code:***
        Yes, the authors have re-used the original author's code repository and their trained model with small modifications.

***Communication with original authors***
      Yes, the authors connected with original authors through the original authors' Github repo.

***Hyperparameter Search:***
      Yes, the authors have attempted to reproduced the hyperparameter search.  The authors have also expanded the hyperparameter search to involve image size, the ""hard examples factor $m$"", different kernel sizes, lambda parameters, and other analysis not present in the original paper.

***Ablation Study:***
      I did not notice any ablation in the study.

***Discussion on results:***
      Yes, the reproducibility report contains a brief discussion on the state of reproducibility of the original papers, but also provides  notes on where to modify the original work to fix the issue in the original pyTorch model.  Despite differences in the experimental results, the authors carried out a paired t-test to further confirm the reproducibility of result.

***Recommendations for reproducibility:***
      No, the authors did not provide any recommendation to the original authors for improving reproducibility.

***Results beyond the paper:***
      The authors have tried additional results that are not mentioned in the original paper.  The authors include significantly more quantitative and qualitative results than the original paper.

***Overall organization and clarity:***
         Nicely written and coherent.

***Pros:***
         Significantly more quantitative and qualitative results.
         Extensive further exploration of the hyperparameter search and added new dimensions to the hyperparameter search (e.g. image size).",nZvCsz5CjqN
203,"Authors of this report provide summary of report, scope of reproducibility and communicated with original author of the “Interactive Two-Stream Decoder 3 for Accurate and Fast Saliency Detection”.

This paper consists of mainly three parts: evaluation using the pretrained model, evaluation using retrained model and hyperparameter ablations

Basically, there are available codes from the original authors, thus it is important to perform plenty of “hyperparameter search” and “ablation”.

The good point of the paper is that the author tried to verify the original model provided by the original authors and reproduce and retrain the exact same model, then perform ablations. It is helpful for the following works.


In order to quantitatively justify the reproducibility of the results from the retrained model, they carried out statistical analysis using paired t-test. I think that it is a good value of this report.

Also, this report provides more various ablation studies not included in the original paper, including “image size and inference speed”, “factor m”, “dilation and erosion”, “lambda”, etc. Those plenties of experiments may be helpful for following researchers.
",nZvCsz5CjqN
204,"The scope definition is good and clear.

The design choice in choosing training and testing datasets would need a bit better motivation. 

This paper did make efforts in exploring different hyperparameter searches.

There are communications between authors and the original authors. All are done in Github issues. I am happy to see that the authors have made a few changes to the original code.

This paper picked up some experiments that are not done in the original papers. e.g. effects of image sizes and inference speed; hard examples factor m; dilation and erosion; lambda; optimized vs default parameters; constrained lambda.

It would be better if the authors could look further into the compatibility of the pre-trained models with other frameworks e.g. TensorFlow, or ONNX.

I find the following items could be improved in the paper.
1. Table 2-6 lacks the detailed description of ""our results"". How exactly the parameters are set to get the results?
2. I think figure 1 could be significantly improved if there are more descriptions to summarize the trends in the figures. 
3. Figure 3 is a bit confusing, how the figures related to the proposed approach?
4. Figure 6 provides very little information, and it should be replaced by a table or even description in the text.",nZvCsz5CjqN
